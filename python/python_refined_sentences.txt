The “Python library” contains several different kinds of components.
It contains data types that would normally be considered part of the “core” of a language, such as numbers and lists.
For
these types, the Python language core defines the form of literals and places some constraints on their semantics, but does
not fully define the semantics.
The library also contains built-in functions and exceptions — objects that can be used by all Python code without the
need of an import statement.
Some of these are defined by the core language, but many are not essential for the core
semantics and are only described here.
The bulk of the library, however, consists of a collection of modules.
There are many ways to dissect this collection.
Some
modules are written in C and built in to the Python interpreter; others are written in Python and imported in source form.
Some modules provide interfaces that are highly specific to Python, like printing a stack trace; some provide interfaces
that are specific to particular operating systems, such as access to specific hardware; others provide interfaces that are
specific to a particular application domain, like the World Wide Web.
Some modules are available in all versions and
ports of Python; others are only available when the underlying system supports or requires them; yet others are available
only when a particular configuration option was chosen at the time when Python was compiled and installed.
This manual is organized “from the inside out:” it first describes the built-in functions, data types and exceptions, and
finally the modules, grouped in chapters of related modules.
This means that if you start reading this manual from the start, and skip to the next chapter when you get bored, you will
get a reasonable overview of the available modules and application areas that are supported by the Python library.
Of
course, you don’t have to read it like a novel — you can also browse the table of contents (in front of the manual), or
look for a specific function, module or term in the index (in the back).
And finally, if you enjoy learning about random
subjects, you choose a random page number (see module random) and read a section or two.
Regardless of the order
in which you read the sections of this manual, it helps to start with chapter Built-in Functions, as the remainder of the
manual assumes familiarity with this material.
Let the show begin!
It does not make any
claims about its existence on a specific operating system.
If an availability note contains both a minimum Kernel version and a minimum libc version, then both conditions
must hold.
For example a feature with note Availability: Linux >= 3.17 with glibc >= 2.27 requires both Linux
3.17 or newer and glibc 2.27 or newer.
WebAssembly runtimes and browsers are sandboxed and have limited access to the host and external
resources.
Any Python standard library module that uses processes, threading, networking, signals, or other forms of
inter-process communication (IPC), is either not available or may not work as on other Unix-like systems.
File I/O,
file system, and Unix permission-related functions are restricted, too.
Emscripten does not permit blocking I/O. Other
blocking operations like sleep() block the browser event loop.
The properties and behavior of Python on WebAssembly platforms depend on the Emscripten-SDK or WASI-SDK ver-
sion, WASM runtimes (browser, NodeJS, wasmtime), and Python build time flags.
WebAssembly, Emscripten, and
WASI are evolving standards; some features like networking may be supported in the future.
For Python in the browser, users should consider Pyodide or PyScript.
PyScript is built on top of Pyodide, which itself
is built on top of CPython and Emscripten.
Pyodide provides access to browsers’ JavaScript and DOM APIs as well as
limited networking capabilities with JavaScript’s XMLHttpRequest and Fetch APIs.
That includes APIs that spawn new processes
(fork(), execve()), wait for processes (waitpid()), send signals (kill()), or otherwise interact with
processes.
The subprocess is importable but does not work.
The socket module is available, but is limited and behaves differently from other platforms.
On Emscripten,
sockets are always non-blocking and require additional JavaScript code and helpers on the server to proxy TCP
through WebSockets; see Emscripten Networking for more information.
WASI snapshot preview 1 only permits
sockets from an existing file descriptor.
Some functions are stubs that either don’t do anything and always return hardcoded values.
For example, WASI does not permit symlinks with absolute file names.
Introduction
CHAPTER
TWO
BUILT-IN FUNCTIONS
The Python interpreter has a number of functions and types built into it that are always available.
They are listed here in
alphabetical order.
The argument may be an integer, a floating point number, or an object
implementing __abs__().
If the argument is a complex number, its magnitude is returned.
Return an asynchronous iterator for an asynchronous iterable.
Equivalent to calling x.__aiter__().
Note: Unlike iter(), aiter() has no 2-argument variant.
New in version 3.10.
Return True if all elements of the iterable are true (or if the iterable is empty).
When awaited, return the next item from the given asynchronous iterator, or default if given and the iterator is
exhausted.
This is the async variant of the next() builtin, and behaves similarly.
This calls the __anext__() method of async_iterator, returning an awaitable.
Awaiting this returns
the next value of the iterator.
If default is given, it is returned if the iterator is exhausted, otherwise
StopAsyncIteration is raised.
New in version 3.10.
Return True if any element of the iterable is true.
If the iterable is empty, return False.
As repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters
in the string returned by repr() using \x, \u, or \U escapes.
This generates a string similar to that returned by
repr() in Python 2.
bin(x)
Convert an integer number to a binary string prefixed with “0b”.
The result is a valid Python expression.
If x is not
a Python int object, it has to define an __index__() method that returns an integer.
See also format() for more information.
If
x is false or omitted, this returns False; otherwise, it returns True.
The bool class is a subclass of int (see
Numeric Types — int, float, complex).
It cannot be subclassed further.
Its only instances are False and True
(see Boolean Type - bool).
This function drops you into the debugger at the call site.
Specifically, it calls sys.breakpointhook(),
passing args and kws straight through.
By default, sys.breakpointhook() calls pdb.set_trace()
expecting no arguments.
In this case, it is purely a convenience function so you don’t have to explicitly import pdb
or type as much code to enter the debugger.
However, sys.breakpointhook() can be set to some other
function and breakpoint() will automatically call that, allowing you to drop into the debugger of choice.
If
sys.breakpointhook() is not accessible, this function will raise RuntimeError.
By default, the behavior of breakpoint() can be changed with the PYTHONBREAKPOINT environment vari-
able.
See sys.breakpointhook() for usage details.
Note that this is not guaranteed if sys.breakpointhook() has been replaced.
Raises an auditing event builtins.breakpoint with argument breakpointhook.
New in version 3.7.
It has most of the usual methods of mutable sequences, described in Mutable Sequence Types, as well as most
methods that the bytes type has, see Bytes and Bytearray Operations.
If it is a string, you must also give the encoding (and optionally, errors) parameters; bytearray() then
converts the string to bytes using str.encode().
If it is an integer, the array will have that size and will be initialized with null bytes.
If it is an object conforming to the buffer interface, a read-only buffer of the object will be used to initialize
the bytes array.
Without an argument, an array of size 0 is created.
See also Binary Sequence Types — bytes, bytearray, memoryview and Bytearray Objects.
Accordingly, constructor arguments are interpreted as for bytearray().
Bytes objects can also be created with literals, see strings.
Return True if the object argument appears callable, False if not.
If this returns True, it is still possible that a
call fails, but if it is False, calling object will never succeed.
Note that classes are callable (calling a class returns
a new instance); instances are callable if their class has a __call__() method.
New in version 3.2: This function was first removed in Python 3.0 and then brought back in Python 3.2.
chr(i)
Return the string representing a character whose Unicode code point is the integer i.
For example, chr(97)
returns the string 'a', while chr(8364) returns the string '€'.
This is the inverse of ord().
The valid range for the argument is from 0 through 1,114,111 (0x10FFFF in base 16).
ValueError will be
raised if i is outside that range.
A class method receives the class as an implicit first argument, just like an instance method receives the instance.
The @classmethod form is a function decorator – see function for details.
A class method can be called either on the class (such as C.f()) or on an instance (such as C().f()).
The
instance is ignored except for its class.
If a class method is called for a derived class, the derived class object is
passed as the implied first argument.
Class methods are different than C++ or Java static methods.
If you want those, see staticmethod() in this
section.
For more information on class methods, see types.
Class methods can now wrap other descriptors such as property().
Changed in version 3.10:
Class methods now inherit the method attributes (__module__, __name__,
__qualname__, __doc__ and __annotations__) and have a new __wrapped__ attribute.
Changed in version 3.11: Class methods can no longer wrap other descriptors such as property().
Compile the source into a code or AST object.
Code objects can be executed by exec() or eval().
Refer to the ast module documentation for information
on how to work with AST objects.
The filename argument should give the file from which the code was read; pass some recognizable value if it wasn’t
read from a file ('<string>' is commonly used).
The optional arguments flags and dont_inherit control which compiler options should be activated and which future
features should be allowed.
If neither is present (or both are zero) the code is compiled with the same flags that
affect the code that is calling compile().
If the flags argument is given and dont_inherit is not (or is zero) then
the compiler options and the future statements specified by the flags argument are used in addition to those that
would be used anyway.
If dont_inherit is a non-zero integer then the flags argument is it – the flags (future features
and compiler options) in the surrounding code are ignored.
Compiler options and future statements are specified by bits which can be bitwise ORed together to specify multiple
options.
Compiler flags can be found in ast module, with PyCF_
prefix.
The argument optimize specifies the optimization level of the compiler; the default value of -1 selects the opti-
mization level of the interpreter as given by -O options.
Explicit levels are 0 (no optimization; __debug__ is
true), 1 (asserts are removed, __debug__ is false) or 2 (docstrings are removed too).
This function raises SyntaxError if the compiled source is invalid, and ValueError if the source contains
null bytes.
If you want to parse Python code into its AST representation, see ast.parse().
Raises an auditing event compile with arguments source and filename.
This event may also be raised by
implicit compilation.
Note: When compiling a string with multi-line code in 'single' or 'eval' mode, input must be terminated
by at least one newline character.
This is to facilitate detection of incomplete and complete statements in the code
module.
Warning: It is possible to crash the Python interpreter with a sufficiently large/complex string when compiling
to an AST object due to stack depth limitations in Python’s AST compiler.
Changed in version 3.2: Allowed use of Windows and Mac newlines.
Also, input in 'exec' mode does not have
to end in a newline anymore.
Added the optimize parameter.
Changed in version 3.5: Previously, TypeError was raised when null bytes were encountered in source.
New in version 3.8: ast.
PyCF_ALLOW_TOP_LEVEL_AWAIT can now be passed in flags to enable support for
top-level await, async for, and async with.
If
the first parameter is a string, it will be interpreted as a complex number and the function must be called without a
second parameter.
The second parameter can never be a string.
Each argument may be any numeric type (including
complex).
If imag is omitted, it defaults to zero and the constructor serves as a numeric conversion like int and
float.
If both arguments are omitted, returns 0j.
For a general Python object x, complex(x) delegates to x.__complex__().
If __complex__()
is not defined then it falls back to __float__().
If __float__() is not defined then it falls back to
__index__().
For example, complex('1+2j') is fine, but complex('1 + 2j') raises ValueError.
The complex type is described in Numeric Types — int, float, complex.
Grouping digits with underscores as in code literals is allowed.
Falls back to __index__() if __complex__() and __float__() are not defined.
This is a relative of setattr().
The arguments are an object and a string.
The string must be the name of
one of the object’s attributes.
The function deletes the named attribute, provided the object allows it.
For exam-
ple, delattr(x, 'foobar') is equivalent to del x.foobar.
The dict object is the dictionary class.
See dict and Mapping Types — dict for
documentation about this class.
For other containers see the built-in list, set, and tuple classes, as well as the collections module.
Without arguments, return the list of names in the current local scope.
With an argument, attempt to return a list
of valid attributes for that object.
If the object has a method named __dir__(), this method will be called and must return the list of attributes.
If the object does not provide __dir__(), the function tries its best to gather information from the object’s
__dict__ attribute, if defined, and from its type object.
If the object is a module object, the list contains the names of the module’s attributes.
If the object is a type or class object, the list contains the names of its attributes, and recursively of the
attributes of its bases.
Otherwise, the list contains the object’s attributes’ names, the names of its class’s attributes, and recursively
of the attributes of its class’s base classes.
The resulting list is sorted alphabetically.
For example, metaclass attributes are not in the result list when the
argument is a class.
With mixed operand types, the rules for binary arithmetic operators apply.
For integers, the result is the same as (a // b, a % b).
For floating point numbers the result is (q, a %
b), where q is usually math.floor(a / b) but may be 1 less than that.
In any case q * b + a % b is
very close to a, if a % b is non-zero it has the same sign as b, and 0 <= abs(a % b) < abs(b).
enumerate(iterable, start=0)
Return an enumerate object.
The __next__() method of the iterator returned by enumerate() returns a tuple containing a count (from
start which defaults to 0) and the values obtained from iterating over iterable.
The arguments are a string and optional globals and locals.
If provided, globals must be a dictionary.
If provided,
locals can be any mapping object.
The expression argument is parsed and evaluated as a Python expression (technically speaking, a condition list)
using the globals and locals dictionaries as global and local namespace.
If the globals dictionary is present and does
not contain a value for the key __builtins__, a reference to the dictionary of the built-in module builtins
is inserted under that key before expression is parsed.
That way you can control what builtins are available to the
executed code by inserting your own __builtins__ dictionary into globals before passing it to eval().
If the
locals dictionary is omitted it defaults to the globals dictionary.
If both dictionaries are omitted, the expression is
executed with the globals and locals in the environment where eval() is called.
Note, eval() does not have access
to the nested scopes (non-locals) in the enclosing environment.
The return value is the result of the evaluated expression.
Syntax errors are reported as exceptions.
In this
case, pass a code object instead of a string.
If the code object has been compiled with 'exec' as the mode
argument, eval()’s return value will be None.
Hints: dynamic execution of statements is supported by the exec() function.
The globals() and locals()
functions return the current global and local dictionary, respectively, which may be useful to pass around for use
by eval() or exec().
If the given source is a string, then leading and trailing spaces and tabs are stripped.
See ast.literal_eval() for a function that can safely evaluate strings with expressions containing only
literals.
Raises an auditing event exec with the code object as the argument.
Code compilation events may also be raised.
This function supports dynamic execution of Python code.
If
it is a code object, it is simply executed.
In all cases, the code that’s executed is expected to be valid as file input (see
the section file-input in the Reference Manual).
Be aware that the nonlocal, yield, and return statements
may not be used outside of function definitions even within the context of code passed to the exec() function.
The return value is None.
In all cases, if the optional parts are omitted, the code is executed in the current scope.
If only globals is provided,
it must be a dictionary (and not a subclass of dictionary), which will be used for both the global and the local
variables.
If globals and locals are given, they are used for the global and local variables, respectively.
If provided,
locals can be any mapping object.
Remember that at the module level, globals and locals are the same dictionary.
If exec gets two separate objects as globals and locals, the code will be executed as if it were embedded in a class
definition.
If the globals dictionary does not contain a value for the key __builtins__, a reference to the dictionary of the
built-in module builtins is inserted under that key.
That way you can control what builtins are available to the
executed code by inserting your own __builtins__ dictionary into globals before passing it to exec().
The closure argument specifies a closure–a tuple of cellvars.
It’s only valid when the object is a code object containing
free variables.
The length of the tuple must exactly match the number of free variables referenced by the code object.
Raises an auditing event exec with the code object as the argument.
Code compilation events may also be raised.
Note: The built-in functions globals() and locals() return the current global and local dictionary, respec-
tively, which may be useful to pass around for use as the second and third argument to exec().
Note:
The default locals act as described for function locals() below: modifications to the default locals
dictionary should not be attempted.
Pass an explicit locals dictionary if you need to see effects of the code on locals
after function exec() returns.
If you are reading the code from a file, make sure to use newline conversion
mode to convert Windows or Mac-style newlines.
If function is None, the identity function is assumed, that is,
all elements of iterable that are false are removed.
If the argument is a string, it should contain a decimal number, optionally preceded by a sign, and optionally
embedded in whitespace.
The optional sign may be '+' or '-'; a '+' sign has no effect on the value produced.
The argument may also be a string representing a NaN (not-a-number), or positive or negative infinity.
Here digit is a Unicode decimal digit (character in the Unicode general category Nd).
Case is not significant,
so, for example, “inf”, “Inf”, “INFINITY”, and “iNfINity” are all acceptable spellings for positive infinity.
Otherwise, if the argument is an integer or a floating point number, a floating point number with the same value
(within Python’s floating point precision) is returned.
If the argument is outside the range of a Python float, an
OverflowError will be raised.
For a general Python object x, float(x) delegates to x.__float__().
If __float__() is not defined
then it falls back to __index__().
If no argument is given, 0.0 is returned.
The float type is described in Numeric Types — int, float, complex.
Grouping digits with underscores as in code literals is allowed.
Changed in version 3.7: x is now a positional-only parameter.
Falls back to __index__() if __float__() is not defined.
The interpretation of format_spec
will depend on the type of the value argument; however, there is a standard formatting syntax that is used by most
built-in types: Format Specification Mini-Language.
The default format_spec is an empty string which usually gives the same effect as calling str(value).
A call to format(value, format_spec) is translated to type(value).__format__(value,
format_spec) which bypasses the instance dictionary when searching for the value’s __format__()
method.
A TypeError exception is raised if the method search reaches object and the format_spec is non-
empty, or if either the format_spec or the return value are not strings.
Changed in version 3.4: object().__format__(format_spec) raises TypeError if format_spec is
not an empty string.
See frozenset and Set Types — set, frozenset for documentation about this class.
For other containers see the built-in set, list, tuple, and dict classes, as well as the collections
module.
If the string is the name of one of the ob-
ject’s attributes, the result is the value of that attribute.
For example, getattr(x, 'foobar') is equivalent to
x.foobar.
If the named attribute does not exist, default is returned if provided, otherwise AttributeError
is raised.
Note: Since private name mangling happens at compilation time, one must manually mangle a private attribute’s
(attributes with two leading underscores) name in order to retrieve it with getattr().
For code within functions, this is set when the
function is defined and remains the same regardless of where the function is called.
The arguments are an object and a string.
The result is True if the string is the name of one of the object’s
attributes, False if not.
Hash values are integers.
They are used to quickly compare
dictionary keys during a dictionary lookup.
Numeric values that compare equal have the same hash value (even if
they are of different types, as is the case for 1 and 1.0).
Note: For objects with custom __hash__() methods, note that hash() truncates the return value based on
the bit width of the host machine.
See __hash__ for details.
If no argument is given, the inter-
active help system starts on the interpreter console.
If the argument is a string, then the string is looked up as the
name of a module, function, class, method, keyword, or documentation topic, and a help page is printed on the
console.
If the argument is any other kind of object, a help page on the object is generated.
Note that if a slash(/) appears in the parameter list of a function when invoking help(), it means that the param-
eters prior to the slash are positional-only.
For more info, see the FAQ entry on positional-only parameters.
This function is added to the built-in namespace by the site module.
Changes to pydoc and inspect mean that the reported signatures for callables are now
more comprehensive and consistent.
If x is not a Python int object,
it has to define an __index__() method that returns an integer.
Some examples:
'0xff'
'-0x2a'
If you want to convert an integer number to an uppercase or lower hexadecimal string with prefix or not, you can
use either of the following ways:
('0xff', 'ff', 'FF')
('0xff', 'ff', 'FF')
('0xff', 'ff', 'FF')
See also format() for more information.
See also int() for converting a hexadecimal string to an integer using a base of 16.
Note: To obtain a hexadecimal string representation for a float, use the float.hex() method.
This is an integer which is guaranteed to be unique and constant for this object
during its lifetime.
Two objects with non-overlapping lifetimes may have the same id() value.
CPython implementation detail: This is the address of the object in memory.
If the prompt argument is present, it is written to standard output without a trailing newline.
The function then
reads a line from input, converts it to a string (stripping a trailing newline), and returns that.
When EOF is read,
EOFError is raised.
Raises an auditing event builtins.input with argument prompt before reading input
Raises an auditing event builtins.input/result with the result after successfully reading input.
If x defines
__int__(), int(x) returns x.__int__().
If x defines __index__(), it returns x.__index__().
If
x defines __trunc__(), it returns x.__trunc__().
For floating point numbers, this truncates towards zero.
If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an
integer in radix base.
Optionally, the string can be preceded by + or - (with no space in between), have leading
zeros, be surrounded by whitespace, and have single underscores interspersed between digits.
A base-n integer string contains digits, each representing a value from 0 to n-1.
The values 0–9 can be represented
by any Unicode decimal digit.
The values 10–35 can be represented by a to z (or A to Z).
The default base is 10.
The allowed bases are 0 and 2–36.
Base-2, -8, and -16 strings can be optionally prefixed with 0b/0B, 0o/0O, or
0x/0X, as with integer literals in code.
For base 0, the string is interpreted in a similar way to an integer literal
in code, in that the actual base is 2, 8, 10, or 16 as determined by the prefix.
Base 0 also disallows leading zeros:
int('010', 0) is not legal, while int('010') and int('010', 8) are.
The integer type is described in Numeric Types — int, float, complex.
If base is not an instance of int and the base object has a base.__index__ method,
that method is called to obtain an integer for the base.
Previous versions used base.__int__ instead of base.
__index__.
Grouping digits with underscores as in code literals is allowed.
Changed in version 3.7: x is now a positional-only parameter.
Falls back to __index__() if __int__() is not defined.
The delegation to __trunc__() is deprecated.
Changed in version 3.11: int string inputs and string representations can be limited to help avoid denial of ser-
vice attacks.
A ValueError is raised when the limit is exceeded while converting a string x to an int or
when converting an int into a string would exceed the limit.
See the integer string conversion length limitation
documentation.
If object is not an object of the given type, the function always returns False.
If classinfo is a
tuple of type objects (or recursively, other such tuples) or a Union Type of multiple types, return True if object is
an instance of any of the types.
If classinfo is not a type or tuple of types and such tuples, a TypeError exception
is raised.
TypeError may not be raised for an invalid type if an earlier check succeeds.
Changed in version 3.10: classinfo can be a Union Type.
issubclass(class, classinfo)
Return True if class is a subclass (direct, indirect, or virtual) of classinfo.
A class is considered a subclass of itself.
In any other case, a TypeError exception is raised.
The first argument is interpreted very differently depending on the presence of the sec-
ond argument.
Without a second argument, object must be a collection object which supports the iterable protocol
(the __iter__() method), or it must support the sequence protocol (the __getitem__() method with in-
teger arguments starting at 0).
If it does not support either of those protocols, TypeError is raised.
If the
second argument, sentinel, is given, then object must be a callable object.
The iterator created in this case will
call object with no arguments for each call to its __next__() method; if the value returned is equal to sentinel,
StopIteration will be raised, otherwise the value will be returned.
See also Iterator Types.
One useful application of the second form of iter() is to build a block-reader.
For example, reading fixed-width
blocks from a binary database file until the end of file is reached:
from functools import partial
with open('mydata.db', 'rb') as f:
for block in iter(partial(f.read, 64), b''):
process_block(block)
len(s)
Return the length (the number of items) of an object.
The argument may be a sequence (such as a string, bytes,
tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).
CPython implementation detail: len raises OverflowError on lengths larger than sys.maxsize, such
as range(2 ** 100).
Free variables are returned by
locals() when it is called in function blocks, but not in class blocks.
Note that at the module level, locals()
and globals() are the same dictionary.
Note: The contents of this dictionary should not be modified; changes may not affect the values of local and free
variables used by the interpreter.
If additional iterables arguments
are passed, function must take that many arguments and is applied to the items from all iterables in parallel.
With
multiple iterables, the iterator stops when the shortest iterable is exhausted.
For cases where the function inputs are
already arranged into argument tuples, see itertools.starmap().
If one positional argument is provided, it should be an iterable.
The largest item in the iterable is returned.
If two
or more positional arguments are provided, the largest of the positional arguments is returned.
The key argument specifies a one-argument ordering function like
that used for list.sort().
The default argument specifies an object to return if the provided iterable is empty.
If the iterable is empty and default is not provided, a ValueError is raised.
If multiple items are maximal, the function returns the first one encountered.
This is consistent with other
sort-stability preserving tools such as sorted(iterable, key=keyfunc, reverse=True)[0] and
heapq.nlargest(1, iterable, key=keyfunc).
The default keyword-only argument.
The key can be None.
class memoryview(object)
Return a “memory view” object created from the given argument.
See Memory Views for more information.
If one positional argument is provided, it should be an iterable.
The smallest item in the iterable is returned.
If two
or more positional arguments are provided, the smallest of the positional arguments is returned.
There are two optional keyword-only arguments.
The key argument specifies a one-argument ordering function like
that used for list.sort().
The default argument specifies an object to return if the provided iterable is empty.
If the iterable is empty and default is not provided, a ValueError is raised.
If multiple items are minimal, the function returns the first one encountered.
This is consistent with other sort-
stability preserving tools such as sorted(iterable, key=keyfunc)[0] and heapq.nsmallest(1,
iterable, key=keyfunc).
The default keyword-only argument.
None.
next(iterator)
next(iterator, default)
Retrieve the next item from the iterator by calling its __next__() method.
If default is given, it is returned if
the iterator is exhausted, otherwise StopIteration is raised.
It has methods that are common to all instances
of Python classes.
This function does not accept any arguments.
Note: object does not have a __dict__, so you can’t assign arbitrary attributes to an instance of the object
class.
The result is a valid Python expression.
If x is not
a Python int object, it has to define an __index__() method that returns an integer.
See also format() for more information.
If the file cannot be opened, an OSError is raised.
See tut-files
for more examples of how to use this function.
It defaults to 'r' which means open
for reading in text mode.
Other common values are 'w' for writing (truncating the file if it already exists), 'x' for
exclusive creation, and 'a' for appending (which on some Unix systems, means that all writes append to the end
of the file regardless of the current seek position).
In text mode, if encoding is not specified the encoding used is
platform-dependent: locale.getencoding() is called to get the current locale encoding.
The default mode is 'r' (open for reading text, a synonym of 'rt').
Modes 'w+' and 'w+b' open and truncate
the file.
Modes 'r+' and 'r+b' open the file with no truncation.
As mentioned in the Overview, Python distinguishes between binary and text I/O. Files opened in binary mode
(including 'b' in the mode argument) return contents as bytes objects without any decoding.
In text mode (the
default, or when 't' is included in the mode argument), the contents of the file are returned as str, the bytes
having been first decoded using a platform-dependent encoding or using the specified encoding if given.
Note: Python doesn’t depend on the underlying operating system’s notion of text files; all the processing is done
by Python itself, and is therefore platform-independent.
Pass 0 to switch buffering off (only allowed in binary
mode), 1 to select line buffering (only usable in text mode), and an integer > 1 to indicate the size in bytes of a fixed-
size chunk buffer.
Note that specifying a buffer size this way applies for binary buffered I/O, but TextIOWrapper
(i.e., files opened with mode='r+') would have another buffering.
To disable buffering in TextIOWrapper,
consider using the write_through flag for io.TextIOWrapper.reconfigure().
When no buffering
argument is given, the default buffering policy works as follows:
• Binary files are buffered in fixed-size chunks; the size of the buffer is chosen using a heuristic trying to
determine the underlying device’s “block size” and falling back on io.DEFAULT_BUFFER_SIZE.
On
many systems, the buffer will typically be 4096 or 8192 bytes long.
Other text files use the
policy described above for binary files.
This should only be used in text mode.
The
default encoding is platform dependent (whatever locale.getencoding() returns), but any text encoding
supported by Python can be used.
See the codecs module for the list of supported encodings.
A variety of standard error handlers are available (listed under Error Handlers), though any error
handling name that has been registered with codecs.register_error() is also valid.
The standard names
include:
• 'strict' to raise a ValueError exception if there is an encoding error.
The default value of None has
the same effect.
Note that ignoring encoding errors can lead to data loss.
These surrogate code units will then be turned back into the same bytes when the
surrogateescape error handler is used when writing data.
This is useful for processing files in an un-
known encoding.
Characters not supported by the en-
coding are replaced with the appropriate XML character reference &#nnn;.
It can be None, '', '\n', '\r', and
'\r\n'.
When reading input from the stream, if newline is None, universal newlines mode is enabled.
Lines in the
input can end in '\n', '\r', or '\r\n', and these are translated into '\n' before being returned to the
caller.
If it is '', universal newlines mode is enabled, but line endings are returned to the caller untranslated.
If it has any of the other legal values, input lines are only terminated by the given string, and the line ending
is returned to the caller untranslated.
When writing output to the stream, if newline is None, any '\n' characters written are translated to the
system default line separator, os.linesep.
If newline is '' or '\n', no translation takes place.
If newline
is any of the other legal values, any '\n' characters written are translated to the given string.
If closefd is False and a file descriptor rather than a filename was given, the underlying file descriptor will be kept
open when the file is closed.
If a filename is given closefd must be True (the default); otherwise, an error will be
raised.
A custom opener can be used by passing a callable as opener.
The underlying file descriptor for the file object is
then obtained by calling opener with (file, flags).
The newly created file is non-inheritable.
When open() is used to open a
file in a text mode ('w', 'r', 'wt', 'rt', etc.), it returns a subclass of io.TextIOBase (specifically io.
TextIOWrapper).
When used to open a file in a binary mode with buffering, the returned class is a subclass of
io.BufferedIOBase.
The exact class varies: in read binary mode, it returns an io.
BufferedReader; in
write binary and append binary modes, it returns an io.
BufferedWriter, and in read/write mode, it returns
an io.
BufferedRandom.
When buffering is disabled, the raw stream, a subclass of io.RawIOBase, io.
FileIO, is returned.
See also the file handling modules, such as fileinput, io (where open() is declared), os, os.path,
tempfile, and shutil.
Raises an auditing event open with arguments file, mode, flags.
The mode and flags arguments may have been modified or inferred from the original call.
The opener parameter was added.
The file is now non-inheritable.
If the system call is interrupted and the signal handler does not raise an exception, the function now retries
the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).
Changed in version 3.6:
• Support added to accept objects implementing os.
PathLike.
Changed in version 3.11: The 'U' mode has been removed.
Given a string representing one Unicode character, return an integer representing the Unicode code point of that
character.
For example, ord('a') returns the integer 97 and ord('€') (Euro sign) returns 8364.
This is the
inverse of chr().
The two-argument form pow(base, exp) is equivalent to
using the power operator: base**exp.
The arguments must have numeric types.
With mixed operand types, the coercion rules for binary arithmetic
operators apply.
For int operands, the result has the same type as the operands (after coercion) unless the second
argument is negative; in that case, all arguments are converted to float and a float result is delivered.
For a negative base of type int or float and
a non-integral exponent, a complex result is delivered.
For example, pow(-9, 0.5) returns a value close to 3j.
For int operands base and exp, if mod is present, mod must also be of integer type and mod must be nonzero.
If
mod is present and exp is negative, base must be relatively prime to mod.
In that case, pow(inv_base, -exp,
mod) is returned, where inv_base is an inverse to base modulo mod.
Here’s an example of computing an inverse for 38 modulo 97:

True
Changed in version 3.8: For int operands, the three-argument form of pow now allows the second argument to
be negative, permitting computation of modular inverses.
Changed in version 3.8: Allow keyword arguments.
Formerly, only positional arguments were supported.
All non-keyword arguments are converted to strings like str() does and written to the stream, separated by sep
and followed by end.
Both sep and end must be strings; they can also be None, which means to use the default
values.
If no objects are given, print() will just write end.
The file argument must be an object with a write(string) method; if it is not present or None, sys.stdout
will be used.
Since printed arguments are converted to text strings, print() cannot be used with binary mode
file objects.
For these, use file.write(...) instead.
Output buffering is usually determined by file.
However, if flush is true, the stream is forcibly flushed.
Added the flush keyword argument.
And doc creates a docstring for the attribute.
If c is an instance of C, c.x will invoke the getter, c.x = value will invoke the setter, and del c.x the
deleter.
If given, doc will be the docstring of the property attribute.
Otherwise, the property will copy fget’s docstring (if it
exists).
A property object has getter, setter, and deleter methods usable as decorators that create a copy of the
property with the corresponding accessor function set to the decorated function.
The returned property object also has the attributes fget, fset, and fdel corresponding to the constructor
arguments.
The docstrings of property objects are now writeable.
Rather than being a function, range is actually an immutable sequence type, as documented in Ranges and Se-
quence Types — list, tuple, range.
repr(object)
Return a string containing a printable representation of an object.
For many types, this function makes an attempt to

is a string enclosed in angle brackets that contains the name of the type of the object together with additional
information often including the name and address of the object.
A class can control what this function returns for
its instances by defining a __repr__() method.
If sys.displayhook() is not accessible, this function will
raise RuntimeError.
If ndigits is omitted or is None, it returns the
nearest integer to its input.
For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power mi-
nus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both
round(0.5) and round(-0.5) are 0, and round(1.5) is 2).
Any integer value is valid for ndigits (pos-
itive, zero, or negative).
The return value is an integer if ndigits is omitted or None.
Otherwise, the return value
has the same type as number.
For a general Python object number, round delegates to number.__round__.
Note: The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67
instead of the expected 2.68.
This is not a bug: it’s a result of the fact that most decimal fractions can’t be
represented exactly as a float.
See tut-fp-issues for more information.
See set and Set
Types — set, frozenset for documentation about this class.
For other containers see the built-in frozenset, list, tuple, and dict classes, as well as the
collections module.
This is the counterpart of getattr().
The arguments are an object, a string, and an arbitrary value.
The string
may name an existing attribute or a new attribute.
The function assigns the value to the attribute, provided the
object allows it.
For example, setattr(x, 'foobar', 123) is equivalent to x.foobar = 123.
An attribute whose name is not an identifier will not
be accessible using the dot notation, but is accessible through getattr() etc..
Note: Since private name mangling happens at compilation time, one must manually mangle a private attribute’s
(attributes with two leading underscores) name in order to set it with setattr().
The start
and step arguments default to None.
Slice objects have read-only data attributes start, stop, and step which
merely return the argument values (or their default).
They have no other explicit functionality; however, they are
used by NumPy and other third-party packages.
Slice objects are also generated when extended indexing syntax is
used.
For example: a[start:stop:step] or a[start:stop, i].
See itertools.islice() for
an alternate version that returns an iterator.
Changed in version 3.12: Slice objects are now hashable (provided start, stop, and step are hashable).
Has two optional arguments which must be specified as keyword arguments.
The default value is None (compare the elements directly).
If set to True, then the list elements are sorted as if each comparison were reversed.
Use functools.cmp_to_key() to convert an old-style cmp function to a key function.
The built-in sorted() function is guaranteed to be stable.
A sort is stable if it guarantees not to change the
relative order of elements that compare equal — this is helpful for sorting in multiple passes (for example, sort by
department, then by salary grade).
The sort algorithm uses only < comparisons between items.
While defining an __lt__() method will suffice for
sorting, PEP 8 recommends that all six rich comparisons be implemented.
This will help avoid bugs when using
the same data with other ordering tools such as max() that rely on a different underlying method.
Implementing
all six comparisons also helps avoid confusion for mixed type comparisons which can call reflected the __gt__()
method.
For sorting examples and a brief sorting tutorial, see sortinghowto.
A static method does not receive an implicit first argument.
The @staticmethod form is a function decorator – see function for details.
A static method can be called either on the class (such as C.f()) or on an instance (such as C().f()).
Moreover,
they can be called as regular functions (such as f()).
Static methods in Python are similar to those found in Java or C++.
Also, see classmethod() for a variant that
is useful for creating alternate class constructors.
Like all decorators, it is also possible to call staticmethod as a regular function and do something with its
result.
This is needed in some cases where you need a reference to a function from a class body and you want to
avoid the automatic transformation to instance method.
For more information on static methods, see types.
Changed in version 3.10:
Static methods now inherit the method attributes (__module__, __name__,
__qualname__, __doc__ and __annotations__), have a new __wrapped__ attribute, and are now
callable as regular functions.
See str() for details.
Sums start and the items of an iterable from left to right and returns the total.
The iterable’s items are normally
numbers, and the start value is not allowed to be a string.
For some use cases, there are good alternatives to sum().
The preferred, fast way to concatenate a sequence of
strings is by calling ''.join(sequence).
To add floating point values with extended precision, see math.
fsum().
To concatenate a series of iterables, consider using itertools.chain().
Changed in version 3.8: The start parameter can be specified as a keyword argument.
Changed in version 3.12: Summation of floats switched to an algorithm that gives higher accuracy on most builds.
This is useful for accessing
inherited methods that have been overridden in a class.
The object_or_type determines the method resolution order to be searched.
The search starts from the class right
after the type.
A -> object and the value of type is B,
then super() searches C -> A -> object.
The __mro__ attribute of the object_or_type lists the method resolution search order used by both getattr()
and super().
The attribute is dynamic and can change whenever the inheritance hierarchy is updated.
If the second argument is omitted, the super object returned is unbound.
If the second argument is an object,
isinstance(obj, type) must be true.
If the second argument is a type, issubclass(type2, type)
must be true (this is useful for classmethods).
There are two typical use cases for super.
In a class hierarchy with single inheritance, super can be used to refer to
parent classes without naming them explicitly, thus making the code more maintainable.
This use closely parallels
the use of super in other programming languages.
The second use case is to support cooperative multiple inheritance in a dynamic execution environment.
This use
case is unique to Python and is not found in statically compiled languages or languages that only support single
inheritance.
This makes it possible to implement “diamond diagrams” where multiple base classes implement
the same method.
Good design dictates that such implementations have the same calling signature in every case
(because the order of calls is determined at runtime, because that order adapts to changes in the class hierarchy,
and because that order can include sibling classes that are unknown prior to runtime).
In addition to method lookups, super() also works for attribute lookups.
One possible use case for this is calling
descriptors in a parent or sibling class.
Note that super() is implemented as part of the binding process for explicit dotted attribute lookups such as
super().__getitem__(name).
It does so by implementing its own __getattribute__() method
for searching classes in a predictable order that supports cooperative multiple inheritance.
Accordingly, super()
is undefined for implicit lookups using statements or operators such as super()[name].
Also note that, aside from the zero argument form, super() is not limited to use inside methods.
The two
argument form specifies the arguments exactly and makes the appropriate references.
The zero argument form
only works inside a class definition, as the compiler fills in the necessary details to correctly retrieve the class being
defined, as well as accessing the current instance for ordinary methods.
For practical suggestions on how to design cooperative classes using super(), see guide to using super().
With one argument, return the type of an object.
The return value is a type object and generally the same object as
returned by object.__class__.
The isinstance() built-in function is recommended for testing the type of an object, because it takes subclasses
into account.
With three arguments, return a new type object.
This is essentially a dynamic form of the class statement.
The
name string is the class name and becomes the __name__ attribute.
The bases tuple contains the base classes
and becomes the __bases__ attribute; if empty, object, the ultimate base of all classes, is added.
The dict
dictionary contains attribute and method definitions for the class body; it may be copied or wrapped before becoming
the __dict__ attribute.
See also Type Objects.
Keyword arguments provided to the three argument form are passed to the appropriate metaclass machinery (usually
__init_subclass__()) in the same way that keywords in a class definition (besides metaclass) would.
See also class-customization.
MappingProxyType to
prevent direct dictionary updates).
Without an argument, vars() acts like locals().
Note, the locals dictionary is only useful for reads since
updates to the locals dictionary are ignored.
Another way to think of zip() is that it turns rows into columns, and columns into rows.
This is similar to
transposing a matrix.
zip() is lazy: The elements won’t be processed until the iterable is iterated on, e.g. by a for loop or by wrapping
in a list.
One thing to consider is that the iterables passed to zip() could have different lengths; sometimes by design, and
sometimes because of a bug in the code that prepared these iterables.
Python offers three different approaches to
dealing with this issue:
• By default, zip() stops when the shortest iterable is exhausted.
In such cases, it’s
recommended to use the strict=True option.
Traceback (most recent call last):

ValueError: zip() argument 2 is longer than argument 1
Without the strict=True argument, any bug that results in iterables of different lengths will be silenced,
possibly manifesting as a hard-to-find bug in another part of the program.
This is
done by itertools.zip_longest().
Edge cases: With a single iterable argument, zip() returns an iterator of 1-tuples.
With no arguments, it returns
an empty iterator.
The left-to-right evaluation order of the iterables is guaranteed.
This makes possible an idiom for clustering
a data series into n-length groups using zip(*[iter(s)]*n, strict=True).
This repeats the same
iterator n times so that each output tuple has the result of n calls to the iterator.
This has the effect of dividing
the input into n-length chunks.
Note: This is an advanced function that is not needed in everyday Python programming, unlike importlib.
import_module().
This function is invoked by the import statement.
Direct use of
__import__() is also discouraged in favor of importlib.import_module().
The function imports the module name, potentially using the given globals and locals to determine how to interpret
the name in a package context.
The fromlist gives the names of objects or submodules that should be imported
from the module given by name.
The standard implementation does not use its locals argument at all and uses its
globals only to determine the package context of the import statement.
Positive values for level indicate the number of parent directories to search relative to the directory of the module
calling __import__() (see PEP 328 for the details).
When the name variable is of the form package.module, normally, the top-level package (the name up till the
first dot) is returned, not the module named by name.
However, when a non-empty fromlist argument is given, the
module named by name is returned.
The statement import spam.ham results in this call:
spam = __import__('spam.ham', globals(), locals(), [], 0)
Note how __import__() returns the toplevel module here because this is the object that is bound to a name by
the import statement.
On the other hand, the statement from spam.ham import eggs, sausage as saus results in
_temp = __import__('spam.ham', globals(), locals(), ['eggs', 'sausage'], 0)
eggs = _temp.eggs
saus = _temp.sausage
Here, the spam.ham module is returned from __import__().
From this object, the names to import are
retrieved and assigned to their respective names.
If you simply want to import a module (potentially within a package) by name, use importlib.
import_module().
Negative values for level are no longer supported (which also changes the default value to
0).
Changed in version 3.9: When the command line options -E or -I are being used, the environment variable
PYTHONCASEOK is now ignored.
Built-in Functions
CHAPTER
THREE
BUILT-IN CONSTANTS
A small number of constants live in the built-in namespace.
They are:
False
The false value of the bool type.
Assignments to False are illegal and raise a SyntaxError.
True
The true value of the bool type.
Assignments to True are illegal and raise a SyntaxError.
None
An object frequently used to represent the absence of a value, as when default arguments are not passed to a function.
Assignments to None are illegal and raise a SyntaxError.
None is the sole instance of the NoneType type.
It should not be evaluated in a boolean context.
NotImplemented is the sole instance of the
types.
NotImplementedType type.
Note:
When a binary (or in-place) method returns NotImplemented the interpreter will try the
reflected operation on the other type (or some other fallback, depending on the operator).
If all at-
tempts return NotImplemented, the interpreter will raise an appropriate exception.
Incorrectly returning
NotImplemented will result in a misleading error message or the NotImplemented value being returned to
Python code.
See Implementing the arithmetic operations for examples.
Note:
NotImplementedError and NotImplemented are not interchangeable, even though they have
similar names and purposes.
See NotImplementedError for details on when to use it.
Changed in version 3.9: Evaluating NotImplemented in a boolean context is deprecated.
While it currently
evaluates as true, it will emit a DeprecationWarning.
It will raise a TypeError in a future version of
Python.
Ellipsis
The same as the ellipsis literal “...”.
Special value used mostly in conjunction with extended slicing syntax for
user-defined container data types.
Ellipsis is the sole instance of the types.
This constant is true if Python was not started with an -O option.
See also the assert statement.
They are useful for the interactive interpreter shell and should not be used in
programs.
The following sections describe the standard types that are built into the interpreter.
The principal built-in types are numerics, sequences, mappings, classes, instances and exceptions.
Some collection classes are mutable.
Some operations are supported by several object types; in particular, practically all objects can be compared for equality,
tested for truth value, and converted to a string (with the repr() function or the slightly different str() function).
The latter function is implicitly used when an object is written by the print() function.
None and False
• zero of any numeric type: 0, 0.0, 0j, Decimal(0), Fraction(0, 1)
• empty sequences and collections: '', (), [], {}, set(), range(0)
Operations and built-in functions that have a Boolean result always return 0 or False for false and 1 or True for true,
unless otherwise stated.
They all have the same priority (which is higher than that of the Boolean
operations).
Comparisons can be chained arbitrarily; for example, x < y <= z is equivalent to x < y and y <=
z, except that y is evaluated only once (but in both cases z is not evaluated at all when x < y is found to be false).
This table summarizes the comparison operations:
Operation
Meaning
<
strictly less than
<=
less than or equal
>
strictly greater than
>=
greater than or equal
==
equal
!=
not equal
is
object identity
is not
negated object identity
Objects of different types, except different numeric types, never compare equal.
The == operator is always defined but for
some object types (for example, class objects) is equivalent to is.
The <, <=, > and >= operators are only defined where
they make sense; for example, they raise a TypeError exception when one of the arguments is a complex number.
Non-identical instances of a class normally compare as non-equal unless the class defines the __eq__() method.
Instances of a class cannot be ordered with respect to other instances of the same class, or other types of object, unless the
class defines enough of the methods __lt__(), __le__(), __gt__(), and __ge__() (in general, __lt__()
and __eq__() are sufficient, if you want the conventional meanings of the comparison operators).
The behavior of the is and is not operators cannot be customized; also they can be applied to any two objects and
never raise an exception.
Two more operations with the same syntactic priority, in and not in, are supported by types that are iterable or
implement the __contains__() method.
In addition, Booleans are a
subtype of integers.
Integers have unlimited precision.
Floating point numbers are usually implemented using double
in C; information about the precision and internal representation of floating point numbers for the machine on which your
program is running is available in sys.float_info.
Complex numbers have a real and imaginary part, which are
each a floating point number.
To extract these parts from a complex number z, use z.real and z.imag.
Fraction, for rationals, and decimal.
Numbers are created by numeric literals or as the result of built-in functions and operators.
Unadorned integer literals
(including hex, octal and binary numbers) yield integers.
Appending 'j' or 'J' to a numeric literal yields an imaginary number (a complex number
with a zero real part) which you can add to an integer or float to get a complex number with real and imaginary parts.
Python fully supports mixed arithmetic: when a binary arithmetic operator has operands of different numeric types, the
operand with the “narrower” type is widened to that of the other, where integer is narrower than floating point, which is
narrower than complex.
A comparison between numbers of different types behaves as though the exact values of those
numbers were being compared.2
The constructors int(), float(), and complex() can be used to produce numbers of a specific type.
For operands of type int, the result has type int.
For operands of type
float, the result has type float.
In general, the result is a whole integer, though the result’s type is not necessarily
int.
The result is always rounded towards minus infinity: 1//2 is 0, (-1)//2 is -1, 1//(-2) is -1, and
(-1)//(-2) is 0.
Instead convert to floats using abs() if appropriate.
(3) Conversion from float to int truncates, discarding the fractional part.
See functions math.floor() and
math.ceil() for alternative conversions.
See the Unicode Standard for a complete list of code points with the Nd property.
All numbers.
As a consequence, the list [1, 2] is considered equal to [1.0, 2.0], and similarly for tuples.
If n is omitted, it defaults to 0.
math.floor(x)
the greatest Integral <= x
math.ceil(x)
the least Integral >= x
For additional numeric operations see the math and cmath modules.
The result of bitwise operations is calculated as though carried out in
two’s complement with an infinite number of sign bits.
The priorities of the binary bitwise operations are all lower than the numeric operations and higher than the comparisons;
the unary operation ~ has the same priority as the other unary numeric operations (+ and -).
The int type implements the numbers.
Integral abstract base class.
Equivalently, when abs(x) is small enough to have a correctly rounded logarithm, then
k = 1 + int(log(abs(x), 2)).
This is also known as
the population count.
Example:
'0b10011'


Equivalent to:

New in version 3.10.
int.to_bytes(length=1, byteorder=’big’, *, signed=False)
Return an array of bytes representing an integer.
The integer is represented using length bytes, and defaults to 1.
An OverflowError is raised if the integer is
not representable with the given number of bytes.
The byteorder argument determines the byte order used to represent the integer, and defaults to "big".
If byteorder
is "big", the most significant byte is at the beginning of the byte array.
If byteorder is "little", the most
significant byte is at the end of the byte array.
The signed argument determines whether two’s complement is used to represent the integer.
If signed is False
and a negative integer is given, an OverflowError is raised.
The default value for signed is False.
Equivalent to:
4.4.
Changed in version 3.11: Added default argument values for length and byteorder.
The argument bytes must either be a bytes-like object or an iterable producing bytes.
The byteorder argument determines the byte order used to represent the integer, and defaults to "big".
If byteorder
is "big", the most significant byte is at the beginning of the byte array.
If byteorder is "little", the most
significant byte is at the end of the byte array.
To request the native byte order of the host system, use sys.
byteorder as the byte order value.
The signed argument indicates whether two’s complement is used to represent the integer.
Equivalent to:

little_ordered = list(bytes)
elif byteorder == 'big':
little_ordered = list(reversed(bytes))
else:
raise ValueError("byteorder must be either 'little' or 'big'")
n = sum(b << i*8 for i, b in enumerate(little_ordered))
if signed and little_ordered and (little_ordered[-1] & 0x80):
n -= 1 << 8*len(little_ordered)

New in version 3.2.
Changed in version 3.11: Added default argument value for byteorder.
The integer
ratio of integers (whole numbers) is always the integer as the numerator and 1 as the denominator.
Exists for duck type compatibility with float.is_integer().
New in version 3.12.
Real abstract base class.
The ratio is in lowest terms and has a
positive denominator.
Raises OverflowError on infinities and a ValueError on NaNs.
Since Python’s floats are stored internally as binary
numbers, converting a float to or from a decimal string usually involves a small rounding error.
In contrast, hexadecimal
strings allow exact representation and specification of floating-point numbers.
This can be useful when debugging, and in
numerical work.
Class method to return the float represented by a hexadecimal string s.
The string s may have leading and trailing
whitespace.
Note that float.hex() is an instance method, while float.fromhex() is a class method.
Case is not significant, and there must be at least one
hexadecimal digit in either the integer or the fraction.
This syntax is similar to the syntax specified in section 6.4.4.2 of
the C99 standard, and also to the syntax used in Java 1.5 onwards.
In particular, the output of float.hex() is usable
as a hexadecimal floating-point literal in C or Java code, and hexadecimal strings produced by C’s %a format character
or Java’s Double.toHexString are accepted by float.fromhex().
Note that the exponent is written in decimal rather than hexadecimal, and that it gives the power of 2 by which to multiply
the coefficient.
For example, the hexadecimal string 0x3.a7p10 represents the floating-point number (3 + 10./16
+ 7./16**2) * 2.0**10, or 3740.0:
3740.0
Applying the reverse conversion to 3740.0 gives a different hexadecimal string representing the same number:
4.4.
For ease of implementation and efficiency across a
variety of numeric types (including int, float, decimal.
Decimal and fractions.
Python’s hash
for numeric types is based on a single mathematical function that’s defined for any rational number, and hence applies
to all instances of int and fractions.
Fraction, and all finite instances of float and decimal.
Decimal.
Essentially, this function is given by reduction modulo P for a fixed prime P.
The value of P is made available to Python
as the modulus attribute of sys.hash_info.
Currently, the prime used is P = 2**31 - 1 on machines with 32-bit C longs and
P = 2**61 - 1 on machines with 64-bit C longs.
If x = m / n is a nonnegative rational number and n is divisible by P (but m is not) then n has no inverse modulo
P and the rule above doesn’t apply; in this case define hash(x) to be the constant value sys.hash_info.inf.
If x = m / n is a negative rational number define hash(x) as -hash(-x).
If the resulting hash is -1,
replace it with -2.
The particular values sys.hash_info.inf and -sys.hash_info.inf are used as hash values for positive
infinity or negative infinity (respectively).
Again, if the result is -1, it’s replaced with -2.
To clarify the above rules, here’s some example Python code, equivalent to the built-in hash, for computing the hash of a
rational number, float, or complex:
import sys, math

Assumes m and n are integers, with n positive.
Equivalent to hash(fractions.
Fraction(m, n)).
The bool type has exactly two constant instances: True and False.
The built-in function bool() converts any value to a boolean, if the value can be interpreted as a truth value (see section
Truth Value Testing above).
For logical operations, use the boolean operators and, or and not.
When applying the bitwise operators &, |, ^ to two
booleans, they return a bool equivalent to the logical operations “and”, “or”, “xor”.
However, the logical operators and,
or and != should be preferred over &, | and ^.
Deprecated since version 3.12: The use of the bitwise inversion operator ~ is deprecated and will raise an error in Python
3.14.
In many numeric contexts, False and True
behave like the integers 0 and 1, respectively.
However, relying on this is discouraged; explicitly convert using int()
instead.
This is implemented using two distinct methods; these are used
to allow user-defined classes to support iteration.
Sequences, described below in more detail, always support the iteration
methods.
One method needs to be defined for container objects to provide iterable support:
container.__iter__()
Return an iterator object.
The object is required to support the iterator protocol described below.
If a container
supports different types of iteration, additional methods can be provided to specifically request iterators for those
iteration types.
This method corresponds to the tp_iter slot of the type
structure for Python objects in the Python/C API.
The iterator objects themselves are required to support the following two methods, which together form the iterator
protocol:
iterator.__iter__()
Return the iterator object itself.
This is required to allow both containers and iterators to be used with the for
and in statements.
This method corresponds to the tp_iter slot of the type structure for Python objects in the
Python/C API.
iterator.__next__()
Return the next item from the iterator.
If there are no further items, raise the StopIteration exception.
This
method corresponds to the tp_iternext slot of the type structure for Python objects in the Python/C API.
Python defines several iterator objects to support iteration over general and specific sequence types, dictionaries, and other
more specialized forms.
The specific types are not important beyond their implementation of the iterator protocol.
Implementations that do not obey this property are deemed broken.
More information about generators can be found in the
documentation for the yield expression.
Additional sequence types tailored for processing
of binary data and text strings are described in dedicated sections.
The
collections.abc.
Sequence ABC is provided to make it easier to correctly implement these operations on custom
sequence types.
This table lists the sequence operations sorted in ascending priority.
In the table, s and t are sequences of the same type,
n, i, j and k are integers and x is an arbitrary object that meets any type and value restrictions imposed by s.
The in and not in operations have the same priorities as the comparison operations.
In particular, tuples and lists are compared lexicographically by
comparing corresponding elements.
This means that to compare equal, every element must compare equal and the two
sequences must be of the same type and have the same length.
Forward and reversed iterators over mutable sequences access values using an index.
That index will continue to march
forward (or backward) even if the underlying sequence is mutated.
The iterator terminates only when an IndexError
or a StopIteration is encountered (or when the index drops below zero).
Notes:
(1) While the in and not in operations are used only for simple containment testing in the general case, some
specialised sequences (such as str, bytes and bytearray) also use them for subsequence testing:
True
(2) Values of n less than 0 are treated as 0 (which yields an empty sequence of the same type as s).
Note that items
in the sequence s are not copied; they are referenced multiple times.
Modifying any of the elements of lists modifies this single list.
But note that -0 is still 0.
If i or j is
greater than len(s), use len(s).
If i is omitted or None, use 0.
If j is omitted or None, use len(s).
If i is
greater than or equal to j, the slice is empty.
In other words, the indices are i, i+k, i+2*k, i+3*k and so on, stopping when j is reached
(but never including j).
When k is positive, i and j are reduced to len(s) if they are greater.
When k is negative,
i and j are reduced to len(s) - 1 if they are greater.
If i or j are omitted or None, they become “end” values
(which end depends on the sign of k).
Note, k cannot be zero.
If k is None, it is treated like 1.
This means that building up a sequence by
repeated concatenation will have a quadratic runtime cost in the total sequence length.
To get a linear runtime cost,
you must switch to one of the alternatives below:
• if concatenating str objects, you can build a list and use str.join() at the end or else write to an io.
StringIO instance and retrieve its value when complete
• if concatenating bytes objects, you can similarly use bytes.join() or io.BytesIO, or you can do
in-place concatenation with a bytearray object.
Not all implementations support passing the additional
arguments i and j.
These arguments allow efficient searching of subsections of the sequence.
Passing the extra ar-
guments is roughly equivalent to using s[i:j].index(x), only without copying any data and with the returned
index being relative to the start of the sequence rather than the start of the slice.
This support allows immutable sequences, such as tuple instances, to be used as dict keys and stored in set and
frozenset instances.
Attempting to hash an immutable sequence that contains unhashable values will result in TypeError.
The collections.abc.
MutableSequence ABC is provided to make it easier to correctly implement these operations on custom sequence
types.
To remind users that it operates by side effect, it does not return the reversed sequence.
MutableSequence ABC, but most concrete mutable sequence classes provide it.
New in version 3.3: clear() and copy() methods.
Zero and negative values of n clear the
sequence.
Items in the sequence are not copied; they are referenced multiple times, as explained for s * n under
Common Sequence Operations.
The constructor builds a list whose items are the same and in the same order as iterable’s items.
If iterable is already a list, a copy is
made and returned, similar to iterable[:].
For example, list('abc') returns ['a', 'b', 'c']
and list( (1, 2, 3) ) returns [1, 2, 3].
If no argument is given, the constructor creates a new empty
list, [].
Many other operations also produce lists, including the sorted() built-in.
Lists implement all of the common and mutable sequence operations.
This method sorts the list in place, using only < comparisons between items.
Exceptions are not suppressed -
if any comparison operations fail, the entire sort operation will fail (and the list will likely be left in a partially
modified state).
The key corresponding to each item in the list is calculated once and then used
for the entire sorting process.
The default value of None means that list items are sorted directly without
calculating a separate key value.
The functools.cmp_to_key() utility is available to convert a 2.x style cmp function to a key function.
If set to True, then the list elements are sorted as if each comparison were
reversed.
This method modifies the sequence in place for economy of space when sorting a large sequence.
To remind
users that it operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request
a new sorted list instance).
The sort() method is guaranteed to be stable.
A sort is stable if it guarantees not to change the relative
order of elements that compare equal — this is helpful for sorting in multiple passes (for example, sort by
department, then by salary grade).
For sorting examples and a brief sorting tutorial, see sortinghowto.
CPython implementation detail: While a list is being sorted, the effect of attempting to mutate, or even
inspect, the list is undefined.
Tuples are also used for cases where an immutable sequence of homogeneous data is
needed (such as allowing storage in a set or dict instance).
The constructor builds a tuple whose items are the same and in the same order as iterable’s items.
If iterable is already a tuple, it is
returned unchanged.
For example, tuple('abc') returns ('a', 'b', 'c') and tuple( [1, 2, 3]
) returns (1, 2, 3).
If no argument is given, the constructor creates a new empty tuple, ().
Note that it is actually the comma which makes a tuple, not the parentheses.
The parentheses are optional, except
in the empty tuple case, or when they are needed to avoid syntactic ambiguity.
For example, f(a, b, c) is a
function call with three arguments, while f((a, b, c)) is a function call with a 3-tuple as the sole argument.
Tuples implement all of the common sequence operations.
For heterogeneous collections of data where access by name is clearer than access by index, collections.
The arguments to the range constructor must be integers (either built-in int or any object that implements the
__index__() special method).
If the step argument is omitted, it defaults to 1.
If the start argument is omitted,
it defaults to 0.
If step is zero, ValueError is raised.
For a positive step, the contents of a range r are determined by the formula r[i] = start + step*i where
i >= 0 and r[i] < stop.
For a negative step, the contents of the range are still determined by the formula r[i] = start + step*i,
but the constraints are i >= 0 and r[i] > stop.
A range object will be empty if r[0] does not meet the value constraint.
Ranges do support negative indices, but
these are interpreted as indexing from the end of the sequence determined by the positive indices.
Ranges containing absolute values larger than sys.maxsize are permitted but some features (such as len())
may raise OverflowError.
The advantage of the range type over a regular list or tuple is that a range object will always take the same
(small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step
values, calculating individual items and subranges as needed).
Range objects implement the collections.abc.
Sequence ABC, and provide features such as containment tests,
element index lookup, slicing and support for negative indices (see Sequence Types — list, tuple, range):
range(0, 20, 2)
False
True


range(0, 10, 2)

Testing range objects for equality with == and !
That is, two range objects are considered
equal if they represent the same sequence of values.
Support slicing and negative indices.
Test int objects for
membership in constant time instead of iterating through all items.
Changed in version 3.3: Define ‘==’ and ‘!=’ to compare range objects based on the sequence of values they define (instead
of comparing based on object identity).
The linspace recipe shows how to implement a lazy version of range suitable for floating point applications.
Strings are immutable sequences of Unicode code points.
String literals are written in a variety of ways:
• Single quotes: 'allows embedded "double" quotes'
• Double quotes: "allows embedded 'single' quotes"
• Triple quoted: '''Three single quotes''', """Three double quotes"""
Triple quoted strings may span multiple lines - all associated whitespace will be included in the string literal.
String literals that are part of a single expression and have only whitespace between them will be implicitly converted to
a single string literal.
That is, ("spam " "eggs") == "spam eggs".
See strings for more about the various forms of string literal, including supported escape sequences, and the r (“raw”)
prefix that disables most escape sequence processing.
Strings may also be created from other objects using the str constructor.
Since there is no separate “character” type, indexing a string produces strings of length 1.
That is, for a non-empty string
s, s[0] == s[0:1].
There is also no mutable string type, but str.join() or io.StringIO can be used to efficiently construct strings
from multiple fragments.
Changed in version 3.3: For backwards compatibility with the Python 2 series, the u prefix is once again permitted on
string literals.
It has no effect on the meaning of string literals and cannot be combined with the r prefix.
If object is not provided, returns the empty string.
Otherwise, the behavior of
str() depends on whether encoding or errors is given, as follows.
If neither encoding nor errors is given, str(object) returns type(object).__str__(object), which
is the “informal” or nicely printable string representation of object.
For string objects, this is the string itself.
If
object does not have a __str__() method, then str() falls back to returning repr(object).
If at least one of encoding or errors is given, object should be a bytes-like object (e.g. bytes or bytearray).
In
this case, if object is a bytes (or bytearray) object, then str(bytes, encoding, errors) is equiv-
alent to bytes.decode(encoding, errors).
Otherwise, the bytes object underlying the buffer object
is obtained before calling bytes.decode().
See Binary Sequence Types — bytes, bytearray, memoryview and
bufferobjects for information on buffer objects.
Passing a bytes object to str() without the encoding or errors arguments falls under the first case of returning
the informal string representation (see also the -b command-line option to Python).
For example:
"b'Zoot!'"
For more information on the str class and its methods, see Text Sequence Type — str and the String Methods
section below.
To output formatted strings, see the f-strings and Format String Syntax sections.
In addition, see the
Text Processing Services section.
Strings also support two styles of string formatting, one providing a large degree of flexibility and customization (see str.
format(), Format String Syntax and Custom String Formatting) and the other based on C printf style formatting that
handles a narrower range of types and is slightly harder to use correctly, but is often faster for the cases it can handle
(printf-style String Formatting).
The Text Processing Services section of the standard library covers a number of other modules that provide various text
related utilities (including regular expression support in the re module).
Changed in version 3.8: The first character is now put into titlecase rather than uppercase.
This means that char-
acters like digraphs will only have their first letter capitalized, instead of the full character.
Casefolded strings may be used for caseless matching.
Casefolding is similar to lowercasing but more aggressive because it is intended to remove all case distinctions in
a string.
For example, the German lowercase letter 'ß' is equivalent to "ss".
Since it is already lowercase,
lower() would do nothing to 'ß'; casefold() converts it to "ss".
The casefolding algorithm is described in section 3.13 ‘Default Case Folding’ of the Unicode Standard.
New in version 3.3.
Padding is done using the specified fillchar (default is an ASCII space).
The original string is returned if width is less than or equal to len(s).
Optional arguments
start and end are interpreted as in slice notation.
If sub is empty, returns the number of empty strings between characters which is the length of the string plus one.
str.encode(encoding=’utf-8’, errors=’strict’)
Return the string encoded to bytes.
If 'strict' (the default), a UnicodeError ex-
ception is raised.
Other possible values are 'ignore', 'replace', 'xmlcharrefreplace',
'backslashreplace' and any other name registered via codecs.register_error().
See Error Han-
dlers for details.
For performance reasons, the value of errors is not checked for validity unless an encoding error actually occurs,
Python Development Mode is enabled or a debug build is used.
Changed in version 3.1: Added support for keyword arguments.
The value of the errors argument is now checked in Python Development Mode and in
debug mode.
With optional start, test beginning at that position.
With optional end, stop comparing at that
position.
Return a copy of the string where all tab characters are replaced by one or more spaces, depending on the current
column and the given tab size.
Tab positions occur every tabsize characters (default is 8, giving tab positions at
columns 0, 8, 16 and so on).
To expand the string, the current column is set to zero and the string is examined
character by character.
If the character is a tab (\t), one or more space characters are inserted in the result until the
current column is equal to the next tab position.
If the character is a newline
(\n) or return (\r), it is copied and the current column is reset to zero.
Any other character is copied unchanged
and the current column is incremented by one regardless of how the character is represented when printed.
Optional
arguments start and end are interpreted as in slice notation.
Note:
The find() method should be used only if you need to know the position of sub.
To check if sub is a
substring or not, use the in operator:
True
str.format(*args, **kwargs)
Perform a string formatting operation.
The string on which this method is called can contain literal text or re-
placement fields delimited by braces {}.
Each replacement field contains either the numeric index of a positional
argument, or the name of a keyword argument.
Returns a copy of the string where each replacement field is replaced
with the string value of the corresponding argument.
Note: When formatting a number (int, float, complex, decimal.
Decimal and subclasses) with the n
type (ex: '{:n}'.format(1234)), the function temporarily sets the LC_CTYPE locale to the LC_NUMERIC
locale to decode decimal_point and thousands_sep fields of localeconv() if they are non-ASCII or
longer than 1 byte, and the LC_NUMERIC locale is different than the LC_CTYPE locale.
This temporary change
affects other threads.
Changed in version 3.7: When formatting a number with the n type, the function sets temporarily the LC_CTYPE
locale to the LC_NUMERIC locale in some cases.
Similar to str.format(**mapping), except that mapping is used directly and not copied to a dict.
Like find(), but raise ValueError when the substring is not found.
str.isalnum()
Return True if all characters in the string are alphanumeric and there is at least one character, False otherwise.
A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.
isdigit(), or c.isnumeric().
str.isalpha()
Return True if all characters in the string are alphabetic and there is at least one character, False otherwise.
Alphabetic characters are those characters defined in the Unicode character database as “Letter”, i.e., those with
general category property being one of “Lm”, “Lt”, “Lu”, “Ll”, or “Lo”.
Note that this is different from the Alpha-
betic property defined in the section 4.10 ‘Letters, Alphabetic, and Ideographic’ of the Unicode Standard.
ASCII characters
have code points in the range U+0000-U+007F.
New in version 3.7.
str.isdecimal()
Return True if all characters in the string are decimal characters and there is at least one character, False
otherwise.
Decimal characters are those that can be used to form numbers in base 10, e.g. U+0660, ARABIC-
INDIC DIGIT ZERO.
Formally a decimal character is a character in the Unicode General Category “Nd”.
Digits
include decimal characters and digits that need special handling, such as the compatibility superscript digits.
This
covers digits which cannot be used to form numbers in base 10, like the Kharosthi numbers.
Formally, a digit is a
character that has the property value Numeric_Type=Digit or Numeric_Type=Decimal.
str.isidentifier()
Return True if the string is a valid identifier according to the language definition, section identifiers.
keyword.iskeyword() can be used to test whether string s is a reserved identifier, such as def and class.
Example:
(True, False)
(True, True)
str.islower()
Return True if all cased characters4 in the string are lowercase and there is at least one cased character, False
otherwise.
Numeric characters include digit characters, and all characters that have the Unicode numeric value
property, e.g. U+2155, VULGAR FRACTION ONE FIFTH.
Formally, numeric characters are those with the
property value Numeric_Type=Digit, Numeric_Type=Decimal or Numeric_Type=Numeric.
Nonprintable
characters are those characters defined in the Unicode character database as “Other” or “Separator”, excepting the
ASCII space (0x20) which is considered printable.
A character is whitespace if in the Unicode character database (see unicodedata), either its general category is
Zs (“Separator, space”), or its bidirectional class is one of WS, B, or S.
str.istitle()
Return True if the string is a titlecased string and there is at least one character, for example uppercase characters
may only follow uncased characters and lowercase characters only cased ones.
Return False otherwise.
True
False
False
False
str.join(iterable)
Return a string which is the concatenation of the strings in iterable.
A TypeError will be raised if there are any
non-string values in iterable, including bytes objects.
The separator between elements is the string providing this
method.
Padding is done using the specified fillchar (default is an
ASCII space).
The original string is returned if width is less than or equal to len(s).
The lowercasing algorithm used is described in section 3.13 ‘Default Case Folding’ of the Unicode Standard.
The chars argument is a string specifying the set of
characters to be removed.
If omitted or None, the chars argument defaults to removing whitespace.
The chars
argument is not a prefix; rather, all combinations of its values are stripped:
4.8.
This static method returns a translation table usable for str.translate().
If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters (strings
of length 1) to Unicode ordinals, strings (of arbitrary lengths) or None.
Character keys will then be converted to
ordinals.
If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in
x will be mapped to the character at the same position in y.
If there is a third argument, it must be a string, whose
characters will be mapped to None in the result.
Split the string at the first occurrence of sep, and return a 3-tuple containing the part before the separator, the
separator itself, and the part after the separator.
If the string starts with the prefix string, return string[len(prefix):].
If the string ends with the suffix string and that suffix is not empty, return string[:-len(suffix)].
Oth-
erwise, return a copy of the original string:
'Misc'
'TmpDirMixin'
New in version 3.9.
str.replace(old, new[, count])
Return a copy of the string with all occurrences of substring old replaced by new.
If the optional argument count
is given, only the first count occurrences are replaced.
Optional arguments start and end are interpreted as in slice notation.
Return -1 on failure.
Like rfind() but raises ValueError when the substring sub is not found.
Padding is done using the specified fillchar (default is
an ASCII space).
The original string is returned if width is less than or equal to len(s).
If the separator is not found, return a 3-tuple containing two
empty strings, followed by the string itself.
If maxsplit is given, at most maxsplit
splits are done, the rightmost ones.
If sep is not specified or None, any whitespace string is a separator.
Except for
splitting from the right, rsplit() behaves like split() which is described in detail below.
The chars argument is a string specifying the set of
characters to be removed.
If omitted or None, the chars argument defaults to removing whitespace.
The chars
argument is not a suffix; rather, all combinations of its values are stripped:
spacious
'.rstrip()
'
spacious'
'mississ'
See str.removesuffix() for a method that will remove a single suffix string rather than all of a set of
characters.
For example:
'M'
'Monty'
str.split(sep=None, maxsplit=- 1)
Return a list of the words in the string, using sep as the delimiter string.
If maxsplit is given, at most maxsplit splits
are done (thus, the list will have at most maxsplit+1 elements).
If maxsplit is not specified or -1, then there is
no limit on the number of splits (all possible splits are made).
The sep argument may consist of multiple charac-
ters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']).
Splitting an empty string with
a specified separator returns [''].
Consequently, splitting an empty string or a string consisting of just whitespace with
a None separator returns [].
4.8.
Line breaks are not included in the resulting list
unless keepends is given and true.
This method splits on the following line boundaries.
In particular, the boundaries are a superset of universal new-
lines.
Representation
Description
\n
Line Feed
\r
Carriage Return
\r\n
Carriage Return + Line Feed
\v or \x0b
Line Tabulation
\f or \x0c
Form Feed
\x1c
File Separator
\x1d
Group Separator
\x1e
Record Separator
\x85
Next Line (C1 Control Code)
\u2028
Line Separator
\u2029
Paragraph Separator
Changed in version 3.2: \v and \f added to list of line boundaries.
With optional start, test string beginning at that position.
With optional end, stop comparing string at that
position.
str.strip([chars])
Return a copy of the string with the leading and trailing characters removed.
The chars argument is a string
specifying the set of characters to be removed.
If omitted or None, the chars argument defaults to removing
whitespace.
The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped:
spacious
'.strip()
'spacious'
'example'
The outermost leading and trailing chars argument values are stripped from the string.
Characters are removed
from the leading end until reaching a string character that is not contained in the set of characters in chars.
A
similar action takes place on the trailing end.
For example:
'Section 3.2.1 Issue #32'
str.swapcase()
Return a copy of the string with uppercase characters converted to lowercase and vice versa.
Note that it is not
necessarily true that s.swapcase().swapcase() == s.
str.title()
Return a titlecased version of the string where words start with an uppercase character and the remaining characters
are lowercase.
For example:
'Hello World'
The algorithm uses a simple language-independent definition of a word as groups of consecutive letters.
The
table must be an object that implements indexing via __getitem__(), typically a mapping or sequence.
When
indexed by a Unicode ordinal (an integer), the table object can do any of the following: return a Unicode ordinal or
4.8.
You can use str.maketrans() to create a translation map from character-to-character mappings in different
formats.
See also the codecs module for a more flexible approach to custom character mappings.
Note that s.upper().
isupper() might be False if s contains uncased characters or if the Unicode category of the resulting char-
acter(s) is not “Lu” (Letter, uppercase), but e.g. “Lt” (Letter, titlecase).
The uppercasing algorithm used is described in section 3.13 ‘Default Case Folding’ of the Unicode Standard.
A leading sign prefix
('+'/'-') is handled by inserting the padding after the sign character rather than before.
The original string is
returned if width is less than or equal to len(s).
For example:
'00042'
'-0042'
4.8.2 printf-style String Formatting
Note: The formatting operations described here exhibit a variety of quirks that lead to a number of common errors (such
as failing to display tuples and dictionaries correctly).
Using the newer formatted string literals, the str.format()
interface, or template strings may help avoid these errors.
Each of these alternatives provides their own trade-offs and
benefits of simplicity, flexibility, and/or extensibility.
String objects have one unique built-in operation: the % operator (modulo).
This is also known as the string formatting
or interpolation operator.
Given format % values (where format is a string), % conversion specifications in format
are replaced with zero or more elements of values.
The effect is similar to using the sprintf() in the C language.
If format requires a single argument, values may be a single non-tuple object.5 Otherwise, values must be a tuple with
exactly the number of items specified by the format string, or a single mapping object (for example, a dictionary).
The '%' character, which marks the start of the specifier.
Conversion flags (optional), which affect the result of some conversion types.
If specified as an '*' (asterisk), the actual width is read from the next element
of the tuple in values, and the object to convert comes after the minimum field width and optional precision.
If specified as '*' (an asterisk), the actual
precision is read from the next element of the tuple in values, and the value to convert comes after the precision.
Conversion type.
When the right argument is a dictionary (or other mapping type), then the formats in the string must include a parenthesised
mapping key into that dictionary inserted immediately after the '%' character.
The mapping key selects the value to be
formatted from the mapping.
For example:

{'language': "Python", "number": 2})
Python has 002 quote types.
In this case no * specifiers may occur in a format (since they require a sequential parameter list).
The conversion flag characters are:
Flag
Meaning
'#'
The value conversion will use the “alternate form” (where defined below).
The conversion will be zero padded for numeric values.
A blank should be left before a positive number (or empty string) produced by a signed conversion.
A length modifier (h, l, or L) may be present, but is ignored as it is not necessary for Python – so e.g. %ld is identical
to %d.
The conversion types are:
Conver-
sion
Meaning
Notes
'd'
Signed integer decimal.
Uses lowercase exponential format if exponent is less than -4 or not less
than precision, decimal format otherwise.
Uses uppercase exponential format if exponent is less than -4 or not less
than precision, decimal format otherwise.
Notes:
(1) The alternate form causes a leading octal specifier ('0o') to be inserted before the first digit.
The precision determines the number of digits after the decimal point and defaults to 6.
(4) The alternate form causes the result to always contain a decimal point, and trailing zeroes are not removed as they
would otherwise be.
The precision determines the number of significant digits before and after the decimal point and defaults to 6.
(5) If precision is N, the output is truncated to N characters.
Since Python strings have an explicit length, %s conversions do not assume that '\0' is the end of the string.
They are supported by memoryview
which uses the buffer protocol to access the memory of other binary objects without needing to make a copy.
The array module supports efficient storage of basic data types like 32-bit integers and IEEE754 double-precision
floating values.
Since many major binary protocols are based on the ASCII text
encoding, bytes objects offer several methods that are only valid when working with ASCII compatible data and are closely
related to string objects in a variety of other ways.
Firstly, the syntax for bytes literals is largely the same as that for string literals, except that a b prefix is added:
• Single quotes: b'still allows embedded "double" quotes'
• Double quotes: b"still allows embedded 'single' quotes"
• Triple quoted: b'''3 single quotes''', b"""3 double quotes"""
Only ASCII characters are permitted in bytes literals (regardless of the declared source code encoding).
Any binary
values over 127 must be entered into bytes literals using the appropriate escape sequence.
As with string literals, bytes literals may also use a r prefix to disable processing of escape sequences.
See strings
for more about the various forms of bytes literal, including supported escape sequences.
This is done deliberately to emphasise that while many binary formats
include ASCII based elements and can be usefully manipulated with some text-oriented algorithms, this is not
generally the case for arbitrary binary data (blindly applying text processing algorithms to binary data formats that
are not ASCII compatible will usually lead to data corruption).
Since 2 hexadecimal digits correspond precisely to a single byte, hexadecimal numbers are a commonly used format
for describing binary data.
This bytes class method returns a bytes object, decoding the given string object.
The string must contain
two hexadecimal digits per byte, with ASCII whitespace being ignored.
A reverse conversion function exists to transform a bytes object into its hexadecimal representation.
By default, this separator will be included between each byte.
A second optional
bytes_per_sep parameter controls the spacing.
Positive values calculate the separator position from the right,
negative values from the left.
Changed in version 3.8: bytes.hex() now supports optional sep and bytes_per_sep parameters to insert
separators between bytes in the hex output.
Since bytes objects are sequences of integers (akin to a tuple), for a bytes object b, b[0] will be an integer, while b[0:1]
will be a bytes object of length 1.
The representation of bytes objects uses the literal format (b'...') since it is often more useful than e.g. bytes([46,
46, 46]).
You can always convert a bytes object into a list of integers using list(b).
As bytearray objects are mutable, they support the mutable sequence operations in addition to the common bytes
and bytearray operations described in Bytes and Bytearray Operations.
Also see the bytearray built-in.
Since 2 hexadecimal digits correspond precisely to a single byte, hexadecimal numbers are a commonly used format
for describing binary data.
This bytearray class method returns bytearray object, decoding the given string object.
The string must
contain two hexadecimal digits per byte, with ASCII whitespace being ignored.
A reverse conversion function exists to transform a bytearray object into its hexadecimal representation.
Since bytearray objects are sequences of integers (akin to a list), for a bytearray object b, b[0] will be an integer, while
b[0:1] will be a bytearray object of length 1.
The representation of bytearray objects uses the bytes literal format (bytearray(b'...')) since it is often more
useful than e.g. bytearray([46, 46, 46]).
You can always convert a bytearray object into a list of integers
using list(b).
They interoperate not just with operands of
the same type, but with any bytes-like object.
Due to this flexibility, they can be freely mixed in operations without causing
errors.
However, the return type of the result may depend on the order of operands.
Note: The methods on bytes and bytearray objects don’t accept strings as their arguments, just as the methods on strings
don’t accept bytes as their arguments.
Some bytes and bytearray operations assume the use of ASCII compatible binary formats, and hence should be avoided
when working with arbitrary binary data.
These restrictions are covered below.
Note: Using these ASCII based operations to manipulate binary data that is not stored in an ASCII based format may
lead to data corruption.
The following methods on bytes and bytearray objects can be used with arbitrary binary data.
Optional arguments
start and end are interpreted as in slice notation.
The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.
If sub is empty, returns the number of empty slices between characters which is the length of the bytes object plus
one.
If the binary data starts with the prefix string, return bytes[len(prefix):].
The prefix may be any bytes-like object.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
New in version 3.9.
If the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)].
The suffix may be any bytes-like object.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
New in version 3.9.
bytes.decode(encoding=’utf-8’, errors=’strict’)
bytearray.decode(encoding=’utf-8’, errors=’strict’)
Return the bytes decoded to a str.
encoding defaults to 'utf-8'; see Standard Encodings for possible values.
If 'strict' (the default), a UnicodeError exception
is raised.
Other possible values are 'ignore', 'replace', and any other name registered via codecs.
See Error Handlers for details.
For performance reasons, the value of errors is not checked for validity unless a decoding error actually occurs,
Python Development Mode is enabled or a debug build is used.
Note:
Passing the encoding argument to str allows decoding any bytes-like object directly, without needing to
make a temporary bytes or bytearray object.
Changed in version 3.1: Added support for keyword arguments.
The value of the errors argument is now checked in Python Development Mode and in
debug mode.
With optional start, test beginning at that position.
With optional end, stop comparing at
that position.
The suffix(es) to search for may be any bytes-like object.
Optional arguments start and end are interpreted as in slice notation.
To check if sub is a
substring or not, use the in operator:
True
Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.
Like find(), but raise ValueError when the subsequence is not found.
The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.
Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.
A
TypeError will be raised if there are any values in iterable that are not bytes-like objects, including str ob-
jects.
The separator between elements is the contents of the bytes or bytearray object providing this method.
This static method returns a translation table usable for bytes.translate() that will map each character in
from into the character at the same position in to; from and to must both be bytes-like objects and have the same
length.
New in version 3.1.
bytes.partition(sep)
bytearray.partition(sep)
Split the sequence at the first occurrence of sep, and return a 3-tuple containing the part before the separator, the
separator itself or its bytearray copy, and the part after the separator.
If the separator is not found, return a 3-tuple
containing a copy of the original sequence, followed by two empty bytes or bytearray objects.
The separator to search for may be any bytes-like object.
If the optional argument
count is given, only the first count occurrences are replaced.
The subsequence to search for and its replacement may be any bytes-like object.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
Optional arguments start and end are interpreted as in slice notation.
Return -1 on failure.
The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.
4.9.
The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.
Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.
bytes.rpartition(sep)
bytearray.rpartition(sep)
Split the sequence at the last occurrence of sep, and return a 3-tuple containing the part before the separator, the
separator itself or its bytearray copy, and the part after the separator.
If the separator is not found, return a 3-tuple
containing two empty bytes or bytearray objects, followed by a copy of the original sequence.
The separator to search for may be any bytes-like object.
With optional start, test beginning at that position.
With optional end, stop comparing at
that position.
The prefix(es) to search for may be any bytes-like object.
You can use the bytes.maketrans() method to create a translation table.
Set the table argument to None for translations that only delete characters:
b'rd ths shrt txt'
Changed in version 3.6: delete is now supported as a keyword argument.
The following methods on bytes and bytearray objects have default behaviours that assume the use of ASCII compatible
binary formats, but can still be used with arbitrary binary data by passing appropriate arguments.
Note that all of the
bytearray methods in this section do not operate in place, and instead produce new objects.
Padding is done using the specified fillbyte
(default is an ASCII space).
For bytes objects, the original sequence is returned if width is less than or equal to
len(s).
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
Padding is done using the specified fillbyte
(default is an ASCII space).
For bytes objects, the original sequence is returned if width is less than or equal to
len(s).
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
The chars argument is a binary sequence
specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII
characters.
If omitted or None, the chars argument defaults to removing ASCII whitespace.
The chars argument
is not a prefix; rather, all combinations of its values are stripped:
spacious
'.lstrip()
b'spacious
'
b'example.com'
The binary sequence of byte values to remove may be any bytes-like object.
See removeprefix() for a method
that will remove a single prefix string rather than all of a set of characters.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
Padding is done using the specified fillbyte
(default is an ASCII space).
For bytes objects, the original sequence is returned if width is less than or equal to
len(s).
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
If maxsplit is given,
at most maxsplit splits are done, the rightmost ones.
If sep is not specified or None, any subsequence consisting
solely of ASCII whitespace is a separator.
Except for splitting from the right, rsplit() behaves like split()
which is described in detail below.
The chars argument is a binary sequence
specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII
characters.
If omitted or None, the chars argument defaults to removing ASCII whitespace.
The binary sequence of byte values to remove may be any bytes-like object.
See removesuffix() for a method
that will remove a single suffix string rather than all of a set of characters.
For example:
b'M'
b'Monty'
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
If maxsplit is given
and non-negative, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements).
If
maxsplit is not specified or is -1, then there is no limit on the number of splits (all possible splits are made).
If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty subsequences
(for example, b'1,,2'.split(b',') returns [b'1', b'', b'2']).
Split-
ting an empty sequence with a specified separator returns [b''] or [bytearray(b'')] depending on the
type of object being split.
The sep argument may be any bytes-like object.
For example:
[b'1', b'2', b'3']
[b'1', b'2,3']
[b'1', b'2', b'', b'3', b'']
If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive ASCII whitespace
are regarded as a single separator, and the result will contain no empty strings at the start or end if the sequence has
leading or trailing whitespace.
The chars argument is a binary
sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used
with ASCII characters.
If omitted or None, the chars argument defaults to removing ASCII whitespace.
The binary sequence of byte values to remove may be any bytes-like object.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
The following methods on bytes and bytearray objects assume the use of ASCII compatible binary formats and should
not be applied to arbitrary binary data.
Note that all of the bytearray methods in this section do not operate in place, and
instead produce new objects.
Non-ASCII byte values are passed through unchanged.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
bytes.expandtabs(tabsize=8)
bytearray.expandtabs(tabsize=8)
Return a copy of the sequence where all ASCII tab characters are replaced by one or more ASCII spaces, depending
on the current column and the given tab size.
Tab positions occur every tabsize bytes (default is 8, giving tab positions
at columns 0, 8, 16 and so on).
To expand the sequence, the current column is set to zero and the sequence is
examined byte by byte.
If the byte is an ASCII tab character (b'\t'), one or more space characters are inserted
in the result until the current column is equal to the next tab position.
If
the current byte is an ASCII newline (b'\n') or carriage return (b'\r'), it is copied and the current column is
reset to zero.
Alphabetic ASCII characters are those byte values in the se-
quence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'.
For example:
True
False
bytes.isalpha()
bytearray.isalpha()
Return True if all bytes in the sequence are alphabetic ASCII characters and the sequence is
not empty, False otherwise.
Alphabetic ASCII characters are those byte values in the sequence
b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'.
For example:
True
False
bytes.isascii()
bytearray.isascii()
Return True if the sequence is empty or all bytes in the sequence are ASCII, False otherwise.
ASCII bytes are
in the range 0-0x7F.
New in version 3.7.
ASCII decimal digits are those byte values in the sequence b'0123456789'.
For example:
True
False
bytes.islower()
bytearray.islower()
Return True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters,
False otherwise.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
ASCII whitespace characters are those byte values in the sequence b' \t\n\r\x0b\f' (space, tab, newline,
carriage return, vertical tab, form feed).
See bytes.
For example:
True
False
bytes.isupper()
bytearray.isupper()
Return True if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII
characters, False otherwise.
For example:
True
False
Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
For example:
b'hello world'
Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
4.9.
This method uses the universal
newlines approach to splitting lines.
Line breaks are not included in the resulting list unless keepends is given and
true.
For example:
b'hELLO wORLD'
Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
Unlike str.swapcase(), it is always the case that bin.swapcase().swapcase() == bin for the
binary versions.
Case conversions are symmetrical in ASCII, even though that is not generally true for arbitrary
Unicode code points.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
Uncased byte values are left unmodified.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
All other byte values are uncased.
The algorithm uses a simple language-independent definition of a word as groups of consecutive letters.
Bill's Friends.
For example:
b'HELLO WORLD'
Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'.
Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
bytes.zfill(width)
bytearray.zfill(width)
Return a copy of the sequence left filled with ASCII b'0' digits to make a sequence of length width.
A leading sign
prefix (b'+'/ b'-') is handled by inserting the padding after the sign character rather than before.
For bytes
objects, the original sequence is returned if width is less than or equal to len(seq).
For example:
b'00042'
b'-0042'
4.9.
Note: The bytearray version of this method does not operate in place - it always produces a new object, even if
no changes were made.
If the value being printed may be a tuple or dictionary, wrap it in a
tuple.
Bytes objects (bytes/bytearray) have one unique built-in operation: the % operator (modulo).
This is also known as
the bytes formatting or interpolation operator.
Given format % values (where format is a bytes object), % conversion
specifications in format are replaced with zero or more elements of values.
The effect is similar to using the sprintf()
in the C language.
If format requires a single argument, values may be a single non-tuple object.
Page 58, 5 Otherwise, values must be a
tuple with exactly the number of items specified by the format bytes object, or a single mapping object (for example, a
dictionary).
The '%' character, which marks the start of the specifier.
Conversion flags (optional), which affect the result of some conversion types.
If specified as an '*' (asterisk), the actual width is read from the next element
of the tuple in values, and the object to convert comes after the minimum field width and optional precision.
If specified as '*' (an asterisk), the actual
precision is read from the next element of the tuple in values, and the value to convert comes after the precision.
When the right argument is a dictionary (or other mapping type), then the formats in the bytes object must include a
parenthesised mapping key into that dictionary inserted immediately after the '%' character.
The mapping key selects
the value to be formatted from the mapping.
In this case no * specifiers may occur in a format (since they require a sequential parameter list).
The conversion flag characters are:
Flag
Meaning
'#'
The value conversion will use the “alternate form” (where defined below).
The conversion will be zero padded for numeric values.
A blank should be left before a positive number (or empty string) produced by a signed conversion.
A length modifier (h, l, or L) may be present, but is ignored as it is not necessary for Python – so e.g. %ld is identical
to %d.
The conversion types are:
Conver-
sion
Meaning
Notes
'd'
Signed integer decimal.
Uses lowercase exponential format if exponent is less than -4 or not less
than precision, decimal format otherwise.
Uses uppercase exponential format if exponent is less than -4 or not less
than precision, decimal format otherwise.
No argument is converted, results in a '%' character in the result.
Notes:
(1) The alternate form causes a leading octal specifier ('0o') to be inserted before the first digit.
The precision determines the number of digits after the decimal point and defaults to 6.
(4) The alternate form causes the result to always contain a decimal point, and trailing zeroes are not removed as they
would otherwise be.
The precision determines the number of significant digits before and after the decimal point and defaults to 6.
(5) If precision is N, the output is truncated to N characters.
Note:
The bytearray version of this method does not operate in place - it always produces a new object, even if no
changes were made.
Adding % formatting to bytes and bytearray
New in version 3.5.
Built-in objects that support
the buffer protocol include bytes and bytearray.
A memoryview has the notion of an element, which is the atomic memory unit handled by the originating object.
For many simple types such as bytes and bytearray, an element is a single byte, but other types such as
array.array may have bigger elements.
len(view) is equal to the length of tolist, which is the nested list representation of the view.
If view.ndim
= 1, this is equal to the number of elements in the view.
Changed in version 3.12: If view.ndim == 0, len(view) now raises TypeError instead of returning 1.
The itemsize attribute will give you the number of bytes in a single element.
A memoryview supports slicing and indexing to expose its data.
One-dimensional slicing will result in a subview:


<memory at 0x7f3ddc9f4350>
b'bce'
If format is one of the native format specifiers from the struct module, indexing with an integer or a tuple of
integers is also supported and returns a single element with the correct type.
One-dimensional memoryviews can
be indexed with an integer or a one-integer tuple.
Multi-dimensional memoryviews can be indexed with tuples of
exactly ndim integers where ndim is the number of dimensions.
Zero-dimensional memoryviews can be indexed
with the empty tuple.
If the underlying object is writable, the memoryview supports one-dimensional slice assignment.
One-dimensional memoryviews of hashable (read-only) types with formats ‘B’, ‘b’ or ‘c’ are also hashable.
The hash
is defined as hash(m) == hash(m.tobytes()):
True
True
True
Changed in version 3.3: One-dimensional memoryviews can now be sliced.
One-dimensional memoryviews with
formats ‘B’, ‘b’ or ‘c’ are now hashable.
Changed in version 3.4: memoryview is now registered automatically with collections.abc.
Sequence
Changed in version 3.5: memoryviews can now be indexed with tuple of integers.
For the subset of struct format strings currently supported by tolist(), v and w are equal if v.
tolist() == w.tolist():
True
True
True
(continues on next page)
4.9.
Changed in version 3.3: Previous versions compared the raw memory disregarding the item format and the
logical array structure.
tobytes(order=’C’)
Return the data in the buffer as a bytestring.
This is equivalent to calling the bytes constructor on the
memoryview.
New in version 3.8: order can be {‘C’, ‘F’, ‘A’}.
When order is ‘C’ or ‘F’, the data of the original array is
converted to C or Fortran order.
For contiguous views, ‘A’ returns an exact copy of the physical memory.
In
particular, in-memory Fortran order is preserved.
For non-contiguous views, the data is converted to C first.
The original memoryview object is unchanged.
Many objects take special actions when
a view is held on them (for example, a bytearray would temporarily forbid resizing); therefore, calling
release() is handy to remove these restrictions (and free any dangling resources) as soon as possible.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: operation forbidden on released memoryview object
The context management protocol can be used for a similar effect, using the with statement:

m[0]


Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: operation forbidden on released memoryview object
New in version 3.2.
The return value is a new memoryview, but the
buffer itself is not copied.
Supported casts are 1D -> C-contiguous and C-contiguous -> 1D.
The destination format is restricted to a single element native format in struct syntax.
One of the formats
must be a byte format (‘B’, ‘b’ or ‘c’).
The byte length of the result must be the same as the original length.
Note that all byte lengths may depend on the operating system.
The source format is no longer restricted when casting to a byte view.
True
New in version 3.3.
This is the amount of
space in bytes that the array would use in a contiguous representation.
It is not necessarily equal to len(m):





4.9.
A memoryview can
be created from exporters with arbitrary format strings, but some methods (e.g. tolist()) are restricted
to native single element formats.
Changed in version 3.3: format 'B' is now handled according to the struct module syntax.
This means that
memoryview(b'abc')[0] == b'abc'[0] == 97.
itemsize
The size in bytes of each element of the memoryview:


True
ndim
An integer indicating how many dimensions of a multi-dimensional array the memory represents.
The value is informational only.
New in version 3.3.
New in version 3.3.
New in version 3.3.
4.10 Set Types — set, frozenset
A set object is an unordered collection of distinct hashable objects.
Common uses include membership testing, removing
duplicates from a sequence, and computing mathematical operations such as intersection, union, difference, and symmetric
difference.
Like other collections, sets support x in set, len(set), and for x in set.
Being an unordered collection,
sets do not record element position or order of insertion.
Accordingly, sets do not support indexing, slicing, or other
sequence-like behavior.
There are currently two built-in set types, set and frozenset.
The set type is mutable — the contents can be
changed using methods like add() and remove().
Since it is mutable, it has no hash value and cannot be used as
either a dictionary key or as an element of another set.
The frozenset type is immutable and hashable — its contents
cannot be altered after it is created; it can therefore be used as a dictionary key or as an element of another set.
Non-empty sets (not frozensets) can be created by placing a comma-separated list of elements within braces, for example:
{'jack', 'sjoerd'}, in addition to the set constructor.
The constructors for both classes work the same:
class set([iterable])
class frozenset([iterable])
Return a new set or frozenset object whose elements are taken from iterable.
The elements of a set must be hashable.
To represent sets of sets, the inner sets must be frozenset objects.
If iterable is not specified, a new empty set
is returned.
Sets can be created by several means:
• Use a comma-separated list of elements within braces: {'jack', 'sjoerd'}
• Use a set comprehension: {c for c in 'abracadabra' if c not in 'abc'}
• Use the type constructor: set(), set('foobar'), set(['a', 'b', 'foo'])
Instances of set and frozenset provide the following operations:
len(s)
Return the number of elements in set s (cardinality of s).
Sets are disjoint if and only if their intersection
is the empty set.
4.10.
Note,
the
non-operator
versions
of
union(),
intersection(),
difference(),
symmetric_difference(), issubset(), and issuperset() methods will accept any iterable
as an argument.
In contrast, their operator based counterparts require their arguments to be sets.
This precludes
error-prone constructions like set('abc') & 'cbs' in favor of the more readable set('abc').
Both set and frozenset support set to set comparisons.
Two sets are equal if and only if every element of
each set is contained in the other (each is a subset of the other).
A set is less than another set if and only if the first
set is a proper subset of the second set (is a subset, but is not equal).
A set is greater than another set if and only if
the first set is a proper superset of the second set (is a superset, but is not equal).
Instances of set are compared to instances of frozenset based on their members.
For example, set('abc')
== frozenset('abc') returns True and so does set('abc') in set([frozenset('abc')]).
The subset and equality comparisons do not generalize to a total ordering function.
For example, any two nonempty
disjoint sets are not equal and are not subsets of each other, so all of the following return False: a<b, a==b, or
a>b.
Since sets only define partial ordering (subset relationships), the output of the list.sort() method is undefined
for lists of sets.
Set elements, like dictionary keys, must be hashable.
Binary operations that mix set instances with frozenset return the type of the first operand.
For example:
frozenset('ab') | set('bc') returns an instance of frozenset.
The following table lists operations available for set that do not apply to immutable instances of frozenset:
update(*others)
set |= other | ...
Update the set, adding elements from all others.
intersection_update(*others)
set &= other & ...
Update the set, keeping only elements found in it and all others.
difference_update(*others)
set -= other | ...
Update the set, removing elements found in others.
Add element elem to the set.
Remove element elem from the set.
Raises KeyError if elem is not contained in the set.
Remove element elem from the set if it is present.
pop()
Remove and return an arbitrary element from the set.
Raises KeyError if the set is empty.
Remove all elements from the set.
Note,
the
non-operator
versions
of
the
update(),
intersection_update(),
difference_update(), and symmetric_difference_update() methods will accept any it-
erable as an argument.
Note, the elem argument to the __contains__(), remove(), and discard() methods may be a set.
To
support searching for an equivalent frozenset, a temporary one is created from elem.
Mappings are mutable objects.
There is currently only one
standard mapping type, the dictionary.
A dictionary’s keys are almost arbitrary values.
Values that are not hashable, that is, values containing lists, dictionaries
or other mutable types (that are compared by value rather than by object identity) may not be used as keys.
Values that
compare equal (such as 1, 1.0, and True) can be used interchangeably to index the same dictionary entry.
If no positional argument is given, an empty dictionary is created.
If a positional argument is given and it is
a mapping object, a dictionary is created with the same key-value pairs as the mapping object.
Otherwise, the
positional argument must be an iterable object.
Each item in the iterable must itself be an iterable with exactly two
objects.
The first object of each item becomes a key in the new dictionary, and the second object the corresponding
value.
If a key occurs more than once, the last value for that key becomes the corresponding value in the new
dictionary.
If keyword arguments are given, the keyword arguments and their values are added to the dictionary created from
the positional argument.
If a key being added is already present, the value from the keyword argument replaces the
value from the positional argument.
To illustrate, the following examples all return a dictionary equal to {"one": 1, "two": 2, "three":
3}:
True
Providing keyword arguments as in the first example only works for keys that are valid Python identifiers.
Otherwise,
any valid keys can be used.
These are the operations that dictionaries support (and therefore, custom mapping types should support too):
list(d)
Return a list of all the keys used in the dictionary d.
len(d)
Return the number of items in the dictionary d.
d[key]
Return the item of d with key key.
Raises a KeyError if key is not in the map.
If a subclass of dict defines a method __missing__() and key is not present, the d[key] operation calls
that method with the key key as argument.
The d[key] operation then returns or raises whatever is returned
or raised by the __missing__(key) call.
No other operations or methods invoke __missing__().
If __missing__() is not defined, KeyError is raised.
Counter.
This is a shortcut for iter(d.keys()).
Remove all items from the dictionary.
All of the values
refer to just a single instance, so it generally doesn’t make sense for value to be a mutable object such as an
empty list.
If default is not given, it defaults to None, so
that this method never raises a KeyError.
items()
Return a new view of the dictionary’s items ((key, value) pairs).
See the documentation of view objects.
If key is in the dictionary, remove it and return its value, else return default.
If default is not given and key is
not in the dictionary, a KeyError is raised.
Pairs are returned in LIFO (last-in, first-out)
order.
If the dictio-
nary is empty, calling popitem() raises a KeyError.
Changed in version 3.7: LIFO order is now guaranteed.
In prior versions, popitem() would return an
arbitrary key/value pair.
This is a shortcut for reversed(d.keys()).
New in version 3.8.
If key is in the dictionary, return its value.
If not, insert key with a value of default and return default.
Return None.
update() accepts either another dictionary object or an iterable of key/value pairs (as tuples or other iter-
ables of length two).
If keyword arguments are specified, the dictionary is then updated with those key/value
pairs: d.update(red=1, blue=2).
See the documentation of view objects.
An equality comparison between one dict.values() view and another will always return False.
This
also applies when comparing dict.values() to itself:
False
d | other
Create a new dictionary with the merged keys and values of d and other, which must both be dictionaries.
The values of other take priority when d and other share keys.
New in version 3.9.
The values of other take priority when d and other share keys.
New in version 3.9.
Dictionaries compare equal if and only if they have the same (key, value) pairs (regardless of ordering).
Order comparisons (‘<’, ‘<=’, ‘>=’, ‘>’) raise TypeError.
Dictionaries preserve insertion order.
Note that updating a key does not affect the order.
Keys added after deletion
are inserted at the end.
Dictionary order is guaranteed to be insertion order.
This behavior was an implementation
detail of CPython from 3.6.
Dictionaries and dictionary views are reversible.
See also:
types.
MappingProxyType can be used to create a read-only view of a dict.
They provide a
dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.
Dictionary views can be iterated over to yield their respective data, and support membership tests:
len(dictview)
Return the number of entries in the dictionary.
iter(dictview)
Return an iterator over the keys, values or items (represented as tuples of (key, value)) in the dictionary.
Keys and values are iterated over in insertion order.
This allows the creation of (value, key) pairs using
zip(): pairs = zip(d.values(), d.keys()).
Another way to create the same list is pairs =
[(v, k) for (k, v) in d.items()].
Iterating views while adding or deleting entries in the dictionary may raise a RuntimeError or fail to iterate
over all entries.
Dictionary order is guaranteed to be insertion order.
The view will be iterated in reverse order
of the insertion.
Dictionary views are now reversible.
MappingProxyType that wraps the original dictionary to which the view refers.
New in version 3.10.
Keys views are set-like since their entries are unique and hashable.
If all values are hashable, so that (key, value)
pairs are unique and hashable, then the items view is also set-like.
For set-like views, all of the operations defined for the abstract base class collections.
Set are available (for example, ==, <, or ^).
While using set operators, set-like views accept any iterable as the
other operand, unlike sets which only accept sets as the input.
This is implemented
using a pair of methods that allow user-defined classes to define a runtime context that is entered before the statement
body is executed and exited when the statement ends:
contextmanager.__enter__()
Enter the runtime context and return either this object or another object related to the runtime context.
The value
returned by this method is bound to the identifier in the as clause of with statements using this context manager.
An example of a context manager that returns itself is a file object.
File objects return themselves from __enter__()
to allow open() to be used as the context expression in a with statement.
An example of a context manager that returns a related object is the one returned by decimal.
These managers set the active decimal context to a copy of the original decimal context
and then return the copy.
Exit the runtime context and return a Boolean flag indicating if any exception that occurred should be suppressed.
If an exception occurred while executing the body of the with statement, the arguments contain the exception
type, value and traceback information.
Otherwise, all three arguments are None.
Returning a true value from this method will cause the with statement to suppress the exception and continue
execution with the statement immediately following the with statement.
Otherwise the exception continues prop-
agating after this method has finished executing.
Exceptions that occur during execution of this method will replace
any exception that occurred in the body of the with statement.
The exception passed in should never be reraised explicitly - instead, this method should return a false value to
indicate that the method completed successfully and does not want to suppress the raised exception.
This allows
context management code to easily detect whether or not an __exit__() method has actually failed.
Python defines several context managers to support easy thread synchronisation, prompt closure of files or other objects,
and simpler manipulation of the active decimal arithmetic context.
The specific types are not treated specially beyond
their implementation of the context management protocol.
See the contextlib module for some examples.
Python’s generators and the contextlib.contextmanager decorator provide a convenient way to implement these
protocols.
If a generator function is decorated with the contextlib.contextmanager decorator, it will return
a context manager implementing the necessary __enter__() and __exit__() methods, rather than the iterator
produced by an undecorated generator function.
Note that there is no specific slot for any of these methods in the type structure for Python objects in the Python/C API.
Extension types wanting to define these methods must provide them as a normal Python accessible method.
Compared
to the overhead of setting up the runtime context, the overhead of a single class dictionary lookup is negligible.
They are most often used with container classes,
such as list or dict.
For example, list[int] is a GenericAlias object created by subscripting the list class
with the argument int.
GenericAlias objects are intended primarily for use with type annotations.
Note:
It is generally only possible to subscript a class if the class implements the special method
__class_getitem__().
A GenericAlias object acts as a proxy for a generic type, implementing parameterized generics.
For a container class, the argument(s) supplied to a subscription of the class may indicate the type(s) of the elements an
object contains.
For example, set[bytes] can be used in type annotations to signify a set in which all the elements
are of type bytes.
For a class which defines __class_getitem__() but is not a container, the argument(s) supplied to a subscription
of the class will often indicate the return type(s) of one or more methods defined on an object.
If x = re.search('foo', 'foo'), x will be a re.Match object where the return values of x.group(0)
and x[0] will both be of type str.
We can represent this kind of object in type annotations with the
GenericAlias re.Match[str].
If y = re.search(b'bar', b'bar'), (note the b for bytes), y will also be an instance of re.Match,
but the return values of y.group(0) and y[0] will both be of type bytes.
In type annotations, we would
represent this variety of re.Match objects with re.Match[bytes].
GenericAlias objects are instances of the class types.
GenericAlias, which can also be used to create
GenericAlias objects directly.
T[X, Y, ...]
Creates a GenericAlias representing a type T parameterized by types X, Y, and more depending on the T used.
Another example for mapping objects, using a dict, which is a generic type expecting two type parameters rep-
resenting the key type and the value type.
The builtin functions isinstance() and issubclass() do not accept GenericAlias types for their second
argument:
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: isinstance() argument 2 cannot be a parameterized generic
The Python runtime does not enforce type annotations.
This extends to generic types and their type parameters.
When
creating a container object from a GenericAlias, the elements in the container are not checked against their type.
GenericAlias'>
<class 'list'>
Calling repr() or str() on a generic shows the parameterized type:
'list[int]'
'list[int]'
The __getitem__() method of generic containers will raise an exception to disallow mistakes like
dict[str][str]:
Traceback (most recent call last):

TypeError: dict[str] is not a generic class
However, such expressions are valid when type variables are used.
The index must have as many elements as there are
type variable items in the GenericAlias object’s __args__.
dict[str, int]
Standard Generic Classes
The following standard library classes support parameterized generics.
This list is non-exhaustive.
OrderedDict
4.13.
Counter
• collections.
ChainMap
• collections.abc.
Awaitable
• collections.abc.
Coroutine
• collections.abc.
AsyncIterable
• collections.abc.
AsyncIterator
• collections.abc.
AsyncGenerator
• collections.abc.
Iterable
• collections.abc.
Iterator
• collections.abc.
Generator
• collections.abc.
Reversible
• collections.abc.
Container
• collections.abc.
Collection
• collections.abc.
Callable
• collections.abc.
Set
• collections.abc.
MutableSet
• collections.abc.
Mapping
• collections.abc.
MutableMapping
• collections.abc.
Sequence
• collections.abc.
MutableSequence
• collections.abc.
ByteString
• collections.abc.
MappingView
• collections.abc.
KeysView
• collections.abc.
ItemsView
• collections.abc.
ValuesView
• contextlib.
AbstractContextManager
• contextlib.
AbstractAsyncContextManager
• dataclasses.
Field
• functools.cached_property
• functools.partialmethod
• os.
PathLike
• queue.
LifoQueue
• queue.
Queue
• queue.
PriorityQueue
• queue.
SimpleQueue
• re.
DbfilenameShelf
• shelve.
Shelf
• types.
MappingProxyType
• weakref.
WeakKeyDictionary
• weakref.
WeakMethod
• weakref.
WeakSet
• weakref.
WeakValueDictionary
Special Attributes of GenericAlias objects
All parameterized generics implement special read-only attributes.
Note:
A GenericAlias object with typing.
ParamSpec is intended primarily for static type
checking.
A boolean that is true if the alias has been unpacked using the * operator (see TypeVarTuple).
New in version 3.11.
See also:
PEP 484 - Type Hints Introducing Python’s framework for type annotations.
PEP 585 - Type Hinting Generics In Standard Collections Introducing the ability to natively parameterize standard-
library classes, provided they implement the special class method __class_getitem__().
4.13.
Generic Documentation on how to implement generic classes that can
be parameterized at runtime and understood by static type-checkers.
New in version 3.9.
These types are intended primarily
for type annotations.
The union type expression enables cleaner type hinting syntax compared to typing.
Union.
X | Y | ...
Defines a union object which holds types X, Y, and so forth.
X | Y means either X or Y.
It is equivalent to
typing.
Union[X, Y].
For example, the following function expects an argument of type int or float:
def square(number: int | float) -> int | float:

Note:
The | operand cannot be used at runtime to define unions where one or more members is a forward
reference.
For example, int | "Foo", where "Foo" is a reference to a class not yet defined, will fail at
runtime.
For unions which include forward references, present the whole expression as a string, e.g. "int |
Foo".
It is compatible with typing.
Union:
int | str == typing.
Union[int, str]
• Optional types can be spelled as a union with None:
str | None == typing.
TypeError: isinstance() argument 2 cannot be a parameterized generic
The user-exposed type for the union object can be accessed from types.
UnionType and used for isinstance()
checks.
True
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: cannot create 'types.
UnionType' instances
Note:
The __or__() method for type objects was added to support the syntax X | Y.
New in version 3.10.
The interpreter supports several other kinds of objects.
Most of these support only one or two operations.
Module attributes can be assigned to.
A special attribute of every module is __dict__.
This is the dictionary containing the module’s symbol table.
Modifying __dict__ directly is not recommended.
Modules built into the interpreter are written like this: <module 'sys' (built-in)>.
If loaded from a file, they
are written as <module 'os' from '/usr/local/lib/pythonX.Y/os.pyc'>.
The only operation on a function object is to call it:
func(argument-list).
There are really two flavors of function objects: built-in functions and user-defined functions.
Both support the same
operation (to call the function), but the implementation is different, hence the different object types.
See function for more information.
There are two flavors: built-in methods (such as
append() on lists) and class instance methods.
Built-in methods are described with the types that support them.
If you access a method (a function defined in a class namespace) through an instance, you get a special object: a bound
method (also called instance method) object.
When called, it will add the self argument to the argument list.
Calling m(arg-1, arg-2, ..., arg-n) is completely
equivalent to calling m.__func__(m.__self__, arg-1, arg-2, ..., arg-n).
Like function objects, bound method objects support getting arbitrary attributes.
However, since method attributes are
actually stored on the underlying function object (meth.__func__), setting method attributes on bound methods is
disallowed.
Attempting to set an attribute on a method results in an AttributeError being raised.
File "<stdin>", line 1, in <module>
AttributeError: 'method' object has no attribute 'whoami'
'my name is method'
See types for more information.
They differ from function objects because they don’t contain a reference to their global execution environment.
Code objects are returned by the built-in compile() function and can be extracted from function objects through their
__code__ attribute.
See also the code module.
A code object can be executed or evaluated by passing it (instead of a source string) to the exec() or eval() built-in
functions.
See types for more information.
An object’s type is accessed by the built-in function type().
There are
no special operations on types.
The standard module types defines names for all standard built-in types.
Types are written like this: <class 'int'>.
It supports no special operations.
There is exactly
one null object, named None (a built-in name).
It is written as None.
The Ellipsis Object
This object is commonly used by slicing (see slicings).
It supports no special operations.
There is exactly one ellipsis
object, named Ellipsis (a built-in name).
It is written as Ellipsis or ....
4.14.
See comparisons for more information.
There is exactly one NotImplemented object.
It is written as NotImplemented.
It describes stack frame objects, traceback objects, and slice objects.
Some of these
are not reported by the dir() built-in function.
The class to which a class instance belongs.
The tuple of base classes of a class object.
The qualified name of the class, function, method, descriptor, or generator instance.
New in version 3.3.
The type parameters of generic classes, functions, and type aliases.
New in version 3.12.
This attribute is a tuple of classes that are considered when looking for base classes during method resolution.
This method can be overridden by a metaclass to customize the method resolution order for its instances.
It is called
at class instantiation, and its result is stored in __mro__.
This method returns a list of all those
references still alive.
The list is in definition order.
This limit only
applies to decimal or other non-power-of-two number bases.
Hexadecimal, octal, and binary conversions are unlimited.
The limit can be configured.
The int type in CPython is an arbitrary length number stored in binary form (commonly known as a “bignum”).
There
exists no algorithm that can convert a string to a binary integer or a binary integer to a string in linear time, unless the base
is a power of 2.
Even the best known algorithms for base 10 have sub-quadratic complexity.
Converting a large value
such as int('1' * 500_000) can take over a second on a fast CPU.
Limiting conversion size offers a practical way to avoid CVE-2020-10735.
The limit is applied to the number of digit characters in the input or output string when a non-linear conversion algorithm
would be involved.
Underscores and the sign are not counted towards the limit.
When an operation would exceed the limit, a ValueError is raised:
# Illustrative, this is the default.
Exceeds the limit (4300 digits) for integer string conversion; use sys.
The default limit is 4300 digits as provided in sys.int_info.default_max_str_digits.
The lowest limit
that can be configured is 640 digits as provided in sys.int_info.str_digits_check_threshold.
New in version 3.11.
The limitation only applies to potentially slow conversions between int and str or bytes:
• int(string) with default base 10.
• int(string, base) for all bases that are not a power of 2.
• str(integer).
The limitations do not apply to functions with a linear algorithm:
• int(string, base) with base 2, 4, 8, 16, or 32.
• int.from_bytes() and int.to_bytes().
Decimal.
If both the env var and the -X option are set, the -X option takes precedence.
A
value of -1 indicates that both were unset, thus a value of sys.int_info.default_max_str_digits
was used during initialization.
From code, you can inspect the current limit and set a new one using these sys APIs:
• sys.get_int_max_str_digits() and sys.set_int_max_str_digits() are a getter and setter
for the interpreter-wide limit.
Subinterpreters have their own limit.
Information about the default and minimum can be found in sys.int_info:
• sys.int_info.default_max_str_digits is the compiled-in default limit.
New in version 3.11.
Caution: Setting a low limit can lead to problems.
While rare, code exists that contains integer constants in decimal
in their source that exceed the minimum threshold.
A consequence of setting the limit is that Python source code
containing decimal integer literals longer than the limit will encounter an error during parsing, usually at startup time
or import time or even at installation time - anytime an up to date .pyc does not already exist for the code.
A
workaround for source that contains such large constants is to convert them to 0x hexadecimal form as it has no limit.
Recommended configuration
The default sys.int_info.default_max_str_digits is expected to be reasonable for most applications.
If
your application requires a different limit, set it from your main entry point using Python version agnostic code as these
APIs were added in security patch releases in versions before 3.12.
If you need to disable it entirely, set it to 0.
4.16.
In Python, all exceptions must be instances of a class that derives from BaseException.
In a try statement with
an except clause that mentions a particular class, that clause also handles any exception classes derived from that class
(but not exception classes from which it is derived).
Two exception classes that are not related via subclassing are never
equivalent, even if they have the same name.
The built-in exceptions listed below can be generated by the interpreter or built-in functions.
Except where mentioned,
they have an “associated value” indicating the detailed cause of the error.
This may be a string or a tuple of several items
of information (e.g., an error code and a string explaining the code).
The associated value is usually passed as arguments
to the exception class’s constructor.
User code can raise built-in exceptions.
This can be used to test an exception handler or to report an error condition “just
like” the situation in which the interpreter raises the same exception; but beware that there is nothing to prevent user code
from raising an inappropriate error.
The built-in exception classes can be subclassed to define new exceptions; programmers are encouraged to derive new
exceptions from the Exception class or one of its subclasses, and not from BaseException.
More information on
defining exceptions is available in the Python Tutorial under tut-userexceptions.
An exception may be handled when an except or finally
clause, or a with statement, is used.
This implicit exception context can be supplemented with an explicit cause by using from with raise:
raise new_exc from original_exc
The expression following from must be an exception or None.
The default traceback display code shows these chained exceptions in addition to the traceback for the exception itself.
An explicitly chained exception in __cause__ is always shown when present.
In either case, the exception itself is always shown after any chained exceptions so that the final line of the traceback
always shows the last exception that was raised.
It’s recommended to only subclass one exception
type at a time to avoid any possible conflicts between how the bases handle the args attribute, as well as due to possible
memory layout incompatibilities.
CPython implementation detail:
Most built-in exceptions are implemented in C for efficiency, see:
Ob-
jects/exceptions.c.
Some have custom memory layouts which makes it impossible to create a subclass that inherits from
multiple exception types.
The memory layout of a type is an implementation detail and might change between Python
versions, leading to new conflicts in the future.
Therefore, it’s recommended to avoid subclassing multiple exception types
altogether.
The base class for all built-in exceptions.
It is not meant to be directly inherited by user-defined classes (for that,
use Exception).
If str() is called on an instance of this class, the representation of the argument(s) to the
instance are returned, or the empty string when there were no arguments.
Some built-in exceptions (like OSError) expect
a certain number of arguments and assign a special meaning to the elements of this tuple, while others are
usually called only with a single string giving an error message.
This method sets tb as the new traceback for the exception and returns the exception object.
It was more
commonly used before the exception chaining features of PEP 3134 became available.
The following exam-
ple shows how we can convert an instance of SomeException into an instance of OtherException
while preserving the traceback.
Once raised, the current frame is pushed onto the traceback of the
OtherException, as would have happened to the traceback of the original SomeException had we
allowed it to propagate to the caller.
Add the string note to the exception’s notes which appear in the standard traceback after the exception
string.
A TypeError is raised if note is not a string.
New in version 3.11.
A list of the notes of this exception, which were added with add_note().
This attribute is created when
add_note() is called.
New in version 3.11.
All user-defined exceptions should also be
derived from this class.
The base class for those built-in exceptions that are raised for various arithmetic errors: OverflowError,
ZeroDivisionError, FloatingPointError.
exception BufferError
Raised when a buffer related operation cannot be performed.
The base class for the exceptions that are raised when a key or index used on a mapping or sequence is invalid:
IndexError, KeyError.
This can be raised directly by codecs.lookup().
The name and obj attributes can be set using keyword-only arguments to the constructor.
When set they represent
the name of the attribute that was attempted to be accessed and the object that was accessed for said attribute,
respectively.
Changed in version 3.10: Added the name and obj attributes.
It
directly inherits from BaseException instead of Exception since it is technically not an error.
Also raised when the “from list” in from
... import has a name that cannot be found.
The optional name and path keyword-only arguments set the corresponding attributes:
name
The name of the module that was attempted to be imported.
Added the name and path attributes.
It is also raised
when None is found in sys.modules.
New in version 3.6.
5.4.
During execution, a check for
interrupts is made regularly.
The exception inherits from BaseException so as to not be accidentally caught
by code that catches Exception and thus prevent the interpreter from exiting.
Note:
Catching a KeyboardInterrupt requires special consideration.
Because it can be raised at unpre-
dictable points, it may, in some circumstances, leave the running program in an inconsistent state.
It is generally
best to allow KeyboardInterrupt to end the program as quickly as possible or avoid raising it entirely.
The associated value is a string indicating what kind of (internal) operation ran out of memory.
Note that because
of the underlying memory management architecture (C’s malloc() function), the interpreter may not always be
able to completely recover from this situation; it nevertheless raises an exception so that a stack traceback can be
printed, in case a run-away program was the cause.
This applies only to unqualified names.
The associated value is
an error message that includes the name that could not be found.
The name attribute can be set using a keyword-only argument to the constructor.
When set it represent the name
of the variable that was attempted to be accessed.
Added the name attribute.
This exception is derived from RuntimeError.
In user defined base classes, abstract methods should raise this
exception when they require derived classes to override the method, or while the class is being developed to indicate
that the real implementation still needs to be added.
Note: It should not be used to indicate that an operator or method is not meant to be supported at all – in that case
either leave the operator / method undefined or, if a subclass, set it to None.
Note:
NotImplementedError and NotImplemented are not interchangeable, even though they have
similar names and purposes.
See NotImplemented for details on when to use it.
This exception is raised when a system function returns a system-related error, including I/O failures such as “file
not found” or “disk full” (not for illegal argument types or other incidental errors).
The second form of the constructor sets the corresponding attributes, described below.
The attributes default to
None if not specified.
For backwards compatibility, if three arguments are passed, the args attribute contains
only a 2-tuple of the first two constructor arguments.
The constructor often actually returns a subclass of OSError, as described in OS exceptions below.
The particular
subclass depends on the final errno value.
This behaviour only occurs when constructing OSError directly or
via an alias, and is not inherited when subclassing.
The errno attribute is then an approximate
translation, in POSIX terms, of that native error code.
Under Windows, if the winerror constructor argument is an integer, the errno attribute is determined from
the Windows error code, and the errno argument is ignored.
On other platforms, the winerror argument is
ignored, and the winerror attribute does not exist.
It is formatted by the C functions
perror() under POSIX, and FormatMessage() under Windows.
For exceptions that involve a file system path (such as open() or os.unlink()), filename is the file
name passed to the function.
For functions that involve two file system paths (such as os.rename()),
filename2 corresponds to the second file name passed to the function.
EnvironmentError, IOError, WindowsError, socket.error, select.
error and mmap.error have been merged into OSError, and the constructor may return a subclass.
The filename attribute is now the original file name passed to the function, instead of
the name encoded to or decoded from the filesystem encoding and error handler.
Also, the filename2 constructor
argument and attribute was added.
This cannot occur for integers
(which would rather raise MemoryError than give up).
However, for historical reasons, OverflowError is some-
times raised for integers that are outside a required range.
Because of the lack of standardization of floating point
exception handling in C, most floating point operations are not checked.
It is raised when the interpreter detects that the maximum
recursion depth (see sys.getrecursionlimit()) is exceeded.
New in version 3.5: Previously, a plain RuntimeError was raised.
This exception is raised when a weak reference proxy, created by the weakref.proxy() function, is used to
access an attribute of the referent after it has been garbage collected.
For more information on weak references,
see the weakref module.
The associated value is a string
indicating what precisely went wrong.
The exception object has a single attribute value, which is given as an argument when constructing the exception,
and defaults to None.
5.4.
When a generator or coroutine function returns, a new StopIteration instance is raised, and the value returned
by the function is used as the value parameter to the constructor of the exception.
If a generator code directly or indirectly raises StopIteration, it is converted into a RuntimeError (re-
taining the StopIteration as the new exception’s cause).
Changed in version 3.3: Added value attribute and the ability for generator functions to use it to return a value.
Changed in version 3.7: Enable PEP 479 for all code by default: a StopIteration error raised in a generator
is transformed into a RuntimeError.
New in version 3.5.
exception SyntaxError(message, details)
Raised when the parser encounters a syntax error.
This may occur in an import statement, in a call to the
built-in functions compile(), exec(), or eval(), or when reading the initial script or standard input (also
interactively).
The str() of the exception instance returns only the error message.
Details is a tuple whose members are also
available as separate attributes.
This is 1-indexed: the first line in the file has a lineno
of 1.
offset
The column in the line where the error occurred.
This is 1-indexed: the first character in the line has an
offset of 1.
text
The source code text involved in the error.
This is 1-indexed: the first line in the file has a
lineno of 1.
end_offset
The column in the end line where the error occurred finishes.
This is 1-indexed: the first character in the line
has an offset of 1.
For errors in f-string fields, the message is prefixed by “f-string: ” and the offsets are offsets in a text constructed
from the replacement expression.
For example, compiling f’Bad {a b} field’ results in this args attribute: (‘f-string:
…’, (‘’, 1, 2, ‘(a b)n’, 1, 5)).
Changed in version 3.10: Added the end_lineno and end_offset attributes.
This is a subclass of SyntaxError.
This is a subclass of
IndentationError.
The associated value is a string indicating what went wrong (in low-level terms).
You should report this to the author or maintainer of your Python interpreter.
Be sure to report the version of the
Python interpreter (sys.version; it is also printed at the start of an interactive Python session), the exact error
message (the exception’s associated value) and if possible the source of the program that triggered the error.
It inherits from BaseException instead of
Exception so that it is not accidentally caught by code that catches Exception.
This allows the exception to
properly propagate up and cause the interpreter to exit.
When it is not handled, the Python interpreter exits; no
stack traceback is printed.
The constructor accepts the same optional argument passed to sys.exit().
If the
value is an integer, it specifies the system exit status (passed to C’s exit() function); if it is None, the exit status
is zero; if it has another type (such as a string), the object’s value is printed and the exit status is one.
A call to sys.exit() is translated into an exception so that clean-up handlers (finally clauses of try state-
ments) can be executed, and so that a debugger can execute a script without running the risk of losing control.
The
os._exit() function can be used if it is absolutely positively necessary to exit immediately (for example, in the
child process after a call to os.fork()).
The associated value is a string
giving details about the type mismatch.
This exception may be raised by user code to indicate that an attempted operation on an object is not supported, and
is not meant to be.
If an object is meant to support a given operation but has not yet provided an implementation,
NotImplementedError is the proper exception to raise.
Passing arguments of the wrong type (e.g.
passing a list when an int is expected) should result in a
TypeError, but passing arguments with the wrong value (e.g. a number outside expected boundaries) should
result in a ValueError.
This is a subclass of NameError.
It is a subclass of ValueError.
UnicodeError has attributes that describe the encoding or decoding error.
For example, err.object[err.
It is a subclass of UnicodeError.
It is a subclass of UnicodeError.
It is a subclass of UnicodeError.
The associated value is a string
indicating the type of the operands and the operation.
The following exceptions are kept for compatibility with previous versions; starting from Python 3.3, they are aliases of
OSError.
Corresponds to
errno EAGAIN, EALREADY, EWOULDBLOCK and EINPROGRESS.
In addition to those of OSError, BlockingIOError can have one more attribute:
characters_written
An integer containing the number of characters written to the stream before it blocked.
This attribute is
available when using the buffered I/O classes from the io module.
Corresponds to errno ECHILD.
Subclasses are BrokenPipeError, ConnectionAbortedError, ConnectionRefusedError and
ConnectionResetError.
Corresponds to errno EPIPE and ESHUTDOWN.
Corresponds to
errno ECONNABORTED.
Corresponds to
errno ECONNREFUSED.
Corresponds to errno
ECONNRESET.
Corresponds to errno EEXIST.
Corresponds to errno ENOENT.
Corresponds to errno EINTR.
Python now retries system calls when a syscall is interrupted by a signal, except if the
signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
Corresponds to errno
EISDIR.
On most POSIX platforms, it may also be raised if an operation attempts to open or traverse a non-directory file
as if it were a directory.
Corresponds to errno ENOTDIR.
Corresponds to errno EACCES, EPERM, and ENOTCAPABLE.
Changed in version 3.11.1: WASI’s ENOTCAPABLE is now mapped to PermissionError.
Corresponds to errno ESRCH.
Corresponds to errno ETIMEDOUT.
New in version 3.3: All the above OSError subclasses were added.
Reworking the OS and IO exception hierarchy
5.4.
Ignored by the default warning filters, except in the __main__ module (PEP 565).
Enabling the Python Devel-
opment Mode shows this warning.
The deprecation policy is described in PEP 387.
This class is rarely used as emitting a warning about a possible upcoming deprecation is unusual, and
DeprecationWarning is preferred for already active deprecations.
Ignored by the default warning filters.
Enabling the Python Development Mode shows this warning.
The deprecation policy is described in PEP 387.
Ignored by the default warning filters.
Enabling the Python Development Mode shows this warning.
See Opt-in EncodingWarning for details.
New in version 3.10.
Ignored by the default warning filters.
Enabling the Python Development Mode shows this warning.
New in version 3.2.
They are part of the exception hierarchy
so they can be handled with except like all other exceptions.
In addition, they are recognised by except*, which
matches their subgroups based on the types of the contained exceptions.
Both of these exception types wrap the exceptions in the sequence excs.
The msg parameter must be a string.
The
difference between the two classes is that BaseExceptionGroup extends BaseException and it can wrap
any exception, while ExceptionGroup extends Exception and it can only wrap subclasses of Exception.
This design is so that except Exception catches an ExceptionGroup but not BaseExceptionGroup.
The
BaseExceptionGroup
constructor
returns
an
ExceptionGroup
rather
than
a
BaseExceptionGroup if all contained exceptions are Exception instances, so it can be used to
make the selection automatic.
The ExceptionGroup constructor, on the other hand, raises a TypeError if
any contained exception is not an Exception subclass.
message
The msg argument to the constructor.
This is a read-only attribute.
None if the result is empty.
The condition can be either a function that accepts an exception and returns true for those that should be in
the subgroup, or it can be an exception type or a tuple of exception types, which is used to check for a match
using the same check that is used in an except clause.
The nesting structure of the current exception is preserved in the result, as are the values of its message,
__traceback__, __cause__, __context__ and __notes__ fields.
Empty nested groups are
omitted from the result.
The condition is checked for all exceptions in the nested exception group, including the top-level and any
nested exception groups.
If the condition is true for such an exception group, it is included in the result in
full.
split(condition)
Like subgroup(), but returns the pair (match, rest) where match is subgroup(condition)
and rest is the remaining non-matching part.
This method is used by subgroup() and split().
A subclass needs to override it in order to make
subgroup() and split() return instances of the subclass rather than ExceptionGroup.
For example, the following defines an exception group subclass
which accepts an exit_code and and constructs the group’s message from it.
The modules described in this chapter provide a wide range of string manipulation operations and other text processing
services.
The codecs module described under Binary Data Services is also highly relevant to text processing.
In addition, see the
documentation for Python’s built-in string type in Text Sequence Type — str.
The constants defined in this module are:
string.ascii_letters
The concatenation of the ascii_lowercase and ascii_uppercase constants described below.
This value
is not locale-dependent.
This value is not locale-dependent and will not
change.
This value is not locale-dependent and will not
change.
This is a combination of digits, ascii_letters,
punctuation, and whitespace.
This includes the characters space, tab,
linefeed, return, formfeed, and vertical tab.
The Formatter class in the string module allows you to create and customize
your own string formatting behaviors using the same implementation as the built-in format() method.
The primary API method.
It takes a format string and an arbitrary set of positional and keyword arguments.
It is just a wrapper that calls vformat().
A format string argument is now positional-only.
This function does the actual work of formatting.
It is exposed as a separate function for cases where you
want to pass in a predefined dictionary of arguments, rather than unpacking and repacking the dictionary as
individual arguments using the *args and **kwargs syntax.
It calls the various methods described below.
In addition, the Formatter defines a number of methods that are intended to be replaced by subclasses:
parse(format_string)
Loop over the format_string and return an iterable of tuples (literal_text, field_name, format_spec, conversion).
This is used by vformat() to break the string into either literal text, or replacement fields.
The values in the tuple conceptually represent a span of literal text followed by a single replacement field.
If there is no literal text (which can happen if two replacement fields occur consecutively), then literal_text
will be a zero-length string.
If there is no replacement field, then the values of field_name, format_spec and
conversion will be None.
get_field(field_name, args, kwargs)
Given field_name as returned by parse() (see above), convert it to an object to be formatted.
Returns a
tuple (obj, used_key).
The default version takes strings of the form defined in PEP 3101, such as “0[name]”
or “label.title”.
Retrieve a given field value.
The key argument will be either an integer or a string.
If it is an integer, it
represents the index of the positional argument in args; if it is a string, then it represents a named argument
in kwargs.
The args parameter is set to the list of positional arguments to vformat(), and the kwargs parameter is set
to the dictionary of keyword arguments.
So for example, the field expression ‘0.name’ would cause get_value() to be called with a key argument
of 0.
The name attribute will be looked up after get_value() returns by calling the built-in getattr()
function.
If the index or keyword refers to an item that does not exist, then an IndexError or KeyError should
be raised.
Implement checking for unused arguments if desired.
The arguments to this function is the set of all argument
keys that were actually referred to in the format string (integers for positional arguments, and strings for named
arguments), and a reference to the args and kwargs that was passed to vformat.
The set of unused args can be
calculated from these parameters.
The method is provided so that subclasses
can override it.
The default version understands ‘s’ (str), ‘r’ (repr) and ‘a’ (ascii) conversion types.
The syntax is related to that of formatted string
literals, but it is less sophisticated and, in particular, does not support arbitrary expressions.
Format strings contain “replacement fields” surrounded by curly braces {}.
Anything that is not contained in braces is
considered literal text, which is copied unchanged to the output.
If you need to include a brace character in the literal
text, it can be escaped by doubling: {{ and }}.
In less formal terms, the replacement field can start with a field_name that specifies the object whose value is to be
formatted and inserted into the output instead of the replacement field.
The field_name is optionally followed by a
conversion field, which is preceded by an exclamation point '!', and a format_spec, which is preceded by a colon ':'.
These specify a non-default format for the replacement value.
See also the Format Specification Mini-Language section.
The field_name itself begins with an arg_name that is either a number or a keyword.
If it’s a number, it refers to a
positional argument, and if it’s a keyword, it refers to a named keyword argument.
An arg_name is treated as a number if
a call to str.isdecimal() on the string would return true.
Because arg_name is not quote-delimited, it is not possible to specify arbitrary dictionary keys (e.g., the strings
'10' or ':-]') within a format string.
The arg_name can be followed by any number of index or attribute expressions.
An expression of the form '.name' selects the named attribute using getattr(), while an expression of the form
'[index]' does an index lookup using __getitem__().
Changed in version 3.1: The positional argument specifiers can be omitted for str.format(), so '{} {}'.
The positional argument specifiers can be omitted for Formatter.
Some simple format string examples:
"First, thou shalt count to {0}"
# References first positional argument
"Bring me a {}"
# Implicitly references the first positional␣
�→argument
"From {} to {}"
# Same as "From {0} to {1}"
"My quest is {name}"
# References keyword argument 'name'
"Weight in tons {0.weight}"
# 'weight' attribute of first positional arg
"Units destroyed: {players[0]}"
# First element of keyword argument 'players'.
The conversion field causes a type coercion before formatting.
Normally, the job of formatting a value is done by the
__format__() method of the value itself.
However, in some cases it is desirable to force a type to be formatted as a
string, overriding its own definition of formatting.
By converting the value to a string before calling __format__(),
the normal formatting logic is bypassed.
Three conversion flags are currently supported: '!s' which calls str() on the value, '!r' which calls repr() and
'!
The format_spec field contains a specification of how the value should be presented, including such details as field width,
alignment, padding, decimal precision and so on.
Each value type can define its own “formatting mini-language” or
interpretation of the format_spec.
Most built-in types support a common formatting mini-language, which is described in the next section.
A format_spec field can also include nested replacement fields within it.
These nested replacement fields may contain
a field name, conversion flag and format specification, but deeper nesting is not allowed.
The replacement fields within
the format_spec are substituted before the format_spec string is interpreted.
This allows the formatting of a value to be
dynamically specified.
See the Format examples section for some examples.
Format Specification Mini-Language
“Format specifications” are used within replacement fields contained within a format string to define how individual
values are presented (see Format String Syntax and f-strings).
They can also be passed directly to the built-in format()
function.
Each formattable type may define how the format specification is to be interpreted.
Most built-in types implement the following options for format specifications, although some of the formatting options
are only supported by the numeric types.
A general convention is that an empty format specification produces the same result as if you had called str() on the
value.
It is not possible to use a literal curly brace (”{” or “}”) as the fill character in a formatted string literal or when
using the str.format() method.
However, it is possible to insert a curly brace with a nested replacement field.
This
limitation doesn’t affect the format() function.
The meaning of the various alignment options is as follows:
Op-
tion
Meaning
'<'
Forces the field to be left-aligned within the available space (this is the default for most objects).
This is used for printing fields in the
form ‘+000000120’.
This alignment option is only valid for numeric types.
It becomes the default for numbers
when ‘0’ immediately precedes the field width.
Note that unless a minimum field width is defined, the field width will always be the same size as the data to fill it, so that
the alignment option has no meaning in this case.
The sign option is only valid for number types, and can be one of the following:
Option
Meaning
'+'
indicates that a sign should be used for both positive as well as negative numbers.
The 'z' option coerces negative zero floating-point values to positive zero after rounding to the format precision.
This
option is only valid for floating-point presentation types.
Changed in version 3.11: Added the 'z' option (see also PEP 682).
The '#' option causes the “alternate form” to be used for the conversion.
The alternate form is defined differently
for different types.
This option is only valid for integer, float and complex types.
For integers, when binary, octal, or
hexadecimal output is used, this option adds the respective prefix '0b', '0o', '0x', or '0X' to the output value.
For
float and complex the alternate form causes the result of the conversion to always contain a decimal-point character, even
if no digits follow it.
Normally, a decimal-point character appears in the result of these conversions only if a digit follows
it.
In addition, for 'g' and 'G' conversions, trailing zeros are not removed from the result.
The ',' option signals the use of a comma for a thousands separator.
For a locale aware separator, use the 'n' integer
presentation type instead.
Changed in version 3.1: Added the ',' option (see also PEP 378).
For integer presentation types 'b', 'o', 'x', and 'X', underscores will be inserted
every 4 digits.
For other presentation types, specifying this option is an error.
Added the '_' option (see also PEP 515).
If not specified, then the field width will be determined by the content.
When no explicit alignment is given, preceding the width field by a zero ('0') character enables sign-aware zero-padding
for numeric types.
This is equivalent to a fill character of '0' with an alignment type of '='.
Preceding the width field by '0' no longer affects the default alignment for strings.
The precision is a decimal integer indicating how many digits should be displayed after the decimal point for presentation
types 'f' and 'F', or before and after the decimal point for presentation types 'g' or 'G'.
For string presentation
types the field indicates the maximum field size - in other words, how many characters will be used from the field content.
The precision is not allowed for integer presentation types.
Finally, the type determines how the data should be presented.
The available string presentation types are:
Type
Meaning
's'
String format.
This is the default type for strings and may be omitted.
None
The same as 's'.
The available integer presentation types are:
Type Meaning
'b'
Binary format.
Outputs the number in base 2.
'c'
Character.
Converts the integer to the corresponding unicode character before printing.
Outputs the number in base 10.
Outputs the number in base 8.
'x'
Hex format.
Outputs the number in base 16, using lower-case letters for the digits above 9.
'X'
Hex format.
Outputs the number in base 16, using upper-case letters for the digits above 9.
In
case '#' is specified, the prefix '0x' will be upper-cased to '0X' as well.
This is the same as 'd', except that it uses the current locale setting to insert the
appropriate number separator characters.
None The same as 'd'.
In addition to the above presentation types, integers can be formatted with the floating point presentation types listed
below (except 'n' and None).
When doing so, float() is used to convert the integer to a floating point number
before formatting.
For a given precision p, formats the number in scientific notation with the
letter ‘e’ separating the coefficient from the exponent.
The coefficient has one digit before and p
digits after the decimal point, for a total of p + 1 significant digits.
With no precision given,
uses a precision of 6 digits after the decimal point for float, and shows all coefficient digits for
Decimal.
If no digits follow the decimal point, the decimal point is also removed unless the #
option is used.
Same as 'e' except it uses an upper case ‘E’ as the separator character.
For a given precision p, formats the number as a decimal number with
exactly p digits following the decimal point.
With no precision given, uses a precision of 6 digits
after the decimal point for float, and uses a precision large enough to show all coefficient digits
for Decimal.
If no digits follow the decimal point, the decimal point is also removed unless the
# option is used.
Same as 'f', but converts nan to NAN and inf to INF.
For a given precision p >= 1, this rounds the number to p significant digits
and then formats the result in either fixed-point format or in scientific notation, depending on its
magnitude.
A precision of 0 is treated as equivalent to a precision of 1.
The precise rules are as follows: suppose that the result formatted with presentation type 'e' and
precision p-1 would have exponent exp.
Then, if m <= exp < p, where m is -4 for floats and
-6 for Decimals, the number is formatted with presentation type 'f' and precision p-1-exp.
Otherwise, the number is formatted with presentation type 'e' and precision p-1.
In both cases
insignificant trailing zeros are removed from the significand, and the decimal point is also removed
if there are no remaining digits following it, unless the '#' option is used.
With no precision given, uses a precision of 6 significant digits for float.
For Decimal, the
coefficient of the result is formed from the coefficient digits of the value; scientific notation is
used for values smaller than 1e-6 in absolute value and values where the place value of the least
significant digit is larger than 1, and fixed-point notation is used otherwise.
Same as 'g' except switches to 'E' if the number gets too large.
The repre-
sentations of infinity and NaN are uppercased, too.
This is the same as 'g', except that it uses the current locale setting to insert the
appropriate number separator characters.
Multiplies the number by 100 and displays in fixed ('f') format, followed by a
percent sign.
None For float this is the same as 'g', except that when fixed-point notation is used to format the
result, it always includes at least one digit past the decimal point.
The precision used is as large as
needed to represent the given value faithfully.
For Decimal, this is the same as either 'g' or 'G' depending on the value of context.
The overall effect is to match the output of str() as altered by the other format modifiers.
Format examples
This section contains examples of the str.format() syntax and comparison with the old %-formatting.
In most of the cases the syntax is similar to the old %-formatting, with the addition of the {} and with : used instead of
%.
For example, '%03.2f' can be translated to '{:03.2f}'.
The new format syntax also supports new and different options, shown in the following examples.
A primary use case for template strings is
for internationalization (i18n) since in that context, the simpler syntax and functionality makes it easier to translate than
other built-in string formatting facilities in Python.
As an example of a library built on template strings for i18n, see the
flufl.i18n package.
Template strings support $-based substitutions, using the following rules:
• $$ is an escape; it is replaced with a single $.
• $identifier names a substitution placeholder matching a mapping key of "identifier".
By default,
"identifier" is restricted to any case-insensitive ASCII alphanumeric string (including underscores) that starts
with an underscore or ASCII letter.
The first non-identifier character after the $ character terminates this place-
holder specification.
It is required when valid identifier characters follow the
placeholder but are not part of the placeholder, such as "${noun}ification".
Any other appearance of $ in the string will result in a ValueError being raised.
The string module provides a Template class that implements these rules.
The methods of Template are:
class string.
The constructor takes a single argument which is the template string.
substitute(mapping={}, /, **kwds)
Performs the template substitution, returning a new string.
Alternatively, you can provide keyword arguments, where the
keywords are the placeholders.
When both mapping and kwds are given and there are duplicates, the place-
holders from kwds take precedence.
Also, unlike with
substitute(), any other appearances of the $ will simply return $ instead of raising ValueError.
While other exceptions may still occur, this method is called “safe” because it always tries to return a usable
string instead of raising an exception.
In another sense, safe_substitute() may be anything other than
safe, since it will silently ignore malformed templates containing dangling delimiters, unmatched braces, or
placeholders that are not valid Python identifiers.
Returns false if the template has invalid placeholders that will cause substitute() to raise
ValueError.
New in version 3.11.
Template instances also provide one public data attribute:
template
This is the object passed to the constructor’s template argument.
In general, you shouldn’t change it, but
read-only access is not enforced.
ValueError: Invalid placeholder in string: line 1, col 11
Traceback (most recent call last):

KeyError: 'what'
'tim likes $what'
Advanced usage: you can derive subclasses of Template to customize the placeholder syntax, delimiter character, or
the entire regular expression used to parse template strings.
To do this, you can override these class attributes:
• delimiter – This is the literal string describing a placeholder introducing delimiter.
The default value is $.
Note that
this should not be a regular expression, as the implementation will call re.escape() on this string as needed.
Note further that you cannot change the delimiter after class creation (i.e. a different delimiter must be set in the
subclass’s class namespace).
This is the regular expression describing the pattern for non-braced placeholders.
The default value is
the regular expression (?a:[_a-z][_a-z0-9]*).
If this is given and braceidpattern is None this pattern will
also apply to braced placeholders.
Note:
Since default flags is re.IGNORECASE, pattern [a-z] can match with some non-ASCII characters.
That’s why we use the local a flag here.
Changed in version 3.7: braceidpattern can be used to define separate patterns used inside and outside the braces.
Defaults to None which
means to fall back to idpattern (i.e. the same pattern is used both inside and outside braces).
If given, this allows
you to define different patterns for braced and unbraced placeholders.
New in version 3.7.
• flags – The regular expression flags that will be applied when compiling the regular expression used for recognizing
substitutions.
The default value is re.IGNORECASE.
Note that re.VERBOSE will always be added to the flags,
so custom idpatterns must follow conventions for verbose regular expressions.
New in version 3.2.
If you do
this, the value must be a regular expression object with four named capturing groups.
The capturing groups correspond
to the rules given above, along with the invalid placeholder rule:
• escaped – This group matches the escape sequence, e.g. $$, in the default pattern.
• named – This group matches the unbraced placeholder name; it should not include the delimiter in capturing group.
The methods on this class will raise ValueError if the pattern matches the template without one of these named groups
matching.
If the optional second argument sep is absent or None, runs of
whitespace characters are replaced by a single space and leading and trailing whitespace are removed, otherwise
sep is used to split and join the words.
Both patterns and strings to be searched can be Unicode strings (str) as well as 8-bit strings (bytes).
However, Unicode
strings and 8-bit strings cannot be mixed: that is, you cannot match a Unicode string with a byte pattern or vice-versa;
similarly, when asking for a substitution, the replacement string must be of the same type as both the pattern and the
search string.
Regular expressions use the backslash character ('\') to indicate special forms or to allow special characters to be used
without invoking their special meaning.
This collides with Python’s usage of the same character for the same purpose in
string literals; for example, to match a literal backslash, one might have to write '\\\\' as the pattern string, because
the regular expression must be \\, and each backslash must be expressed as \\ inside a regular Python string literal.
Also, please note that any invalid escape sequences in Python’s usage of the backslash in string literals now generate a
SyntaxWarning and in the future this will become a SyntaxError.
This behaviour will happen even if it is a valid
escape sequence for a regular expression.
The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any
special way in a string literal prefixed with 'r'.
So r"\n" is a two-character string containing '\' and 'n', while
"\n" is a one-character string containing a newline.
Usually patterns will be expressed in Python code using this raw
string notation.
It is important to note that most regular expression operations are available as module-level functions and methods on
compiled regular expressions.
The functions are shortcuts that don’t require you to compile a regex object first, but miss
some fine-tuning parameters.
Regular expressions can be concatenated to form new regular expressions; if A and B are both regular expressions, then
AB is also a regular expression.
In general, if a string p matches A and another string q matches B, the string pq will
match AB.
This holds unless A or B contain low precedence operations; boundary conditions between A and B; or have
numbered group references.
Thus, complex expressions can easily be constructed from simpler primitive expressions like
the ones described here.
For details of the theory and implementation of regular expressions, consult the Friedl book
[Frie09], or almost any textbook about compiler construction.
A brief explanation of the format of regular expressions follows.
For further information and a gentler presentation,
consult the regex-howto.
Regular expressions can contain both special and ordinary characters.
Most ordinary characters, like 'A', 'a', or '0',
are the simplest regular expressions; they simply match themselves.
You can concatenate ordinary characters, so last
matches the string 'last'.
Some characters, like '|' or '(', are special.
Special characters either stand for classes of ordinary characters, or affect
how the regular expressions around them are interpreted.
Repetition operators or quantifiers (*, +, ?, {m,n}, etc) cannot be directly nested.
This avoids ambiguity with the
non-greedy modifier suffix ?, and with other modifiers in other implementations.
To apply a second repetition to an
inner repetition, parentheses may be used.
For example, the expression (?:a{6})* matches any multiple of six 'a'
characters.
The special characters are:
.
In the default mode, this matches any character except a newline.
If the DOTALL flag has been specified, this
matches any character including a newline.
Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.
More
interestingly, searching for foo.$ in 'foo1\nfoo2\n' matches ‘foo2’ normally, but ‘foo1’ in MULTILINE
mode; searching for a single $ in 'foo\n' will find two (empty) matches: one just before the newline, and one
at the end of the string.
Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab?
Sometimes this
behaviour isn’t desired; if the RE <.
Adding ?
Using the RE <.*?> will match only '<a>'.
However, unlike the true greedy quantifiers, these do not allow back-tracking when the expression following it fails
to match.
These are known as possessive quantifiers.
For example, a*a will match 'aaaa' because the a* will
match all 4 'a's, but, when the final 'a' is encountered, the expression is backtracked so that in the end the a*
ends up matching 3 'a's total, and the fourth 'a' is matched by the final 'a'.
However, when a*+a is used to
match 'aaaa', the a*+ will match all 4 'a', but when the final 'a' fails to find any more characters to match,
the expression cannot be backtracked and will thus fail to match.
New in version 3.11.
{m} Specifies that exactly m copies of the previous RE should be matched; fewer matches cause the entire RE not to
match.
For example, a{6} will match exactly six 'a' characters, but not five.
{m,n} Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many
repetitions as possible.
For example, a{3,5} will match from 3 to 5 'a' characters.
Omitting m specifies a lower
bound of zero, and omitting n specifies an infinite upper bound.
As an example, a{4,}b will match 'aaaab' or
a thousand 'a' characters followed by a 'b', but not 'aaab'.
The comma may not be omitted or the modifier
would be confused with the previously described form.
Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as few
repetitions as possible.
This is the non-greedy version of the previous quantifier.
For example, on the 6-character
string 'aaaaaa', a{3,5} will match 5 'a' characters, while a{3,5}?
This is the possessive version of the quantifier
above.
For example, on the 6-character string 'aaaaaa', a{3,5}+aa attempt to match 5 'a' characters, then,
requiring 2 more 'a's, will need more characters than available and thus fail, while a{3,5}aa will match with
a{3,5} capturing 5, then 4 'a's by backtracking and then the final 2 'a's are matched by the final aa in the
pattern.
New in version 3.11.
Either escapes special characters (permitting you to match characters like '*', '?', and so forth), or signals a special
sequence; special sequences are discussed below.
If you’re not using a raw string to express the pattern, remember that Python also uses the backslash as an escape
sequence in string literals; if the escape sequence isn’t recognized by Python’s parser, the backslash and subsequent
character are included in the resulting string.
However, if Python would recognize the resulting sequence, the
backslash should be repeated twice.
This is complicated and hard to understand, so it’s highly recommended that
you use raw strings for all but the simplest expressions.
In a set:
• Characters can be listed individually, e.g.
If - is escaped (e.g. [a\-z]) or if it’s
placed as the first or last character (e.g. [-a] or [a-]), it will match a literal '-'.
For example, [(+*)] will match any of the literal
characters '(', '+', '*', or ')'.
If the first character of the
set is '^', all the characters that are not in the set will be matched.
For example, [^5] will match any
character except '5', and [^^] will match any character except '^'.
For
example, both [()[\]{}] and []()[{}] will match a right bracket, as well as left bracket, braces, and
parentheses.
This would change the syntax, so to facilitate this change a FutureWarning will be raised in ambiguous
cases for the time being.
That includes sets starting with a literal '[' or containing literal character sequences
'--', '&&', '~~', and '||'.
To avoid a warning escape them with a backslash.
FutureWarning is raised if a character set contains constructs that will change seman-
tically in the future.
This can be used inside groups (see below) as well.
As the
target string is scanned, REs separated by '|' are tried from left to right.
When one pattern completely matches,
that branch is accepted.
This means that once A matches, B will not be tested further, even if it would produce a
longer overall match.
In other words, the '|' operator is never greedy.
To match a literal '|', use \|, or enclose
it inside a character class, as in [|].
The first character after
the '?' determines what the meaning and further syntax of the construct is.
Following are the currently supported extensions.
The group matches the empty
string; the letters set the corresponding flags: re.
A (ASCII-only matching), re.I (ignore case), re.L (locale
dependent), re.M (multi-line), re.S (dot matches all), re.U (Unicode matching), and re.X (verbose), for the
entire regular expression.
This is useful if you wish to include the
flags as part of the regular expression, instead of passing a flag argument to the re.compile() function.
Flags
should be used first in the expression string.
This construction can only be used at the start of the expression.
A non-capturing version of regular parentheses.
Matches whatever regular expression is inside the parenthe-
ses, but the substring matched by the group cannot be retrieved after performing a match or referenced later in the
pattern.
The letters set or remove the
corresponding flags: re.
A (ASCII-only matching), re.I (ignore case), re.L (locale dependent), re.M (multi-
line), re.S (dot matches all), re.U (Unicode matching), and re.X (verbose), for the part of the expression.
The letters 'a', 'L' and 'u' are mutually exclusive when used as inline flags, so they can’t be combined or follow
'-'.
Instead, when one of them appears in an inline group, it overrides the matching mode in the enclosing group.
In Unicode patterns (?a:...) switches to ASCII-only matching, and (?u:...) switches to Unicode matching
(default).
In byte pattern (?L:...) switches to locale depending matching, and (?a:...) switches to ASCII-
only matching (default).
This override is only in effect for the narrow inline group, and the original matching mode
is restored outside of the group.
New in version 3.6.
The letters 'a', 'L' and 'u' also can be used in a group.
Attempts to match ...
If the subsequent pattern fails to match, the stack can only be unwound to a point
6.2.
Thus, (?>.*). would never match anything because first the .
Since there are no stack points saved
in the Atomic Group, and there is no stack point before it, the entire expression would thus fail to match.
Similar to regular parentheses, but the substring matched by the group is accessible via the symbolic
group name name.
Group names must be valid Python identifiers, and in bytes patterns they can only contain
bytes in the ASCII range.
Each group name must be defined only once within a regular expression.
A symbolic
group is also a numbered group, just as if the group were not named.
Named groups can be referenced in three contexts.
If the pattern is (?P<quote>['"]).*?(?P=quote)
(i.e. matching a string quoted with either single or double quotes):
Context of reference to group “quote”
Ways to reference it
in the same pattern itself
• (?P=quote) (as shown)
• \1
when processing match object m
• m.group('quote')
• m.end('quote') (etc.)
in a string passed to the repl argument of re.sub()
• \g<quote>
• \g<1>
• \1
Changed in version 3.12: In bytes patterns, group name can only contain bytes in the ASCII range (b'\x00'-
b'\x7f').
Matches if ... matches next, but doesn’t consume any of the string.
This is called a lookahead assertion.
Matches if ... doesn’t match next.
This is a negative lookahead assertion.
Matches if the current position in the string is preceded by a match for ... that ends at the current po-
sition.
This is called a positive lookbehind assertion.
The contained pattern must only
match strings of some fixed length, meaning that abc or a|b are allowed, but a* and a{3,4} are not.
Matches if the current position in the string is not preceded by a match for ....
This is called a negative
lookbehind assertion.
Similar to positive lookbehind assertions, the contained pattern must only match strings of
some fixed length.
Patterns which start with negative lookbehind assertions may match at the beginning of the
string being searched.
For
example, (<)?(\w+@\w+(?:\.\w+)+)(?(1)>|$) is a poor email matching pattern, which will match
with '<user@host.com>' as well as 'user@host.com', but not with '<user@host.com' nor
'user@host.com>'.
Group id can only contain ASCII digits.
In bytes patterns, group name can only contain
bytes in the ASCII range (b'\x00'-b'\x7f').
The special sequences consist of '\' and a character from the list below.
If the ordinary character is not an ASCII digit
or an ASCII letter, then the resulting RE will match the second character.
For example, \$ matches the character '$'.
Groups are numbered starting from 1.
This special
sequence can only be used to match one of the first 99 groups.
If the first digit of number is 0, or number is 3 octal
digits long, it will not be interpreted as a group match, but as the character with octal value number.
Inside the '['
and ']' of a character class, all numeric escapes are treated as characters.
A word is defined as a sequence of word
characters.
Note that formally, \b is defined as the boundary between a \w and a \W character (or vice versa),
or between \w and the beginning/end of the string.
This means that r'\bfoo\b' matches 'foo', 'foo.',
'(foo)', 'bar foo baz' but not 'foobar' or 'foo3'.
By default Unicode alphanumerics are the ones used in Unicode patterns, but this can be changed by using the
ASCII flag.
Word boundaries are determined by the current locale if the LOCALE flag is used.
Inside a character
range, \b represents the backspace character, for compatibility with Python’s string literals.
This means that r'py\B'
matches 'python', 'py3', 'py2', but not 'py', 'py.', or 'py!'.
Word boundaries are determined by the current locale if the LOCALE flag is used.
This includes [0-9], and also many other digit characters.
If the ASCII flag is used only
[0-9] is matched.
For 8-bit (bytes) patterns: Matches any decimal digit; this is equivalent to [0-9].
This is the opposite of \d.
If the ASCII flag is used this becomes
the equivalent of [^0-9].
\s
For Unicode (str) patterns: Matches Unicode whitespace characters (which includes [ \t\n\r\f\v], and
also many other characters, for example the non-breaking spaces mandated by typography rules in many
languages).
If the ASCII flag is used, only [ \t\n\r\f\v] is matched.
This is the opposite of \s.
If the ASCII flag is used
this becomes the equivalent of [^ \t\n\r\f\v].
\w
For Unicode (str) patterns: Matches Unicode word characters; this includes alphanumeric characters (as defined
by str.isalnum()) as well as the underscore (_).
If the ASCII flag is used, only [a-zA-Z0-9_] is
matched.
For 8-bit (bytes) patterns: Matches characters considered alphanumeric in the ASCII character set; this is equiv-
alent to [a-zA-Z0-9_].
If the LOCALE flag is used, matches characters considered alphanumeric in the
current locale and the underscore.
This is the opposite of \w.
If the ASCII flag is used this
becomes the equivalent of [^a-zA-Z0-9_].
If the LOCALE flag is used, matches characters which are neither
alphanumeric in the current locale nor the underscore.
Most of the escape sequences supported by Python string literals are also accepted by the regular expression parser:
\a
\b
\f
\n
\N
\r
\t
\u
\U
\v
\x
\\
(Note that \b is used to represent word boundaries, and means “backspace” only inside character classes.)
'\u', '\U', and '\N' escape sequences are only recognized in Unicode patterns.
In bytes patterns they are errors.
Unknown escapes of ASCII letters are reserved for future use and treated as errors.
Octal escapes are included in a limited form.
If the first digit is a 0, or if there are three octal digits, it is considered an
octal escape.
Otherwise, it is a group reference.
As for string literals, octal escapes are always at most three digits in
length.
The '\u' and '\U' escape sequences have been added.
Unknown escapes consisting of '\' and an ASCII letter now are errors.
The '\N{name}' escape sequence has been added.
As in string literals, it expands to the
named Unicode character (e.g. '\N{EM DASH}').
Some of the functions are simplified versions of the
full featured methods for compiled regular expressions.
Most non-trivial applications always use the compiled form.
IntFlag.
IntFlag class containing the regex options listed below.
New in version 3.11: - added to __all__
re.
A
re.ASCII
Make \w, \W, \b, \B, \d, \D, \s and \S perform ASCII-only matching instead of full Unicode matching.
This
is only meaningful for Unicode patterns, and is ignored for byte patterns.
Corresponds to the inline flag (?a).
Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its
embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings
(and Unicode matching isn’t allowed for bytes).
No corresponding inline flag.
Full Unicode match-
ing (such as Ü matching ü) also works unless the re.ASCII flag is used to disable non-ASCII matches.
The
current locale does not change the effect of this flag unless the re.LOCALE flag is also used.
Corresponds to the
inline flag (?i).
Note that when the Unicode patterns [a-z] or [A-Z] are used in combination with the IGNORECASE flag,
they will match the 52 ASCII letters and 4 additional non-ASCII letters: ‘İ’ (U+0130, Latin capital letter I with dot
above), ‘ı’ (U+0131, Latin small letter dotless i), ‘ſ’ (U+017F, Latin small letter long s) and ‘K’ (U+212A, Kelvin
sign).
If the ASCII flag is used, only letters ‘a’ to ‘z’ and ‘A’ to ‘Z’ are matched.
This flag can be used only
with bytes patterns.
The use of this flag is discouraged as the locale mechanism is very unreliable, it only handles
one “culture” at a time, and it only works with 8-bit locales.
Unicode matching is already enabled by default in
Python 3 for Unicode (str) patterns, and it is able to handle different locales/languages.
Corresponds to the inline
flag (?L).
Changed in version 3.6: re.LOCALE can be used only with bytes patterns and is not compatible with re.ASCII.
Changed in version 3.7: Compiled regular expression objects with the re.LOCALE flag no longer depend on the
locale at compile time.
Only the locale at matching time affects the result of matching.
By default, '^' matches only at the beginning of the
string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string.
Corresponds to the inline flag (?m).
This flag may be used as a default value for a function keyword
argument or as a base value that will be conditionally ORed with other flags.
Example of use as a default value:
6.2.
New in version 3.11.
Corresponds to the inline flag (?s).
Since Python 3, Unicode
characters are matched by default.
See A for restricting matching on ASCII characters instead.
This flag allows you to write regular expressions that look nicer and are more readable by allowing you to visually
separate logical sections of the pattern and add comments.
Whitespace within the pattern is ignored, except when
in a character class, or when preceded by an unescaped backslash, or within tokens like *?, (?: or (?P<...>.
For example, (? : and * ? are not allowed.
When a line contains a # that is not in a character class and is not
preceded by an unescaped backslash, all characters from the leftmost such # through the end of the line are ignored.
Corresponds to the inline flag (?x).
Functions
re.compile(pattern, flags=0)
Compile a regular expression pattern into a regular expression object, which can be used for matching using its
match(), search() and other methods, described below.
The expression’s behaviour can be modified by specifying a flags value.
Values can be any of the following variables,
combined using bitwise OR (the | operator).
The sequence
prog = re.compile(pattern)
result = prog.match(string)
is equivalent to
result = re.match(pattern, string)
but using re.compile() and saving the resulting regular expression object for reuse is more efficient when the
expression will be used several times in a single program.
Return None if no position in the string matches the pattern; note that this is different
from finding a zero-length match at some point in the string.
If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding
Match.
Return None if the string does not match the pattern; note that this is different from a zero-length match.
Note that even in MULTILINE mode, re.match() will only match at the beginning of the string and not at the
beginning of each line.
If the whole string matches the regular expression pattern, return a corresponding Match.
Return None if the
string does not match the pattern; note that this is different from a zero-length match.
New in version 3.4.
If capturing parentheses are used in pattern, then the text of all groups in
the pattern are also returned as part of the resulting list.
If maxsplit is nonzero, at most maxsplit splits occur, and
the remainder of the string is returned as the final element of the list.
Empty matches for the pattern split the string only when not adjacent to a previous empty match.
Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.
The string is scanned left-to-
right, and matches are returned in the order found.
Empty matches are included in the result.
The result depends on the number of capturing groups in the pattern.
If there are no groups, return a list of strings
matching the whole pattern.
If there is exactly one group, return a list of strings matching that group.
If multiple
groups are present, return a list of tuples of strings matching the groups.
Non-capturing groups do not affect the
form of the result.
The string
is scanned left-to-right, and matches are returned in the order found.
Empty matches are included in the result.
Changed in version 3.7: Non-empty matches can now start just after a previous empty match.
If the pattern isn’t found, string is returned unchanged.
That is, \n is converted to a single newline character, \r is
converted to a carriage return, and so forth.
Unknown escapes of ASCII letters are reserved for future use and
treated as errors.
Other unknown escapes such as \& are left alone.
Backreferences, such as \6, are replaced with
the substring matched by group 6 in the pattern.
For example:

r'static PyObject*\npy_\1(void)\n{',

'def myfunc():')
'static PyObject*\npy_myfunc(void)\n{'
If repl is a function, it is called for every non-overlapping occurrence of pattern.
The function takes a single Match
argument, and returns the replacement string.
The pattern may be a string or a Pattern.
The optional argument count is the maximum number of pattern occurrences to be replaced; count must be a non-
negative integer.
If omitted or zero, all occurrences will be replaced.
Empty matches for the pattern are replaced
only when not adjacent to a previous empty match, so sub('x*', '-', 'abxd') returns '-a-b--d-'.
In string-type repl arguments, in addition to the character escapes and backreferences described above, \g<name>
will use the substring matched by the group named name, as defined by the (?P<name>...) syntax.
The backreference \g<0> substitutes in the entire substring matched by
the RE.
Changed in version 3.1: Added the optional flags argument.
Unmatched groups are replaced with an empty string.
Unknown escapes in pattern consisting of '\' and an ASCII letter now are errors.
Unknown escapes in repl consisting of '\' and an ASCII letter now are errors.
Changed in version 3.7: Empty matches for the pattern are replaced when adjacent to a previous non-empty match.
Group id can only contain ASCII digits.
In bytes replacement strings, group name can
only contain bytes in the ASCII range (b'\x00'-b'\x7f').
Changed in version 3.1: Added the optional flags argument.
Unmatched groups are replaced with an empty string.
This is useful if you want to match an arbitrary literal string that may have
regular expression metacharacters in it.
This function must not be used for the replacement string in sub() and subn(), only backslashes should be
escaped.
For example:
/usr/sbin/sendmail - \d+ errors, \d+ warnings
Changed in version 3.3: The '_' character is no longer escaped.
Changed in version 3.7: Only characters that can have special meaning in a regular expression are escaped.
As a
result, '!', '"', '%', "'", ',', '/', ':', ';', '<', '=', '>', '@', and "`" are no longer escaped.
Clear the regular expression cache.
It is
never an error if a string contains no match for a pattern.
The error instance has the following additional attributes:
msg
The unformatted error message.
Changed in version 3.5: Added additional attributes.
Changed in version 3.9: re.
See Generic Alias
Type.
Return None if no position in the string matches the pattern; note that this is different
from finding a zero-length match at some point in the string.
The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0.
This is
not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string
and at positions just after a newline, but not necessarily at the index where the search is to start.
The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos
characters long, so only the characters from pos to endpos - 1 will be searched for a match.
If endpos is less
than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string,
0, 50) is equivalent to rx.search(string[:50], 0).
If zero or more characters at the beginning of string match this regular expression, return a corresponding Match.
Return None if the string does not match the pattern; note that this is different from a zero-length match.
The optional pos and endpos parameters have the same meaning as for the search() method.
If the whole string matches this regular expression, return a corresponding Match.
Return None if the string does
not match the pattern; note that this is different from a zero-length match.
The optional pos and endpos parameters have the same meaning as for the search() method.
Pattern.split(string, maxsplit=0)
Identical to the split() function, using the compiled pattern.
Similar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos param-
eters that limit the search region like for search().
Similar to the finditer() function, using the compiled pattern, but also accepts optional pos and endpos pa-
rameters that limit the search region like for search().
Pattern.sub(repl, string, count=0)
Identical to the sub() function, using the compiled pattern.
Pattern.groups
The number of capturing groups in the pattern.
Pattern.groupindex
A dictionary mapping any symbolic group names defined by (?P<id>) to group numbers.
The dictionary is
empty if no symbolic groups were used in the pattern.
The pattern string from which the pattern object was compiled.
Changed in version 3.7: Added support of copy.copy() and copy.deepcopy().
Compiled regular expression
objects are considered atomic.
Match
Match object returned by successful matches and searches.
See Generic Alias
Type.
Match.expand(template)
Return the string obtained by doing backslash substitution on the template string template, as done by the sub()
method.
Escapes such as \n are converted to the appropriate characters, and numeric backreferences (\1, \2)
and named backreferences (\g<1>, \g<name>) are replaced by the contents of the corresponding group.
Unmatched groups are replaced with an empty string.
Match.group([group1, ...])
Returns one or more subgroups of the match.
If there is a single argument, the result is a single string; if there are
multiple arguments, the result is a tuple with one item per argument.
Without arguments, group1 defaults to zero
(the whole match is returned).
If a groupN argument is zero, the corresponding return value is the entire matching
string; if it is in the inclusive range [1..99], it is the string matching the corresponding parenthesized group.
If a
group number is negative or larger than the number of groups defined in the pattern, an IndexError exception
is raised.
If a group is contained in a part of the pattern that did not match, the corresponding result is None.
If a
group is contained in a part of the pattern that matched multiple times, the last match is returned.
If a string argument is not used as a group name in the pattern, an IndexError
exception is raised.
This is identical to m.group(g).
The first parenthesized subgroup.
Match.groups(default=None)
Return a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern.
The default argument is used for groups that did not participate in the match; it defaults to None.
If we make the decimal place and everything after it optional, not all groups might participate in the match.
These
groups will default to None unless the default argument is given:
# Second group defaults to None.
('24', None)
# Now, the second group defaults to '0'.
The default
argument is used for groups that did not participate in the match; it defaults to None.
Return -1 if group exists but did not contribute to the match.
For a match object m,
and a group g that did contribute to the match, the substring matched by group g (equivalent to m.group(g)) is
m.string[m.start(g):m.end(g)]
Note that m.start(group) will equal m.end(group) if group matched a null string.
For example, after m
= re.search('b(c?)', 'cba'), m.start(0) is 1, m.end(0) is 2, m.start(1) and m.end(1)
are both 2, and m.start(2) raises an IndexError exception.
An example that will remove remove_this from email addresses:
'tony@tiger.net'
Match.span([group])
For a match m, return the 2-tuple (m.start(group), m.end(group)).
Note that if group did not con-
tribute to the match, this is (-1, -1).
Match.pos
The value of pos which was passed to the search() or match() method of a regex object.
This is the index
into the string at which the RE engine started looking for a match.
The value of endpos which was passed to the search() or match() method of a regex object.
This is the index
into the string beyond which the RE engine will not go.
Match.lastindex
The integer index of the last matched capturing group, or None if no group was matched at all.
For example, the
expressions (a)b, ((a)(b)), and ((ab)) will have lastindex == 1 if applied to the string 'ab', while
the expression (a)(b) will have lastindex == 2, if applied to the same string.
Match.lastgroup
The name of the last matched capturing group, or None if the group didn’t have a name, or if no group was matched
at all.
Match.re
The regular expression object whose match() or search() method produced this match instance.
Match.string
The string passed to match() or search().
Changed in version 3.7: Added support of copy.copy() and copy.deepcopy().
Match objects are considered
atomic.
To see if a given string is a valid hand, one could do the following:
# Valid.
To match this with a regular expression,
one could use backreferences as such:
# Pair of 7s.
AttributeError: 'NoneType' object has no attribute 'group'
'a'
6.2.
Regular expressions are generally more powerful, though also
more verbose, than scanf() format strings.
The table below offers some more-or-less equivalent mappings between
scanf() format tokens and regular expressions.
scanf() Token
Regular Expression
%c
.
Python offers different primitive operations based on regular expressions:
• re.match() checks for a match only at the beginning of the string
• re.search() checks for a match anywhere in the string (this is what Perl does by default)
• re.fullmatch() checks for entire string to be a match
For example:
# No match
# Match
<re.
Match object; span=(2, 3), match='c'>
<re.
No match
# Match
<re.
Match object; span=(4, 5), match='X'>
Making a Phonebook
split() splits a string into a list delimited by the passed pattern.
The method is invaluable for converting textual data
into data structures that can be easily read and modified by Python as demonstrated in the following example that creates
a phonebook.
First, here is the input.
Heather Albrecht: 548.326.4584 919 Park Place"""
The entries are separated by one or more newlines.
Now we convert the string into a list with each nonempty line having
its own entry:
['Ross McFluff: 834.345.1254 155 Elm Street',
'Ronald Heathmore: 892.345.3428 436 Finley Avenue',
'Frank Burger: 925.541.7625 662 South Dogwood Way',
'Heather Albrecht: 548.326.4584 919 Park Place']
Finally, split each entry into a list with first name, last name, telephone number, and address.
Finding all Adverbs
findall() matches all occurrences of a pattern, not just the first one as search() does.
Continuing with the previous example, if a writer wanted to find all of the adverbs and
their positions in some text, they would use finditer() in the following manner:

print('%02d-%02d: %s' % (m.start(), m.end(), m.group(0)))
07-16: carefully
40-47: quickly
Raw String Notation
Raw string notation (r"text") keeps regular expressions sane.
Without it, every backslash ('\') in a regular expression
would have to be prefixed with another one to escape it.
For example, the two following lines of code are functionally
identical:
<re.
Match object; span=(0, 4), match=' ff '>
<re.
When one wants to match a literal backslash, it must be escaped in the regular expression.
With raw string notation, this
means r"\\".
Match object; span=(0, 1), match='\\'>
<re.
Match object; span=(0, 1), match='\\'>
Writing a Tokenizer
A tokenizer or scanner analyzes a string to categorize groups of characters.
This is a useful first step in writing a compiler
or interpreter.
The text categories are specified with regular expressions.
Token(type='END', value=';', line=5, column=9)
6.3 difflib — Helpers for computing deltas
Source code: Lib/difflib.py
This module provides classes and functions for comparing sequences.
It can be used for example, for comparing files,
and can produce information about file differences in various formats, including HTML and context and unified diffs.
For
comparing directories and files, see also, the filecmp module.
class difflib.
SequenceMatcher
This is a flexible class for comparing pairs of sequences of any type, so long as the sequence elements are hashable.
The idea is to find the longest contiguous
matching subsequence that contains no “junk” elements; these “junk” elements are ones that are uninteresting
in some sense, such as blank lines or whitespace.
The same idea is then applied recursively to the pieces of the sequences to the left and to the right of
the matching subsequence.
This does not yield minimal edit sequences, but does tend to yield matches that “look
right” to people.
Timing: The basic Ratcliff-Obershelp algorithm is cubic time in the worst case and quadratic time in the expected
case.
SequenceMatcher is quadratic time for the worst case and has expected-case behavior dependent in a
complicated way on how many elements the sequences have in common; best case time is linear.
The heuristic counts how many times each individual item appears in the sequence.
If an item’s
duplicates (after the first one) account for more than 1% of the sequence and the sequence is at least 200 items
long, this item is marked as “popular” and is treated as junk for the purpose of sequence matching.
This heuristic
can be turned off by setting the autojunk argument to False when creating the SequenceMatcher.
The autojunk parameter.
This is a class for comparing sequences of lines of text, and producing human-readable differences or deltas.
Differ
uses SequenceMatcher both to compare sequences of lines, and to compare sequences of characters within
similar (near-matching) lines.
Each line of a Differ delta begins with a two-letter code:
Code
Meaning
'- '
line unique to sequence 1
'+ '
line unique to sequence 2
' '
line common to both sequences
'?
These lines can be confusing if the sequences contain whitespace characters, such as spaces, tabs or line
breaks.
HtmlDiff
This class can be used to create an HTML table (or a complete HTML file containing the table) showing a side by
side, line by line comparison of text with inter-line and intra-line change highlights.
The table can be generated in
either full or contextual difference mode.
Initializes instance of HtmlDiff.
See ndiff() documentation for argument default values and
descriptions.
The following methods are public:
make_file(fromlines, tolines, fromdesc=”, todesc=”, context=False, numlines=5, *, charset=’utf-8’)
Compares fromlines and tolines (lists of strings) and returns a string which is a complete HTML file containing
a table showing line by line differences with inter-line and intra-line changes highlighted.
Set context to True when contextual differences
are to be shown, else the default is False to show the full files.
When context is
True numlines controls the number of context lines which surround the difference highlights.
When context
is False numlines controls the number of lines which are shown before a difference highlight when using
the “next” hyperlinks (setting to zero would cause the “next” hyperlinks to place the next difference highlight
at the top of the browser without any leading context).
Changed in version 3.5: charset keyword-only argument was added.
The default charset of HTML document
changed from 'ISO-8859-1' to 'utf-8'.
The arguments for this method are the same as those for the make_file() method.
Context diffs are a compact way of showing just the lines that have changed plus a few lines of context.
The changes
are shown in a before/after style.
The number of context lines is set by n which defaults to three.
By default, the diff control lines (those with *** or ---) are created with a trailing newline.
This is helpful so
that inputs created from io.IOBase.readlines() result in diffs that are suitable for use with io.
IOBase.
writelines() since both the inputs and outputs have trailing newlines.
For inputs that do not have trailing newlines, set the lineterm argument to "" so that the output will be uniformly
newline free.
The context diff format normally has a header for filenames and modification times.
Any or all of these may be
specified using strings for fromfile, tofile, fromfiledate, and tofiledate.
The modification times are normally expressed
in the ISO 8601 format.
Optional argument n (default 3) is the maximum number of close matches to return; n must be greater than 0.
Optional argument cutoff (default 0.6) is a float in the range [0, 1].
Possibilities that don’t score at least that similar
to word are ignored.
The best (no more than n) matches among the possibilities are returned in a list, sorted by similarity score, most
similar first.
Optional keyword parameters linejunk and charjunk are filtering functions (or None):
linejunk: A function that accepts a single string argument, and returns true if the string is junk, or false if not.
The
default is None.
There is also a module-level function IS_LINE_JUNK(), which filters out lines without visible
characters, except for at most one pound character ('#') – however the underlying SequenceMatcher class
does a dynamic analysis of which lines are so frequent as to constitute noise, and this usually works better than
using this function.
The default is module-level function IS_CHARACTER_JUNK(), which filters out whitespace characters (a
blank or tab; it’s a bad idea to include newline in this!).
Given a sequence produced by Differ.compare() or ndiff(), extract lines originating from file 1 or 2
(parameter which), stripping off line prefixes.
Compare a and b (lists of strings); return a delta (a generator generating the delta lines) in unified diff format.
Unified diffs are a compact way of showing just the lines that have changed plus a few lines of context.
The changes
are shown in an inline style (instead of separate before/after blocks).
The number of context lines is set by n which
defaults to three.
By default, the diff control lines (those with ---, +++, or @@) are created with a trailing newline.
This is helpful so
that inputs created from io.IOBase.readlines() result in diffs that are suitable for use with io.
IOBase.
writelines() since both the inputs and outputs have trailing newlines.
For inputs that do not have trailing newlines, set the lineterm argument to "" so that the output will be uniformly
newline free.
The unified diff format normally has a header for filenames and modification times.
Any or all of these may be
specified using strings for fromfile, tofile, fromfiledate, and tofiledate.
The modification times are normally expressed
in the ISO 8601 format.
If not specified, the strings default to blanks.
Compare a and b (lists of bytes objects) using dfunc; yield a sequence of delta lines (also bytes) in the format
returned by dfunc.
Allows you to compare data with unknown or inconsistent encoding.
All inputs except n must be bytes objects,
not str.
Works by losslessly converting all inputs (except n) to str, and calling dfunc(a, b, fromfile,
tofile, fromfiledate, tofiledate, n, lineterm).
The output of dfunc is then converted
back to bytes, so the delta lines that you receive have the same unknown/inconsistent encodings as a and b.
New in version 3.5.
difflib.
IS_LINE_JUNK(line)
Return True for ignorable lines.
The line line is ignorable if line is blank or contains a single '#', otherwise it is
not ignorable.
Used as a default for parameter linejunk in ndiff() in older versions.
difflib.
IS_CHARACTER_JUNK(ch)
Return True for ignorable characters.
The character ch is ignorable if ch is a space or tab, otherwise it is not
ignorable.
Used as a default for parameter charjunk in ndiff().
Pattern Matching: The Gestalt Approach Discussion of a similar algorithm by John W. Ratcliff and D. E. Metzener.
This was published in Dr. Dobb’s Journal in July, 1988.
SequenceMatcher(isjunk=None, a=”, b=”, autojunk=True)
Optional argument isjunk must be None (the default) or a one-argument function that takes a sequence element
and returns true if and only if the element is “junk” and should be ignored.
For example, pass:
lambda x: x in " \t"
if you’re comparing lines as sequences of characters, and don’t want to synch up on blanks or hard tabs.
The optional arguments a and b are sequences to be compared; both default to empty strings.
The elements of both
sequences must be hashable.
The optional argument autojunk can be used to disable the automatic junk heuristic.
The autojunk parameter.
SequenceMatcher objects get three data attributes: bjunk is the set of elements of b for which isjunk is True;
bpopular is the set of non-junk elements considered popular by the heuristic (if it is not disabled); b2j is a dict
mapping the remaining elements of b to a list of positions where they occur.
All three are reset whenever b is reset
with set_seqs() or set_seq2().
New in version 3.2: The bjunk and bpopular attributes.
SequenceMatcher objects have the following methods:
set_seqs(a, b)
Set the two sequences to be compared.
SequenceMatcher computes and caches detailed information about the second sequence, so if you want to
compare one sequence against many sequences, use set_seq2() to set the commonly used sequence once and
call set_seq1() repeatedly, once for each of the other sequences.
set_seq1(a)
Set the first sequence to be compared.
The second sequence to be compared is not changed.
The first sequence to be compared is not changed.
If isjunk was omitted or None, find_longest_match() returns (i, j, k) such that a[i:i+k] is
equal to b[j:j+k], where alo <= i <= i+k <= ahi and blo <= j <= j+k <= bhi.
For all
(i', j', k') meeting those conditions, the additional conditions k >= k', i <= i', and if i ==
i', j <= j' are also met.
In other words, of all maximal matching blocks, return one that starts earliest
in a, and of all those maximal matching blocks that start earliest in a, return the one that starts earliest in b.
Match(a=0, b=4, size=5)
6.3.
If isjunk was provided, first the longest matching block is determined as above, but with the additional restric-
tion that no junk element appears in the block.
Then that block is extended as far as possible by matching
(only) junk elements on both sides.
So the resulting block never matches on junk except as identical junk
happens to be adjacent to an interesting match.
Here’s the same example as before, but considering blanks to be junk.
That prevents ' abcd' from matching
the ' abcd' at the tail end of the second sequence directly.
If no blocks match, this returns (alo, blo, 0).
This method returns a named tuple Match(a, b, size).
Changed in version 3.9: Added default arguments.
Each triple is of the form (i, j,
n), and means that a[i:i+n] == b[j:j+n].
The triples are monotonically increasing in i and j.
The last triple is a dummy, and has the value (len(a), len(b), 0).
It is the only triple with n == 0.
If (i, j, n) and (i', j', n') are adjacent triples in the list, and the second is not the last triple in
the list, then i+n < i' or j+n < j'; in other words, adjacent triples always describe non-adjacent equal
blocks.
Each tuple is of the form (tag, i1, i2, j1,
j2).
The first tuple has i1 == j1 == 0, and remaining tuples have i1 equal to the i2 from the preceding
tuple, and, likewise, j1 equal to the previous j2.
The tag values are strings, with these meanings:
Value
Meaning
'replace'
a[i1:i2] should be replaced by b[j1:j2].
Note that j1 == j2 in this case.
Note that i1 == i2 in this case.
Starting with the groups returned by get_opcodes(), this method splits out smaller change clusters and
eliminates intervening ranges which have no changes.
The groups are returned in the same format as get_opcodes().
Where T is the total number of elements in both sequences, and M is the number of matches, this is 2.0*M /
T. Note that this is 1.0 if the sequences are identical, and 0.0 if they have nothing in common.
This is expensive to compute if get_matching_blocks() or get_opcodes() hasn’t already been
called, in which case you may want to try quick_ratio() or real_quick_ratio() first to get an
upper bound.
Note: Caution: The result of a ratio() call may depend on the order of the arguments.
For instance:
0.25
0.5
quick_ratio()
Return an upper bound on ratio() relatively quickly.
The three methods that return the ratio of matching to total characters can give different results due to differing levels of
approximation, although quick_ratio() and real_quick_ratio() are always at least as large as ratio():
0.75
0.75
1.0
6.3.2 SequenceMatcher Examples
This example compares two strings, considering blanks to be “junk”:

"private Thread currentThread;",

"private volatile Thread currentThread;")
ratio() returns a float in [0, 1], measuring the similarity of the sequences.
As a rule of thumb, a ratio() value over
0.6 means the sequences are close matches:
0.866
6.3.
If you’re only interested in where the sequences match, get_matching_blocks() is handy:

print("a[%d] and b[%d] match for %d elements" % block)
a[0] and b[0] match for 8 elements
a[8] and b[17] match for 21 elements
a[29] and b[38] match for 0 elements
Note that the last tuple returned by get_matching_blocks() is always a dummy, (len(a), len(b), 0),
and this is the only case in which the last tuple element (number of elements matched) is 0.
The get_close_matches() function in this module which shows how simple code building on
SequenceMatcher can be used to do useful work.
To the contrary, minimal diffs are often counter-
intuitive, because they synch up anywhere possible, sometimes accidental matches 100 pages apart.
Restricting synch
points to contiguous matches preserves some notion of locality, at the occasional cost of producing a longer diff.
The Differ class has this constructor:
class difflib.
Differ(linejunk=None, charjunk=None)
Optional keyword parameters linejunk and charjunk are for filter functions (or None):
linejunk: A function that accepts a single string argument, and returns true if the string is junk.
The default is
None, meaning that no line is considered junk.
The default is None, meaning that no character is considered junk.
These junk-filtering functions speed up matching to find differences and do not cause any differing lines or char-
acters to be ignored.
Read the description of the find_longest_match() method’s isjunk parameter for an
explanation.
Differ objects are used (deltas generated) via a single method:
compare(a, b)
Compare two sequences of lines, and generate the delta (a sequence of lines).
Each sequence must contain individual single-line strings ending with newlines.
Such sequences can be ob-
tained from the readlines() method of file-like objects.
The delta generated also consists of newline-
terminated strings, ready to be printed as-is via the writelines() method of a file-like object.
First we set up the texts, sequences of individual single-line strings ending with newlines
(such sequences can also be obtained from the readlines() method of file-like objects):
1.
See the
Differ() constructor for details.
Beautiful is better than ugly.\n',
'-
2.
Explicit is better than implicit.\n',
'-
3.
Simple is better than complex.\n',
'+
3.
Simple is better than complex.\n',
'?
Complex is better than complicated.\n',
'?
Complicated is better than complex.\n',
'?
Beautiful is better than ugly.
-
2.
Explicit is better than implicit.
-
3.
Simple is better than complex.
Simple is better than complex.
?
Complex is better than complicated.
?
Complicated is better than complex.
Flat is better than nested.
Both inter-
and intra-line differences are noted.
In the second form, recreate file1
(-r1) or file2 (-r2) on stdout, from an ndiff report on stdin.
In the first form, if -q ("quiet") is not specified, the first two lines
of output are
-: file1
+: file2
Each remaining line begins with a two-letter code:
"- "
line unique to file1
"+ "
line unique to file2
"
"
line common to both files
"?
These lines can be
confusing if the source files contain tab characters.
The first file can be recovered by retaining only lines that begin with
"
" or "- ", and deleting those 2-character prefixes; use ndiff with -r1.
Returns a list of output
lines, without final newlines.
Optional keyword arguments correspond to the instance attributes of TextWrapper, documented below.
In particular, fill() accepts exactly the same keyword arguments as wrap().
If the result fits in the width,
it is returned.
Otherwise, enough words are dropped from the end so that the remaining words plus the placeholder
fit within width:
world!
Hello [...]'
'Hello...'
Optional keyword arguments correspond to the instance attributes of TextWrapper, documented below.
Note
that the whitespace is collapsed before the text is passed to the TextWrapper fill() function, so changing the
value of tabsize, expand_tabs, drop_whitespace, and replace_whitespace will have no effect.
Remove any common leading whitespace from every line in text.
This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them
in the source code in indented form.
Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines " hello" and "\
thello" are considered to have no common leading whitespace.
Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output.
For example:

# end first line with \ to avoid the empty line!
Add prefix to the beginning of selected lines in text.
Lines are separated by calling text.splitlines(True).
By default, prefix is added to all lines that do not consist solely of whitespace (including any line endings).
The optional predicate argument can be used to control which lines are indented.
That instance is not reused, so for applications that process many text strings using wrap() and/or fill(), it may be
more efficient to create your own TextWrapper object.
Text is preferably wrapped on whitespaces and right after the hyphens in hyphenated words; only then will long words be
broken if necessary, unless TextWrapper.break_long_words is set to false.
TextWrapper(**kwargs)
The TextWrapper constructor accepts a number of optional keyword arguments.
Each keyword argument cor-
responds to an instance attribute, so for example
wrapper = TextWrapper(initial_indent="* ")
is the same as
wrapper = TextWrapper()
wrapper.initial_indent = "* "
You can re-use the same TextWrapper object many times, and you can change any of its options through direct
assignment to instance attributes between uses.
The maximum length of wrapped lines.
As long as there are no individual words in the input text
longer than width, TextWrapper guarantees that no output line will be longer than width characters.
If true, then all tab characters in text will be expanded to spaces using the expandtabs()
method of text.
New in version 3.3.
If true, after tab expansion but before wrapping, the wrap() method will replace each
whitespace character with a single space.
The whitespace characters replaced are as follows: tab, newline,
vertical tab, formfeed, and carriage return ('\t\n\v\f\r').
Note: If expand_tabs is false and replace_whitespace is true, each tab character will be replaced
by a single space, which is not the same as tab expansion.
Note: If replace_whitespace is false, newlines may appear in the middle of a line and cause strange
output.
For this reason, text should be split into paragraphs (using str.splitlines() or similar) which
are wrapped separately.
Whitespace at the beginning of the paragraph, however, is not dropped if non-whitespace
follows it.
If whitespace being dropped takes up an entire line, the whole line is dropped.
String that will be prepended to the first line of wrapped output.
Counts towards the length of
the first line.
The empty string is not indented.
String that will be prepended to all lines of wrapped output except the first.
Counts towards the
length of each line except the first.
This is generally desired for text in a monospaced font.
However,
the sentence detection algorithm is imperfect: it assumes that a sentence ending consists of a lowercase letter
followed by one of '.', '!', or '?', possibly followed by one of '"' or "'", followed by a space.
Since the sentence detection algorithm relies on string.lowercase for the definition of “lowercase
letter”, and a convention of using two spaces after a period to separate sentences on the same line, it is
specific to English-language texts.
If it is false, long words will not be broken, and some lines may be longer than width.
True) If true, wrapping will occur preferably on whitespaces and right after hyphens in compound
words, as it is customary in English.
If false, only whitespaces will be considered as potentially good places
for line breaks, but you need to set break_long_words to false if you want truly insecable words.
Default
behaviour in previous versions was to always allow breaking hyphenated words.
New in version 3.4.
New in version 3.4.
All wrapping
options are taken from instance attributes of the TextWrapper instance.
Returns a list of output lines,
without final newlines.
Wraps the single paragraph in text, and returns a single string containing the wrapped paragraph.
The data contained in this database is compiled from the UCD version 15.0.0.
The module uses the same names and symbols as defined by Unicode Standard Annex #44, “Unicode Character Database”.
It defines the following functions:
unicodedata.lookup(name)
Look up character by name.
If a character with the given name is found, return the corresponding character.
If not
found, KeyError is raised.
Changed in version 3.3: Support for name aliases1 and named sequences2 has been added.
If no name is defined, default is returned, or, if not
given, ValueError is raised.
If no such value is defined, default is returned,
or, if not given, ValueError is raised.
If no such value is defined, default is returned, or,
if not given, ValueError is raised.
If no such value is defined, default is returned, or,
if not given, ValueError is raised.
Returns the bidirectional class assigned to the character chr as string.
If no such value is defined, an empty string
is returned.
Returns 0 if no combining class is
defined.
Returns 1 if the character has been identified
as a “mirrored” character in bidirectional text, 0 otherwise.
An empty string is returned
in case no such mapping is defined.
Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and
‘NFKD’.
The Unicode standard defines various normalization forms of a Unicode string, based on the definition of canonical
equivalence and compatibility equivalence.
In Unicode, several characters can be expressed in various way.
For
example, the character U+00C7 (LATIN CAPITAL LETTER C WITH CEDILLA) can also be expressed as the
sequence U+0043 (LATIN CAPITAL LETTER C) U+0327 (COMBINING CEDILLA).
For each character, there are two normal forms: normal form C and normal form D. Normal form D (NFD) is
also known as canonical decomposition, and translates each character into its decomposed form.
Normal form C
(NFC) first applies a canonical decomposition, then composes pre-combined characters again.
In addition to these two forms, there are two additional normal forms based on compatibility equivalence.
In
Unicode, certain characters are supported which normally would be unified with other characters.
For example,
U+2160 (ROMAN NUMERAL ONE) is really the same thing as U+0049 (LATIN CAPITAL LETTER I).
How-
ever, it is supported in Unicode for compatibility with existing character sets (e.g. gb2312).
The normal form KD (NFKD) will apply the compatibility decomposition, i.e. replace all compatibility characters
with their equivalents.
The normal form KC (NFKC) first applies the compatibility decomposition, followed by the
canonical composition.
Even if two unicode strings are normalized and look the same to a human reader, if one has combining characters
and the other doesn’t, they may not compare equal.
Valid values for form are ‘NFC’, ‘NFKC’,
‘NFD’, and ‘NFKD’.
New in version 3.8.
In addition, the module exposes the following constant:
unicodedata.unidata_version
The version of the Unicode database used in this module.
Exactly how this comparison is executed may depend on the application domain, e.g. whether it should
be case-insensitive or not.
It may be also necessary to restrict the possible identifications, to allow only identifications
consisting of “printable” characters.
RFC 3454 defines a procedure for “preparing” Unicode strings in internet protocols.
Before passing strings onto the wire,
they are processed with the preparation procedure, after which they have a certain normalized form.
The RFC defines a
set of tables, which can be combined into profiles.
Each profile must define which tables it uses, and what other optional
parts of the stringprep procedure are part of the profile.
One example of a stringprep profile is nameprep,
which is used for internationalized domain names.
The module stringprep only exposes the tables from RFC 3454.
As these tables would be very large to represent
as dictionaries or lists, the module uses the Unicode character database internally.
The module source code itself was
generated using the mkstringprep.py utility.
As a result, these tables are exposed as functions, not as data structures.
There are two kinds of tables in the RFC: sets
and mappings.
For a set, stringprep provides the “characteristic function”, i.e. a function that returns True if the
parameter is part of the set.
For mappings, it provides the mapping function: given the key, it returns the associated value.
Below is a list of all functions available in the module.
stringprep.in_table_a1(code)
Determine whether code is in tableA.1 (Unassigned code points in Unicode 3.2).
Determine whether code is in tableC.3 (Private use).
Determine whether code is in tableC.5 (Surrogate codes).
Determine whether code is in tableC.6 (Inappropriate for plain text).
Determine whether code is in tableC.8 (Change display properties or are deprecated).
This module can be used directly, or via the rlcompleter module, which supports completion of
Python identifiers at the interactive prompt.
Settings made using this module affect the behaviour of both the interpreter’s
interactive prompt and the prompts offered by the built-in input() function.
Readline keybindings may be configured via an initialization file, typically .inputrc in your home directory.
See
Readline Init File in the GNU Readline manual for information about the format and allowable constructs of that file, and
the capabilities of the Readline library in general.
Note:
The underlying Readline library API may be implemented by the libedit library instead of GNU readline.
On macOS the readline module detects which library is being used at run time.
The configuration file for libedit is different from that of GNU readline.
If you programmatically load configuration
strings you can check for the text “libedit” in readline.__doc__ to differentiate between GNU readline and libedit.
If you use editline/libedit readline emulation on macOS, the initialization file located in your home directory is named
.editrc.
This calls rl_parse_and_bind() in the underlying
library.
The default filename is the last filename used.
This calls
rl_read_init_file() in the underlying library.
Insert text into the line buffer at the cursor position.
This calls rl_insert_text() in the underlying library,
but ignores the return value.
This calls
rl_redisplay() in the underlying library.
The default filename is ~/.history.
This calls
read_history() in the underlying library.
The default filename is ~/.history.
This calls write_history() in the underlying library.
Append the last nelements items of history to a file.
The default filename is ~/.history.
The file must already
exist.
This calls append_history() in the underlying library.
This function only exists if Python was compiled
for a version of the library that supports it.
New in version 3.5.
readline.get_history_length()
readline.set_history_length(length)
Set or return the desired number of lines to save in the history file.
The write_history_file() function
uses this value to truncate the history file, by calling history_truncate_file() in the underlying library.
Negative values imply unlimited history file size.
Clear the current history.
This calls clear_history() in the underlying library.
The Python function only
exists if Python was compiled for a version of the library that supports it.
The item index is one-based.
This calls history_get()
in the underlying library.
readline.remove_history_item(pos)
Remove history item specified by its position from the history.
The position is zero-based.
This calls
remove_history() in the underlying library.
The position is zero-based.
This calls
replace_history_entry() in the underlying library.
Append line to the history buffer, as if it was the last line typed.
This calls add_history() in the underlying
library.
The enabled argument
should be a Boolean value that when true, enables auto history, and that when false, disables auto history.
New in version 3.6.
CPython implementation detail: Auto history is enabled by default, and changes to this do not persist across
multiple sessions.
If function is
specified, it will be used as the new hook function; if omitted or None, any function already installed is removed.
The hook is called with no arguments just before readline prints the first prompt.
If function
is specified, it will be used as the new hook function; if omitted or None, any function already installed is removed.
The hook is called with no arguments after the first prompt has been printed and just before readline starts reading
input characters.
This function only exists if Python was compiled for a version of the library that supports it.
The following functions relate to implementing a custom word completion function.
This is typically operated by the
Tab key, and can suggest and automatically complete a word being typed.
By default, Readline is set up to be used by
rlcompleter to complete Python identifiers for the interactive interpreter.
If the readline module is to be used
with a custom completer, a different set of word delimiters should be set.
If function is specified, it will be used as the new completer function;
if omitted or None, any completer function already installed is removed.
The completer function is called as
function(text, state), for state in 0, 1, 2, …, until it returns a non-string value.
It should return the next
possible completion starting with text.
The
installed
completer
function
is
invoked
by
the
entry_func
callback
passed
to
rl_completion_matches() in the underlying library.
The text string comes from the first parame-
ter to the rl_attempted_completion_function callback of the underlying library.
This returns the rl_completion_type variable in the underlying
library as an integer.
These indexes are the start and end arguments passed
to the rl_attempted_completion_function callback of the underlying library.
The values may be
different in the same input editing scenario based on the underlying C readline implementation.
Ex: libedit is
known to behave differently than libreadline.
These determine the start of the word to be considered for completion
(the completion scope).
These functions access the rl_completer_word_break_characters variable
in the underlying library.
If function is specified, it will be used as the new completion
display function; if omitted or None, any completion display function already installed is removed.
This sets or
clears the rl_completion_display_matches_hook callback in the underlying library.
The completion
display function is called as function(substitution, [matches], longest_match_length)
once each time matches need to be displayed.
The code below
would normally be executed automatically during interactive sessions from the user’s PYTHONSTARTUP file.
This code is actually automatically run when Python is run in interactive mode (see Readline configuration).
The following example achieves the same goal but supports concurrent interactive sessions, by only appending the new
history.
The following example extends the code.
InteractiveConsole class to support history save/restore.
When this module is imported on a Unix platform with the readline module available, an instance of the Completer
class is automatically created and its complete() method is set as the readline completer.
Unless Python is run with the -S option,
the module is automatically imported and configured (see Readline configuration).
On platforms without readline, the Completer class defined by this module can still be used for custom purposes.
If called for text that doesn’t include a period character ('.'), it will complete from names currently defined in
__main__, builtins and keywords (as defined by the keyword module).
Any exception raised during the evaluation of the expression is caught, silenced and None is
returned.
Text Processing Services
CHAPTER
SEVEN
BINARY DATA SERVICES
The modules described in this chapter provide some basic services operations for manipulation of binary data.
Other
operations on binary data, specifically in relation to file formats and network protocols, are described in the relevant
sections.
Some libraries described under Text Processing Services also work with either ASCII-compatible binary formats (for
example, re) or all binary data (for example, difflib).
Lib/struct.py
This module converts between Python values and C structs represented as Python bytes objects.
Compact format strings
describe the intended conversions to/from Python values.
The module’s functions and objects can be used for two largely
distinct applications, data exchange with external sources (files or network connections), or data transfer between the
Python application and the C layer.
Note:
When no prefix character is given, native mode is the default.
It packs or unpacks data based on the platform
and compiler on which the Python interpreter was built.
The result of packing a given C struct includes pad bytes which
maintain proper alignment for the C types involved; similarly, alignment is taken into account when unpacking.
In contrast,
when communicating data between external sources, the programmer is responsible for defining byte ordering and padding
between elements.
See Byte Order, Size, and Alignment for details.
Several struct functions (and methods of Struct) take a buffer argument.
This refers to objects that implement the
bufferobjects and provide either a readable or read-writable buffer.
The most common types used for that purpose are
bytes and bytearray, but many other types that can be viewed as an array of bytes implement the buffer protocol,
so that they can be read/filled without additional copying from a bytes object.
The arguments
must match the values required by the format exactly.
struct.pack_into(format, buffer, offset, v1, v2, ...)
Pack the values v1, v2, … according to the format string format and write the packed bytes into the writable buffer
buffer starting at position offset.
Note that offset is a required argument.
The result is a tuple even if it contains exactly one item.
The buffer’s size in bytes must match the size
required by the format, as reflected by calcsize().
struct.unpack_from(format, /, buffer, offset=0)
Unpack from buffer starting at position offset, according to the format string format.
The result is a tuple even if it
contains exactly one item.
The buffer’s size in bytes, starting at position offset, must be at least the size required by
the format, as reflected by calcsize().
This function returns an iterator
which will read equally sized chunks from the buffer until all its contents have been consumed.
The buffer’s size in
bytes must be a multiple of the size required by the format, as reflected by calcsize().
Each iteration yields a tuple as specified by the format string.
They are built up from format characters,
which specify the type of data being packed/unpacked.
In addition, special characters control the byte order, size and
alignment.
Each format string consists of an optional prefix character which describes the overall properties of the data
and one or more format characters which describe the actual data values and padding.
Byte Order, Size, and Alignment
By default, C types are represented in the machine’s native format and byte order, and properly aligned by skipping pad
bytes if necessary (according to the rules used by the C compiler).
This behavior is chosen so that the bytes of a packed
struct correspond exactly to the memory layout of the corresponding C struct.
Whether to use native byte ordering and
padding or standard formats depends on the application.
Native byte order is big-endian or little-endian, depending on the host system.
For example, Intel x86, AMD64 (x86-64),
and Apple M1 are little-endian; IBM z and many legacy architectures are big-endian.
Use sys.byteorder to check
the endianness of your system.
Native size and alignment are determined using the C compiler’s sizeof expression.
This is always combined with
native byte order.
Standard size depends only on the format character; see the table in the Format Characters section.
Note the difference between '@' and '=': both use native byte order, but the size and alignment of the latter is stan-
dardized.
The form '!' represents the network byte order which is always big-endian as defined in IETF RFC 1700.
There is no way to indicate non-native byte order (force byte-swapping); use the appropriate choice of '<' or '>'.
Notes:
(1) Padding is only automatically added between successive structure members.
No padding is added at the beginning
or the end of the encoded struct.
See Examples.
Format Characters
Format characters have the following meaning; the conversion between C and Python values should be obvious given their
types.
When using native size, the size of the packed value is
platform-dependent.
Changed in version 3.6: Added support for the 'e' format.
If this type is not available, it is simulated
using a char.
In standard mode, it is always represented by one byte.
Changed in version 3.2: Added use of the __index__() method for non-integers.
For the standard size, you can use whichever of the other integer formats fits your application.
The byte order character '=' chooses to use little- or big-endian ordering based on the host
system.
The struct module does not interpret this as native ordering, so the 'P' format is not available.
It has
a sign bit, a 5-bit exponent and 11-bit precision (with 10 bits explicitly stored), and can represent numbers between
approximately 6.1e-05 and 6.5e+04 at full precision.
This type is not widely supported by C compilers: on a
typical machine, an unsigned short can be used for storage, but not for math operations.
See the Wikipedia page
on the half-precision floating-point format for more information.
The first byte stored is the length of the string, or 255, whichever is smaller.
The bytes
of the string follow.
If the string is shorter than count-1, it is padded with null bytes so that
exactly count bytes in all are used.
Note that for unpack(), the 'p' format character consumes count bytes,
but that the string returned can never contain more than 255 bytes.
If a count
is not given, it defaults to 1.
For packing, the string is truncated or padded with null bytes as appropriate to make
it fit.
For unpacking, the resulting bytes object always has exactly the specified number of bytes.
As a special case,
'0s' means a single, empty string (while '0c' means 0 characters).
A format character may be preceded by an integral repeat count.
For example, the format string '4h' means exactly the
same as 'hhhh'.
Whitespace characters between formats are ignored; a count and its format must not contain whitespace though.
When packing a value x using one of the integer formats ('b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q'), if
x is outside the valid range for that format then struct.error is raised.
Changed in version 3.1:
Previously, some of the integer formats wrapped out-of-range values and raised
DeprecationWarning instead of struct.error.
For the '?' format character, the return value is either True or False.
When packing, the truth value of the argument
object is used.
Either 0 or 1 in the native or standard bool representation will be packed, and any non-zero value will be
True when unpacking.
Examples
Note: Native byte order examples (designated by the '@' format prefix or lack of any prefix character) may not match
what the reader’s machine produces as that depends on the platform and compiler.
The ordering of format characters may have an impact on size in native mode since padding is implicit.
In standard
mode, the user is responsible for inserting any desired padding.
Note in the first pack call below that three NUL bytes
were added after the packed '#' to align the following integer on a four-byte boundary.
In this example, the output was
produced on a little endian machine:
b'#\x00\x00\x00\x15\x14\x13\x12'
b'\x15\x14\x13\x12#'


The following format 'llh0l' results in two pad bytes being added at the end, assuming the platform’s longs are aligned
on 4-byte boundaries:
b'\x00\x00\x00\x01\x00\x00\x00\x02\x00\x03\x00\x00'
See also:
Module array Packed binary storage of homogeneous data.
Module json JSON encoder and decoder.
Module pickle Python object serialization.
Generally speaking, the format strings constructed for these two domains are
distinct.
Native Formats
When constructing format strings which mimic native layouts, the compiler and machine architecture determine byte
ordering and padding.
In such cases, the @ format character should be used to specify native byte ordering and data sizes.
Internal pad bytes are normally inserted automatically.
It is possible that a zero-repeat format code will be needed at the
end of a format string to round up to the correct byte boundary for proper alignment of consecutive chunks of data.
A
zero-repeat format code solves that problem:

The 'x' format code can be used to specify the repeat, but for native formats it is better to use a zero-repeat format like
'0l'.
By default, native byte ordering and alignment is used, but it is better to be explicit and use the '@' prefix character.
Standard Formats
When exchanging data beyond your process such as networking or storage, be precise.
Specify the exact byte order, size,
and alignment.
Do not assume they match the native order of a particular machine.
For example, network byte order
is big-endian, while many popular CPUs are little-endian.
By defining this explicitly, the user need not care about the
specifics of the platform their code is running on.
The first character should typically be < or > (or !).
Padding is the
responsibility of the programmer.
The zero-repeat format character won’t work.
Instead, the user must explicitly add
'x' pad bytes where needed.
For
example, the examples below were executed on a 32-bit machine:


False
7.1.4 Classes
The struct module also defines the following type:
class struct.
Struct(format)
Return a new Struct object which writes and reads binary data according to the format string format.
Creating a
Struct object once and calling its methods is more efficient than calling module-level functions with the same
format since the format string is only compiled once.
Identical to the pack() function, using the compiled format.
Identical to the unpack() function, using the compiled format.
Identical to the unpack_from() function, using the compiled format.
The buffer’s size in bytes, starting
at position offset, must be at least size.
iter_unpack(buffer)
Identical to the iter_unpack() function, using the compiled format.
The buffer’s size in bytes must be a
multiple of size.
New in version 3.4.
The format string type is now str instead of bytes.
Most standard codecs are text
encodings, which encode text to bytes (and decode bytes to text), but there are also codecs provided that encode text to
text, and bytes to bytes.
Custom codecs may encode and decode between arbitrary types, but some module features are
restricted to be used specifically with text encodings or with codecs that encode to bytes.
Encodes obj using the codec registered for encoding.
Errors may be given to set the desired error handling scheme.
The default error handler is 'strict' meaning
that encoding errors raise ValueError (or a more codec specific subclass, such as UnicodeEncodeError).
Refer to Codec Base Classes for more information on codec error handling.
Decodes obj using the codec registered for encoding.
Errors may be given to set the desired error handling scheme.
The default error handler is 'strict' meaning
that decoding errors raise ValueError (or a more codec specific subclass, such as UnicodeDecodeError).
Refer to Codec Base Classes for more information on codec error handling.
The full details for each codec can also be looked up directly:
codecs.lookup(encoding)
Looks up the codec info in the Python codec registry and returns a CodecInfo object as defined below.
Encodings are first looked up in the registry’s cache.
If not found, the list of registered search functions is scanned.
If no CodecInfo object is found, a LookupError is raised.
Otherwise, the CodecInfo object is stored in
the cache and returned to the caller.
CodecInfo(encode, decode, streamreader=None, streamwriter=None, incrementalencoder=None,
incrementaldecoder=None, name=None)
Codec details when looking up the codec registry.
The constructor arguments are stored in attributes of the same
name:
name
The name of the encoding.
These must be functions or methods which have the same in-
terface as the encode() and decode() methods of Codec instances (see Codec Interface).
The functions
or methods are expected to work in a stateless mode.
These have to provide the interface defined
by the base classes IncrementalEncoder and IncrementalDecoder, respectively.
Incremental
codecs can maintain state.
These have to provide the interface defined by the base
classes StreamWriter and StreamReader, respectively.
Stream codecs can maintain state.
To simplify access to the various codec components, the module provides these additional functions which use lookup()
for the codec lookup:
codecs.getencoder(encoding)
Look up the codec for the given encoding and return its encoder function.
Raises a LookupError in case the encoding cannot be found.
Raises a LookupError in case the encoding cannot be found.
Look up the codec for the given encoding and return its incremental encoder class or factory function.
Raises a LookupError in case the encoding cannot be found or the codec doesn’t support an incremental encoder.
Raises a LookupError in case the encoding cannot be found or the codec doesn’t support an incremental decoder.
Raises a LookupError in case the encoding cannot be found.
Look up the codec for the given encoding and return its StreamWriter class or factory function.
Raises a LookupError in case the encoding cannot be found.
Custom codecs are made available by registering a suitable codec search function:
codecs.register(search_function)
Register a codec search function.
Search functions are expected to take one argument, being the encoding name in
all lower case letters with hyphens and spaces converted to underscores, and return a CodecInfo object.
In case
a search function cannot find a given encoding, it should return None.
Changed in version 3.9: Hyphens and spaces are converted to underscore.
Unregister a codec search function and clear the registry’s cache.
If the search function is not registered, do nothing.
New in version 3.10.
While the builtin open() and the associated io module are the recommended approach for working with encoded text
files, this module provides additional utility functions and classes that allow the use of a wider range of codecs when
working with binary files:
codecs.open(filename, mode=’r’, encoding=None, errors=’strict’, buffering=- 1)
Open an encoded file using the given mode and return an instance of StreamReaderWriter, providing trans-
parent encoding/decoding.
The default file mode is 'r', meaning to open the file in read mode.
Note: If encoding is not None, then the underlying encoded files are always opened in binary mode.
No automatic
conversion of '\n' is done on reading and writing.
The mode argument may be any binary mode acceptable to
the built-in open() function; the 'b' is automatically added.
Any encoding that encodes to and decodes from
bytes is allowed, and the data types supported by the file methods depend on the codec used.
It defaults to 'strict' which causes a ValueError to be
raised in case an encoding error occurs.
It defaults to -1 which means that the default
buffer size will be used.
Changed in version 3.11: The 'U' mode has been removed.
codecs.
EncodedFile(file, data_encoding, file_encoding=None, errors=’strict’)
Return a StreamRecoder instance, a wrapped version of file which provides transparent transcoding.
The
original file is closed when the wrapped version is closed.
Data written to the wrapped file is decoded according to the given data_encoding and then written to the original
file as bytes using file_encoding.
If file_encoding is not given, it defaults to data_encoding.
It defaults to 'strict', which causes ValueError to be
raised in case an encoding error occurs.
This function is a generator.
The
errors argument (as well as any other keyword argument) is passed through to the incremental encoder.
This function requires that the codec accept text str objects to encode.
Therefore it does not support bytes-to-bytes
encoders such as base64_codec.
This function is a generator.
The
errors argument (as well as any other keyword argument) is passed through to the incremental decoder.
This function requires that the codec accept bytes objects to decode.
Therefore it does not support text-to-text
encoders such as rot_13, although rot_13 may be used equivalently with iterencode().
The module also provides the following constants which are useful for reading and writing to platform dependent files:
codecs.
BOM
codecs.
BOM_BE
codecs.
BOM_LE
codecs.
BOM_UTF8
codecs.
BOM_UTF16
codecs.
BOM_UTF16_BE
codecs.
BOM_UTF16_LE
codecs.
BOM_UTF32
codecs.
BOM_UTF32_BE
codecs.
These constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings.
They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode
signature.
BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform’s native byte
order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE.
The
others represent the BOM in UTF-8 and UTF-32 encodings.
Each codec has to define four interfaces to make it usable as codec in Python: stateless encoder, stateless decoder, stream
reader and stream writer.
The stream reader and writers typically reuse the stateless encoder/decoder to implement the
file protocols.
Codec authors also need to define how the codec will handle encoding and decoding errors.
The following error handlers can be used with all Python Standard Encodings codecs:
Value
Meaning
'strict'
Raise UnicodeError (or a subclass), this is the default.
Implemented in
strict_errors().
Implemented in
ignore_errors().
On encoding, use ?
On
decoding, use � (U+FFFD, the official REPLACEMENT CHARACTER).
Implemented in replace_errors().
On encoding, use hexadecimal form of
Unicode code point with formats \xhh \uxxxx \Uxxxxxxxx.
On decoding, use
hexadecimal form of byte value with format \xhh.
Implemented in
backslashreplace_errors().
This code will then be turned back into the same byte when the
'surrogateescape' error handler is used when encoding the data.
The following error handlers are only applicable to encoding (within text encodings):
Value
Meaning
'xmlcharrefreplace'
Replace with XML/HTML numeric character reference, which is a decimal form of Unicode code
point with format &#num;.
Implemented in xmlcharrefreplace_errors().
Implemented in namereplace_errors().
In addition, the following error handler is specific to the given codecs:
Value
Codecs
Meaning
'surrogatepass'
utf-8, utf-16, utf-32, utf-
16-be, utf-16-le, utf-32-
be, utf-32-le
Allow encoding and decoding surrogate code point (U+D800 - U+DFFF) as
normal code point.
Otherwise these codecs treat the presence of surrogate
code point in str as an error.
The 'surrogateescape' and 'surrogatepass' error handlers.
The 'surrogatepass' error handler now works with utf-16* and utf-32* codecs.
The 'backslashreplace' error handler now works with decoding and translating.
The error_handler argument will be
called during encoding and decoding in case of an error, when name is specified as the errors parameter.
For encoding, error_handler will be called with a UnicodeEncodeError instance, which contains information
about the location of the error.
The error handler must either raise this or a different exception, or return a tuple
with a replacement for the unencodable part of the input and a position where encoding should continue.
The
replacement may be either str or bytes.
If the replacement is bytes, the encoder will simply copy them into
the output buffer.
If the replacement is a string, the encoder will encode the replacement.
Encoding continues on
original input at the specified position.
Negative position values will be treated as being relative to the end of the
input string.
If the resulting position is out of bound an IndexError will be raised.
Decoding and translating works similarly, except UnicodeDecodeError or UnicodeTranslateError
will be passed to the handler and that the replacement from the error handler will be put into the output directly.
Return the error handler previously registered under the name name.
Raises a LookupError in case the handler cannot be found.
Implements the 'strict' error handling.
Implements the 'ignore' error handling.
Malformed data is ignored; encoding or decoding is continued without further notice.
Implements the 'replace' error handling.
Substitutes ?
Malformed data is replaced by a backslashed escape sequence.
On encoding, use the hexadecimal form of Unicode
code point with formats \xhh \uxxxx \Uxxxxxxxx.
On decoding, use the hexadecimal form of byte value
with format \xhh.
Implements the 'xmlcharrefreplace' error handling (for encoding within text encoding only).
The unencodable character is replaced by an appropriate XML/HTML numeric character reference, which is a
decimal form of Unicode code point with format &#num; .
The unencodable character is replaced by a \N{...} escape sequence.
The set of characters that appear in the
braces is the Name property from Unicode Character Database.
SMALL LETTER SHARP S} .
Encodes the object input and returns a tuple (output object, length consumed).
For instance, text encod-
ing converts a string object to a bytes object using a particular character set encoding (e.g., cp1252 or
iso-8859-1).
The errors argument defines the error handling to apply.
It defaults to 'strict' handling.
The method may not store state in the Codec instance.
Use StreamWriter for codecs which have to
keep state in order to make encoding efficient.
The encoder must be able to handle zero length input and return an empty object of the output object type in
this situation.
Decodes the object input and returns a tuple (output object, length consumed).
For instance, for a text encod-
ing, decoding converts a bytes object encoded using a particular character set encoding to a string object.
For text encodings and bytes-to-bytes codecs, input must be a bytes object or one which provides the read-only
buffer interface – for example, buffer objects and memory mapped files.
The errors argument defines the error handling to apply.
It defaults to 'strict' handling.
The method may not store state in the Codec instance.
Use StreamReader for codecs which have to
keep state in order to make decoding efficient.
The decoder must be able to handle zero length input and return an empty object of the output object type in
this situation.
Incremental Encoding and Decoding
The IncrementalEncoder and IncrementalDecoder classes provide the basic interface for incremental en-
coding and decoding.
Encoding/decoding the input isn’t done with one call to the stateless encoder/decoder function,
but with multiple calls to the encode()/decode() method of the incremental encoder/decoder.
The incremental
encoder/decoder keeps track of the encoding/decoding process during method calls.
The joined output of calls to the encode()/decode() method is the same as if all the single inputs were joined into
one, and this input was encoded/decoded with the stateless encoder/decoder.
IncrementalEncoder Objects
The IncrementalEncoder class is used for encoding an input in multiple steps.
It defines the following methods
which every incremental encoder must define in order to be compatible with the Python codec registry.
Constructor for an IncrementalEncoder instance.
All incremental encoders must provide this constructor interface.
They are free to add additional keyword argu-
ments, but only the ones defined here are used by the Python codec registry.
The IncrementalEncoder may implement different error handling schemes by providing the errors keyword
argument.
The errors argument will be assigned to an attribute of the same name.
Assigning to this attribute makes it possible
to switch between different error handling strategies during the lifetime of the IncrementalEncoder object.
Encodes object (taking the current state of the encoder into account) and returns the resulting encoded object.
If this is the last call to encode() final must be true (the default is false).
The output is discarded: call .encode(object, final=True),
passing an empty byte or text string if necessary, to reset the encoder and to get the output.
The implementation should make sure that
0 is the most common state.
IncrementalDecoder Objects
The IncrementalDecoder class is used for decoding an input in multiple steps.
It defines the following methods
which every incremental decoder must define in order to be compatible with the Python codec registry.
Constructor for an IncrementalDecoder instance.
All incremental decoders must provide this constructor interface.
They are free to add additional keyword argu-
ments, but only the ones defined here are used by the Python codec registry.
The IncrementalDecoder may implement different error handling schemes by providing the errors keyword
argument.
See Error Handlers for possible values.
The errors argument will be assigned to an attribute of the same name.
Assigning to this attribute makes it possible
to switch between different error handling strategies during the lifetime of the IncrementalDecoder object.
Decodes object (taking the current state of the decoder into account) and returns the resulting decoded object.
If this is the last call to decode() final must be true (the default is false).
If final is true the decoder must
decode the input completely and must flush all buffers.
If this isn’t possible (e.g. because of incomplete byte
sequences at the end of the input) it must initiate error handling just like in the stateless case (which might
raise an exception).
This must be a tuple with two items, the first must be the buffer
containing the still undecoded input.
The second must be an integer and can be additional state info.
If this additional state
info is 0 it must be possible to set the decoder to the state which has no input buffered and 0 as the additional
state info, so that feeding the previously buffered input to the decoder returns it to the previous state without
producing any output.
See encodings.utf_8 for an example of how this is done.
StreamWriter Objects
The StreamWriter class is a subclass of Codec and defines the following methods which every stream writer must
define in order to be compatible with the Python codec registry.
StreamWriter(stream, errors=’strict’)
Constructor for a StreamWriter instance.
All stream writers must provide this constructor interface.
They are free to add additional keyword arguments, but
only the ones defined here are used by the Python codec registry.
The stream argument must be a file-like object open for writing text or binary data, as appropriate for the specific
codec.
The StreamWriter may implement different error handling schemes by providing the errors keyword argument.
See Error Handlers for the standard error handlers the underlying stream codec may support.
The errors argument will be assigned to an attribute of the same name.
Writes the object’s contents encoded to the stream.
Infinite
or very large iterables are not supported.
The standard bytes-to-bytes codecs do not support this method.
Calling this method should ensure that the data on the output is put into a clean state that allows appending
of new fresh data without having to rescan the whole stream to recover state.
In addition to the above methods, the StreamWriter must also inherit all other methods and attributes from the
underlying stream.
StreamReader Objects
The StreamReader class is a subclass of Codec and defines the following methods which every stream reader must
define in order to be compatible with the Python codec registry.
Constructor for a StreamReader instance.
All stream readers must provide this constructor interface.
They are free to add additional keyword arguments, but
only the ones defined here are used by the Python codec registry.
The stream argument must be a file-like object open for reading text or binary data, as appropriate for the specific
codec.
The StreamReader may implement different error handling schemes by providing the errors keyword argument.
See Error Handlers for the standard error handlers the underlying stream codec may support.
The errors argument will be assigned to an attribute of the same name.
Assigning to this attribute makes it possible
to switch between different error handling strategies during the lifetime of the StreamReader object.
The set of allowed values for the errors argument can be extended with register_error().
The chars argument indicates the number of decoded code points or bytes to return.
The read() method
will never return more data than requested, but it might return less, if there is not enough available.
The size argument indicates the approximate maximum number of encoded bytes or code points to read for
decoding.
The decoder can modify this setting as appropriate.
The default value -1 indicates to read and
decode as much as possible.
This parameter is intended to prevent having to decode huge files in one step.
The firstline flag indicates that it would be sufficient to only return the first line, if there are decoding errors
on later lines.
The method should use a greedy read strategy meaning that it should read as much data as is allowed within the
definition of the encoding and the given size, e.g. if optional encoding endings or state markers are available
on the stream, these should be read too.
If keepends is false line-endings will be stripped from the lines returned.
Line-endings are implemented using the codec’s decode() method and are included in the list entries if
keepends is true.
Note that no stream repositioning should take place.
This method is primarily intended to be able to recover
from decoding errors.
In addition to the above methods, the StreamReader must also inherit all other methods and attributes from the
underlying stream.
StreamReaderWriter Objects
The StreamReaderWriter is a convenience class that allows wrapping streams which work in both read and write
modes.
The design is such that one can use the factory functions returned by the lookup() function to construct the instance.
class codecs.
StreamReaderWriter(stream, Reader, Writer, errors=’strict’)
Creates a StreamReaderWriter instance.
Reader and Writer must be factory
functions or classes providing the StreamReader and StreamWriter interface resp.
Error handling is done
in the same way as defined for the stream readers and writers.
StreamReaderWriter instances define the combined interfaces of StreamReader and StreamWriter classes.
They inherit all other methods and attributes from the underlying stream.
The design is such that one can use the factory functions returned by the lookup() function to construct the instance.
class codecs.
StreamRecoder(stream, encode, decode, Reader, Writer, errors=’strict’)
Creates a StreamRecoder instance which implements a two-way conversion: encode and decode work on the
frontend — the data visible to code calling read() and write(), while Reader and Writer work on the backend
— the data in stream.
You can use these objects to do transparent transcodings, e.g., from Latin-1 to UTF-8 and back.
The stream argument must be a file-like object.
The encode and decode arguments must adhere to the Codec interface.
Reader and Writer must be factory functions
or classes providing objects of the StreamReader and StreamWriter interface respectively.
Error handling is done in the same way as defined for the stream readers and writers.
StreamRecoder instances define the combined interfaces of StreamReader and StreamWriter classes.
They
inherit all other methods and attributes from the underlying stream.
Once a string object is used outside of CPU and memory, endianness and how these arrays are
stored as bytes become an issue.
As with other codecs, serialising a string into a sequence of bytes is known as encoding,
and recreating the string from the sequence of bytes is known as decoding.
There are a variety of different text serialisation
codecs, which are collectivity referred to as text encodings.
The simplest text encoding (called 'latin-1' or 'iso-8859-1') maps the code points 0–255 to the bytes 0x0–
0xff, which means that a string object that contains code points above U+00FF can’t be encoded with this codec.
Doing so will raise a UnicodeEncodeError that looks like the following (although the details of the error message
may differ): UnicodeEncodeError: 'latin-1' codec can't encode character '\u1234' in
position 3: ordinal not in range(256).
There’s another group of encodings (the so called charmap encodings) that choose a different subset of all Unicode
code points and how these code points are mapped to the bytes 0x0–0xff.
To see how this is done simply open e.g.
encodings/cp1252.py (which is an encoding that is used primarily on Windows).
There’s a string constant with
256 characters that shows you which character is mapped to which byte value.
All of these encodings can only encode 256 of the 1114112 code points defined in Unicode.
A simple and straightfor-
ward way that can store each Unicode code point, is to store each code point as four consecutive bytes.
There are two
possibilities: store the bytes in big endian or in little endian order.
These two encodings are called UTF-32-BE and
UTF-32-LE respectively.
Their disadvantage is that if e.g. you use UTF-32-BE on a little endian machine you will
always have to swap bytes on encoding and decoding.
UTF-32 avoids this problem: bytes will always be in natural endi-
anness.
When these bytes are read by a CPU with a different endianness, then bytes have to be swapped though.
To be
able to detect the endianness of a UTF-16 or UTF-32 byte sequence, there’s the so called BOM (“Byte Order Mark”).
This is the Unicode character U+FEFF.
This character can be prepended to every UTF-16 or UTF-32 byte sequence.
The byte swapped version of this character (0xFFFE) is an illegal character that may not appear in a Unicode text.
So
when the first character in a UTF-16 or UTF-32 byte sequence appears to be a U+FFFE the bytes have to be swapped
on decoding.
Unfortunately the character U+FEFF had a second purpose as a ZERO WIDTH NO-BREAK SPACE: a
character that has no width and doesn’t allow a word to be split.
It can e.g. be used to give hints to a ligature algorithm.
With Unicode 4.0 using U+FEFF as a ZERO WIDTH NO-BREAK SPACE has been deprecated (with U+2060 (WORD
JOINER) assuming this role).
UTF-8.
UTF-8 is an 8-bit encoding,
which means there are no issues with byte order in UTF-8.
Each byte in a UTF-8 byte sequence consists of two parts:
marker bits (the most significant bits) and payload bits.
The marker bits are a sequence of zero to four 1 bits followed by
a 0 bit.
As UTF-8 is an 8-bit encoding no BOM is required and any U+FEFF character in the decoded string (even if it’s the first
character) is treated as a ZERO WIDTH NO-BREAK SPACE.
Without external information it’s impossible to reliably determine which encoding was used for encoding a string.
Each
charmap encoding can decode any random byte sequence.
However that’s not possible with UTF-8, as UTF-8 byte
sequences have a structure that doesn’t allow arbitrary byte sequences.
To increase the reliability with which a UTF-8
encoding can be detected, Microsoft invented a variant of UTF-8 (that Python calls "utf-8-sig") for its Notepad
program: Before any of the Unicode characters is written to the file, a UTF-8 encoded BOM (which looks like this as a
byte sequence: 0xef, 0xbb, 0xbf) is written.
As it’s rather improbable that any charmap encoded file starts with these
byte values (which would e.g. map to
LATIN SMALL LETTER I WITH DIAERESIS
RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
INVERTED QUESTION MARK
in iso-8859-1), this increases the probability that a utf-8-sig encoding can be correctly guessed from the byte se-
quence.
So here the BOM is not used to be able to determine the byte order used for generating the byte sequence, but
as a signature that helps in guessing the encoding.
On encoding the utf-8-sig codec will write 0xef, 0xbb, 0xbf as the
first three bytes to the file.
On decoding utf-8-sig will skip those three bytes if they appear as the first three bytes in
the file.
In UTF-8, the use of the BOM is discouraged and should generally be avoided.
The following table lists the codecs by name, together with a few common aliases, and the languages for which
the encoding is likely used.
Neither the list of aliases nor the list of languages is meant to be exhaustive.
Notice that
spelling alternatives that only differ in case or use a hyphen instead of an underscore are also valid aliases; therefore, e.g.
'utf-8' is a valid alias for the 'utf_8' codec.
CPython implementation detail: Some common encodings can bypass the codecs lookup machinery to improve per-
formance.
These optimization opportunities are only recognized by CPython for a limited set of (case insensitive) aliases:
utf-8, utf8, latin-1, latin1, iso-8859-1, iso8859-1, mbcs (Windows only), ascii, us-ascii, utf-16, utf16, utf-32, utf32, and
the same using underscores instead of dashes.
Using alternative aliases for these encodings may result in slower execution.
Optimization opportunity recognized for us-ascii.
Many of the character sets support the same languages.
They vary in individual characters (e.g. whether the EURO SIGN
is supported or not), and in the assignment of characters to code positions.
The utf-16* and utf-32* encoders no longer allow surrogate code points (U+D800–U+DFFF)
to be encoded.
The utf-32* decoders no longer decode byte sequences that correspond to surrogate code points.
Changed in version 3.8: cp65001 is now an alias to utf_8.
These
are listed in the tables below based on the expected input and output types (note that while text encodings are the most
common use case for codecs, the underlying codec infrastructure supports arbitrary data transforms rather than just text
encodings).
For asymmetric codecs, the stated meaning describes the encoding direction.
Only
errors='strict'
is
sup-
ported.
New in version 3.6.
palmos
Encoding of PalmOS 3.5.
Stateful
codecs are not supported.
Existing backslashes are not
escaped in any way.
It is used in the
Python pickle protocol.
The error
handler is ignored.
Decode
from Latin-1 source code.
Beware
that Python source code actually
uses UTF-8 by default.
Changed in version 3.8: “unicode_internal” codec is removed.
Binary Transforms
The following codecs provide binary transforms: bytes-like object to bytes mappings.
They are not supported by
bytes.decode() (which only produces str output).
Changed in version 3.4: accepts any bytes-like object as
input for encoding and decoding
base64.
encodebytes() /
base64.
decodebytes()
bz2_codec
bz2
Compress the operand using bz2.
Changed in version 3.4: Restoration of the aliases for the binary transforms.
Text Transforms
The following codec provides a text transform: a str to str mapping.
It is not supported by str.encode() (which
only produces bytes output).
Codec
Aliases
Meaning
rot_13
rot13
Return the Caesar-cypher encryption of the operand.
New in version 3.2: Restoration of the rot_13 text transform.
Changed in version 3.4: Restoration of the rot13 alias.
This module implements RFC 3490 (Internationalized Domain Names in Applications) and RFC 3492 (Nameprep:
A Stringprep Profile for Internationalized Domain Names (IDN)).
It builds upon the punycode encoding and
stringprep.
If you need the IDNA 2008 standard from RFC 5891 and RFC 5895, use the third-party idna module.
These RFCs together define a protocol to support non-ASCII characters in domain names.
A domain name containing
non-ASCII characters (such as www.Alliancefrançaise.nu) is converted into an ASCII-compatible encoding
(ACE, such as www.xn--alliancefranaise-npb.nu).
This conversion is carried out in the application; if possible invisible to the user: The application should transparently
convert Unicode domain labels to IDNA on the wire, and convert back ACE labels to Unicode before presenting them to
the user.
Python supports this conversion in several ways: the idna codec performs conversion between Unicode and ACE,
separating an input string into labels based on the separator characters defined in section 3.1 of RFC 3490 and converting
each label to ACE as required, and conversely separating an input byte string into labels based on the .
Furthermore, the socket module transparently converts Unicode host
names to ACE, so that applications need not be concerned about converting host names themselves when they pass them
to the socket module.
On top of that, modules that have host names as function parameters, such as http.client and
ftplib, accept Unicode host names (http.client then also transparently sends an IDNA hostname in the Host
field if it sends that field at all).
When receiving host names from the wire (such as in reverse name lookup), no automatic conversion to Unicode is
performed: applications wishing to present such host names to the user should decode them to Unicode.
The module encodings.idna also implements the nameprep procedure, which performs certain normalizations on
host names, to achieve case-insensitivity of international domain names, and to unify similar characters.
The nameprep
functions can be used directly if desired.
encodings.idna.nameprep(label)
Return the nameprepped version of label.
The implementation currently assumes query strings, so
AllowUnassigned is true.
ToASCII(label)
Convert a label to ASCII, as specified in RFC 3490.
UseSTD3ASCIIRules is assumed to be false.
Availability: Windows.
Changed in version 3.3: Support any error handler.
Changed in version 3.2: Before 3.2, the errors argument was ignored; 'replace' was always used to encode, and
'ignore' to decode.
On encoding, a UTF-8 encoded BOM will be prepended to the
UTF-8 encoded bytes.
For the stateful encoder this is only done once (on the first write to the byte stream).
On decoding,
an optional UTF-8 encoded BOM at the start of the data will be skipped.
Binary Data Services
CHAPTER
EIGHT
DATA TYPES
The modules described in this chapter provide a variety of specialized data types such as dates and times, fixed-type
arrays, heap queues, double-ended queues, and enumerations.
Python also provides some built-in data types, in particular, dict, list, set and frozenset, and tuple.
The
str class is used to hold Unicode strings, and the bytes and bytearray classes are used to hold binary data.
The following modules are documented in this chapter:
8.1 datetime — Basic date and time types
Source code: Lib/datetime.py
The datetime module supplies classes for manipulating dates and times.
While date and time arithmetic is supported, the focus of the implementation is on efficient attribute extraction for output
formatting and manipulation.
Skip to the format codes.
See also:
Module calendar General calendar related functions.
Module time Time access and conversions.
Module zoneinfo Concrete time zones representing the IANA time zone database.
Package dateutil Third-party library with expanded time zone and parsing support.
Package DateType Third-party library that introduces distinct static types to e.g. allow static type checkers to differen-
tiate between naive and aware datetimes.
With sufficient knowledge of applicable algorithmic and political time adjustments, such as time zone and daylight saving
time information, an aware object can locate itself relative to other aware objects.
An aware object represents a specific
moment in time that is not open to interpretation.1
A naive object does not contain enough information to unambiguously locate itself relative to other date/time objects.
Whether a naive object represents Coordinated Universal Time (UTC), local time, or time in some other timezone is
purely up to the program, just like it is up to the program whether a particular number represents metres, miles, or mass.
Naive objects are easy to understand and to work with, at the cost of ignoring some aspects of reality.
For applications requiring aware objects, datetime and time objects have an optional time zone information attribute,
tzinfo, that can be set to an instance of a subclass of the abstract tzinfo class.
These tzinfo objects capture
information about the offset from UTC time, the time zone name, and whether daylight saving time is in effect.
Only one concrete tzinfo class, the timezone class, is supplied by the datetime module.
The timezone class
can represent simple timezones with fixed offsets from UTC, such as UTC itself or North American EST and EDT
timezones.
Supporting timezones at deeper levels of detail is up to the application.
The rules for time adjustment across
the world are more political than rational, change frequently, and there is no standard suitable for every application aside
from UTC.
MINYEAR
The smallest year number allowed in a date or datetime object.
MINYEAR is 1.
datetime.
MAXYEAR
The largest year number allowed in a date or datetime object.
MAXYEAR is 9999.
UTC
Alias for the UTC timezone singleton datetime.timezone.utc.
New in version 3.11.
At-
tributes: year, month, and day.
Attributes: hour, minute, second, microsecond, and
tzinfo.
Attributes:
year, month, day, hour, minute, second,
microsecond, and tzinfo.
These are used by the datetime and time classes to
provide a customizable notion of time adjustment (for example, to account for time zone and/or daylight saving
time).
New in version 3.2.
Objects of these types are immutable.
Subclass relationships:
object
timedelta
tzinfo
timezone
time
date
datetime
Common Properties
The date, datetime, time, and timezone types share these common features:
• Objects of these types are immutable.
Determining if an Object is Aware or Naive
Objects of the date type are always naive.
An object of type time or datetime may be aware or naive.
A datetime object d is aware if both of the following hold:
1.
A time object t is aware if both of the following hold:
1. t.tzinfo is not None
2. t.tzinfo.utcoffset(None) does not return None.
Otherwise, t is naive.
The distinction between aware and naive doesn’t apply to timedelta objects.
All arguments are optional and default to 0.
Arguments may be integers or floats, and may be positive or negative.
Only days, seconds and microseconds are stored internally.
A millisecond is converted to 1000 microseconds.
A minute is converted to 60 seconds.
An hour is converted to 3600 seconds.
If any argument is a float and there are fractional microseconds, the fractional microseconds left over from all
arguments are combined and their sum is rounded to the nearest microsecond using round-half-to-even tiebreaker.
If no argument is a float, the conversion and normalization processes are exact (no information is lost).
If the normalized value of days lies outside the indicated range, OverflowError is raised.
Note that normalization of negative values may be surprising at first.
For example:
(-1, 86399, 999999)
Class attributes:
timedelta.min
The most negative timedelta object, timedelta(-999999999).
Note that, because of normalization, timedelta.max > -timedelta.min.
Instance attributes (read-only):
Attribute
Value
days
Between -999999999 and 999999999 inclusive
seconds
Between 0 and 86399 inclusive
microseconds
Between 0 and 999999 inclusive
Supported operations:
Operation
Result
t1 = t2 + t3
Sum of t2 and t3.
Afterwards t1-t2 == t3 and t1-t3 == t2 are true.
Afterwards t1 == t2 - t3 and t2 == t1 + t3 are true.
The result is rounded to the nearest multiple of
timedelta.resolution using round-half-to-even.
Returns a float object.
The result is rounded to the nearest multiple of
timedelta.resolution using round-half-to-even.
The floor is computed and the remainder (if any) is thrown away.
In the second case,
an integer is returned.
The remainder is computed as a timedelta object.
This leads
to somewhat unusual results for negative timedeltas.
For example:
8.1.
In addition to the operations listed above, timedelta objects support certain additions and subtractions with date and
datetime objects (see below).
Floor division and true division of a timedelta object by another timedelta object are now
supported, as are remainder operations and the divmod() function.
True division and multiplication of a timedelta
object by a float object are now supported.
Comparisons of timedelta objects are supported, with some caveats.
The comparisons == or != always return a bool, no matter the type of the compared object:
True
False
For all other comparisons (such as < and >), when a timedelta object is compared to an object of a different type,
TypeError is raised:
True
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: '>' not supported between instances of 'datetime.timedelta' and 'int'
In Boolean contexts, a timedelta object is considered to be true if and only if it isn’t equal to timedelta(0).
Instance methods:
timedelta.total_seconds()
Return
the
total
number
of
seconds
contained
in
the
duration.
Equivalent
to
td /
timedelta(seconds=1).
For interval units other than seconds, use the division form directly (e.g.
td / timedelta(microseconds=1)).
True
31536000.0
Examples of timedelta arithmetic:
datetime.timedelta(days=3650)

datetime.timedelta(days=3285)
(datetime.timedelta(days=1095), 3)
8.1.5 date Objects
A date object represents a date (year, month and day) in an idealized calendar, the current Gregorian calendar indefinitely
extended in both directions.
All arguments are required.
Arguments must be integers, in the following ranges:
• MINYEAR <= year <= MAXYEAR
• 1 <= month <= 12
• 1 <= day <= number of days in the given month and year
If an argument outside those ranges is given, ValueError is raised.
Other constructors, all class methods:
classmethod date.today()
Return the current local date.
This is equivalent to date.fromtimestamp(time.time()).
This matches the definition of the “proleptic Gregorian” calendar in Dershowitz and Reingold’s book Calendrical Calculations, where it’s the base
calendar for all computations.
See the book for algorithms for converting between proleptic Gregorian ordinals and many other calendar systems.
This may raise OverflowError, if the timestamp is out of the range of values supported by the platform C
localtime() function, and OSError on localtime() failure.
It’s common for this to be restricted to
years from 1970 through 2038.
Note that on non-POSIX systems that include leap seconds in their notion of a
timestamp, leap seconds are ignored by fromtimestamp().
Changed in version 3.3: Raise OverflowError instead of ValueError if the timestamp is out of the range
of values supported by the platform C localtime() function.
Raise OSError instead of ValueError on
localtime() failure.
ValueError is raised unless 1 <= ordinal <= date.max.toordinal().
Changed in version 3.11: Previously, this method only supported the format YYYY-MM-DD.
classmethod date.fromisocalendar(year, week, day)
Return a date corresponding to the ISO calendar date specified by year, week and day.
This is the inverse of the
function date.isocalendar().
New in version 3.8.
Class attributes:
date.min
The earliest representable date, date(MINYEAR, 1, 1).
Instance attributes (read-only):
date.year
Between MINYEAR and MAXYEAR inclusive.
Supported operations:
Operation
Result
date2 = date1 + timedelta
date2 will be timedelta.days days after date1.
Afterward date2 - date1 == timedelta.days.
OverflowError is raised if date2.year would be smaller than MINYEAR
or larger than MAXYEAR.
Date
comparison raises TypeError if the other comparand isn’t also a date object.
However, NotImplemented
is returned instead if the other comparand has a timetuple() attribute.
This hook gives other kinds of date
objects a chance at implementing mixed-type comparison.
If not, when a date object is compared to an object
of a different type, TypeError is raised unless the comparison is == or !
The latter cases return False or
True, respectively.
In Boolean contexts, all date objects are considered to be true.
Instance methods:
date.replace(year=self.year, month=self.month, day=self.day)
Return a date with the same value, except for those parameters given new values by whichever keyword arguments
are specified.
Example:
datetime.date(2002, 12, 26)
date.timetuple()
Return a time.struct_time such as returned by time.localtime().
The hours, minutes and seconds are 0, and the DST flag is -1.
For any date object
d, date.fromordinal(d.toordinal()) == d.
date.weekday()
Return the day of the week as an integer, where Monday is 0 and Sunday is 6.
For example, date(2002, 12,
4).weekday() == 2, a Wednesday.
See also isoweekday().
date.isoweekday()
Return the day of the week as an integer, where Monday is 1 and Sunday is 7.
For example, date(2002, 12,
4).isoweekday() == 3, a Wednesday.
See also weekday(), isocalendar().
The ISO calendar is a widely used variant of the Gregorian calendar.3
The ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday.
The first
week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday.
This is called week
number 1, and the ISO year of that Thursday is the same as its Gregorian year.
IsoCalendarDate(year=2004, week=1, weekday=1)
datetime.
For a date d, str(d) is equivalent to d.isoformat().
Format codes referring to hours,
minutes or seconds will see 0 values.
Same as date.strftime().
This makes it possible to specify a format string for a date object in for-
matted string literals and when using str.format().
See also strftime() and strptime() Behavior and date.
March 2002'
'Mon Mar 11 00:00:00 2002'
'The day is 11, the month is March.
Like a date object, datetime assumes the current Gregorian calendar extended in both directions; like a time object,
datetime assumes there are exactly 3600*24 seconds in every day.
The year, month and day arguments are required.
If an argument outside those ranges is given, ValueError is raised.
Added the fold argument.
Other constructors, all class methods:
classmethod datetime.today()
Return the current local datetime, with tzinfo None.
This method is functionally equivalent to now(), but without a tz parameter.
If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than
can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms
supplying the C gettimeofday() function).
If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz’s
time zone.
This function is preferred over today() and utcnow().
This is like now(), but returns the current UTC date and time, as a naive datetime object.
An aware current
UTC datetime can be obtained by calling datetime.now(timezone.utc).
See also now().
Warning:
Because naive datetime objects are treated by many datetime methods as local times, it is
preferred to use aware datetimes to represent times in UTC.
As such, the recommended way to create an object
representing the current time in UTC is by calling datetime.now(timezone.utc).
Deprecated since version 3.12: Use datetime.now() with UTC instead.
If optional argument tz is None or not specified, the timestamp is converted to the platform’s local date and time,
and the returned datetime object is naive.
If tz is not None, it must be an instance of a tzinfo subclass, and the timestamp is converted to tz’s time zone.
It’s common for this to be restricted to years in 1970 through 2038.
Note that on non-POSIX systems that
include leap seconds in their notion of a timestamp, leap seconds are ignored by fromtimestamp(), and then
it’s possible to have two timestamps differing by a second that yield identical datetime objects.
This method is
preferred over utcfromtimestamp().
Changed in version 3.3: Raise OverflowError instead of ValueError if the timestamp is out of the range
of values supported by the platform C localtime() or gmtime() functions.
Raise OSError instead of
ValueError on localtime() or gmtime() failure.
Return the UTC datetime corresponding to the POSIX timestamp, with tzinfo None.
This may raise OverflowError, if the timestamp is out of the range of values supported by the platform C
gmtime() function, and OSError on gmtime() failure.
It’s common for this to be restricted to years in 1970
through 2038.
To get an aware datetime object, call fromtimestamp():
8.1.
Warning:
Because naive datetime objects are treated by many datetime methods as local times, it is
preferred to use aware datetimes to represent times in UTC.
As such, the recommended way to create an ob-
ject representing a specific timestamp in UTC is by calling datetime.fromtimestamp(timestamp,
tz=timezone.utc).
Changed in version 3.3: Raise OverflowError instead of ValueError if the timestamp is out of the range
of values supported by the platform C gmtime() function.
Raise OSError instead of ValueError on
gmtime() failure.
Deprecated since version 3.12: Use datetime.fromtimestamp() with UTC instead.
ValueError is raised unless 1 <= ordinal <= datetime.max.toordinal().
The hour, minute,
second and microsecond of the result are all 0, and tzinfo is None.
classmethod datetime.combine(date, time, tzinfo=time.tzinfo)
Return a new datetime object whose date components are equal to the given date object’s, and whose time
components are equal to the given time object’s.
If the tzinfo argument is provided, its value is used to set the
tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used.
If the date argument
is a datetime object, its time components and tzinfo attributes are ignored.
For any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo).
Added the tzinfo argument.
Time zone offsets may have fractional seconds.
The T separator may be replaced by any single unicode character.
Ordinal dates are not currently supported.
Fractional hours and minutes are not supported.
New in version 3.7.
Changed in version 3.11: Previously, this method only supported formats that could be emitted by date.
isoformat() or datetime.isoformat().
classmethod datetime.fromisocalendar(year, week, day)
Return a datetime corresponding to the ISO calendar date specified by year, week and day.
The non-date
components of the datetime are populated with their normal default values.
This is the inverse of the function
datetime.isocalendar().
New in version 3.8.
ValueError is raised if the date_string and format can’t be parsed by time.strptime() or if it returns a
value which isn’t a time tuple.
See also strftime() and strptime() Behavior and datetime.fromisoformat().
Class attributes:
datetime.min
The earliest representable datetime, datetime(MINYEAR, 1, 1, tzinfo=None).
Instance attributes (read-only):
datetime.year
Between MINYEAR and MAXYEAR inclusive.
In [0, 1].
Used to disambiguate wall times during a repeated interval.
The value 0 (1) represents the earlier (later) of the two moments with the same wall time
representation.
New in version 3.6.
Supported operations:
Operation
Result
datetime2 = datetime1 + timedelta
(1)
datetime2 = datetime1 - timedelta
(2)
timedelta = datetime1 - datetime2
(3)
datetime1 < datetime2
Compares datetime to datetime.
The result has the same tzinfo attribute as the input datetime, and
datetime2 - datetime1 == timedelta after.
OverflowError is raised if datetime2.year would be smaller than
MINYEAR or larger than MAXYEAR.
Note that no time zone adjustments are done even if the input is an aware
object.
As for addition, the result has the same
tzinfo attribute as the input datetime, and no time zone adjustments are done even if the input is aware.
If one is aware and the other is naive, TypeError is raised.
If both are naive, or both are aware and have the same tzinfo attribute, the tzinfo attributes are ignored, and
the result is a timedelta object t such that datetime2 + t == datetime1.
No time zone adjustments
are done in this case.
If both are aware and have different tzinfo attributes, a-b acts as if a and b were first converted to
naive UTC datetimes first.
The result is (a.replace(tzinfo=None) - a.utcoffset()) - (b.
replace(tzinfo=None) - b.utcoffset()) except that the implementation never overflows.
If one comparand is naive and the other is aware, TypeError is raised if an order comparison is attempted.
For
equality comparisons, naive instances are never equal to aware instances.
If both comparands are aware, and have the same tzinfo attribute, the common tzinfo attribute is ignored
and the base datetimes are compared.
If both comparands are aware and have different tzinfo attributes, the
comparands are first adjusted by subtracting their UTC offsets (obtained from self.utcoffset()).
Equality comparisons between aware and naive datetime instances don’t raise
TypeError.
However,
NotImplemented is returned instead if the other comparand has a timetuple() attribute.
This hook gives
other kinds of date objects a chance at implementing mixed-type comparison.
If not, when a datetime object
is compared to an object of a different type, TypeError is raised unless the comparison is == or !
The latter
cases return False or True, respectively.
Instance methods:
datetime.date()
Return date object with same year, month and day.
See also method
timetz().
The fold value is copied to the returned time object.
See also method
time().
The fold value is copied to the returned time object.
Note that tzinfo=None can be specified to create a naive datetime from an aware
datetime with no conversion of date and time data.
Added the fold argument.
If self is naive, it is presumed to represent time in the system timezone.
If called without arguments (or with tz=None) the system local timezone is assumed for the target timezone.
The
.tzinfo attribute of the converted datetime instance will be set to an instance of timezone with the zone name
and offset obtained from the OS.
If self.tzinfo is tz, self.astimezone(tz) is equal to self: no adjustment of date or time data is per-
formed.
Else the result is local time in the timezone tz, representing the same UTC time as self: after astz
= dt.astimezone(tz), astz - astz.utcoffset() will have the same date and time data as dt -
dt.utcoffset().
If you merely want to attach a time zone object tz to a datetime dt without adjustment of date and time data, use
dt.replace(tzinfo=tz).
If you merely want to remove the time zone object from an aware datetime dt
without conversion of date and time data, use dt.replace(tzinfo=None).
Note that the default tzinfo.fromutc() method can be overridden in a tzinfo subclass to affect the result
returned by astimezone().
Changed in version 3.3: tz now can be omitted.
The astimezone() method can now be called on naive instances that are presumed to
represent system local time.
If tzinfo is None, returns None, else returns self.tzinfo.utcoffset(self), and raises an exception
if the latter doesn’t return None or a timedelta object with magnitude less than one day.
The UTC offset is not restricted to a whole number of minutes.
If tzinfo is None, returns None, else returns self.tzinfo.dst(self), and raises an exception if the
latter doesn’t return None or a timedelta object with magnitude less than one day.
The DST offset is not restricted to a whole number of minutes.
If tzinfo is None, returns None, else returns self.tzinfo.tzname(self), raises an exception if the
latter doesn’t return None or a string object,
datetime.timetuple()
Return a time.struct_time such as returned by time.localtime().
The tm_isdst flag of the result is set according to the
dst() method: tzinfo is None or dst() returns None, tm_isdst is set to -1; else if dst() returns a
non-zero value, tm_isdst is set to 1; else tm_isdst is set to 0.
If datetime instance d is naive, this is the same as d.timetuple() except that tm_isdst is forced to 0
regardless of what d.dst() returns.
DST is never in effect for a UTC time.
If d is aware, d is normalized to UTC time, by subtracting d.utcoffset(), and a time.struct_time for
the normalized time is returned.
Note that an OverflowError may be raised if
d.year was MINYEAR or MAXYEAR and UTC adjustment spills over a year boundary.
Warning:
Because naive datetime objects are treated by many datetime methods as local
times, it is preferred to use aware datetimes to represent times in UTC; as a result, using datetime.
The same as self.date().toordinal().
datetime.timestamp()
Return POSIX timestamp corresponding to the datetime instance.
The return value is a float similar to that
returned by time.time().
Naive datetime instances are assumed to represent local time and this method relies on the platform C
mktime() function to perform the conversion.
Since datetime supports wider range of values than
mktime() on many platforms, this method may raise OverflowError or OSError for times far in the
past or far in the future.
For aware datetime instances, the return value is computed as:
(dt - datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()
New in version 3.3.
The timestamp() method uses the fold attribute to disambiguate the times during a
repeated interval.
Note: There is no method to obtain the POSIX timestamp directly from a naive datetime instance representing
UTC time.
If your application uses this convention and your system timezone is not set to UTC, you can obtain the
POSIX timestamp by supplying tzinfo=timezone.utc:
timestamp = dt.replace(tzinfo=timezone.utc).timestamp()
or by calculating the timestamp directly:
timestamp = (dt - datetime(1970, 1, 1)) / timedelta(seconds=1)
datetime.weekday()
Return the day of the week as an integer, where Monday is 0 and Sunday is 6.
The same as self.date().
See also isoweekday().
The same as self.date().
See also weekday(), isocalendar().
The same as self.date().
For example:

"""A time zone with an arbitrary, constant -06:39 offset.
Same as 'seconds' if microsecond is 0, same as 'microseconds' otherwise.
HH:MM:SS.
Note: Excluded time components are truncated, not rounded.
ValueError will be raised on an invalid timespec argument:
'2002-12-25T00:00'
'2015-01-01T12:30:59.000000'
New in version 3.6: Added the timespec argument.
The output string will not include time zone information, regardless of whether the input is aware or naive.
d.ctime() is equivalent to:
time.ctime(time.mktime(d.timetuple()))
on platforms where the native C ctime() function (which time.ctime() invokes, but which datetime.
See also strftime() and
strptime() Behavior and datetime.isoformat().
Same as datetime.strftime().
This makes it possible to specify a format string for a datetime object
in formatted string literals and when using str.format().
See also strftime() and strptime() Behavior and
datetime.isoformat().
The example below defines a tzinfo subclass capturing time zone information for Kabul, Afghanistan, which used +4
UTC until 1945 and then +4:30 UTC thereafter:
from datetime import timedelta, datetime, tzinfo, timezone
class KabulTz(tzinfo):
# Kabul used +4 until 1945, when they moved to +4:30
UTC_MOVE_DATE = datetime(1944, 12, 31, 20, tzinfo=timezone.utc)


elif (1945, 1, 1, 0, 0) <= dt.timetuple()[:5] < (1945, 1, 1, 0, 30):
# An ambiguous ("imaginary") half-hour range representing
# a 'fold' in time due to the shift from +4 to +4:30.
UTC_MOVE_DATE:

else:


# Kabul does not observe daylight saving time.
True
8.1.7 time Objects
A time object represents a (local) time of day, independent of any particular day, and subject to adjustment via a tzinfo
object.
All arguments are optional.
If an argument outside those ranges is given, ValueError is raised.
All default to 0 except tzinfo, which defaults
to None.
Class attributes:
time.min
The earliest representable time, time(0, 0, 0, 0).
Instance attributes (read-only):
time.hour
In range(24).
The object passed as the tzinfo argument to the time constructor, or None if none was passed.
In [0, 1].
Used to disambiguate wall times during a repeated interval.
The value 0 (1) represents the earlier (later) of the two moments with the same wall time
representation.
New in version 3.6.
If
one comparand is naive and the other is aware, TypeError is raised if an order comparison is attempted.
For equality
comparisons, naive instances are never equal to aware instances.
If both comparands are aware, and have the same tzinfo attribute, the common tzinfo attribute is ignored and
the base times are compared.
If both comparands are aware and have different tzinfo attributes, the comparands
are first adjusted by subtracting their UTC offsets (obtained from self.utcoffset()).
In order to stop mixed-type
comparisons from falling back to the default comparison by object address, when a time object is compared to an object
of a different type, TypeError is raised unless the comparison is == or !
The latter cases return False or True,
respectively.
Equality comparisons between aware and naive time instances don’t raise TypeError.
In Boolean contexts, a time object is always considered to be true.
Changed in version 3.5: Before Python 3.5, a time object was considered to be false if it represented midnight in UTC.
This behavior was considered obscure and error-prone and has been removed in Python 3.5.
See bpo-13936 for full
details.
Other constructor:
classmethod time.fromisoformat(time_string)
Return a time corresponding to a time_string in any valid ISO 8601 format, with the following exceptions:
1.
Time zone offsets may have fractional seconds.
The leading T, normally required in cases where there may be ambiguity between a date and a time, is not
required.
Fractional seconds may have any number of digits (anything beyond 6 will be truncated).
Fractional hours and minutes are not supported.
Changed in version 3.11: Previously, this method only supported formats that could be emitted by time.
isoformat().
Instance methods:
time.replace(hour=self.hour, minute=self.minute, second=self.second, microsecond=self.microsecond,
tzinfo=self.tzinfo, *, fold=0)
Return a time with the same value, except for those attributes given new values by whichever keyword arguments
are specified.
Note that tzinfo=None can be specified to create a naive time from an aware time, without
conversion of the time data.
Added the fold argument.
Return a string representing the time in ISO 8601 format, one of:
• HH:MM:SS.ffffff, if microsecond is not 0
• HH:MM:SS, if microsecond is 0
• HH:MM:SS.ffffff+HH:MM[:SS[.ffffff]], if utcoffset() does not return None
• HH:MM:SS+HH:MM[:SS[.ffffff]], if microsecond is 0 and utcoffset() does not return
None
The optional argument timespec specifies the number of additional components of the time to include (the default
is 'auto').
Same as 'seconds' if microsecond is 0, same as 'microseconds' otherwise.
HH:MM:SS.
ValueError will be raised on an invalid timespec argument.
Example:
�→'minutes')
'12:34'
'12:34:56.000000'
'12:34:56'
New in version 3.6: Added the timespec argument.
Behavior and time.isoformat().
Same as time.strftime().
This makes it possible to specify a format string for a time object in for-
matted string literals and when using str.format().
See also strftime() and strptime() Behavior and time.
If tzinfo is None, returns None, else returns self.tzinfo.utcoffset(None), and raises an exception
if the latter doesn’t return None or a timedelta object with magnitude less than one day.
The UTC offset is not restricted to a whole number of minutes.
If tzinfo is None, returns None, else returns self.tzinfo.dst(None), and raises an exception if the
latter doesn’t return None, or a timedelta object with magnitude less than one day.
The DST offset is not restricted to a whole number of minutes.
If tzinfo is None, returns None, else returns self.tzinfo.tzname(None), or raises an exception if the
latter doesn’t return None or a string object.
Define a subclass of
tzinfo to capture information about a particular time zone.
An instance of (a concrete subclass of) tzinfo can be passed to the constructors for datetime and time
objects.
The latter objects view their attributes as being in local time, and the tzinfo object supports methods
revealing offset of local time from UTC, the name of the time zone, and DST offset, all relative to a date or time
object passed to them.
You need to derive a concrete subclass, and (at least) supply implementations of the standard tzinfo methods
needed by the datetime methods you use.
The datetime module provides timezone, a simple concrete
subclass of tzinfo which can represent timezones with fixed offset from UTC such as UTC itself or North
American EST and EDT.
Special requirement for pickling: A tzinfo subclass must have an __init__() method that can be called with
no arguments, otherwise it can be pickled but possibly not unpickled again.
This is a technical requirement that
may be relaxed in the future.
A concrete subclass of tzinfo may need to implement the following methods.
Exactly which methods are needed
depends on the uses made of aware datetime objects.
If in doubt, simply implement all of them.
tzinfo.utcoffset(dt)
Return offset of local time from UTC, as a timedelta object that is positive east of UTC.
If local time is west
of UTC, this should be negative.
This represents the total offset from UTC; for example, if a tzinfo object represents both time zone and
DST adjustments, utcoffset() should return their sum.
If the UTC offset isn’t known, return None.
Else the value returned must be a timedelta object strictly between -timedelta(hours=24) and
timedelta(hours=24) (the magnitude of the offset must be less than one day).
If utcoffset() does not return None, dst() should not return None either.
The default implementation of utcoffset() raises NotImplementedError.
The UTC offset is not restricted to a whole number of minutes.
Return timedelta(0) if DST is not in effect.
If DST is in effect, return the offset as a timedelta object
(see utcoffset() for details).
Note that DST offset, if applicable, has already been added to the UTC offset
returned by utcoffset(), so there’s no need to consult dst() unless you’re interested in obtaining DST info
separately.
For example, datetime.timetuple() calls its tzinfo attribute’s dst() method to determine
how the tm_isdst flag should be set, and tzinfo.fromutc() calls dst() to account for DST changes
when crossing time zones.
An instance tz of a tzinfo subclass that models both standard and daylight times must be consistent in this sense:
tz.utcoffset(dt) - tz.dst(dt)
must return the same result for every datetime dt with dt.tzinfo == tz For sane tzinfo subclasses,
this expression yields the time zone’s “standard offset”, which should not depend on the date or the time, but only
on geographic location.
The implementation of datetime.astimezone() relies on this, but cannot detect
violations; it’s the programmer’s responsibility to ensure it.
If a tzinfo subclass cannot guarantee this, it may be
able to override the default implementation of tzinfo.fromutc() to work correctly with astimezone()
regardless.
Most implementations of dst() will probably look like one of these two:

# a fixed-offset class:
doesn't account for DST

or:

# Code to set dston and dstoff to the time zone's DST
# transition times based on the input dt.year, and expressed
# in standard local time.
The default implementation of dst() raises NotImplementedError.
The DST offset is not restricted to a whole number of minutes.
Nothing about string names is
defined by the datetime module, and there’s no requirement that it mean anything in particular.
For example,
“GMT”, “UTC”, “-500”, “-5:00”, “EDT”, “US/Eastern”, “America/New York” are all valid replies.
Return None
if a string name isn’t known.
Note that this is a method rather than a fixed string primarily because some tzinfo
subclasses will wish to return different names depending on the specific value of dt passed, especially if the tzinfo
class is accounting for daylight time.
The default implementation of tzname() raises NotImplementedError.
These methods are called by a datetime or time object, in response to their methods of the same names.
A
datetime object passes itself as the argument, and a time object passes None as the argument.
A tzinfo subclass’s
methods should therefore be prepared to accept a dt argument of None, or of class datetime.
When None is passed, it’s up to the class designer to decide the best response.
For example, returning None is appro-
priate if the class wishes to say that time objects don’t participate in the tzinfo protocols.
It may be more useful for
utcoffset(None) to return the standard UTC offset, as there is no other convention for discovering the standard
offset.
When a datetime object is passed in response to a datetime method, dt.tzinfo is the same object as self.
The intent is that the tzinfo
methods interpret dt as being in local time, and not need worry about objects in other timezones.
This is called from the default datetime.astimezone() implementation.
When called from that, dt.
tzinfo is self, and dt’s date and time data are to be viewed as expressing a UTC time.
The purpose of
fromutc() is to adjust the date and time data, returning an equivalent datetime in self’s local time.
Most tzinfo subclasses should be able to inherit the default fromutc() implementation without problems.
It’s
strong enough to handle fixed-offset time zones, and time zones accounting for both standard and daylight time,
and the latter even if the DST transition times differ in different years.
An example of a time zone the default
fromutc() implementation may not handle correctly in all cases is one where the standard offset (from UTC)
depends on the specific date and time passed, which can happen for political reasons.
The default implementations
of astimezone() and fromutc() may not produce the result you want if the result is one of the hours
straddling the moment the standard offset changes.
For a complete and up-to-date set of DST rules
# and timezone definitions, visit the Olson Database (or try pytz):
# http://www.twinsun.com/tz/tz-link.htm
# https://sourceforge.net/projects/pytz/ (might not be up-to-date)
#
# In the US, since 2007, DST starts at 2am (standard time) on the second
# Sunday in March, which is the first Sunday on or after Mar 8.
DSTSTART_2007 = datetime(1, 3, 8, 2)
# and ends at 2am (DST time) on the first Sunday of Nov.
DSTEND_2007 = datetime(1, 11, 1, 2)
# From 1987 to 2006, DST used to start at 2am (standard time) on the first
# Sunday in April and to end at 2am (DST time) on the last
# Sunday of October, which is the first Sunday on or after Oct 25.
DSTSTART_1987_2006 = datetime(1, 4, 1, 2)
DSTEND_1987_2006 = datetime(1, 10, 25, 2)
# From 1967 to 1986, DST used to start at 2am (standard time) on the last
# Sunday in April (the one on or after April 24) and to end at 2am (DST time)
# on the last Sunday of October, which is the first Sunday
# on or after Oct 25.
DSTSTART_1967_1986 = datetime(1, 4, 24, 2)
DSTEND_1967_1986 = DSTEND_1987_2006

# Find start and end times for US DST.
For years before 1967, return
# start = end for no DST.
The default
# fromutc() implementation (called by the default astimezone()
# implementation) passes a datetime with dt.tzinfo is self.
Note that there are unavoidable subtleties twice per year in a tzinfo subclass accounting for both standard and daylight
time, at the DST transition points.
A wall time of the form 2:MM doesn’t
really make sense on that day, so astimezone(Eastern) won’t deliver a result with hour == 2 on the day DST
begins.
In Eastern, that’s times of the form 5:MM UTC on the day daylight time
ends.
The local wall clock leaps from 1:59 (daylight time) back to 1:00 (standard time) again.
Local times of the form
1:MM are ambiguous.
In the Eastern example, UTC times of the form 5:MM and 6:MM both map to 1:MM when
converted to Eastern, but earlier times have the fold attribute set to 0 and the later times have it set to 1.
UTC = 00:00:00 EDT 0
05:00:00 UTC = 01:00:00 EDT 0
06:00:00 UTC = 01:00:00 EST 1
07:00:00 UTC = 02:00:00 EST 0
Note that the datetime instances that differ only by the value of the fold attribute are considered equal in comparisons.
Applications that can’t bear wall-time ambiguities should explicitly check the value of the fold attribute or avoid using
hybrid tzinfo subclasses; there are no ambiguities when using timezone, or any other fixed-offset tzinfo subclass
(such as a class representing only EST (fixed offset -5 hours), or only EDT (fixed offset -4 hours)).
See also:
zoneinfo The datetime module has a basic timezone class (for handling arbitrary fixed offsets from
UTC) and its timezone.utc attribute (a UTC timezone instance).
It is updated periodically to
reflect changes made by political bodies to time zone boundaries, UTC offsets, and daylight-saving rules.
The timezone class is a subclass of tzinfo, each instance of which represents a timezone defined by a fixed offset
from UTC.
Objects of this class cannot be used to represent timezone information in the locations where different offsets are used in
different days of the year or where historical changes have been made to civil time.
The offset argument must be specified as a timedelta object representing the difference between the local time
and UTC.
It must be strictly between -timedelta(hours=24) and timedelta(hours=24), otherwise
ValueError is raised.
The name argument is optional.
If specified it must be a string that will be used as the value returned by the
datetime.tzname() method.
New in version 3.2.
The UTC offset is not restricted to a whole number of minutes.
The dt argument is ignored.
The return value is a timedelta instance equal to the difference between the local
time and UTC.
The UTC offset is not restricted to a whole number of minutes.
If name is not provided in the constructor, the name returned by tzname(dt) is generated from the value of
the offset as follows.
If offset is timedelta(0), the name is “UTC”, otherwise it is a string in the format
UTC±HH:MM, where ± is the sign of offset, HH and MM are two digits of offset.hours and offset.
minutes respectively.
Name generated from offset=timedelta(0) is now plain 'UTC', not
'UTC+00:00'.
timezone.dst(dt)
Always returns None.
timezone.fromutc(dt)
Return dt + offset.
The dt argument must be an aware datetime instance, with tzinfo set to self.
Class attributes:
timezone.utc
The UTC timezone, timezone(timedelta(0)).
Conversely, the datetime.strptime() class method creates a datetime object from a string representing a date
and time and a corresponding format string.
The following is a list of all the format codes that the 1989 C standard requires, and these work on all platforms with a
standard C implementation.
A
Weekday as locale’s full
name.
Sunday, Monday, …,
Saturday (en_US);
Sonntag, Montag, …,
Samstag (de_DE)
(1)
%w
Weekday
as
a
decimal
number,
where

is
Sunday and 6 is Saturday.
Jan, Feb, …, Dec
(en_US);
Jan, Feb, …, Dez (de_DE)
(1)
%B
Month
as
locale’s
full
name.
January, February, …,
December (en_US);
Januar, Februar, …,
Dezember (de_DE)
(1)
%m
Month as a zero-padded
decimal number.
These parameters all corre-
spond to ISO 8601 date values.
Di-
rec-
tive
Meaning
Example
Notes
%G
ISO 8601 year with century representing the year that contains
the greater part of the ISO week (%V).
0001, 0002, …, 2013, 2014, …,
9998, 9999
(8)
%u
ISO 8601 weekday as a decimal number where 1 is Monday.
Week 01 is the week containing Jan 4.
01, 02, …, 53
(8),
(9)
%:z
UTC offset in the form ±HH:MM[:SS[.ffffff]] (empty
string if the object is naive).
These may not be available on all platforms when used with the strftime() method.
The ISO 8601 year and ISO
8601 week directives are not interchangeable with the year and week number directives above.
Calling strptime()
with incomplete or ambiguous ISO 8601 directives will raise a ValueError.
The full set of format codes supported varies across platforms, because Python calls the platform C library’s
strftime() function, and platform variations are common.
To see the full set of format codes supported on your
platform, consult the strftime(3) documentation.
There are also differences between platforms in handling of un-
supported format specifiers.
New in version 3.12: %:z was added.
For the datetime.strptime() class method, the default value is 1900-01-01T00:00:00.000: any compo-
nents not specified in the format string will be pulled from the default value.4
Using datetime.strptime(date_string, format) is equivalent to:
datetime(*(time.strptime(date_string, format)[0:6]))
except when the format includes sub-second components or timezone offset information, which are supported in
datetime.strptime but are discarded by time.strptime.
For time objects, the format codes for year, month, and day should not be used, as time objects have no such values.
If they’re used anyway, 1900 is substituted for the year, and 1 for the month and day.
For date objects, the format codes for hours, minutes, seconds, and microseconds should not be used, as date objects
have no such values.
If they’re used anyway, 0 is substituted for them.
For the same reason, handling of format strings containing Unicode code points that can’t be represented in the charset
of the current locale is also platform-dependent.
On some platforms such code points are preserved intact in the output,
while on others strftime may raise UnicodeError or return an empty string instead.
Notes:
(1) Because the format depends on the current locale, care should be taken when making assumptions about the output
value.
Field orderings will vary (for example, “month/day/year” versus “day/month/year”), and the output may
contain non-ASCII characters.
Changed in version 3.2: In previous versions, strftime() method was restricted to years >= 1900.
Changed in version 3.3: In version 3.2, strftime() method was restricted to years >= 1000.
For an aware object:
%z utcoffset() is transformed into a string of the form ±HHMM[SS[.ffffff]], where HH is a 2-digit
string giving the number of UTC offset hours, MM is a 2-digit string giving the number of UTC offset minutes,
SS is a 2-digit string giving the number of UTC offset seconds and ffffff is a 6-digit string giving the
number of UTC offset microseconds.
The ffffff part is omitted when the offset is a whole number of
seconds and both the ffffff and the SS part is omitted when the offset is a whole number of minutes.
For
example, if utcoffset() returns timedelta(hours=-3, minutes=-30), %z is replaced with
the string '-0330'.
The UTC offset is not restricted to a whole number of minutes.
Changed in version 3.7: When the %z directive is provided to the strptime() method, the UTC offsets can
have a colon as a separator between hours, minutes and seconds.
For example, '+01:00:00' will be parsed as
an offset of one hour.
In addition, providing 'Z' is identical to '+00:00'.
In strftime(), %Z is replaced by an empty string if tzname() returns None; otherwise %Z is replaced
by the returned value, which must be a string.
So someone living in Japan may have JST, UTC, and GMT as valid values, but probably not EST.
It will raise
ValueError for invalid values.
Changed in version 3.2: When the %z directive is provided to the strptime() method, an aware datetime
object will be produced.
The tzinfo of the result will be set to a timezone instance.
W are only used in calculations when the day of the week
and the calendar year (%Y) are specified.
Also note that %G and %Y are not interchangeable.
Source code: Lib/zoneinfo
The zoneinfo module provides a concrete time zone implementation to support the IANA time zone database as
originally specified in PEP 615.
By default, zoneinfo uses the system’s time zone data if available; if no system time
zone data is available, the library will fall back to using the first-party tzdata package available on PyPI.
See also:
Module: datetime Provides the time and datetime types with which the ZoneInfo class is designed to be
used.
Package tzdata First-party package maintained by the CPython core developers to supply time zone data via PyPI.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
During offset transitions which induce am-
biguous times (such as a daylight saving time to standard time transition), the offset from before the transition is used
when fold=0, and the offset after the transition is used when fold=1, for example:
2020-11-01 01:00:00-07:00
(continues on next page)
8.2.
When converting from another time zone, the fold will be set to the correct value:
2020-11-01 01:00:00-07:00
2020-11-01 01:00:00-08:00
8.2.2 Data sources
The zoneinfo module does not directly provide time zone data, and instead pulls time zone information from the
system time zone database or the first-party PyPI package tzdata, if available.
Some systems, including notably Windows
systems, do not have an IANA database available, and so for projects targeting cross-platform compatibility that require
time zone data, it is recommended to declare a dependency on tzdata.
If neither system data nor tzdata are available, all
calls to ZoneInfo will raise ZoneInfoNotFoundError.
Configuring the data sources
When ZoneInfo(key) is called, the constructor first searches the directories specified in TZPATH for a file matching
key, and on failure looks for a match in the tzdata package.
This behavior can be configured in three ways:
1.
The default TZPATH when not otherwise specified can be configured at compile time.
At runtime, the search path can be manipulated using the reset_tzpath() function.
Compile-time configuration
The default TZPATH includes several common deployment locations for the time zone database (except on Windows,
where there are no “well-known” locations for time zone data).
On POSIX systems, downstream distributors and those
building Python from source who know where their system time zone data is deployed may change the default time zone
path by specifying the compile-time option TZPATH (or, more likely, the configure flag --with-tzpath),
which should be a string delimited by os.pathsep.
On all platforms, the configured value is available as the TZPATH key in sysconfig.get_config_var().
This is an os.pathsep-separated string containing the time zone search path to use.
It must consist of
only absolute rather than relative paths.
Relative components specified in PYTHONTZPATH will not be used,
but otherwise the behavior when a relative path is specified is implementation-defined; CPython will raise
InvalidTZPathWarning, but other implementations are free to silently ignore the erroneous component or
raise an exception.
To set the system to ignore the system data and use the tzdata package instead, set PYTHONTZPATH="".
Runtime configuration
The TZ search path can also be configured at runtime using the reset_tzpath() function.
This is generally not an
advisable operation, though it is reasonable to use it in test functions that require the use of a specific time zone path (or
require disabling access to the system time zones).
A concrete datetime.tzinfo subclass that represents an IANA time zone specified by the string key.
The constructor will
raise ValueError if a non-conforming key is passed.
If no file matching key is found, the constructor will raise ZoneInfoNotFoundError.
The ZoneInfo class has two alternate constructors:
classmethod ZoneInfo.from_file(fobj, /, key=None)
Constructs a ZoneInfo object from a file-like object returning bytes (e.g. a file opened in binary mode or an
io.
BytesIO object).
Unlike the primary constructor, this always constructs a new object.
Objects created via this constructor cannot be pickled (see pickling).
An alternate constructor that bypasses the constructor’s cache.
It is identical to the primary constructor, but returns
a new object on each call.
This is most likely to be useful for testing or demonstration purposes, but it can also be
used to create a system with a different cache invalidation strategy.
Objects created via this constructor will also bypass the cache of a deserializing process when unpickled.
A method for invalidating the cache on the ZoneInfo class.
If no arguments are passed, all caches are invalidated
and the next call to the primary constructor for each key will return a new instance.
If an iterable of key names is passed to the only_keys parameter, only the specified keys will be removed from
the cache.
Keys passed to only_keys but not found in the cache are ignored.
Warning: Invoking this function may change the semantics of datetimes using ZoneInfo in surprising ways;
this modifies module state and thus may have wide-ranging effects.
Only use it if you know that you need to.
This is a read-only attribute that returns the value of key passed to the constructor, which should be a lookup key
in the IANA time zone database (e.g. America/New_York, Europe/Paris or Asia/Tokyo).
For zones constructed from file without specifying a key parameter, this will be set to None.
Note: Although it is a somewhat common practice to expose these to end users, these values are designed to be
primary keys for representing the relevant zones and not necessarily user-facing elements.
Projects like CLDR (the
Unicode Common Locale Data Repository) can be used to get more user-friendly strings from these keys.
For objects constructed from a file without specifying a key parameter, str falls back to calling repr().
ZoneInfo’s
repr is implementation-defined and not necessarily stable between versions, but it is guaranteed not to be a valid
ZoneInfo key.
The behavior of a ZoneInfo file depends on how it was constructed:
1.
When constructed with the primary constructor, a ZoneInfo object is serialized by key,
and when deserialized, the deserializing process uses the primary and thus it is expected that these are expected
to be the same object as other references to the same time zone.
When constructed from the cache-bypassing constructor, the ZoneInfo
object is also serialized by key, but when deserialized, the deserializing process uses the cache bypassing
constructor.
If europe_berlin_pkl_nc is a string containing a pickle constructed from ZoneInfo.
ZoneInfo.from_file(fobj, /, key=None): When constructed from a file, the ZoneInfo object
raises an exception on pickling.
If an end user wants to pickle a ZoneInfo constructed from a file, it is recom-
mended that they use a wrapper type or a custom serialization function: either serializing by key or storing the
contents of the file object and serializing that.
This method of serialization requires that the time zone data for the required key be available on both the serializing and
deserializing side, similar to the way that references to classes and functions are expected to exist in both the serializing and
deserializing environments.
It also means that no guarantees are made about the consistency of results when unpickling a
ZoneInfo pickled in an environment with a different version of the time zone data.
This is
recalculated on every call to the function.
This function may open a large number of files, as the best way to determine if a file on the time
zone path is a valid time zone is to read the “magic string” at the beginning.
Note: These values are not designed to be exposed to end-users; for user facing elements, applications should use
something like CLDR (the Unicode Common Locale Data Repository) to get more user-friendly strings.
See also
8.2.
When called with no arguments, TZPATH is
set to the default value.
Calling reset_tzpath will not invalidate the ZoneInfo cache, and so calls to the primary ZoneInfo con-
structor will only use the new TZPATH in the case of a cache miss.
The to parameter must be a sequence of strings or os.
PathLike and not a string, all of which must be absolute
paths.
ValueError will be raised if something other than an absolute path is passed.
TZPATH
A read-only sequence representing the time zone search path – when constructing a ZoneInfo from a key, the
key is joined to each entry in the TZPATH, and the first file found is used.
TZPATH may contain only absolute paths, never relative paths, regardless of how it is configured.
The object that zoneinfo.
TZPATH points to may change in response to a call to reset_tzpath(), so it
is recommended to use zoneinfo.
TZPATH rather than importing TZPATH from zoneinfo or assigning a
long-lived variable to zoneinfo.
TZPATH.
For more information on configuring the time zone search path, see Configuring the data sources.
Raised when construction of a ZoneInfo object fails because the specified key could not be found on the system.
This is a subclass of KeyError.
InvalidTZPathWarning
Raised when PYTHONTZPATH contains an invalid component that will be filtered out, such as a relative path.
By default, these calendars have Monday as the first day of the week, and Sunday as the last (the European
convention).
Use setfirstweekday() to set the first day of the week to Sunday (6) or to any other weekday.
Parameters that specify dates are given as integers.
For related functionality, see also the datetime and time modules.
The functions and classes defined in this module use an idealized calendar, the current Gregorian calendar extended indef-
initely in both directions.
This matches the definition of the “proleptic Gregorian” calendar in Dershowitz and Reingold’s
book “Calendrical Calculations”, where it’s the base calendar for all computations.
Zero and negative years are interpreted
as prescribed by the ISO 8601 standard.
Calendar(firstweekday=0)
Creates a Calendar object.
MONDAY is 0 (the
default), SUNDAY is 6.
A Calendar object provides several methods that can be used for preparing the calendar data for formatting.
This class doesn’t do any formatting itself.
This is the job of subclasses.
Calendar instances have the following methods:
iterweekdays()
Return an iterator for the week day numbers that will be used for one week.
The first value from the iterator
will be the same as the value of the firstweekday property.
This iterator will return all days (as
datetime.date objects) for the month and all days before the start of the month or after the end of
the month that are required to get a complete week.
Days returned will simply be day of the month numbers.
For the days
outside of the specified month, the day number is 0.
itermonthdays2(year, month)
Return an iterator for the month month in the year year similar to itermonthdates(), but not restricted
by the datetime.date range.
Days returned will be tuples consisting of a day of the month number and
a week day number.
Days returned will be tuples consisting of a year, a month and a day of the
month numbers.
New in version 3.7.
itermonthdays4(year, month)
Return an iterator for the month month in the year year similar to itermonthdates(), but not restricted
by the datetime.date range.
Days returned will be tuples consisting of a year, a month, a day of the
month, and a day of the week numbers.
New in version 3.7.
Weeks are lists of seven datetime.
Weeks are lists of seven tuples of
day numbers and weekday numbers.
Weeks are lists of seven day numbers.
The return value is a list of month rows.
Each
month row contains up to width months (defaulting to 3).
Each month contains between 4 and 6 weeks and
each week contains 1–7 days.
Days are datetime.date objects.
Entries
in the week lists are tuples of day numbers and weekday numbers.
Day numbers outside this month are zero.
Entries
in the week lists are day numbers.
Day numbers outside this month are zero.
This class can be used to generate plain text calendars.
TextCalendar instances have the following methods:
formatmonth(theyear, themonth, w=0, l=0)
Return a month’s calendar in a multi-line string.
If w is provided, it specifies the width of the date columns,
which are centered.
If l is given, it specifies the number of lines that each week will use.
Depends on the first
weekday as specified in the constructor or set by the setfirstweekday() method.
Print a month’s calendar as returned by formatmonth().
Optional parameters w, l, and c are for
date column width, lines per week, and number of spaces between month columns, respectively.
Depends on
the first weekday as specified in the constructor or set by the setfirstweekday() method.
The earliest
year for which a calendar can be generated is platform-dependent.
This class can be used to generate HTML calendars.
HTMLCalendar instances have the following methods:
formatmonth(theyear, themonth, withyear=True)
Return a month’s calendar as an HTML table.
If withyear is true the year will be included in the header,
otherwise just the month name will be used.
None can be passed if no style sheet should
be used.
HTMLCalendar has the following attributes you can override to customize the CSS classes used by the calendar:
cssclasses
A list of CSS classes used for each weekday.
New in version 3.7.
The default is the same as cssclasses.
New in version 3.7.
The month’s head CSS class (used by formatmonthname()).
The default value is "month".
New in version 3.7.
The default value is "month".
New in version 3.7.
The default value is "year".
New in version 3.7.
The CSS class for the table head for the whole year (used by formatyear()).
The default value is
"year".
New in version 3.7.
Note that although the naming for the above described class attributes is singular (e.g. cssclass_month
cssclass_noday), one can replace the single CSS class with a space separated list of CSS classes, for ex-
ample:
"text-bold text-red"
Here is an example how HTMLCalendar can be customized:
class CustomHTMLCal(calendar.
This subclass of TextCalendar can be passed a locale name in the constructor and will return month and
weekday names in the specified locale.
This subclass of HTMLCalendar can be passed a locale name in the constructor and will return month and
weekday names in the specified locale.
Because the current locale is a process-wide setting, they are not thread-
safe.
Sets the weekday (0 is Monday, 6 is Sunday) to start each week.
The values MONDAY, TUESDAY, WEDNESDAY,
THURSDAY, FRIDAY, SATURDAY, and SUNDAY are provided for convenience.
Returns True if year is a leap year, otherwise False.
This function works for ranges spanning a century change.
Each row represents a week; days outside of the month are
represented by zeros.
Each week begins with Monday unless set by setfirstweekday().
In fact, time.gmtime() and timegm() are each others’ inverse.
MONDAY
calendar.
TUESDAY
calendar.
WEDNESDAY
calendar.
THURSDAY
calendar.
FRIDAY
calendar.
SATURDAY
calendar.
SUNDAY
Aliases for the days of the week, where MONDAY is 0 and SUNDAY is 6.
New in version 3.12.
class calendar.
Day
Enumeration defining days of the week as integer constants.
The members of this enumeration are exported to the
module scope as MONDAY through SUNDAY.
New in version 3.12.
This follows normal convention of January
being month number 1, so it has a length of 13 and month_name[0] is the empty string.
This follows normal convention
of January being month number 1, so it has a length of 13 and month_abbr[0] is the empty string.
calendar.
JANUARY
calendar.
FEBRUARY
calendar.
MARCH
calendar.
APRIL
calendar.
MAY
calendar.
JUNE
calendar.
JULY
calendar.
AUGUST
calendar.
SEPTEMBER
calendar.
OCTOBER
calendar.
NOVEMBER
calendar.
DECEMBER
Aliases for the months of the year, where JANUARY is 1 and DECEMBER is 12.
New in version 3.12.
class calendar.
Month
Enumeration defining months of the year as integer constants.
The members of this enumeration are exported to
the module scope as JANUARY through DECEMBER.
New in version 3.12.
The calendar module defines the following exceptions:
8.3.
A subclass of ValueError, raised when the given month number is outside of the range 1-12 (inclusive).
A subclass of ValueError, raised when the given weekday number is outside of the range 0-6 (inclusive).
See also:
Module datetime Object-oriented interface to dates and times with similar functionality to the time module.
Module time Low-level time related functions.
The calendar module can be executed as a script from the command line to interactively print a calendar.
The locale to use for month and weekday names.
Defaults to English.
Must be a number between 1 and 9999.
Defaults to the current year.
Must be a number between 1 and 12, and may only be
used in text mode.
Defaults to printing a calendar for the full year.
The width of the date column in terminal columns.
The date is printed centred in the column.
Any value lower
than 2 is ignored.
The number of lines for each week in terminal rows.
The date is printed top-aligned.
Any value lower than 1 is
ignored.
SPACING, -s SPACING
The space between months in columns.
Any value lower than 2 is ignored.
The number of months printed per row.
Defaults to 3.
HTML-mode options:
--css CSS, -c CSS
The path of a CSS stylesheet to use for the calendar.
This must either be relative to the generated HTML, or an
absolute HTTP or file:/// URL.
This module implements specialized container datatypes providing alternatives to Python’s general purpose built-in con-
tainers, dict, list, set, and tuple.
A ChainMap class is provided for quickly linking a number of mappings so they can be treated as a single unit.
It is
often much faster than creating a new dictionary and running multiple update() calls.
The class can be used to simulate nested scopes and is useful in templating.
A ChainMap groups multiple dicts or other mappings together to create a single, updateable view.
If no maps are
specified, a single empty dictionary is provided so that a new chain always has at least one mapping.
The underlying mappings are stored in a list.
That list is public and can be accessed or updated using the maps
attribute.
There is no other state.
Lookups search the underlying mappings successively until a key is found.
In contrast, writes, updates, and deletions
only operate on the first mapping.
A ChainMap incorporates the underlying mappings by reference.
So, if one of the underlying mappings gets
updated, those changes will be reflected in ChainMap.
All of the usual dictionary methods are supported.
In addition, there is a maps attribute, a method for creating new
subcontexts, and a property for accessing all but the first mapping:
maps
A user updateable list of mappings.
The list is ordered from first-searched to last-searched.
It is the only
stored state and can be modified to change which mappings are searched.
The list should always contain at
least one mapping.
If m is
specified, it becomes the new map at the front of the list of mappings; if not specified, an empty dict is used, so
that a call to d.new_child() is equivalent to: ChainMap({}, *d.maps).
If any keyword arguments
are specified, they update passed map or new empty dict.
This method is used for creating subcontexts that
can be updated without altering values in any of the parent mappings.
The optional m parameter was added.
Keyword arguments support was added.
This is useful for skipping the first map in the search.
Use cases are similar to those for the nonlocal
keyword used in nested scopes.
The use cases also parallel those for the built-in super() function.
A
reference to d.parents is equivalent to: ChainMap(*d.maps[1:]).
The MultiContext class in the Enthought CodeTools package has options to support writing to any mapping in the
chain.
Django’s Context class for templating is a read-only chain of mappings.
It also features pushing and popping of
contexts similar to the new_child() method and the parents property.
The Nested Contexts recipe has options to control whether writes and other mutations apply only to the first mapping
or to any mapping in the chain.
This section shows various approaches to working with chained maps.
Example of letting user specified command-line arguments take precedence over environment variables which in turn
take precedence over default values:
import os, argparse
defaults = {'color': 'red', 'user': 'guest'}
parser = argparse.
ArgumentParser()
parser.add_argument('-u', '--user')
parser.add_argument('-c', '--color')
namespace = parser.parse_args()
command_line_args = {k: v for k, v in vars(namespace).items() if v is not None}
(continues on next page)
8.4.
A Counter is a dict subclass for counting hashable objects.
It is a collection where elements are stored as
dictionary keys and their counts are stored as dictionary values.
Counts are allowed to be any integer value including
zero or negative counts.
The Counter class is similar to bags or multisets in other languages.
Elements are counted from an iterable or initialized from another mapping (or counter):
# a new, empty counter
# a new counter from an iterable
# a new counter from a mapping
# a new counter from keyword args
Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising
a KeyError:
# count of a missing element is zero

Setting a count to zero does not remove an element from a counter.
Use del to remove it entirely:
# counter entry with a zero count
# del actually removes the entry
New in version 3.1.
As a dict subclass, Counter inherited the capability to remember insertion order.
Math operations on Counter objects also preserve order.
Results are ordered according to when an element is first
encountered in the left operand and then by the order encountered in the right operand.
Counter objects support additional methods beyond those available for all dictionaries:
elements()
Return an iterator over elements repeating each as many times as its count.
Elements are returned in the order
first encountered.
If an element’s count is less than one, elements() will ignore it.
If n is
omitted or None, most_common() returns all elements in the counter.
Like dict.update() but
subtracts counts instead of replacing them.
Both inputs and outputs may be zero or negative.
New in version 3.2.
Compute the sum of the counts.
The usual dictionary methods are available for Counter objects except for two which work differently for counters.
This class method is not implemented for Counter objects.
Elements are counted from an iterable or added-in from another mapping (or counter).
Like dict.
update() but adds counts instead of replacing them.
Also, the iterable is expected to be a sequence of
elements, not a sequence of (key, value) pairs.
Counters support rich comparison operators for equality, subset, and superset relationships: ==, !=, <, <=, >, >=.
All of
those tests treat missing elements as having zero counts so that Counter(a=1) == Counter(a=1, b=0) returns
true.
Rich comparison operations were added.
In equality tests, missing elements are treated as having zero counts.
Formerly,
Counter(a=3) and Counter(a=3, b=0) were considered distinct.
Addition and subtraction combine counters by adding or subtracting the counts of corresponding
elements.
Intersection and union return the minimum and maximum of corresponding counts.
Equality and inclusion
compare corresponding counts.
Each operation can accept inputs with signed counts, but the output will exclude results
with counts of zero or less.
New in version 3.3: Added support for unary plus, unary minus, and in-place multiset operations.
Note: Counters were primarily designed to work with positive integers to represent running counts; however, care was
taken to not unnecessarily preclude use cases needing other types or negative values.
To help with those use cases, this
section documents the minimum range and type restrictions.
The Counter class itself is a dictionary subclass with no restrictions on its keys and values.
The values are intended
to be numbers representing counts, but you could store anything in the value field.
So
fractions, floats, and decimals would work and negative values are supported.
The same is also true for update()
and subtract() which allow negative and zero values for both inputs and outputs.
The multiset methods are designed only for use cases with positive values.
The inputs may be negative or zero, but
only outputs with positive values are created.
There are no type restrictions, but the value type needs to support
addition, subtraction, and comparison.
The elements() method requires integer counts.
It ignores zero and negative counts.
See also:
• Bag class in Smalltalk.
C++ multisets tutorial with examples.
The Art of Computer Programming
Volume II, Section 4.6.3, Exercise 19.
If iterable is not
specified, the new deque is empty.
Deques are a generalization of stacks and queues (the name is pronounced “deck” and is short for “double-ended
queue”).
Deques support thread-safe, memory efficient appends and pops from either side of the deque with ap-
proximately the same O(1) performance in either direction.
Though list objects support similar operations, they are optimized for fast fixed-length operations and incur O(n)
memory movement costs for pop(0) and insert(0, v) operations which change both the size and position
of the underlying data representation.
If maxlen is not specified or is None, deques may grow to an arbitrary length.
Otherwise, the deque is bounded to
the specified maximum length.
Once a bounded length deque is full, when new items are added, a corresponding
number of items are discarded from the opposite end.
Bounded length deques provide functionality similar to the
tail filter in Unix.
They are also useful for tracking transactions and other pools of data where only the most
recent activity is of interest.
Deque objects support the following methods:
append(x)
Add x to the right side of the deque.
Add x to the left side of the deque.
Remove all elements from the deque leaving it with length 0.
copy()
Create a shallow copy of the deque.
New in version 3.5.
count(x)
Count the number of deque elements equal to x.
New in version 3.2.
extend(iterable)
Extend the right side of the deque by appending elements from the iterable argument.
Note, the series of left appends results
in reversing the order of elements in the iterable argument.
Returns the first match
or raises ValueError if not found.
Insert x into the deque at position i.
If the insertion would cause a bounded deque to grow beyond maxlen, an IndexError is raised.
New in version 3.5.
pop()
Remove and return an element from the right side of the deque.
If no elements are present, raises an
IndexError.
Remove the first occurrence of value.
If not found, raises a ValueError.
If n is negative, rotate to the left.
When the deque is not empty, rotating one step to the right is equivalent to d.appendleft(d.pop()),
and rotating one step to the left is equivalent to d.append(d.popleft()).
Deque objects also provide one read-only attribute:
maxlen
Maximum size of a deque or None if unbounded.
New in version 3.1.
In addition to the above, deques support iteration, pickling, len(d), reversed(d), copy.copy(d), copy.
Indexed access is O(1) at both ends but slows to O(n) in the middle.
For fast random access, use lists instead.
Starting in version 3.5, deques support __add__(), __mul__(), and __imul__().
Example:
# make a new deque with three items
# iterate over the deque's elements

print(elem.upper())
G
H
I
(continues on next page)
8.4.
This section shows various approaches to working with deques.
Values are yielded from the active
iterator in position zero.
The rotate() method provides a way to implement deque slicing and deletion.
To implement deque slicing, use a similar approach applying rotate() to bring a target element to the left side of
the deque.
Remove old entries with popleft(), add new entries with extend(), and then reverse the rotation.
With
minor variations on that approach, it is easy to implement Forth style stack manipulations such as dup, drop, swap,
over, pick, rot, and roll.
It overrides one
method and adds one writable instance variable.
The remaining functionality is the same as for the dict class and
is not documented here.
The first argument provides the initial value for the default_factory attribute; it defaults to None.
All
remaining arguments are treated the same as if they were passed to the dict constructor, including keyword
arguments.
If the default_factory attribute is None, this raises a KeyError exception with the key as argument.
If default_factory is not None, it is called without arguments to provide a default value for the given
key, this value is inserted in the dictionary for the key, and returned.
If calling default_factory raises an exception this exception is propagated unchanged.
This method is called by the __getitem__() method of the dict class when the requested key is not
found; whatever it returns or raises is then returned or raised by __getitem__().
Note that __missing__() is not called for any operations besides __getitem__().
This means that
get() will, like normal dictionaries, return None as a default rather than using default_factory.
Changed in version 3.9: Added merge (|) and update (|=) operators, specified in PEP 584.
When each key is encountered for the first time, it is not already in the mapping; so an entry is automatically created using
the default_factory function which returns an empty list.
The list.append() operation then attaches the
value to the new list.
When keys are encountered again, the look-up proceeds normally (returning the list for that key) and
the list.append() operation adds another value to the list.
When a letter is first encountered, it is missing from the mapping, so the default_factory function calls int() to
supply a default count of zero.
The increment operation then builds up the count for each letter.
The function int() which always returns zero is just a special case of constant functions.
They can
be used wherever regular tuples are used, and they add the ability to access fields by name instead of position index.
The new subclass is used to create tuple-like objects that have fields
accessible by attribute lookup as well as being indexable and iterable.
The field_names are a sequence of strings such as ['x', 'y'].
Alternatively, field_names can be a single string
with each fieldname separated by whitespace and/or commas, for example 'x y' or 'x, y'.
Any valid Python identifier may be used for a fieldname except for names starting with an underscore.
Valid
identifiers consist of letters, digits, and underscores but do not start with a digit or underscore and cannot be a
keyword such as class, for, return, global, pass, or raise.
If rename is true, invalid fieldnames are automatically replaced with positional names.
For example, ['abc',
'def', 'ghi', 'abc'] is converted to ['abc', '_1', 'ghi', '_3'], eliminating the keyword
def and the duplicate fieldname abc.
Since fields with a default value must come after any fields
without a default, the defaults are applied to the rightmost parameters.
For example, if the fieldnames are ['x',
'y', 'z'] and the defaults are (1, 2), then x will be a required argument, y will default to 1, and z will
default to 2.
If module is defined, the __module__ attribute of the named tuple is set to that value.
Named tuple instances do not have per-instance dictionaries, so they are lightweight and require no more memory
than regular tuples.
To support pickling, the named tuple class should be assigned to a variable that matches typename.
Changed in version 3.1: Added support for rename.
The verbose and rename parameters became keyword-only arguments.
Added the module parameter.
Removed the verbose parameter and the _source attribute.
Added the defaults parameter and the _field_defaults attribute.
In addition to the methods inherited from tuples, named tuples support three additional methods and two attributes.
To
prevent conflicts with field names, the method and attribute names start with an underscore.
Point(x=11, y=22)
somenamedtuple._asdict()
Return a new dict which maps field names to their corresponding values:
{'x': 11, 'y': 22}
Changed in version 3.1: Returns an OrderedDict instead of a regular dict.
Returns a regular dict instead of an OrderedDict.
As of Python 3.7, regular dicts
are guaranteed to be ordered.
If the extra features of OrderedDict are required, the suggested remediation is
to cast the result to the desired type: OrderedDict(nt._asdict()).
Useful for introspection and for creating new named tuple types from
existing named tuples.
Since a named tuple is a regular Python class, it is easy to add or change functionality with a subclass.
This helps keep memory requirements low by preventing
the creation of instance dictionaries.
Subclassing is not useful for adding new, stored fields.
Property docstrings became writeable.
See also:
• See typing.
NamedTuple for a way to add type hints for named tuples.
SimpleNamespace() for a mutable namespace based on an underlying dictionary instead of a
tuple.
The dataclasses module provides a decorator and functions for automatically adding generated special meth-
ods to user-defined classes.
They have become less important now that the built-in dict class gained the ability to remember insertion order (this
new behavior became guaranteed in Python 3.7).
The regular dict was designed to be very good at mapping operations.
Tracking insertion order was secondary.
The OrderedDict was designed to be good at reordering operations.
Space efficiency, iteration speed, and the
performance of update operations were secondary.
The OrderedDict algorithm can handle frequent reordering operations better than dict.
As shown in the
recipes below, this makes it suitable for implementing various kinds of LRU caches.
The equality operation for OrderedDict checks for matching order.
A regular dict can emulate the order sensitive equality test with p == q and all(k1 == k2 for k1,
k2 in zip(p, q)).
It accepts an optional argument to specify
which item is popped.
A regular dict can emulate OrderedDict’s od.popitem(last=True) with d.popitem() which is guar-
anteed to pop the rightmost (last) item.
A regular dict can emulate OrderedDict’s od.popitem(last=False) with (k := next(iter(d)),
d.pop(k)) which will return and remove the leftmost (first) item if it exists.
A regular dict can emulate OrderedDict’s od.move_to_end(k, last=True) with d[k] = d.
pop(k) which will move the key and its associated value to the rightmost (last) position.
A
regular
dict
does
not
have
an
efficient
equivalent
for
OrderedDict’s
od.move_to_end(k,
last=False) which moves the key and its associated value to the leftmost (first) position.
OrderedDict([items])
Return an instance of a dict subclass that has methods specialized for rearranging dictionary order.
New in version 3.1.
The pairs are
returned in LIFO order if last is true or FIFO (first-in, first-out) order if false.
The item is moved to the right end if last is true
(the default) or to the beginning if last is false.
Raises KeyError if the key does not exist:
'acdeb'
'bacde'
New in version 3.2.
In addition to the usual mapping methods, ordered dictionaries also support reverse iteration using reversed().
Equality tests between OrderedDict objects are order-sensitive and are implemented as list(od1.
items())==list(od2.items()).
Equality tests between OrderedDict objects and other Mapping objects
are order-insensitive like regular dictionaries.
This allows OrderedDict objects to be substituted anywhere a regular
dictionary is used.
The items, keys, and values views of OrderedDict now support reverse iteration using
reversed().
Changed in version 3.6: With the acceptance of PEP 468, order is retained for keyword arguments passed to the
OrderedDict constructor and its update() method.
Changed in version 3.9: Added merge (|) and update (|=) operators, specified in PEP 584.
8.4.
It is straightforward to create an ordered dictionary variant that remembers the order the keys were last inserted.
An OrderedDict would also be useful for implementing variants of functools.lru_cache():
from collections import OrderedDict
from time import time
class TimeBoundedLRU:
"LRU Cache that invalidates and refreshes old entries.
To avoid flushing the LRU cache with one-time requests,
we don't cache until a request has been made more than once.
The need for this class has been partially supplanted
by the ability to subclass directly from dict; however, this class can be easier to work with because the underlying
dictionary is accessible as an attribute.
Class that simulates a dictionary.
The instance’s contents are kept in a regular dictionary, which is accessible via
the data attribute of UserDict instances.
If initialdata is provided, data is initialized with its contents; note
that a reference to initialdata will not be kept, allowing it to be used for other purposes.
In addition to supporting the methods and operations of mappings, UserDict instances provide the following
attribute:
data
A real dictionary used to store the contents of the UserDict class.
It is a useful base class for your own list-like classes which can inherit
from them and override existing methods or add new ones.
In this way, one can add new behaviors to lists.
The need for this class has been partially supplanted by the ability to subclass directly from list; however, this class can
be easier to work with because the underlying list is accessible as an attribute.
Class that simulates a list.
The instance’s contents are kept in a regular list, which is accessible via the data
attribute of UserList instances.
The instance’s contents are initially set to a copy of list, defaulting to the empty
list []. list can be any iterable, for example a real Python list or a UserList object.
In addition to supporting the methods and operations of mutable sequences, UserList instances provide the
following attribute:
data
A real list object used to store the contents of the UserList class.
Subclassing requirements: Subclasses of UserList are expected to offer a constructor which can be called with either
no arguments or one argument.
List operations which return a new sequence attempt to create an instance of the actual
implementation class.
To do so, it assumes that the constructor can be called with a single parameter, which is a sequence
object used as a data source.
If a derived class does not wish to comply with this requirement, all of the special methods supported by this class will
need to be overridden; please consult the sources for information about the methods which need to be provided in that
case.
The need for this class has been partially supplanted
by the ability to subclass directly from str; however, this class can be easier to work with because the underlying string
is accessible as an attribute.
Class that simulates a string object.
The instance’s content is kept in a regular string object, which is accessible via
the data attribute of UserString instances.
The instance’s contents are initially set to a copy of seq.
The seq
argument can be any object which can be converted into a string using the built-in str() function.
In addition to supporting the methods and operations of strings, UserString instances provide the following
attribute:
data
A real str object used to store the contents of the UserString class.
Changed in version 3.5:
New methods __getnewargs__, __rmod__, casefold, format_map,
isprintable, and maketrans.
Source code: Lib/_collections_abc.py
This module provides abstract base classes that can be used to test whether a class provides a particular interface; for
example, whether it is hashable or whether it is a mapping.
An issubclass() or isinstance() test for an interface works in one of three ways.
The class must supply the required
abstract methods.
The remaining mixin methods come from inheritance and can be overridden if desired.
Those classes should define
the full API including all of the abstract methods and all of the mixin methods.
This lets users rely on issubclass()
or isinstance() tests to determine whether the full interface is supported.
True
True
Complex interfaces do not support this last technique because an interface is more than just the presence of method
names.
Interfaces specify semantics and relationships between methods that cannot be inferred solely from the presence
of specific method names.
For example, knowing that a class supplies __getitem__, __len__, and __iter__ is
insufficient for distinguishing a Sequence from a Mapping.
These abstract classes now support [].
See Generic Alias Type and PEP 585.
These ABCs override object.__subclasshook__() to support testing an interface by verifying the required methods are present and have
not been set to None.
This only works for simple interfaces.
More complex interfaces require registration or direct subclassing.
The only reliable way to determine whether an object is iterable is to call
iter(obj).
Container
ABC for classes that provide the __contains__() method.
Hashable
ABC for classes that provide the __hash__() method.
Sized
ABC for classes that provide the __len__() method.
Callable
ABC for classes that provide the __call__() method.
Iterable
ABC for classes that provide the __iter__() method.
Checking isinstance(obj, Iterable) detects classes that are registered as Iterable or that have an
__iter__() method, but it does not detect classes that iterate with the __getitem__() method.
The only
reliable way to determine whether an object is iterable is to call iter(obj).
Collection
ABC for sized iterable container classes.
New in version 3.6.
See also the definition of iterator.
Reversible
ABC for iterable classes that also provide the __reversed__() method.
New in version 3.6.
Generator
ABC for generator classes that implement the protocol defined in PEP 342 that extends iterators with the send(),
throw() and close() methods.
See also the definition of generator.
New in version 3.5.
Sequence
class collections.abc.
MutableSequence
class collections.abc.
ByteString
ABCs for read-only and mutable sequences.
Implementation note: Some of the mixin methods, such as __iter__(), __reversed__() and index(),
make repeated calls to the underlying __getitem__() method.
Consequently, if __getitem__() is imple-
mented with constant access speed, the mixin methods will have linear performance; however, if the underlying
method is linear (as it would be with a linked list), the mixins will have quadratic performance and will likely need
to be overridden.
The ByteString ABC has been deprecated.
For use in typing, prefer a union, like bytes | bytearray, or collections.abc.
Buffer.
For use as
an ABC, prefer Sequence or collections.abc.
Buffer.
Set
8.5.
MutableSet
ABCs for read-only and mutable sets.
Mapping
class collections.abc.
MutableMapping
ABCs for read-only and mutable mappings.
MappingView
class collections.abc.
ItemsView
class collections.abc.
KeysView
class collections.abc.
ValuesView
ABCs for mapping, items, keys, and values views.
Awaitable
ABC for awaitable objects, which can be used in await expressions.
Custom implementations must provide the
__await__() method.
Coroutine objects and instances of the Coroutine ABC are all instances of this ABC.
Note:
In CPython, generator-based coroutines (generators decorated with types.coroutine()) are
awaitables, even though they do not have an __await__() method.
Using isinstance(gencoro,
Awaitable) for them will return False.
Use inspect.isawaitable() to detect them.
New in version 3.5.
Coroutine
ABC for coroutine compatible classes.
These implement the following methods, defined in coroutine-objects:
send(), throw(), and close().
Custom implementations must also implement __await__().
All
Coroutine instances are also instances of Awaitable.
See also the definition of coroutine.
Note:
In CPython, generator-based coroutines (generators decorated with types.coroutine()) are
awaitables, even though they do not have an __await__() method.
Using isinstance(gencoro,
Coroutine) for them will return False.
Use inspect.isawaitable() to detect them.
New in version 3.5.
AsyncIterable
ABC for classes that provide __aiter__ method.
See also the definition of asynchronous iterable.
New in version 3.5.
See also the definition of asynchronous
iterator.
New in version 3.5.
AsyncGenerator
ABC for asynchronous generator classes that implement the protocol defined in PEP 525 and PEP 492.
New in version 3.6.
Buffer
ABC for classes that provide the __buffer__() method, implementing the buffer protocol.
See PEP 688.
Sized):
size = len(myvar)
Several of the ABCs are also useful as mixins that make it easier to develop classes supporting container APIs.
For
example, to write a class supporting the full Set API, it is only necessary to supply the three underlying abstract
methods: __contains__(), __iter__(), and __len__().
The ABC supplies the remaining methods such
as __and__() and isdisjoint():
class ListBasedSet(collections.abc.
Alternate set implementation favoring space over speed
and not requiring the set elements to be hashable.
The __and__() method is supported automatically
Notes on using Set and MutableSet as a mixin:
(1) Since some set operations create new sets, the default mixin methods need a way to create new instances from an
iterable.
The class constructor is assumed to have a signature in the form ClassName(iterable).
That as-
sumption is factored-out to an internal classmethod called _from_iterable() which calls cls(iterable)
to produce a new set.
If the Set mixin is being used in a class with a different constructor signature, you will need
to override _from_iterable() with a classmethod or regular method that can construct new instances from
an iterable argument.
OrderedSet recipe for an example built on MutableSet.
Lib/heapq.py
This module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm.
Heaps are binary trees for which every parent node has a value less than or equal to any of its children.
This implementation
uses arrays for which heap[k] <= heap[2*k+1] and heap[k] <= heap[2*k+2] for all k, counting elements
from zero.
For the sake of comparison, non-existing elements are considered to be infinite.
The interesting property of a
heap is that its smallest element is always the root, heap[0].
The API below differs from textbook heap algorithms in two aspects: (a) We use zero-based indexing.
This makes the
relationship between the index for a node and the indexes for its children slightly less obvious, but is more suitable since
Python uses zero-based indexing.
These two make it possible to view the heap as a regular Python list without surprises: heap[0] is the smallest item,
and heap.sort() maintains the heap invariant!
To create a heap, use a list initialized to [], or you can transform a populated list into a heap via function heapify().
Push the value item onto the heap, maintaining the heap invariant.
heapq.heappop(heap)
Pop and return the smallest item from the heap, maintaining the heap invariant.
If the heap is empty, IndexError
is raised.
Push item on the heap, then pop and return the smallest item from the heap.
The combined action runs more
efficiently than heappush() followed by a separate call to heappop().
heapq.heapify(x)
Transform list x into a heap, in-place, in linear time.
Pop and return the smallest item from the heap, and also push the new item.
The heap size doesn’t change.
If the
heap is empty, IndexError is raised.
This one step operation is more efficient than a heappop() followed by heappush() and can be more ap-
propriate when using a fixed-size heap.
The pop/push combination always returns an element from the heap and
replaces it with item.
The value returned may be larger than the item added.
If that isn’t desired, consider using heappushpop()
instead.
Its push/pop combination returns the smaller of the two values, leaving the larger value on the heap.
The module also offers three general purpose functions based on heaps.
Returns an iterator over the sorted values.
Has two optional arguments which must be specified as keyword arguments.
The
default value is None (compare the elements directly).
If set to True, then the input elements are merged as if each comparison were reversed.
To achieve behavior similar to sorted(itertools.chain(*iterables), reverse=True), all it-
erables must be sorted from largest to smallest.
Added the optional key and reverse parameters.
Equivalent to: sorted(iterable, key=key, reverse=True)[:n].
Equivalent to: sorted(iterable, key=key)[:n].
The latter two functions perform best for smaller values of n. For larger values, it is more efficient to use the sorted()
function.
Also, when n==1, it is more efficient to use the built-in min() and max() functions.
If repeated usage of
these functions is required, consider turning the iterable into an actual heap.
Heap elements can be tuples.
This is useful for assigning comparison values (such as task priorities) alongside the main
record being tracked:
(1, 'write spec')
8.6.
Sort stability: how do you get two tasks with equal priorities to be returned in the order they were originally added?
If the priority of a task changes, how do you move it to a new position in the heap?
• Or if a pending task needs to be deleted, how do you find it and remove it from the queue?
A solution to the first two challenges is to store entries as 3-element list including the priority, an entry count, and the task.
The entry count serves as a tie-breaker so that two tasks with the same priority are returned in the order they were added.
And since no two entry counts are the same, the tuple comparison will never attempt to directly compare two tasks.
The remaining challenges revolve around finding a pending task and making changes to its priority or removing it entirely.
Finding a task can be done with a dictionary pointing to an entry in the queue.
Removing the entry or changing its priority is more difficult because it would break the heap structure invariants.
For
the sake of comparison, non-existing elements are considered to be infinite.
The interesting property of a heap is that
a[0] is always its smallest element.
The strange invariant above is meant to be an efficient memory representation for a tournament.
The numbers below are
k, not a[k]:















15 16
17 18
19 20
21 22
23 24
25 26
27 28
29 30
In the tree above, each cell k is topping 2*k+1 and 2*k+2.
However, in
many computer applications of such tournaments, we do not need to trace the history of a winner.
To be more memory
efficient, when a winner is promoted, we try to replace it by something else at a lower level, and the rule becomes that a
cell and the two cells it tops contain three different items, but the top cell “wins” over the two topped cells.
If this heap invariant is protected at all time, index 0 is clearly the overall winner.
The simplest algorithmic way to remove
it and find the “next” winner is to move some loser (let’s say cell 30 in the diagram above) into the 0 position, and then
percolate this new 0 down the tree, exchanging values, until the invariant is re-established.
This is clearly logarithmic on
the total number of items in the tree.
By iterating over all items, you get an O(n log n) sort.
A nice feature of this sort is that you can efficiently insert new items while the sort is going on, provided that the inserted
items are not “better” than the last 0’th element you extracted.
This is especially useful in simulation contexts, where
the tree holds all incoming events, and the “win” condition means the smallest scheduled time.
When an event schedules
other events for execution, they are scheduled into the future, so they can easily go into the heap.
So, a heap is a good
structure for implementing schedulers (this is what I used for my MIDI sequencer :-).
Various structures for implementing schedulers have been extensively studied, and heaps are good for this, as they are
reasonably speedy, the speed is almost constant, and the worst case is not much different than the average case.
However,
there are other representations which are more efficient overall, yet the worst cases might be terrible.
Heaps are also very useful in big disk sorts.
You most probably all know that a big sort implies producing “runs” (which
are pre-sorted sequences, whose size is usually related to the amount of CPU memory), followed by a merging passes for
these runs, which merging is often very cleverly organised1.
It is very important that the initial sort produces the longest
runs possible.
Tournaments are a good way to achieve that.
If, using all the memory available to hold a tournament, you
replace and percolate items that happen to fit the current run, you’ll produce runs which are twice the size of the memory
for random input, and much better for input fuzzily ordered.
The disk balancing algorithms which are current, nowadays, are more annoying than clever, and this is a consequence of the seeking capabilities
of the disks.
On devices which cannot seek, like big tape drives, the story was quite different, and one had to be very clever to ensure (far in advance)
that each tape movement will be the most effective possible (that is, will best participate at “progressing” the merge).
Some tapes were even able to
read backwards, and this was also used to avoid the rewinding time.
Believe me, real good tape sorts were quite spectacular to watch!
Moreover, if you output the 0’th item on disk and get an input which may not fit in the current tournament (because the
value “wins” over the last output value), it cannot fit in the heap, so the size of the heap decreases.
The freed memory
could be cleverly reused immediately for progressively building a second heap, which grows at exactly the same rate the
first heap is melting.
When the first heap completely vanishes, you switch heaps and start a new run.
Clever and quite
effective!
In a word, heaps are useful memory structures to know.
For
long lists of items with expensive comparison operations, this can be an improvement over linear searches or frequent
resorting.
The module is called bisect because it uses a basic bisection algorithm to do its work.
Unlike other bisection tools
that search for a specific value, the functions in this module are designed to locate an insertion point.
Accordingly, the
functions never call an __eq__() method to determine whether a value has been found.
Instead, the functions only call
the __lt__() method and will return an insertion point between values in an array.
The following functions are provided:
bisect.bisect_left(a, x, lo=0, hi=len(a), *, key=None)
Locate the insertion point for x in a to maintain sorted order.
The parameters lo and hi may be used to specify
a subset of the list which should be considered; by default the entire list is used.
If x is already present in a, the
insertion point will be before (to the left of) any existing entries.
The return value is suitable for use as the first
parameter to list.insert() assuming that a is already sorted.
To support searching complex records, the key function is not applied to the x value.
If key is None, the elements are compared directly and no key function is called.
Added the key parameter.
Added the key parameter.
bisect.insort_left(a, x, lo=0, hi=len(a), *, key=None)
Insert x in a in sorted order.
This function first runs bisect_left() to locate an insertion point.
Next, it runs the insert() method on a
to insert x at the appropriate position to maintain sort order.
To support inserting records in a table, the key function (if any) is applied to x for the search step but not for the
insertion step.
Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step.
Added the key parameter.
bisect.insort_right(a, x, lo=0, hi=len(a), *, key=None)
bisect.insort(a, x, lo=0, hi=len(a), *, key=None)
Similar to insort_left(), but inserting x in a after any existing entries of x.
This function first runs bisect_right() to locate an insertion point.
Next, it runs the insert() method on
a to insert x at the appropriate position to maintain sort order.
To support inserting records in a table, the key function (if any) is applied to x for the search step but not for the
insertion step.
Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step.
Added the key parameter.
For locating specific values, dictionaries are more performant.
The insort() functions are O(n) because the logarithmic search step is dominated by the linear time insertion step.
The search functions are stateless and discard key function results after they are used.
Consequently, if the search
functions are used in a loop, the key function may be called again and again on the same array elements.
If the
key function isn’t fast, consider wrapping it with functools.cache() to avoid duplicate computations.
Alter-
natively, consider searching an array of precomputed keys to locate the insertion point (as shown in the examples
section below).
See also:
• Sorted Collections is a high performance module that uses bisect to managed sorted collections of data.
The SortedCollection recipe uses bisect to build a full-featured collection class with straight-forward search methods
and support for a key-function.
The keys are precomputed to save unnecessary calls to the key function during
searches.
The following five functions show how to transform them into the standard lookups for sorted lists:

i = bisect_left(a, x)
if i !
If the key function is expensive, it is possible to avoid repeated function calls by searching a list of precomputed keys to
find the index of a record:
# Or use operator.itemgetter(1).
Arrays are sequence types and behave very much like lists, except that the type of objects stored in them
is constrained.
The type is specified at object creation time by using a type code, which is a single character.
The following
type codes are defined:
8.8.
Changed in version 3.9: array('u') now uses wchar_t as C type instead of deprecated Py_UNICODE.
This
change doesn’t affect its behavior because Py_UNICODE is alias of wchar_t since Python 3.3.
Deprecated since version 3.3, will be removed in version 4.0.
The actual representation of values is determined by the machine architecture (strictly speaking, by the C implementation).
The actual size can be accessed through the array.itemsize attribute.
The module defines the following item:
array.typecodes
A string with all available type codes.
A new array whose items are restricted by typecode, and initialized from the optional initializer value, which must
be a list, a bytes-like object, or iterable over elements of the appropriate type.
If given a list or string, the initializer is passed to the new array’s fromlist(), frombytes(), or
fromunicode() method (see below) to add initial items to the array.
Otherwise, the iterable initializer is
passed to the extend() method.
Array objects support the ordinary sequence operations of indexing, slicing, concatenation, and multiplication.
When using slice assignment, the assigned value must be an array object with the same type code; in all other cases,
TypeError is raised.
Array objects also implement the buffer interface, and may be used wherever bytes-like
objects are supported.
The size of the memory buffer in bytes can be computed as array.
I/O interfaces that require memory addresses, such as certain ioctl() operations.
The returned numbers are valid as long as the array exists and no length-changing operations are applied to
it.
Note:
When using array objects from code written in C or C++ (the only way to effectively make use of
this information), it makes more sense to use the buffer interface supported by array objects.
This method is
maintained for backward compatibility and should be avoided in new code.
The buffer interface is documented
in bufferobjects.
This is only supported for values which are 1, 2, 4, or 8 bytes in size;
for other types of values, RuntimeError is raised.
It is useful when reading data from a file written on a
machine with a different byte order.
Append items from iterable to the end of the array.
If iterable is another array, it must have exactly the same
type code; if not, TypeError will be raised.
If iterable is not an array, it must be iterable and its elements
must be the right type to be appended to the array.
Appends items from the string, interpreting the string as an array of machine values (as if it had been read
from a file using the fromfile() method).
New in version 3.2: fromstring() is renamed to frombytes() for clarity.
If less than
n items are available, EOFError is raised, but the items that were available are still inserted into the array.
Append items from the list.
Extends this array with data from the given unicode string.
The array must be a type 'u' array; otherwise
a ValueError is raised.
Use array.frombytes(unicodestring.encode(enc)) to append
Unicode data to an array of some other type.
Return the smallest i such that i is the index of the first occurrence of x in the array.
The optional arguments
start and stop can be specified to search for x within a subsection of the array.
Raise ValueError if x is
not found.
Changed in version 3.10: Added optional start and stop parameters.
Removes the item with the index i from the array and returns it.
The optional argument defaults to -1, so
that by default the last item is removed and returned.
Remove the first occurrence of x from the array.
New in version 3.2: tostring() is renamed to tobytes() for clarity.
Convert the array to an ordinary list with the same items.
The array must be a type 'u' array; otherwise a ValueError is
raised.
Use array.tobytes().decode(enc) to obtain a unicode string from an array of some other
type.
When an array object is printed or converted to a string, it is represented as array(typecode, initializer).
The initializer is omitted if the array is empty, otherwise it is a string if the typecode is 'u', otherwise it is a list of numbers.
The string is guaranteed to be able to be converted back to an array with the same type and value using eval(), so long
as the array class has been imported using from array import array.
Module xdrlib Packing and unpacking of External Data Representation (XDR) data as used in some remote proce-
dure call systems.
NumPy The NumPy package defines another array type.
In the following, the term referent means the object which is referred to by a weak reference.
A weak reference to an object is not enough to keep the object alive: when the only remaining references to a referent
are weak references, garbage collection is free to destroy the referent and reuse its memory for something else.
However,
until the object is actually destroyed the weak reference may return the object even if there are no strong references to it.
A primary use for weak references is to implement caches or mappings holding large objects, where it’s desired that a
large object not be kept alive solely because it appears in a cache or mapping.
For example, if you have a number of large binary image objects, you may wish to associate a name with each.
If you
used a Python dictionary to map names to images, or images to names, the image objects would remain alive just because
they appeared as values or keys in the dictionaries.
The WeakKeyDictionary and WeakValueDictionary
classes supplied by the weakref module are an alternative, using weak references to construct mappings that don’t
keep objects alive solely because they appear in the mapping objects.
If, for example, an image object is a value in a
WeakValueDictionary, then when the last remaining references to that image object are the weak references held
by weak mappings, garbage collection can reclaim the object, and its corresponding entries in weak mappings are simply
deleted.
WeakKeyDictionary and WeakValueDictionary use weak references in their implementation, setting up call-
back functions on the weak references that notify the weak dictionaries when a key or value has been reclaimed by
garbage collection.
WeakSet implements the set interface, but keeps weak references to its elements, just like a
WeakKeyDictionary does.
This is simpler to use than setting up a callback function on a raw weak reference, since the module automatically ensures
that the finalizer remains alive until the object is collected.
Most programs should find that using one of these weak container types or finalize is all they need – it’s not usually
necessary to create your own weak references directly.
The low-level machinery is exposed by the weakref module for
the benefit of advanced uses.
Not all objects can be weakly referenced.
Objects which support weak references include class instances, functions written
in Python (but not in C), instance methods, sets, frozensets, some file objects, generators, type objects, sockets, arrays,
deques, regular expression pattern objects, and code objects.
Changed in version 3.2: Added support for thread.lock, threading.
Lock, and code objects.
Extension types can easily be made to support weak references; see weakref-support.
See __slots__ documentation for details.
The original object can be retrieved by calling the reference object if the referent
is still alive; if the referent is no longer alive, calling the reference object will cause None to be returned.
If callback
is provided and not None, and the returned weakref object is still alive, the callback will be called when the object
is about to be finalized; the weak reference object will be passed as the only parameter to the callback; the referent
will no longer be available.
It is allowable for many weak references to be constructed for the same object.
Callbacks registered for each weak
reference will be called from the most recently registered callback to the oldest registered callback.
Exceptions raised by the callback will be noted on the standard error output, but cannot be propagated; they are
handled in exactly the same way as exceptions raised from an object’s __del__() method.
Weak references are hashable if the object is hashable.
They will maintain their hash value even after the object
was deleted.
If hash() is called the first time only after the object was deleted, the call will raise TypeError.
Weak references support tests for equality, but not ordering.
If the referents are still alive, two references have the
same equality relationship as their referents (regardless of the callback).
If either referent has been deleted, the
references are equal only if the reference objects are the same object.
This is a subclassable type rather than a factory function.
This read-only attribute returns the callback currently associated to the weakref.
If there is no callback or if
the referent of the weakref is no longer alive then this attribute will have value None.
Changed in version 3.4: Added the __callback__ attribute.
This supports use of the proxy in most contexts instead
of requiring the explicit dereferencing used with weak reference objects.
The returned object will have a type of
either ProxyType or CallableProxyType, depending on whether object is callable.
Proxy objects are not
hashable regardless of the referent; this avoids a number of problems related to their fundamentally mutable nature,
and prevents their use as dictionary keys.
Accessing an attribute of the proxy object after the referent is garbage collected raises ReferenceError.
Extended the operator support on proxy objects to include the matrix multiplication op-
erators @ and @=.
weakref.getweakrefcount(object)
Return the number of weak references and proxies which refer to object.
weakref.getweakrefs(object)
Return a list of all weak reference and proxy objects which refer to object.
WeakKeyDictionary([dict])
Mapping class that references keys weakly.
Entries in the dictionary will be discarded when there is no longer a
strong reference to the key.
This can be used to associate additional data with an object owned by other parts of
an application without adding attributes to those objects.
This can be especially useful with objects that override
attribute accesses.
Note that when a key with equal value to an existing key (but not equal identity) is inserted into the dictionary,
it replaces the value but does not replace the existing key.
WeakKeyDictionary objects have an additional method that exposes the internal references directly.
The references
are not guaranteed to be “live” at the time they are used, so the result of calling the references needs to be checked before
being used.
This can be used to avoid creating references that will cause the garbage collector to keep the keys around
longer than needed.
WeakKeyDictionary.keyrefs()
Return an iterable of the weak references to the keys.
Entries in the dictionary will be discarded when no strong reference
to the value exists any more.
Changed in version 3.9: Added support for | and |= operators, as specified in PEP 584.
WeakValueDictionary
objects
have
an
additional
method
that
has
the
same
issues
as
the
WeakKeyDictionary.keyrefs() method.
WeakValueDictionary.valuerefs()
Return an iterable of the weak references to the values.
WeakSet([elements])
Set class that keeps weak references to its elements.
An element will be discarded when no strong reference to it
exists any more.
A custom ref subclass which simulates a weak reference to a bound method (i.e., a method defined on a class and
looked up on an instance).
Since a bound method is ephemeral, a standard weak reference cannot keep hold of it.
WeakMethod has special code to recreate the bound method until either the object or the original function dies:


print("method called!")

<bound method C.method of <__main__.C object at 0x7fc859830220>>
method called!

callback is the same as the parameter of the same name to the ref() function.
New in version 3.4.
Unlike an ordinary weak
reference, a finalizer will always survive until the reference object is collected, greatly simplifying lifecycle man-
agement.
A finalizer is considered alive until it is called (either explicitly or at garbage collection), and after that it is dead.
Calling a live finalizer returns the result of evaluating func(*arg, **kwargs), whereas calling a dead final-
izer returns None.
Exceptions raised by finalizer callbacks during garbage collection will be shown on the standard error output, but
cannot be propagated.
They are handled in the same way as exceptions raised from an object’s __del__()
method or a weak reference’s callback.
When the program exits, each remaining live finalizer is called unless its atexit attribute has been set to false.
They are called in reverse order of creation.
If self is alive then mark it as dead and return the result of calling func(*args, **kwargs).
If self is alive then mark it as dead and return the tuple (obj, func, args, kwargs).
If self is alive then return the tuple (obj, func, args, kwargs).
If self is dead then return None.
When the program exits, it calls all remaining live
finalizers for which atexit is true.
They are called in reverse order of creation.
Note:
It is important to ensure that func, args and kwargs do not own any references to obj, either directly or
indirectly, since otherwise obj will never be garbage collected.
In particular, func should not be a bound method
of obj.
New in version 3.4.
ReferenceType
The type object for weak references objects.
The type object for proxies of objects which are not callable.
weakref.
The type object for proxies of callable objects.
ProxyTypes
Sequence containing all the type objects for proxies.
This can make it simpler to test if an object is a proxy without
being dependent on naming both proxy types.
True
If the referent no longer exists, calling the reference object returns None:
None
Testing that a weak reference object is still live should be done using the expression ref() is not None.
Specialized versions of ref objects can be created through subclassing.
This is used in the implementation of the
WeakValueDictionary to reduce the memory overhead for each entry in the mapping.
This may be most useful to
associate additional information with a reference, but could also be used to insert additional processing on calls to retrieve
the referent.
The IDs of
the objects can then be used in other data structures without forcing the objects to remain alive, but the objects can still
be retrieved by ID if they do.
WeakValueDictionary()

_id2obj_dict[oid] = obj


8.9.3 Finalizer Objects
The main benefit of using finalize is that it makes it simple to register a callback without needing to preserve the
returned finalizer object.
You killed Kenny!
The finalizer can be called directly as well.
This kills the finalizer and returns the arguments passed to
the constructor when it was created.
CALLBACK
Unless you set the atexit attribute to False, a finalizer will be called when the program exits if it is still alive.
The directories should be deleted
with their contents when the first of the following events occurs:
• the object is garbage collected,
• the object’s remove() method is called, or
• the program exits.
So this code should work without any issues
on CPython.
However, handling of __del__() methods is notoriously implementation specific, since it depends on internal details
of the interpreter’s garbage collector implementation.
A more robust alternative can be to define a finalizer which only references the specific functions and objects that it needs,
rather than having access to the full state of the object:
class TempDir:

self._finalizer = weakref.finalize(self, shutil.rmtree, self.name)



Defined like this, our finalizer only receives a reference to the details it needs to clean up the directory appropriately.
If
the object never gets garbage collected the finalizer will still be called at exit.
Note: If you create a finalizer object in a daemonic thread just as the program exits then there is the possibility that the
finalizer does not get called at exit.
However, in a daemonic thread atexit.register(), try: ... finally:
... and with: ... do not guarantee that cleanup occurs either.
It also defines names for some object types that are used by the standard Python interpreter, but not exposed as builtins
like int or str are.
The first three arguments are the components that make up a class definition header: the class name, the base classes
(in order), the keyword arguments (such as metaclass).
The exec_body argument is a callback that is used to populate the freshly created class namespace.
It should accept
the class namespace as its sole argument and update the namespace directly with the class contents.
None.
New in version 3.3.
The arguments are the components that make up a class definition header: the class name, the base classes (in
order) and the keyword arguments (such as metaclass).
The return value is a 3-tuple: metaclass, namespace, kwds
metaclass is the appropriate metaclass, namespace is the prepared class namespace and kwds is an updated copy of
the passed in kwds argument with any 'metaclass' entry removed.
If no kwds argument is passed in, this will
be an empty dict.
New in version 3.3.
The default value for the namespace element of the returned tuple has changed.
Now
an insertion-order-preserving mapping is used when the metaclass does not have a __prepare__ method.
Metaclasses in Python 3000 Introduced the __prepare__ namespace hook
types.resolve_bases(bases)
Resolve MRO entries dynamically as specified by PEP 560.
This function looks for items in bases that are not instances of type, and returns a tuple where each such object
that has an __mro_entries__() method is replaced with an unpacked result of calling this method.
New in version 3.7.
This is useful for introspecting Generics.
For classes that have an __orig_bases__ attribute,
this function returns the value of cls.
__orig_bases__.
For classes without the __orig_bases__ attribute, cls.__bases__ is returned.
New in version 3.12.
This module provides names for many of the types that are required to implement a Python interpreter.
It deliberately
avoids including some of the types that arise only incidentally during processing such as the listiterator type.
Typical use of these names is for isinstance() or issubclass() checks.
If you instantiate any of these types, note that signatures may vary between Python versions.
Standard names are defined for the following types:
types.
NoneType
The type of None.
New in version 3.10.
types.
FunctionType
types.
The type of user-defined functions and functions created by lambda expressions.
The audit event only occurs for direct instantiation of function objects, and is not raised for normal compilation.
types.
The type of generator-iterator objects, created by generator functions.
types.
CoroutineType
The type of coroutine objects, created by async def functions.
The type of asynchronous generator-iterator objects, created by asynchronous generator functions.
New in version 3.6.
The type for code objects such as returned by compile().
Note that the audited arguments may not match the names or positions required by the initializer.
The audit event
only occurs for direct instantiation of code objects, and is not raised for normal compilation.
replace(**kwargs)
Return a copy of the code object with new values for the specified fields.
New in version 3.8.
types.
CellType
The type for cell objects: such objects are used as containers for a function’s free variables.
New in version 3.8.
types.
MethodType
The type of methods of user-defined class instances.
types.
BuiltinFunctionType
types.
The type of built-in functions like len() or sys.exit(), and methods of built-in classes.
The type of methods of some built-in data types and base classes such as object.__init__() or object.
__lt__().
New in version 3.7.
types.
The type of bound methods of some built-in data types and base classes.
For example it is the type of object().
__str__.
New in version 3.7.
types.
NotImplementedType
The type of NotImplemented.
New in version 3.10.
types.
The type of methods of some built-in data types such as str.join().
New in version 3.7.
types.
ClassMethodDescriptorType
The type of unbound class methods of some built-in data types such as dict.__dict__['fromkeys'].
New in version 3.7.
The type of modules.
The constructor takes the name of the module to be created and optionally its docstring.
Note: Use importlib.util.module_from_spec() to create a new module if you wish to set the various
import-controlled attributes.
The docstring of the module.
The loader which loaded the module.
Defaults to None.
This attribute is to match importlib.machinery.ModuleSpec.loader as stored in the
__spec__ object.
Note:
A future version of Python may stop setting this attribute by default.
To guard against this
potential change, preferably read from the __spec__ attribute instead or use getattr(module,
"__loader__", None) if you explicitly need to use this attribute.
Changed in version 3.4: Defaults to None.
Previously the attribute was optional.
The name of the module.
Expected to match importlib.machinery.ModuleSpec.name.
__package__
Which package a module belongs to.
If the module is top-level (i.e. not a part of any specific package) then
the attribute should be set to '', else it should be set to the name of the package (which can be __name__
if the module is a package itself).
Defaults to None.
This attribute is to match importlib.machinery.ModuleSpec.parent as stored in the
__spec__ object.
Note:
A future version of Python may stop setting this attribute by default.
To guard against this
potential change, preferably read from the __spec__ attribute instead or use getattr(module,
"__package__", None) if you explicitly need to use this attribute.
Changed in version 3.4: Defaults to None.
Previously the attribute was optional.
A record of the module’s import-system-related state.
Expected to be an instance of importlib.
machinery.
ModuleSpec.
New in version 3.4.
types.
EllipsisType
The type of Ellipsis.
New in version 3.10.
See also:
Generic Alias Types In-depth documentation on instances of types.
GenericAlias
PEP 585 - Type Hinting Generics In Standard Collections Introducing the types.
GenericAlias class
class types.
UnionType
The type of union type expressions.
New in version 3.10.
The type of traceback objects such as found in sys.exception().__traceback__.
See the language reference for details of the available attributes and operations, and guidance on creating tracebacks
dynamically.
types.
FrameType
The type of frame objects such as found in tb.tb_frame if tb is a traceback object.
See the language reference for details of the available attributes and operations.
types.
The type of objects defined in extension modules with PyGetSetDef, such as FrameType.f_locals or
array.array.typecode.
This type is used as descriptor for object attributes; it has the same purpose as the
property type, but for classes defined in extension modules.
types.
MemberDescriptorType
The type of objects defined in extension modules with PyMemberDef, such as datetime.timedelta.
This type is used as descriptor for simple C data members which use standard conversion functions; it has
the same purpose as the property type, but for classes defined in extension modules.
CPython implementation detail:
In other implementations of Python, this type may be identical to
GetSetDescriptorType.
class types.
MappingProxyType(mapping)
Read-only proxy of a mapping.
It provides a dynamic view on the mapping’s entries, which means that when the
mapping changes, the view reflects these changes.
New in version 3.3.
Changed in version 3.9: Updated to support the new union (|) operator from PEP 584, which simply delegates to
the underlying mapping.
Raises a KeyError if key is not in the underlying
mapping.
This is a shortcut for iter(proxy.keys()).
If default is not given, it defaults to
None, so that this method never raises a KeyError.
New in version 3.9.
New in version 3.12.
SimpleNamespace
A simple object subclass that provides attribute access to its namespace, as well as a meaningful repr.
Unlike object, with SimpleNamespace you can add and remove attributes.
If a SimpleNamespace
object is initialized with keyword arguments, those are directly added to the underlying namespace.
However, for a structured record
type use namedtuple() instead.
New in version 3.3.
Attribute order in the repr changed from alphabetical to insertion (like dict).
types.
DynamicClassAttribute(fget=None, fset=None, fdel=None, doc=None)
Route attribute access on a class to __getattr__.
This is a descriptor, used to define attributes that act differently when accessed through an instance and through
a class.
Instance access remains normal, but access to an attribute through a class will be routed to the class’s
__getattr__ method; this is done by raising AttributeError.
This allows one to have properties active on an instance, and have virtual attributes on the class with the same name
(see enum.
Enum for an example).
New in version 3.4.
This function transforms a generator function into a coroutine function which returns a generator-based coroutine.
The generator-based coroutine is still a generator iterator, but is also considered to be a coroutine object and is
awaitable.
However, it may not necessarily implement the __await__() method.
If gen_func is a generator function, it will be modified in-place.
If gen_func is not a generator function, it will be wrapped.
If it returns an instance of collections.abc.
Generator, the instance will be wrapped in an awaitable proxy object.
All other types of objects will be returned
as is.
New in version 3.5.
8.11 copy — Shallow and deep copy operations
Source code: Lib/copy.py
Assignment statements in Python do not copy objects, they create bindings between a target and an object.
For collections
that are mutable or contain mutable items, a copy is sometimes needed so one can change one copy without changing the
other.
This module provides generic shallow and deep copy operations (explained below).
Interface summary:
copy.copy(x)
Return a shallow copy of x.
copy.deepcopy(x[, memo])
Return a deep copy of x.
8.11.
Error
Raised for module specific errors.
A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the
objects found in the original.
A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in
the original.
Two problems often exist with deep copy operations that don’t exist with shallow copy operations:
• Recursive objects (compound objects that, directly or indirectly, contain a reference to themselves) may cause a
recursive loop.
The deepcopy() function avoids these problems by:
• keeping a memo dictionary of objects already copied during the current copying pass; and
• letting user-defined classes override the copying operation or the set of components copied.
This module does not copy types like module, method, stack trace, stack frame, file, socket, window, or any similar types.
It does “copy” functions and classes (shallow and deeply), by returning the original object unchanged; this is compatible
with the way these are treated by the pickle module.
Shallow copies of dictionaries can be made using dict.copy(), and of lists by assigning a slice of the entire list, for
example, copied_list = original_list[:].
Classes can use the same interfaces to control copying that they use to control pickling.
See the description of module
pickle for information on these methods.
In fact, the copy module uses the registered pickle functions from the
copyreg module.
In order for a class to define its own copy implementation, it can define special methods __copy__() and
__deepcopy__().
The former is called to implement the shallow copy operation; no additional arguments are
passed.
The latter is called to implement the deep copy operation; it is passed one argument, the memo dictionary.
If the __deepcopy__() implementation needs to make a deep copy of a component, it should call the deepcopy()
function with the component as first argument and the memo dictionary as second argument.
The memo dictionary should
be treated as an opaque object.
See also:
Module pickle Discussion of the special methods used to support object state retrieval and restoration.
8.12 pprint — Data pretty printer
Source code: Lib/pprint.py
The pprint module provides a capability to “pretty-print” arbitrary Python data structures in a form which can be
used as input to the interpreter.
If the formatted structures include objects which are not fundamental Python types, the
representation may not be loadable.
This may be the case if objects such as files, sockets or classes are included, as well
as many other objects which are not representable as Python literals.
The formatted representation keeps objects on a single line if it can, and breaks them onto multiple lines if they don’t fit
within the allowed width.
Construct PrettyPrinter objects explicitly if you need to adjust the width constraint.
Changed in version 3.9: Added support for pretty-printing types.
SimpleNamespace.
Changed in version 3.10: Added support for pretty-printing dataclasses.dataclass.
The pprint module defines one class:
class pprint.
PrettyPrinter(indent=1, width=80, depth=None, stream=None, *, compact=False,
sort_dicts=True, underscore_numbers=False)
Construct a PrettyPrinter instance.
This constructor understands several keyword parameters.
stream (default sys.stdout) is a file-like object to which the output will be written by calling its write()
method.
If both stream and sys.stdout are None, then pprint() silently returns.
Other values configure the manner in which nesting of complex data structures is displayed.
By default, there is no constraint on the depth of the objects being
formatted.
If a structure cannot
be formatted within the width constraint, a best effort will be made.
If compact is false (the default)
then each item of a sequence will be formatted on a separate line.
If compact is true, as many items as will fit within
the width will be formatted on each output line.
If sort_dicts is true (the default), dictionaries will be formatted with their keys sorted, otherwise they will display
in insertion order.
If underscore_numbers is true, integers will be formatted with the _ character for a thousands separator, otherwise
underscores are not displayed (the default).
Added the compact parameter.
Added the sort_dicts parameter.
Changed in version 3.10: Added the underscore_numbers parameter.
Changed in version 3.11: No longer attempts to write to sys.stdout if it is None.
If sort_dicts is false (the default), dictionaries
will be displayed with their keys in insertion order, otherwise the dict keys will be sorted.
New in version 3.8.
If stream is None, sys.stdout
is used.
This may be used in the interactive interpreter instead of the print() function for inspecting values (you
can even reassign print = pprint.pprint for use within a scope).
The configuration parameters stream, indent, width, depth, compact, sort_dicts and underscore_numbers are passed
to the PrettyPrinter constructor and their meanings are as described in its documentation above.
This always returns False for recursive objects.
Determine if object requires a recursive representation.
This function is subject to the same limitations as noted in
saferepr() below and may raise an RecursionError if it fails to detect a recursive object.
One more support function is also defined:
pprint.saferepr(object)
Return a string representation of object, protected against recursion in some common data structures, namely in-
stances of dict, list and tuple or subclasses whose __repr__ has not been overridden.
If the representation
of object exposes a recursive entry, the recursive reference will be represented as <Recursion on typename
with id=number>.
The representation is not otherwise formatted.
This takes into account the options passed to the PrettyPrinter
constructor.
PrettyPrinter.pprint(object)
Print the formatted representation of object on the configured stream, followed by a newline.
The following methods provide the implementations for the corresponding functions of the same names.
Using these
methods on an instance is slightly more efficient since new PrettyPrinter objects don’t need to be created.
PrettyPrinter.isreadable(object)
Determine if the formatted representation of the object is “readable,” or can be used to reconstruct the value using
eval().
Note that this returns False for recursive objects.
If the depth parameter of the PrettyPrinter
is set and the object is deeper than allowed, this returns False.
PrettyPrinter.isrecursive(object)
Determine if the object requires a recursive representation.
This method is provided as a hook to allow subclasses to modify the way objects are converted to strings.
The default
implementation uses the internals of the saferepr() implementation.
Returns three values: the formatted version of object as a string, a flag indicating whether the result is readable, and
a flag indicating whether recursion was detected.
The first argument is the object to be presented.
The second is a
dictionary which contains the id() of objects that are part of the current presentation context (direct and indirect
containers for object that are affecting the presentation) as the keys; if an object needs to be presented which is
already represented in context, the third return value should be True.
Recursive calls to the format() method
should add additional entries for containers to this dictionary.
The third argument, maxlevels, gives the requested
limit to recursion; this will be 0 if there is no requested limit.
This argument should be passed unmodified to
recursive calls.
The fourth argument, level, gives the current level; recursive calls should be passed a value less than
that of the current call.
It\n'
'will be used to generate the project webpage on PyPI, and '
'should be written for\n'
'that purpose.\n'
'\n'
'Typical contents for this file would include an overview of '
'the project, basic\n'
'usage examples, etc.
It\n'
'will be used to generate the project webpage on PyPI, and '
'should be written for\n'
'that purpose.\n'
'\n'
'Typical contents for this file would include an overview of '
'the project, basic\n'
'usage examples, etc.
Generally, including the project '
'changelog in here is not\n'
'a good idea, although a simple "What\'s New" section for the '
'most recent version\n'
'may be appropriate.',
'description_content_type': None,
'docs_url': None,
'download_url': 'UNKNOWN',
'downloads': {...},
'home_page': 'https://github.com/pypa/sampleproject',
'keywords': 'sample setuptools development',
'license': 'MIT',
'maintainer': None,
'maintainer_email': None,
'name': 'sampleproject',
'package_url': 'https://pypi.org/project/sampleproject/',
'platform': 'UNKNOWN',
'project_url': 'https://pypi.org/project/sampleproject/',
'project_urls': {...},
'release_url': 'https://pypi.org/project/sampleproject/1.2.0/',
'requires_dist': None,
'requires_python': None,
'summary': 'A sample Python project',
'version': '1.2.0'}
Additionally, maximum character width can be suggested.
It\n'
'will be used to generate the project '
'webpage on PyPI, and should be written '
'for\n'
'that purpose.\n'
'\n'
'Typical contents for this file would '
'include an overview of the project, '
'basic\n'
'usage examples, etc.
Generally, including '
'the project changelog in here is not\n'
'a good idea, although a simple "What\'s '
'New" section for the most recent version\n'
'may be appropriate.',
'description_content_type': None,
'docs_url': None,
'download_url': 'UNKNOWN',
'downloads': {...},
'home_page': 'https://github.com/pypa/sampleproject',
'keywords': 'sample setuptools development',
'license': 'MIT',
'maintainer': None,
'maintainer_email': None,
'name': 'sampleproject',
'package_url': 'https://pypi.org/project/sampleproject/',
'platform': 'UNKNOWN',
'project_url': 'https://pypi.org/project/sampleproject/',
'project_urls': {...},
'release_url': 'https://pypi.org/project/sampleproject/1.2.0/',
'requires_dist': None,
'requires_python': None,
'summary': 'A sample Python project',
'version': '1.2.0'}
8.13 reprlib — Alternate repr() implementation
Source code: Lib/reprlib.py
The reprlib module provides a means for producing object representations with limits on the size of the resulting
strings.
This is used in the Python debugger and may be useful in other contexts as well.
This module provides a class, an instance, and a function:
class reprlib.
Repr(*, maxlevel=6, maxtuple=6, maxlist=6, maxarray=5, maxdict=4, maxset=6,
maxfrozenset=6, maxdeque=6, maxstring=30, maxlong=40, maxother=30, fillvalue=’...’,
indent=None)
Class which provides formatting services useful in implementing functions similar to the built-in repr(); size
limits for different object types are added to avoid the generation of representations which are excessively long.
The keyword arguments of the constructor can be used as a shortcut to set the attributes of the Repr instance.
Repr(maxlevel=3)
Is equivalent to:
aRepr = reprlib.
Repr()
aRepr.maxlevel = 3
See section Repr Objects for more information about Repr attributes.
Changed in version 3.12: Allow attributes to be set via keyword arguments.
This is the repr() method of aRepr.
It returns a string similar to that returned by the built-in function of the
same name, but with limits on most sizes.
In addition to size-limiting tools, the module also provides a decorator for detecting recursive calls to __repr__() and
substituting a placeholder string instead.
Decorator for __repr__() methods to detect recursive calls within the same thread.
If a recursive call is made,
the fillvalue is returned, otherwise, the usual __repr__() call is made.
For example:

@recursive_repr()




<'a'|'b'|'c'|...|'x'>
New in version 3.2.
Repr.fillvalue
This string is displayed for recursive references.
It defaults to ....
New in version 3.11.
Repr.maxlevel
Depth limit on the creation of recursive representations.
The default is 6.
Repr.maxdict
Repr.maxlist
Repr.maxtuple
Repr.maxset
8.13.
The default is 4 for maxdict, 5 for
maxarray, and 6 for the others.
Repr.maxlong
Maximum number of characters in the representation for an integer.
Digits are dropped from the middle.
The
default is 40.
Repr.maxstring
Limit on the number of characters in the representation of the string.
Note that the “normal” representation of the
string is used as the character source: if escape sequences are needed in the representation, these may be mangled
when the representation is shortened.
The default is 30.
Repr.maxother
This limit is used to control the size of object types for which no specific formatting method is available on the
Repr object.
It is applied in a similar manner as maxstring.
The default is 20.
Repr.indent
If this attribute is set to None (the default), the output is formatted with no line breaks or indentation, like the
standard repr().
The equivalent to the built-in repr() that uses the formatting imposed by the instance.
Repr.repr1(obj, level)
Recursive implementation used by repr().
This uses the type of obj to determine which formatting method to
call, passing it obj and level.
The type-specific methods should call repr1() to perform recursive formatting,
with level - 1 for the value of level in the recursive call.
Repr.repr_TYPE(obj, level)
Formatting methods for specific types are implemented as methods with a name based on the type name.
In the
method name, TYPE is replaced by '_'.join(type(obj).__name__.split()).
Dispatch to these
methods is handled by repr1().
Type-specific methods which need to recursively format a value should call
self.repr1(subobj, level - 1).
Subclassing Repr Objects
The use of dynamic dispatching by Repr.repr1() allows subclasses of Repr to add support for additional built-in
object types or to modify the handling of types already supported.
This example shows how special support for file objects
could be added:
import reprlib
import sys
class MyRepr(reprlib.
Source code: Lib/enum.py
Important
This page contains the API reference information.
BLUE = 3
Even though we can use class syntax to create Enums, Enums are not normal Python classes.
See How are Enums
different?
The attributes Color.
RED, Color.
GREEN, etc., are enumeration members (or members) and are functionally
constants.
The enum members have names and values (the name of Color.
RED is RED, the value of Color.
The type for Enum and its subclasses.
Enum
Base class for creating enumerated constants.
IntEnum
Base class for creating enumerated constants that are also subclasses of int.
IntFlag
Base class for creating enumerated constants that can be combined using the bitwise operators
without losing their IntFlag membership.
IntFlag members are also subclasses of int.
EnumCheck
An enumeration with the values CONTINUOUS, NAMED_FLAGS, and UNIQUE, for use with
verify() to ensure various constraints are met by a given enumeration.
FlagBoundary
An enumeration with the values STRICT, CONFORM, EJECT, and KEEP which allows for more
fine-grained control over how invalid values are dealt with in an enumeration.
StrEnum defaults to the
lower-cased version of the member name, while other Enums default to 1 and increase from there.
The value
and name attributes are implemented this way.
Can be used as a decorator.
nonmember()
Do not make obj a member.
Can be used as a decorator.
New in version 3.6: Flag, IntFlag, auto
New in version 3.11: StrEnum, EnumCheck, ReprEnum, FlagBoundary, property, member, nonmember,
global_enum, show_flag_values
8.14.2 Data Types
class enum.
EnumType
EnumType is the metaclass for enum enumerations.
It is possible to subclass EnumType – see Subclassing Enum-
Type for details.
The enum class being called.
RED: 1>, <Color.
GREEN: 2>, <Color.
BLUE: 3>, <Color.
GREEN: 2>, <Color.
Before 3.11 enum used EnumMeta type, which is kept as an alias.
Enum
Enum is the base class for all enum enumerations.
If the exact value is unimportant you may use auto
instances and an appropriate value will be chosen for you.
See auto for the details.
A classmethod that is used to further configure subsequent subclasses.
By default, does nothing.
A classmethod for looking up values not found in cls.
DEBUG: 'debug'>
__repr__(self)
Returns the string used for repr() calls.
By default, returns the Enum name, member name, and value, but can
be overridden:

ALTERNATE = auto()

OTHER = auto()

SOMETHING_ELSE = auto()


cls_name = self.__class__.__name__



(OtherStyle.ALTERNATE, 'OtherStyle.ALTERNATE', 'OtherStyle.ALTERNATE')
__str__(self)
Returns the string used for str() calls.
By default, returns the Enum name and member name, but can be
overridden:

ALTERNATE = auto()

OTHER = auto()

SOMETHING_ELSE = auto()




(<OtherStyle.
ALTERNATE: 1>, 'ALTERNATE', 'ALTERNATE')
__format__(self)
Returns the string used for format() and f-string calls.
By default, returns __str__() return value, but can
be overridden:

ALTERNATE = auto()

OTHER = auto()

SOMETHING_ELSE = auto()



(continues on next page)
8.14.
ALTERNATE: 1>, 'OtherStyle.
Note: Using auto with Enum results in integers of increasing value, starting with 1.
Changed in version 3.12: Added enum-dataclass-support
class enum.
IntEnum
IntEnum is the same as Enum, but its members are also integers and can be used anywhere that an integer can be
used.
If any integer operation is performed with an IntEnum member, the resulting value loses its enumeration
status.
THREE: 3>


True
Note: Using auto with IntEnum results in integers of increasing value, starting with 1.
Changed in version 3.11: __str__() is now int.__str__() to better support the replacement of existing
constants use-case.
StrEnum
StrEnum is the same as Enum, but its members are also strings and can be used in most of the same places that a
string can be used.
The result of any string operation performed on or with a StrEnum member is not part of the
enumeration.
Note:
There are places in the stdlib that check for an exact str instead of a str subclass (i.e.
type(unknown) == str instead of isinstance(unknown, str)), and in those locations you will
need to use str(StrEnum.member).
Note: Using auto with StrEnum results in the lower-cased member name as the value.
Note:
__str__() is str.__str__() to better support the replacement of existing constants use-case.
__format__() is likewise str.__format__() for that same reason.
RED: 1>, <Color.
BLUE: 4>]
Changed in version 3.11: Aliases are no longer returned during iteration.
GREEN: 2>
<Color.
GREEN: 2>
<Color.
GREEN|BLUE: 6>
_numeric_repr_()
Function used to format any remaining unnamed numeric values.
Default is the value’s repr; common choices
are hex() and oct().
Note: Using auto with Flag results in integers that are powers of two, starting with 1.
Changed in version 3.11: The repr() of zero-valued flags has changed.
It is now::
<Color: 0>
class enum.
It is now:
<Color: 0>
Note: Using auto with IntFlag results in integers that are powers of two, starting with 1.
Changed in version 3.11: __str__() is now int.__str__() to better support the replacement of existing
constants use-case.
Inversion of an IntFlag now returns a positive value that is the union of all flags not in the given flag, rather than
a negative value.
This matches the existing Flag behavior.
ReprEnum
ReprEnum uses the repr() of Enum, but the str() of the mixed-in data type:
• int.__str__() for IntEnum and IntFlag
• str.__str__() for StrEnum
Inherit from ReprEnum to keep the str() / format() of the mixed-in data type instead of using the Enum-
default str().
New in version 3.11.
EnumCheck
EnumCheck contains the options used by the verify() decorator to ensure various constraints; failed constraints
result in a ValueError.
New in version 3.11.
class enum.
FlagBoundary
FlagBoundary controls how out-of-range values are handled in Flag and its subclasses.
STRICT
Out-of-range values cause a ValueError to be raised.
BLUE = auto()


KEEP
Out-of-range values are kept, and the Flag membership is kept.
BLUE = auto()

<KeepFlag.
It is only available on the class.
Once all the members are created it is no longer used.
For Flag classes the next value chosen will be the next highest power-of-two, regardless of the last value seen.
New in version 3.6: _missing_, _order_, _generate_next_value_
New in version 3.7: _ignore_
8.14.3 Utilities and Decorators
class enum.auto
auto
can
be
used
in
place
of
a
value.
If
used,
the
Enum
machinery
will
call
an
Enum’s
_generate_next_value_() to get an appropriate value.
For Enum and IntEnum that appropriate
value will be the last value plus one; for Flag and IntFlag it will be the first power-of-two greater than the highest
value; for StrEnum it will be the lower-cased version of the member’s name.
Care must be taken if mixing auto()
with manually specified values.
In prior versions, auto() had to be the only thing on the assignment line to work
properly.
Note: in 3.13 the default _generate_next_value_ will always return the highest member value incremented
by 1, and will fail if any member is an incompatible type.
It allows member attributes to have
the same names as members themselves.
Note: the property and the member must be defined in separate classes; for example, the value and name attributes
are defined in the Enum class, and Enum subclasses can define members with the names value and name.
New in version 3.11.
THREE = 3

FOUR = 3

Traceback (most recent call last):

ValueError: duplicate values found in <enum 'Mistake'>: FOUR -> THREE

A class decorator specifically for enumerations.
Members from EnumCheck are used to specify which con-
straints should be checked on the decorated enumeration.
New in version 3.11.
New in version 3.11.
New in version 3.11.
A decorator to change the str() and repr() of an enum to show its members as belonging to the module
instead of its class.
Should only be used when the enum members are exported to the module global namespace
(see re.RegexFlag for an example).
New in version 3.11.
New in version 3.11.
TopologicalSorter(graph=None)
Provides functionality to topologically sort a graph of hashable nodes.
A topological order is a linear ordering of the vertices in a graph such that for every directed edge u -> v from vertex
u to vertex v, vertex u comes before vertex v in the ordering.
For instance, the vertices of the graph may represent
tasks to be performed, and the edges may represent constraints that one task must be performed before another;
in this example, a topological ordering is just a valid sequence for the tasks.
A complete topological ordering is
possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph.
If the optional graph argument is provided it must be a dictionary representing a directed acyclic graph where the
keys are nodes and the values are iterables of all predecessors of that node in the graph (the nodes that have edges
that point to the value in the key).
Additional nodes can be added to the graph using the add() method.
In the general case, the steps required to perform the sorting of a given graph are as follows:
• Create an instance of the TopologicalSorter with an optional initial graph.
While is_active() is True, iterate over the nodes returned by get_ready() and process them.
Call
done() on each node as it finishes processing.
The class is designed to easily support parallel processing of the nodes as they become ready.
The definition of 'is_active()' guarantees that, at this point, at
# least one node has been placed on 'task_queue' that hasn't yet
# been passed to 'done()', so this blocking 'get()' must (eventually)
# succeed.
After calling 'done()', we loop back to call 'get_ready()'
# again, so put newly freed nodes on 'task_queue' as soon as
# logically possible.
Both the node and all elements in predecessors must be
hashable.
If called multiple times with the same node argument, the set of dependencies will be the union of all depen-
dencies passed in.
It is possible to add a node with no dependencies (predecessors is not provided) or to provide a dependency
twice.
If a node that has not been provided before is included among predecessors it will be automatically
added to the graph with no predecessors of its own.
Raises ValueError if called after prepare().
If any cycle is detected, CycleError will
be raised, but get_ready() can still be used to obtain as many nodes as possible until cycles block more
progress.
After a call to this function, the graph cannot be modified, and therefore no more nodes can be
added using add().
Progress can be made if cy-
cles do not block the resolution and either there are still nodes ready that haven’t yet been returned by
TopologicalSorter.get_ready() or the number of nodes marked TopologicalSorter.
Raises ValueError if called without calling prepare() previously.
Raises ValueError if any node in nodes has already been marked as processed by a previous call to this
method or if a node was not added to the graph by using TopologicalSorter.add(), if called without
calling prepare() or if node has not yet been returned by get_ready().
Initially it returns all nodes with no predecessors, and
once those are marked as processed by calling TopologicalSorter.done(), further calls will return
all new nodes that have all their predecessors already processed.
Once no more progress can be made, empty
tuples are returned.
Raises ValueError if called without calling prepare() previously.
When using this method,
prepare() and done() should not be called.
The particular order that is returned may depend on the specific order in which the items were inserted in the
graph.
For example:
[2, 0, 1, 3]
[0, 2, 1, 3]
This is due to the fact that “0” and “2” are in the same level in the graph (they would have been returned in
the same call to get_ready()) and the order between them is determined by the order of insertion.
If any cycle is detected, CycleError will be raised.
New in version 3.9.
CycleError
Subclass of ValueError raised by TopologicalSorter.prepare() if cycles exist in the working graph.
If multiple cycles exist, only one undefined choice among them will be reported and included in the exception.
The detected cycle can be accessed via the second element in the args attribute of the exception instance and
consists in a list of nodes, such that each node is, in the graph, an immediate predecessor of the next node in the
list.
In the reported list, the first and the last node will be the same, to make it clear that it is cyclic.
The modules described in this chapter provide numeric and math-related functions and data types.
The numbers module
defines an abstract hierarchy of numeric types.
The math and cmath modules contain various mathematical functions
for floating-point and complex numbers.
The decimal module supports exact representations of decimal numbers,
using arbitrary precision arithmetic.
The following modules are documented in this chapter:
9.1 numbers — Numeric abstract base classes
Source code: Lib/numbers.py
The numbers module (PEP 3141) defines a hierarchy of numeric abstract base classes which progressively define more
operations.
None of the types defined in this module are intended to be instantiated.
Number
The root of the numeric hierarchy.
If you just want to check if an argument x is a number, without caring what
kind, use isinstance(x, Number).
The numeric tower
class numbers.
Complex
Subclasses of this type describe complex numbers and include the operations that work on the built-in complex
type.
These are: conversions to complex and bool, real, imag, +, -, *, /, **, abs(), conjugate(),
==, and !=.
All except - and != are abstract.
Retrieves the real component of this number.
Retrieves the imaginary component of this number.
Returns the complex conjugate.
For example, (1+3j).conjugate() == (1-3j).
class numbers.
Real
To Complex, Real adds the operations that work on real numbers.
Rational
Subtypes Real and adds numerator and denominator properties.
It also provides a default for float().
The numerator and denominator values should be instances of Integral and should be in lowest terms
with denominator positive.
Integral
Subtypes Rational and adds a conversion to int.
Provides defaults for float(), numerator, and
denominator.
Adds abstract methods for pow() with modulus and bit-string operations: <<, >>, &, ^, |,
~.
9.1.2 Notes for type implementors
Implementors should be careful to make equal numbers equal and hash them to the same values.
This may be subtle if
there are two different extensions of the real numbers.
For example, fractions.
Fraction implements hash()
as follows:

# Get integers right.
Adding More Numeric ABCs
There are, of course, more possible ABCs for numbers, and this would be a poor hierarchy if it precluded the possibility
of adding those.
For subtypes of Integral, this means that __add__() and __radd__() should be defined as:
class MyIntegral(Integral):


elif isinstance(other, OtherTypeIKnowAbout):

else:



elif isinstance(other, OtherTypeIKnowAbout):

elif isinstance(other, Integral):

elif isinstance(other, Real):

elif isinstance(other, Complex):

else:

There are 5 different cases for a mixed-type operation on subclasses of Complex.
I’ll refer to all of the above code that
doesn’t refer to MyIntegral and OtherTypeIKnowAbout as “boilerplate”.
I’ll consider a + b:
1.
If A defines an __add__() which accepts b, all is well.
If A falls back to the boilerplate code, and it were to return a value from __add__(), we’d miss the possibil-
ity that B defines a more intelligent __radd__(), so the boilerplate should return NotImplemented from
__add__().
Then B’s __radd__() gets a chance.
If it accepts a, all is well.
If it falls back to the boilerplate, there are no more possible methods to try, so this is where the default implemen-
tation should live.
This is ok, because it was implemented with knowl-
edge of A, so it can handle those instances before delegating to Complex.
If A <: Complex and B <: Real without sharing any other knowledge, then the appropriate shared operation is
the one involving the built in complex, and both __radd__() s land there, so a+b == b+a.
Because most of the operations on any given type will be very similar, it can be useful to define a helper function which
generates the forward and reverse instances of any given operator.
For example, fractions.
These functions cannot be used with complex numbers; use the functions of the same name from the cmath module if
you require support for complex numbers.
The distinction between functions which support complex numbers and those
which don’t is made since most users do not want to learn quite as much mathematics as required to understand complex
numbers.
Receiving an exception instead of a complex result allows earlier detection of the unexpected complex number
used as a parameter, so that the programmer can determine how and why it was generated in the first place.
The following functions are provided by this module.
Except when explicitly noted otherwise, all return values are floats.
If x is not a float, delegates to x.__ceil__,
which should return an Integral value.
Raises ValueError if either of the arguments
are negative.
New in version 3.8.
Raises ValueError if n is not integral or is negative.
Accepting floats with integral values (like 5.0) is deprecated.
If x is not a float, delegates to x.__floor__,
which should return an Integral value.
The intent of the C standard is that fmod(x, y) be exactly (mathematically; to infinite
precision) equal to x - n*y for some integer n such that the result has the same sign as x and magnitude less than
abs(y).
For this reason, function fmod() is generally preferred when working with floats, while Python’s x % y is pre-
ferred when working with integers.
If x is zero, returns (0.0, 0), otherwise 0.5 <= abs(m) < 1.
This is used to “pick apart”
the internal representation of a float in a portable way.
Avoids loss of precision by tracking multiple
intermediate partial sums.
The algorithm’s accuracy depends on IEEE-754 arithmetic guarantees and the typical case where the rounding
mode is half-even.
On some non-Windows builds, the underlying C library uses extended precision addition and
may occasionally double-round an intermediate sum causing it to be off in its least significant bit.
For further discussion and two alternative approaches, see the ASPN cookbook recipes for accurate floating point
summation.
If any of the arguments is nonzero, then
the returned value is the largest positive integer that is a divisor of all arguments.
If all arguments are zero, then
the returned value is 0. gcd() without arguments returns 0.
New in version 3.5.
Changed in version 3.9: Added support for an arbitrary number of arguments.
Formerly, only two arguments were
supported.
Whether or not two values are considered close is determined according to given absolute and relative tolerances.
For example, to set a tolerance of 5%, pass rel_tol=0.05.
The default tolerance is 1e-09,
which assures that the two values are the same within about 9 decimal digits.
If no errors occur, the result will be:
abs(a-b) <= max(rel_tol * max(abs(a), abs(b)),
abs_tol).
The IEEE 754 special values of NaN, inf, and -inf will be handled according to IEEE rules.
Specifically, NaN
is not considered close to any other value, including NaN. inf and -inf are only considered close to themselves.
New in version 3.5.
See also:
PEP 485 – A function for testing approximate equality
math.isfinite(x)
Return True if x is neither an infinity nor a NaN, and False otherwise.
New in version 3.2.
math.isinf(x)
Return True if x is a positive or negative infinity, and False otherwise.
This is the floor of the exact square root of n, or
equivalently the greatest integer a such that a² ≤ n.
For some applications, it may be more convenient to have the least integer a such that n ≤ a², or in other words the
ceiling of the exact square root of n. For positive n, this can be computed using a = 1 + isqrt(n - 1).
New in version 3.8.
If all arguments are nonzero, then the returned
value is the smallest positive integer that is a multiple of all arguments.
If any of the arguments is zero, then the
returned value is 0. lcm() without arguments returns 1.
New in version 3.9.
Return x * (2**i).
Both results carry the sign of x and are floats.
If x is equal to y, return y, unless steps is zero.
Examples:
• math.nextafter(x, math.inf) goes up: towards positive infinity.
• math.nextafter(x, -math.inf) goes down: towards minus infinity.
See also math.ulp().
Changed in version 3.12: Added the steps argument.
New in version 3.9.
If k is not specified or is None, then k defaults to n and the function returns n!.
Raises TypeError if either of the arguments are not integers.
Raises ValueError if either of the arguments
are negative.
New in version 3.8.
The default start value for the product is 1.
When the iterable is empty, return the start value.
This function is intended specifically for use with numeric values
and may reject non-numeric types.
New in version 3.8.
If x / y is exactly halfway
between two consecutive integers, the nearest even integer is used for n.
The remainder r = remainder(x,
y) thus always satisfies abs(r) <= 0.5 * abs(y).
Special cases follow IEEE 754:
in particular, remainder(x, math.inf) is x for any finite x, and
remainder(x, 0) and remainder(math.inf, x) raise ValueError for any non-NaN x.
If the result
of the remainder operation is zero, that zero will have the same sign as x.
On platforms using IEEE 754 binary floating-point, the result of this operation is always exactly representable: no
rounding error is introduced.
New in version 3.7.
Roughly equivalent to:
9.2.
For float and mixed int/float inputs, the intermediate products and sums are computed with extended precision.
Return x with the fractional part removed, leaving the integer part.
This rounds toward 0: trunc() is equivalent to
floor() for positive x, and equivalent to ceil() for negative x.
If x is not a float, delegates to x.__trunc__,
which should return an Integral value.
If x is negative, return ulp(-x).
If x is equal to zero, return the smallest positive denormalized representable float (smaller than the minimum
positive normalized float, sys.float_info.min).
If x is equal to the largest positive representable float, return the value of the least significant bit of x, such
that the first float smaller than x is x - ulp(x).
ULP stands for “Unit in the Last Place”.
See also math.nextafter() and sys.float_info.epsilon.
New in version 3.9.
Note that frexp() and modf() have a different call/return pattern than their C equivalents: they take a single argument
and return a pair of values, rather than returning their second return value through an ‘output parameter’ (there is no such
thing in Python).
For the ceil(), floor(), and modf() functions, note that all floating-point numbers of sufficiently large magnitude
are exact integers.
Here e is the base of natural logarithms.
For small floats x, the subtraction
in exp(x) - 1 can result in a significant loss of precision; the expm1() function provides a way to compute
this quantity to full precision:
# gives result accurate to 11 places
1.0000050000069649e-05
# result accurate to full precision
1.0000050000166668e-05
New in version 3.2.
With one argument, return the natural logarithm of x (to base e).
With two arguments, return the logarithm of x to the given base, calculated as log(x)/log(base).
The result is calculated in a way which is accurate for x near zero.
math.log2(x)
Return the base-2 logarithm of x.
This is usually more accurate than log(x, 2).
New in version 3.3.
Return the base-10 logarithm of x.
This is usually more accurate than log(x, 10).
Exceptional cases follow the IEEE 754 standard as far as possible.
In particular,
pow(1.0, x) and pow(x, 0.0) always return 1.0, even when x is a zero or a NaN. If both x and y are
finite, x is negative, and y is not an integer then pow(x, y) is undefined, and raises ValueError.
Unlike the built-in ** operator, math.pow() converts both its arguments to type float.
Use ** or the built-in
pow() function for computing exact integer powers.
The special cases pow(0.0, -inf) and pow(-0.0, -inf) were changed to

math.sqrt(x)
Return the square root of x.
9.2.3 Trigonometric functions
math.acos(x)
Return the arc cosine of x, in radians.
The result is between 0 and pi.
math.asin(x)
Return the arc sine of x, in radians.
The result is between -pi/2 and pi/2.
math.atan(x)
Return the arc tangent of x, in radians.
The result is between -pi/2 and pi/2.
9.2.
The result is between -pi and pi.
The vector in the plane from the origin to
point (x, y) makes this angle with the positive X axis.
The point of atan2() is that the signs of both inputs
are known to it, so it can compute the correct quadrant for the angle.
For example, atan(1) and atan2(1,
1) are both pi/4, but atan2(-1, -1) is -3*pi/4.
math.cos(x)
Return the cosine of x radians.
math.dist(p, q)
Return the Euclidean distance between two points p and q, each given as a sequence (or iterable) of coordinates.
The two points must have the same dimension.
New in version 3.8.
math.hypot(*coordinates)
Return the Euclidean norm, sqrt(sum(x**2 for x in coordinates)).
This is the length of the vec-
tor from the origin to the point given by the coordinates.
For a two dimensional point (x, y), this is equivalent to computing the hypotenuse of a right triangle using the
Pythagorean theorem, sqrt(x*x + y*y).
Changed in version 3.8: Added support for n-dimensional points.
Formerly, only the two dimensional case was
supported.
Changed in version 3.10: Improved the algorithm’s accuracy so that the maximum error is under 1 ulp (unit in the
last place).
More typically, the result is almost always correctly rounded to within 1/2 ulp.
Convert angle x from degrees to radians.
The complementary error function is defined as 1.0 - erf(x).
It is used for large values of x where a subtraction from one would cause a loss of significance.
New in version 3.2.
Tau is a circle constant equal to 2π, the ratio of
a circle’s circumference to its radius.
To learn more about Tau, check out Vi Hart’s video Pi is (still) Wrong, and
start celebrating Tau day by eating twice as much pie!
New in version 3.6.
Equivalent to the output of
float('inf').
New in version 3.5.
math.nan
A floating-point “not a number” (NaN) value.
Equivalent to the output of float('nan').
Due to the require-
ments of the IEEE-754 standard, math.nan and float('nan') are not considered to equal to any other
numeric value, including themselves.
To check whether a number is a NaN, use the isnan() function to test for
NaNs instead of is or ==.
Example:
False
False
True
True
Changed in version 3.11: It is now always available.
New in version 3.5.
CPython implementation detail: The math module consists mostly of thin wrappers around the platform C math
library functions.
Behavior in exceptional cases follows Annex F of the C99 standard where appropriate.
The current
implementation will raise ValueError for invalid operations like sqrt(-1.0) or log(0.0) (where C99 Annex F
recommends signaling invalid operation or divide-by-zero), and OverflowError for results that overflow (for example,
exp(1000.0)).
A NaN will not be returned from any of the functions above unless one or more of the input arguments
was a NaN; in that case, most functions will return a NaN, but (again following C99 Annex F) there are some exceptions
to this rule, for example pow(float('nan'), 0.0) or hypot(float('nan'), float('inf')).
Note that Python makes no effort to distinguish signaling NaNs from quiet NaNs, and behavior for signaling NaNs remains
unspecified.
Typical behavior is to treat all NaNs as though they were quiet.
See also:
Module cmath Complex number versions of many of these functions.
The functions in this module accept in-
tegers, floating-point numbers or complex numbers as arguments.
They will also accept any Python object that has
either a __complex__() or a __float__() method: these methods are used to convert the object to a complex or
floating-point number, respectively, and the function is then applied to the result of the conversion.
For functions involving branch cuts, we have the problem of deciding how to define those functions on the cut
itself.
Following Kahan’s “Branch cuts for complex elementary functions” paper, as well as Annex G of C99 and later
C standards, we use the sign of zero to distinguish one side of the branch cut from the other: for a branch cut along (a
portion of) the real axis we look at the sign of the imaginary part, while for a branch cut along the imaginary axis we look
at the sign of the real part.
For example, the cmath.sqrt() function has a branch cut along the negative real axis.
An argument of
complex(-2.0, -0.0) is treated as though it lies below the branch cut, and so gives a result on the negative imagi-
nary axis:
-1.4142135623730951j
But an argument of complex(-2.0, 0.0) is treated as though it lies above the branch cut:
1.4142135623730951j
9.3.1 Conversions to and from polar coordinates
A Python complex number z is stored internally using rectangular or Cartesian coordinates.
It is completely determined
by its real part z.real and its imaginary part z.imag.
In other words:
z == z.real + z.imag*1j
Polar coordinates give an alternative way to represent a complex number.
In polar coordinates, a complex number z is
defined by the modulus r and the phase angle phi.
The modulus r is the distance from z to the origin, while the phase phi
is the counterclockwise angle, measured in radians, from the positive x-axis to the line segment that joins the origin to z.
The following functions can be used to convert from the native rectangular coordinates to polar coordinates and back.
The sign of the result is the same as the sign of x.imag, even when x.imag is zero:
3.141592653589793
-3.141592653589793
Note: The modulus (absolute value) of a complex number x can be computed using the built-in abs() function.
There
is no separate cmath module function for this operation.
cmath.polar(x)
Return the representation of x in polar coordinates.
Returns a pair (r, phi) where r is the modulus of x and
phi is the phase of x. polar(x) is equivalent to (abs(x), phase(x)).
Equivalent to r * (math.cos(phi) +
math.sin(phi)*1j).
Return the base-10 logarithm of x.
This has the same branch cut as log().
This has the same branch cut as log().
Return the arc sine of x.
This has the same branch cuts as acos().
The
other extends from -1j along the imaginary axis to -∞j.
cmath.cos(x)
Return the cosine of x.
cmath.sin(x)
Return the sine of x.
cmath.tan(x)
Return the tangent of x.
9.3.4 Hyperbolic functions
cmath.acosh(x)
Return the inverse hyperbolic cosine of x.
There is one branch cut, extending left from 1 along the real axis to -∞.
cmath.asinh(x)
Return the inverse hyperbolic sine of x.
There are two branch cuts: One extends from 1j along the imaginary axis
to ∞j.
The other extends from -1j along the imaginary axis to -∞j.
cmath.atanh(x)
Return the inverse hyperbolic tangent of x.
There are two branch cuts: One extends from 1 along the real axis to
∞.
New in version 3.2.
cmath.isinf(x)
Return True if either the real or the imaginary part of x is an infinity, and False otherwise.
Whether or not two values are considered close is determined according to given absolute and relative tolerances.
For example, to set a tolerance of 5%, pass rel_tol=0.05.
The default tolerance is 1e-09,
which assures that the two values are the same within about 9 decimal digits.
If no errors occur, the result will be:
abs(a-b) <= max(rel_tol * max(abs(a), abs(b)),
abs_tol).
The IEEE 754 special values of NaN, inf, and -inf will be handled according to IEEE rules.
Specifically, NaN
is not considered close to any other value, including NaN. inf and -inf are only considered close to themselves.
New in version 3.5.
See also:
PEP 485 – A function for testing approximate equality
9.3.6 Constants
cmath.pi
The mathematical constant π, as a float.
New in version 3.6.
Equivalent to float('inf').
New in version 3.6.
Equivalent to complex(0.0,
float('inf')).
New in version 3.6.
cmath.nan
A floating-point “not a number” (NaN) value.
Equivalent to float('nan').
New in version 3.6.
Equivalent to complex(0.0,
float('nan')).
New in version 3.6.
Note that the selection of functions is similar, but not identical, to that in module math.
The reason for having two
modules is that some users aren’t interested in complex numbers, and perhaps don’t even know what they are.
They would
rather have math.sqrt(-1) raise an exception than return a complex number.
Also note that the functions defined
in cmath always return a complex number, even if the answer can be expressed as a real number (in which case the
complex number has an imaginary part of zero).
A note on branch cuts: They are curves along which the given function fails to be continuous.
They are a necessary feature
of many complex functions.
It is assumed that if you need to compute with complex functions, you will understand about
branch cuts.
Consult almost any (not too elementary) book on complex variables for enlightenment.
For information of
the proper choice of branch cuts for numerical purposes, a good reference should be the following:
See also:
Kahan, W: Branch cuts for complex elementary functions; or, Much ado about nothing’s sign bit.
In Iserles, A., and
Powell, M. (eds.), The state of the art in numerical analysis.
Clarendon Press (1987) pp165–211.
In contrast, numbers like 1.1 and 2.2 do not have exact
representations in binary floating point.
End users typically would not expect 1.1 + 2.2 to display as 3.
3000000000000003 as it does with binary floating point.
The exactness carries over into arithmetic.
In decimal floating point, 0.1 + 0.1 + 0.1 - 0.3 is exactly
equal to zero.
In binary floating point, the result is 5.5511151231257827e-017.
While near to zero, the
differences prevent reliable equality testing and differences can accumulate.
For this reason, decimal is preferred
in accounting applications which have strict equality invariants.
The trailing zero
is kept to indicate significance.
This is the customary presentation for monetary applications.
For multiplication,
the “schoolbook” approach uses all the figures in the multiplicands.
For instance, 1.3 * 1.2 gives 1.56 while
1.30 * 1.20 gives 1.5600.
While the built-in float
type exposes only a modest portion of its capabilities, the decimal module exposes all required parts of the standard.
When needed, the programmer has full control over rounding and signal handling.
This includes an option to enforce
exact arithmetic by using exceptions to block any inexact operations.
The module design is centered around three concepts: the decimal number, the context for arithmetic, and signals.
A decimal number is immutable.
It has a sign, coefficient digits, and an exponent.
To preserve significance, the coefficient
digits do not truncate trailing zeros.
Decimals also include special values such as Infinity, -Infinity, and NaN.
The standard also differentiates -0 from +0.
The context for arithmetic is an environment specifying precision, rounding rules, limits on exponents, flags indicat-
ing the results of operations, and trap enablers which determine whether signals are treated as exceptions.
Rounding
options include ROUND_CEILING, ROUND_DOWN, ROUND_FLOOR, ROUND_HALF_DOWN, ROUND_HALF_EVEN,
ROUND_HALF_UP, ROUND_UP, and ROUND_05UP.
Signals are groups of exceptional conditions arising during the course of computation.
Depending on the needs of the
application, signals may be ignored, considered as informational, or treated as exceptions.
The signals in the decimal mod-
ule are: Clamped, InvalidOperation, DivisionByZero, Inexact, Rounded, Subnormal, Overflow,
Underflow and FloatOperation.
For each signal there is a flag and a trap enabler.
When a signal is encountered, its flag is set to one, then, if the trap enabler
is set to one, an exception is raised.
Flags are sticky, so the user needs to reset them before monitoring a calculation.
IBM’s General Decimal Arithmetic Specification, The General Decimal Arithmetic Specification.
Construction from an integer or a float
performs an exact conversion of the value of that integer or float.
If the FloatOperation signal is trapped, accidental mixing of decimals and floats in constructors or ordering com-
parisons raises an exception:
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
decimal.
FloatOperation'>]
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
decimal.
FloatOperation'>]
True
New in version 3.3.
The significance of a new Decimal is determined solely by the number of digits input.
Context precision and rounding
only come into play during arithmetic operations.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
decimal.
InvalidOperation'>]
Changed in version 3.3.
Decimals interact well with much of the rest of Python.
The quantize() method rounds a number to a fixed exponent.
As shown above, the getcontext() function accesses the current context and allows the settings to be changed.
This
approach meets the needs of most applications.
To make an
alternate active, use the setcontext() function.
In accordance with the standard, the decimal module provides two ready to use standard contexts, BasicContext
and ExtendedContext.
DivisionByZero: x / 0
Contexts also have signal flags for monitoring exceptional conditions encountered during computations.
The flags re-
main set until explicitly cleared, so it is best to clear the flags before each set of monitored computations by using the
clear_flags() method.
The flags entry shows that the rational approximation to pi was rounded (digits beyond the context precision were thrown
away) and that the result is inexact (some of the discarded digits were non-zero).
DivisionByZero: x / 0
Most programs adjust the current context only once, at the beginning of the program.
And, in many applications, data is
converted to Decimal with a single cast inside a loop.
Decimal(value=’0’, context=None)
Construct a new Decimal object based from value.
If no value is given, returns
Decimal('0').
These include decimal digits
from various other alphabets (for example, Arabic-Indic and Devanāgarī digits) along with the fullwidth digits
'\uff10' through '\uff19'.
If value is a tuple, it should have three components, a sign (0 for positive or 1 for negative), a tuple of digits, and
an integer exponent.
For example, Decimal((0, (1, 4, 1, 4), -3)) returns Decimal('1.414').
If value is a float, the binary floating point value is losslessly converted to its exact decimal equivalent.
This
conversion can often require 53 or more digits of precision.
For example, Decimal(float('1.1')) converts
to Decimal('1.100000000000000088817841970012523233890533447265625').
The context precision does not affect how many digits are stored.
That is determined exclusively by the number of
digits in value.
For example, Decimal('3.00000') records all five zeros even if the context precision is only
three.
The purpose of the context argument is determining what to do if value is a malformed string.
If the context traps
InvalidOperation, an exception is raised; otherwise, the constructor returns a new Decimal with the value
of NaN.
Once constructed, Decimal objects are immutable.
The argument to the constructor is now permitted to be a float instance.
Changed in version 3.3: float arguments raise an exception if the FloatOperation trap is set.
By default
the trap is off.
Underscores are allowed for grouping, as with integral and floating-point literals in code.
Decimal floating point objects share many properties with the other built-in numeric types such as float and
int.
All of the usual math operations and special methods apply.
Likewise, decimal objects can be copied,
pickled, printed, used as dictionary keys, used as set elements, compared, sorted, and coerced to another type
(such as float or int).
There are some small differences between arithmetic on Decimal objects and arithmetic on integers and floats.
When the remainder operator % is applied to Decimal objects, the sign of the result is the sign of the dividend
rather than the sign of the divisor:

(continues on next page)
9.4.
The % and // operators implement the remainder and divide-integer operations (respectively) as de-
scribed in the specification.
Decimal objects cannot generally be combined with floats or instances of fractions.
Fraction in arithmetic
operations: an attempt to add a Decimal to a float, for example, will raise a TypeError.
However, it is
possible to use Python’s comparison operators to compare a Decimal instance x with another number y.
This
avoids confusing results when doing equality comparisons between numbers of different types.
Changed in version 3.2: Mixed-type comparisons between Decimal instances and other numeric types are now
fully supported.
In addition to the standard numeric properties, decimal floating point objects also have a number of specialized
methods:
adjusted()
Return the adjusted exponent after shifting out the coefficient’s rightmost digits until only the lead digit re-
mains: Decimal('321e+5').adjusted() returns seven.
Used for determining the position of the
most significant digit with respect to the decimal point.
The conversion is exact.
Raise OverflowError on infinities and ValueError on NaNs.
New in version 3.6.
Currently, the encoding of a Decimal instance is always
canonical, so this operation returns its argument unchanged.
compare(other, context=None)
Compare the values of two Decimal instances.
This operation is identical to the compare() method, except that all NaNs signal.
That is, if neither operand
is a signaling NaN then any quiet NaN operand is treated as though it were a signaling NaN.
compare_total(other, context=None)
Compare two operands using their abstract representation rather than their numerical value.
Similar to the
compare() method, but the result gives a total ordering on Decimal instances.
Two Decimal instances
with the same numeric value but different representations compare unequal in this ordering:
Decimal('-1')
Quiet and signaling NaNs are also included in the total ordering.
The result of this function is
Decimal('0') if both operands have the same representation, Decimal('-1') if the first operand
is lower in the total order than the second, and Decimal('1') if the first operand is higher in the total
order than the second operand.
See the specification for details of the total order.
This operation is unaffected by context and is quiet: no flags are changed and no rounding is performed.
As
an exception, the C version may raise InvalidOperation if the second operand cannot be converted exactly.
This operation is unaffected by context and is quiet: no flags are changed and no rounding is performed.
As
an exception, the C version may raise InvalidOperation if the second operand cannot be converted exactly.
Just returns self, this method is only to comply with the Decimal Specification.
This operation is unaffected by the context and is quiet: no flags
are changed and no rounding is performed.
This operation is unaffected by the context and is quiet: no flags are
changed and no rounding is performed.
This operation is unaffected by context and is quiet: no flags are changed and no rounding is performed.
As
an exception, the C version may raise InvalidOperation if the second operand cannot be converted exactly.
The result is correctly
rounded using the ROUND_HALF_EVEN rounding mode.
Decimal('2.718281828459045235360287471')
Decimal('2.561702493119680037517373933E+139')
9.4.
Alternative constructor that only accepts instances of float or int.
Note Decimal.from_float(0.1) is not the same as Decimal('0.1').
Since 0.1 is
not exactly representable in binary floating point,
the value is stored as the nearest repre-
sentable value which is 0x1.999999999999ap-4.
That equivalent value in decimal is 0.
1000000000000000055511151231257827021181583404541015625.
Note: From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.
New in version 3.1.
fma(other, third, context=None)
Fused multiply-add.
Return self*other+third with no rounding of the intermediate product self*other.
Decimal('11')
is_canonical()
Return True if the argument is canonical and False otherwise.
Return True if the argument is either positive or negative infinity and False otherwise.
Return False if the argument is zero, subnormal,
infinite or a NaN.
is_qnan()
Return True if the argument is a quiet NaN, and False otherwise.
is_signed()
Return True if the argument has a negative sign and False otherwise.
Note that zeros and NaNs can both
carry signs.
The result is correctly rounded using the
ROUND_HALF_EVEN rounding mode.
The result is correctly rounded using the ROUND_HALF_EVEN
rounding mode.
If the operand
is a zero then Decimal('-Infinity') is returned and the DivisionByZero flag is raised.
If the
operand is an infinity then Decimal('Infinity') is returned.
The result
is the digit-wise and of the two operands.
The result is the digit-wise inversion of the operand.
The result
is the digit-wise or of the two operands.
The result
is the digit-wise exclusive or of the two operands.
If the two operands are unequal, return the number closest to the first operand in the direction of the second
operand.
If both operands are numerically equal, return a copy of the first operand with the sign set to be the
same as the sign of the second operand.
9.4.
This has the same semantics as the unary plus operation, except that if the final result is finite it is reduced to
its simplest form, with all trailing zeros removed and its sign preserved.
That is, while the coefficient is non-
zero and a multiple of ten the coefficient is divided by ten and the exponent is incremented by 1.
Otherwise
(the coefficient is zero) the exponent is set to 0.
In all cases the sign is unchanged.
For example, Decimal('32.100') and Decimal('0.321000e+2') both normalize to the equiv-
alent value Decimal('32.1').
Note that rounding is applied before reducing to simplest form.
In the latest versions of the specification, this operation is also known as reduce.
The returned value is one of the following ten strings.
Decimal('1.414')
Unlike other operations, if the length of the coefficient after the quantize operation would be greater than
precision, then an InvalidOperation is signaled.
This guarantees that, unless there is an error condition,
the quantized exponent is always equal to that of the right-hand operand.
Also unlike other operations, quantize never signals Underflow, even if the result is subnormal and inexact.
If the exponent of the second operand is larger than that of the first then rounding may be necessary.
In this
case, the rounding mode is determined by the rounding argument if given, else by the given context
argument; if neither argument is given the rounding mode of the current thread’s context is used.
An error is returned whenever the resulting exponent is greater than Emax or less than Etiny().
This differs from self % other in that the sign of the
remainder is chosen so as to minimize its absolute value.
More precisely, the return value is self - n *
other where n is the integer nearest to the exact value of self / other, and if two integers are equally
near then the even one is chosen.
If the result is zero then its sign will be the sign of self.
Decimal('-2')
Decimal('5')
Decimal('-5')
rotate(other, context=None)
Return the result of rotating the digits of the first operand by an amount specified by the second operand.
The second operand must be an integer in the range -precision through precision.
The absolute value of the
second operand gives the number of places to rotate.
If the second operand is positive then rotation is to the
left; otherwise rotation is to the right.
The coefficient of the first operand is padded on the left with zeros to
length precision if necessary.
The sign and exponent of the first operand are unchanged.
As
an exception, the C version may raise InvalidOperation if the second operand cannot be converted exactly.
scaleb(other, context=None)
Return the first operand with exponent adjusted by the second.
Equivalently, return the first operand multiplied
by 10**other.
The second operand must be an integer.
The second operand must be an integer in the range -precision through precision.
The absolute value of the
second operand gives the number of places to shift.
If the second operand is positive then the shift is to the
left; otherwise the shift is to the right.
Digits shifted into the coefficient are zeros.
The sign and exponent of
the first operand are unchanged.
Engineering notation has an exponent which is a multiple of 3.
This can leave up to 3 digits to the left of the
decimal place and may require the addition of either one or two trailing zeros.
For example, this converts Decimal('123E+1') to Decimal('1.23E+3').
The to_integral name has been kept for compat-
ibility with older versions.
The
rounding mode is determined by the rounding parameter if given, else by the given context.
If neither
parameter is given then the rounding mode of the current context is used.
If given, applies rounding; otherwise,
uses the rounding method in either the supplied context or the current context.
Logical operands
The logical_and(), logical_invert(), logical_or(), and logical_xor() methods expect their ar-
guments to be logical operands.
A logical operand is a Decimal instance whose exponent and sign are both zero, and
whose digits are all either 0 or 1.
They govern precision, set rules for rounding, determine which
signals are treated as exceptions, and limit the range for exponents.
Each thread has its own current context which is accessed or changed using the getcontext() and setcontext()
functions:
decimal.getcontext()
Return the current context for the active thread.
You can also use the with statement and the localcontext() function to temporarily change the active context.
If no context is specified, a copy
of the current context is used.
The kwargs argument is used to set the attributes of the new context.
Raises either TypeError or
ValueError if kwargs supplies an invalid value for an attribute.
Changed in version 3.11: localcontext() now supports setting context attributes through the use of keyword
arguments.
New contexts can also be created using the Context constructor described below.
BasicContext
This is a standard context defined by the General Decimal Arithmetic Specification.
Precision is set to nine.
Rounding is set to ROUND_HALF_UP.
All flags are cleared.
All traps are enabled (treated as exceptions) except
Inexact, Rounded, and Subnormal.
Because many of the traps are enabled, this context is useful for debugging.
ExtendedContext
This is a standard context defined by the General Decimal Arithmetic Specification.
Precision is set to nine.
Round-
ing is set to ROUND_HALF_EVEN.
All flags are cleared.
No traps are enabled (so that exceptions are not raised
during computations).
Because the traps are disabled, this context is useful for applications that prefer to have result value of NaN or
Infinity instead of raising exceptions.
This allows an application to complete a run in the presence of conditions
that would otherwise halt the program.
DefaultContext
This context is used by the Context constructor as a prototype for new contexts.
Changing a field (such a
precision) has the effect of changing the default for new contexts created by the Context constructor.
This context is most useful in multi-threaded environments.
Changing one of the fields before threads are started
has the effect of setting system-wide defaults.
Changing the fields after threads have started is not recommended
as it would require thread synchronization to prevent race conditions.
In single threaded environments, it is preferable to not use this context at all.
Instead, simply create contexts
explicitly as described below.
The default values are Context.prec=28, Context.rounding=ROUND_HALF_EVEN, and enabled traps
for Overflow, InvalidOperation, and DivisionByZero.
In addition to the three supplied contexts, new contexts can be created with the Context constructor.
Context(prec=None, rounding=None, Emin=None, Emax=None, capitals=None, clamp=None,
flags=None, traps=None)
Creates a new context.
If a field is not specified or is None, the default values are copied from the
DefaultContext.
If the flags field is not specified or is None, all flags are cleared.
The rounding option is one of the constants listed in the section Rounding Modes.
The traps and flags fields list any signals to be set.
Generally, new contexts should only set traps and leave the flags
clear.
The Emin and Emax fields are integers specifying the outer limits allowable for exponents.
The capitals field is either 0 or 1 (the default).
If set to 1, exponents are printed with a capital E; otherwise, a
lowercase e is used: Decimal('6.02e+23').
The clamp field is either 0 (the default) or 1.
If set to 1, the exponent e of a Decimal instance representable in
this context is strictly limited to the range Emin - prec + 1 <= e <= Emax - prec + 1.
When clamp
is 1, a large normal number will, where possible, have its exponent reduced and a corresponding number of zeros
added to its coefficient, in order to fit the exponent constraints; this preserves the value of the number but loses
information about significant trailing zeros.
For example:
Decimal('1.23000E+999')
9.4.
A clamp value of 1 allows compatibility with the fixed-width decimal interchange formats specified in IEEE 754.
The Context class defines several general purpose methods as well as a large number of methods for doing arith-
metic directly in a given context.
In addition, for each of the Decimal methods described above (with the excep-
tion of the adjusted() and as_tuple() methods) there is a corresponding Context method.
For example,
for a Context instance C and Decimal instance x, C.exp(x) is equivalent to x.exp(context=C).
Each
Context method accepts a Python integer (an instance of int) anywhere that a Decimal instance is accepted.
New in version 3.3.
Unlike the Decimal constructor, the
context precision, rounding method, flags, and traps are applied to the conversion.
This is useful because constants are often given to a greater precision than is needed by the application.
Another benefit is that rounding immediately eliminates unintended effects from digits beyond the current
precision.
This method implements the to-number operation of the IBM specification.
If the argument is a string, no
leading or trailing whitespace or underscores are permitted.
create_decimal_from_float(f)
Creates a new Decimal instance from a float f but rounding using self as the context.
Unlike the Decimal.
from_float() class method, the context precision, rounding method, flags, and traps are applied to the
conversion.
Traceback (most recent call last):

decimal.
None
New in version 3.1.
Etiny()
Returns a value equal to Emin - prec + 1 which is the minimum exponent value for subnormal results.
When underflow occurs, the exponent is set to Etiny.
The usual approach to working with decimals is to create Decimal instances and then apply arithmetic operations
which take place within the current context for the active thread.
An alternative approach is to use context methods
for calculating within a specific context.
The methods are similar to those for the Decimal class and are only
briefly recounted here.
Return x divided by y, truncated to an integer.
Returns True if x is finite; otherwise returns False.
9.4.
True if x is a normal number; otherwise returns False.
Returns True if x is negative; otherwise returns False.
Invert all the digits in x.
logical_or(x, y)
Applies the logical operation or between each operand’s digits.
Reduces x to its simplest form.
This operation applies the context precision and
rounding, so it is not an identity operation.
With two arguments, compute x**y.
If x is negative then y must be integral.
The result will be inexact
unless y is integral and the result is finite and can be expressed exactly in ‘precision’ digits.
The rounding
mode of the context is used.
Results are always correctly rounded in the Python version.
The C module computes power() in terms of the correctly rounded exp() and
ln() functions.
The result is well-defined but only “almost always correctly rounded”.
With three arguments, compute (x**y) % modulo.
The value resulting from Context.power(x, y, modulo) is equal to the value that would be ob-
tained by computing (x**y) % modulo with unbounded precision, but is computed more efficiently.
The exponent of the result is zero, regardless of the exponents of x, y and modulo.
The result is always
exact.
Just returns 10, as this is Decimal, :)
9.4.
The sign of the result, if non-zero, is the same as that of the original dividend.
Engineering notation has an exponent which is a multiple of 3.
Converts a number to a string using scientific notation.
They are also included in the pure Python version for
compatibility.
MAX_PREC


decimal.
MAX_EMAX


decimal.MIN_EMIN
-425000000
-999999999999999999
decimal.
The value is True.
Deprecated, because Python now always has threads.
The
default
value
is
True.
If
Python
is
configured using the
--without-decimal-contextvar option, the C version uses a thread-local rather than a coroutine-
local context and the value is False.
This is slightly faster in some nested context scenarios.
New in version 3.9: backported to 3.7 and 3.8.
ROUND_CEILING
Round towards Infinity.
decimal.
ROUND_DOWN
Round towards zero.
ROUND_HALF_DOWN
Round to nearest with ties going towards zero.
ROUND_HALF_EVEN
Round to nearest with ties going to nearest even integer.
decimal.
Round to nearest with ties going away from zero.
ROUND_05UP
Round away from zero if last digit after rounding towards zero would have been 0 or 5; otherwise round towards
zero.
Each corresponds to one context flag and one context trap
enabler.
The context flag is set whenever the condition is encountered.
After the computation, flags may be checked for informa-
tional purposes (for instance, to determine whether a computation was exact).
After checking the flags, be sure to clear
all flags before starting the next computation.
If the context’s trap enabler is set for the signal, then the condition causes a Python exception to be raised.
For example,
if the DivisionByZero trap is set, then a DivisionByZero exception is raised upon encountering the condition.
Clamped
Altered an exponent to fit representation constraints.
Typically, clamping occurs when an exponent falls outside the context’s Emin and Emax limits.
If possible, the
exponent is reduced to fit by adding zeros to the coefficient.
DecimalException
Base class for other signals and a subclass of ArithmeticError.
DivisionByZero
Signals the division of a non-infinite number by zero.
Can occur with division, modulo division, or when raising a number to a negative power.
If this signal is not trapped,
returns Infinity or -Infinity with the sign determined by the inputs to the calculation.
Inexact
Indicates that rounding occurred and the result is not exact.
Signals when non-zero digits were discarded during rounding.
The rounded result is returned.
The signal flag or
trap is used to detect when results are inexact.
InvalidOperation
An invalid operation was performed.
Indicates that an operation was requested that does not make sense.
Overflow
Numerical overflow.
Indicates the exponent is larger than Context.
Emax after rounding has occurred.
If not trapped, the result
depends on the rounding mode, either pulling inward to the largest representable finite number or rounding outward
to Infinity.
In either case, Inexact and Rounded are also signaled.
Rounded
Rounding occurred though possibly no information was lost.
Signaled whenever rounding discards digits; even if those digits are zero (such as rounding 5.00 to 5.0).
If not
trapped, returns the result unchanged.
This signal is used to detect loss of significant digits.
Subnormal
Exponent was lower than Emin prior to rounding.
Occurs when an operation result is subnormal (the exponent is too small).
If not trapped, returns the result un-
changed.
Underflow
Numerical underflow with result rounded to zero.
Occurs when a subnormal result is pushed to zero by rounding.
Inexact and Subnormal are also signaled.
If the signal is not trapped (default), mixing floats and Decimals is permitted in the Decimal constructor,
create_decimal() and all comparison operators.
Both conversion and comparisons are exact.
Any oc-
currence of a mixed operation is silently recorded by setting FloatOperation in the context flags.
Explicit
conversions with from_float() or create_decimal_from_float() do not set the flag.
Otherwise (the signal is trapped), only equality comparisons and explicit conversions are silent.
All other mixed
operations raise FloatOperation.
The following table summarizes the hierarchy of signals:
exceptions.ArithmeticError(exceptions.
Exception)
DecimalException
Clamped
DivisionByZero(DecimalException, exceptions.
InvalidOperation
Rounded
Subnormal
FloatOperation(DecimalException, exceptions.
TypeError)
9.4.7 Floating Point Notes
Mitigating round-off error with increased precision
The use of decimal floating point eliminates decimal representation error (making it possible to represent 0.1 exactly);
however, some operations can still incur round-off error when non-zero digits exceed the fixed precision.
The effects of round-off error can be amplified by the addition or subtraction of nearly offsetting quantities resulting in
loss of significance.
The decimal module makes it possible to restore the identities by expanding the precision sufficiently to avoid loss of
significance:
Decimal('9.51111111')
(continues on next page)
9.4.
Special values
The number system for the decimal module provides special values including NaN, sNaN, -Infinity, Infinity,
and two zeros, +0 and -0.
Infinities can be constructed directly with: Decimal('Infinity').
Also, they can arise from dividing by zero when
the DivisionByZero signal is not trapped.
Likewise, when the Overflow signal is not trapped, infinity can result
from rounding beyond the limits of the largest representable number.
The infinities are signed (affine) and can be used in arithmetic operations where they get treated as very large, indeterminate
numbers.
For instance, adding a constant to infinity gives another infinite result.
Some operations are indeterminate and return NaN, or if the InvalidOperation signal is trapped, raise an exception.
For example, 0/0 returns NaN which means “not a number”.
This variety of NaN is quiet and, once created, will flow
through other computations always resulting in another NaN. This behavior can be useful for a series of computations that
occasionally have missing inputs — it allows the calculation to proceed while flagging specific results as invalid.
A variant is sNaN which signals rather than remaining quiet after every operation.
This is a useful return value when an
invalid result needs to interrupt a calculation for special handling.
The behavior of Python’s comparison operators can be a little surprising where a NaN is involved.
A test
for equality where one of the operands is a quiet or signaling NaN always returns False (even when doing
Decimal('NaN')==Decimal('NaN')), while a test for inequality always returns True.
An attempt to com-
pare two Decimals using any of the <, <=, > or >= operators will raise the InvalidOperation signal if either
operand is a NaN, and return False if this signal is not trapped.
Note that the General Decimal Arithmetic specifica-
tion does not specify the behavior of direct comparisons; these rules for comparisons involving a NaN were taken from
the IEEE 854 standard (see Table 3 in section 5.7).
To ensure strict standards-compliance, use the compare() and
compare_signal() methods instead.
The signed zeros can result from calculations that underflow.
They keep the sign that would have resulted if the calculation
had been carried out to greater precision.
Since their magnitude is zero, both positive and negative zeros are treated as
equal and their sign is informational.
In addition to the two signed zeros which are distinct yet equal, there are various representations of zero with differing
precisions yet equivalent in value.
This takes a bit of getting used to.
Having separate thread contexts
means that threads may make changes (such as getcontext().prec=10) without interfering with other threads.
Likewise, the setcontext() function automatically assigns its target to the current thread.
If setcontext() has not been called before getcontext(), then getcontext() will automatically create a
new context for use in the current thread.
The new context is copied from a prototype context called DefaultContext.
To control the defaults so that each thread will
use the same values throughout the application, directly modify the DefaultContext object.
This should be done before
any threads are started so that there won’t be a race condition between threads calling getcontext().
Afterwards, the threads can be started
t1.start()
t2.start()
t3.start()
. .
Decimal FAQ
Q.
It is cumbersome to type decimal.Decimal('1234.5').
Is there a way to minimize typing when using the
interactive interpreter?
A.
Q.
In a fixed-point application with two decimal places, some inputs have many places and need to be rounded.
Others
are not supposed to have excess digits and need to be validated.
What methods should be used?
A. The quantize() method rounds to a fixed number of decimal places.
Traceback (most recent call last):

Inexact: None
Q.
Once I have valid two place inputs, how do I maintain that invariant throughout an application?
A.
Some operations like addition, subtraction, and multiplication by an integer will automatically preserve fixed point.
Q. There are many ways to express the same value.
The numbers 200, 200.000, 2E2, and .02E+4 all have the same
value at various precisions.
Is there a way to transform them to a single recognizable canonical value?
A.
Q. When does rounding occur in a computation?
A.
It occurs after the computation.
The philosophy of the decimal specification is that numbers are considered exact and
are created independent of the current context.
They can even have greater precision than current context.
Q.
Some decimal values always print with exponential notation.
Is there a way to get a non-exponential representation?
A.
For some values, exponential notation is the only way to express the number of significant places in the coefficient.
For
example, expressing 5.0E+3 as 5000 keeps the value constant but cannot show the original’s two-place significance.
Q. Is there a way to convert a regular float to a Decimal?
A. Yes, any binary floating point number can be exactly expressed as a Decimal though an exact conversion may take
more precision than intuition would suggest:
9.4.
Q.
Within a complex calculation, how can I make sure that I haven’t gotten a spurious result because of insufficient
precision or rounding anomalies.
A.
The decimal module makes it easy to test results.
A best practice is to re-run calculations using greater precision and
with various rounding modes.
Widely differing results indicate insufficient precision, rounding mode issues, ill-conditioned
inputs, or a numerically unstable algorithm.
Q. I noticed that context precision is applied to the results of operations but not to the inputs.
Is there anything to watch
out for when mixing values of different precisions?
A.
Yes.
The principle is that all values are considered to be exact and so is the arithmetic on those values.
Only the results
are rounded.
The advantage for inputs is that “what you type is what you get”.
Q. Is the CPython implementation fast for large numbers?
A.
Yes.
In the CPython and PyPy3 implementations, the C/CFFI versions of the decimal module integrate the high speed
libmpdec library for arbitrary precision correctly rounded decimal floating point arithmetic1.
The context must be adapted for exact arbitrary precision arithmetic.
Emin and Emax should always be set to the
maximum values, clamp should always be 0 (the default).
Setting prec requires some care.
For inexact results, MAX_PREC is far too large on 64-bit platforms and the available memory will be insufficient:
Traceback (most recent call last):
(continues on next page)

New in version 3.3.
This approach now works for all exact results except for non-integer powers.
File "<stdin>", line 1, in <module>
MemoryError
On systems with overallocation (e.g. Linux), a more sophisticated approach is to adjust prec to the amount of available
RAM.
Suppose that you have 8GB of RAM and expect 10 simultaneous operands using a maximum of 500MB each:

Traceback (most recent call last):
File "<stdin>", line 1, in <module>
decimal.
Inexact'>]
In general (and especially on systems without overallocation), it is recommended to estimate even tighter bounds and set
the Inexact trap if all calculations are expected to be exact.
A Fraction instance can be constructed from a pair of integers, from another rational number, or from a string.
Fraction(numerator=0, denominator=1)
class fractions.
Fraction(other_fraction)
class fractions.
Fraction(float)
class fractions.
Fraction(decimal)
class fractions.
The first version requires that numerator and denominator are instances of numbers.
Rational and re-
turns a new Fraction instance with value numerator/denominator.
If denominator is 0, it raises
a ZeroDivisionError.
The second version requires that other_fraction is an instance of numbers.
Rational and returns a Fraction instance with the same value.
The next two versions accept either a float
or a decimal.
Decimal instance, and return a Fraction instance with exactly the same value.
Note that due
to the usual issues with binary floating-point (see tut-fp-issues), the argument to Fraction(1.1) is not exactly
equal to 11/10, and so Fraction(1.1) does not return Fraction(11, 10) as one might expect.
The last version of the constructor expects
a string or unicode instance.
The usual form for this instance is:
9.5.
In addition, any string that
represents a finite value and is accepted by the float constructor is also accepted by the Fraction constructor.
In either form the input string may also have leading and/or trailing whitespace.
Here are some examples:
Fraction(-8, 5)
Fraction(123, 1)
Fraction(0, 1)
Fraction(3, 7)
Fraction(-3, 7)
Fraction(1414213, 1000000)
Fraction(-1, 8)
Fraction(7, 1000000)
Fraction(9, 4)
Fraction(2476979795053773, 2251799813685248)
Fraction(11, 10)
The Fraction class inherits from the abstract base class numbers.
Rational, and implements all of the
methods and operations from that class.
Fraction instances are hashable, and should be treated as immutable.
The Fraction constructor now accepts float and decimal.
Decimal instances.
Changed in version 3.9: The math.gcd() function is now used to normalize the numerator and denominator.
Previously, the GCD type depended on numerator and denominator.
Changed in version 3.11: Underscores are now permitted when creating a Fraction instance from a string,
following PEP 515 rules.
Fraction implements __int__ now to satisfy typing.
SupportsInt instance
checks.
Changed in version 3.12: Space is allowed around the slash for string inputs: Fraction('2 / 3').
Changed in version 3.12: Fraction instances now support float-style formatting, with presentation types "e",
"E", "f", "F", "g", "G" and "%"".
The ratio is in lowest terms and
has a positive denominator.
New in version 3.8.
New in version 3.12.
Integral.
Beware that
Fraction.from_float(0.3) is not the same value as Fraction(3, 10).
Note: From Python 3.2 onwards, you can also construct a Fraction instance directly from a float.
Decimal or numbers.
Integral.
Note: From Python 3.2 onwards, you can also construct a Fraction instance directly from a decimal.
Decimal instance.
This
method is useful for finding rational approximations to a given floating-point number:
Fraction(355, 113)
or for recovering a rational number that’s represented as a float:
Fraction(4503599627370497, 9007199254740992)
Fraction(1, 2)
Fraction(11, 10)
__floor__()
Returns the greatest int <= self.
This method can also be accessed through the math.floor() func-
tion:

__ceil__()
Returns the least int >= self.
This method can also be accessed through the math.ceil() function.
The first version returns the nearest int to self, rounding half to even.
The second version rounds self
to the nearest multiple of Fraction(1, 10**ndigits) (logically, if ndigits is negative), again
rounding half toward even.
This method can also be accessed through the round() function.
The presentation types "e", "E", "f", "F",
"g", "G" and "%" are supported.
For these presentation types, formatting for a Fraction object x
follows the rules outlined for the float type in the Format Specification Mini-Language section.
Here are some examples:
'0.1428571428571428571428571428571428571429'
'1_234_567.86'
'********3.141593e+00'
'34.67% price increase'
See also:
Module numbers The abstract base classes making up the numeric tower.
For integers, there is uniform selection from a range.
For sequences, there is uniform selection of a random element, a
function to generate a random permutation of a list in-place, and a function for random sampling without replacement.
On the real line, there are functions to compute uniform, normal (Gaussian), lognormal, negative exponential, gamma,
and beta distributions.
For generating distributions of angles, the von Mises distribution is available.
Almost all module functions depend on the basic function random(), which generates a random float uniformly in
the half-open range 0.0 <= X < 1.0.
Python uses the Mersenne Twister as the core generator.
It produces 53-
bit precision floats and has a period of 2**19937-1.
The underlying implementation in C is both fast and threadsafe.
The Mersenne Twister is one of the most extensively tested random number generators in existence.
However, being
completely deterministic, it is not suitable for all purposes, and is completely unsuitable for cryptographic purposes.
The functions supplied by this module are actually bound methods of a hidden instance of the random.
Random class.
You can instantiate your own instances of Random to get generators that don’t share state.
Class Random can also be subclassed if you want to use a different basic generator of your own devising: in that case,
override the random(), seed(), getstate(), and setstate() methods.
Optionally, a new generator can supply
a getrandbits() method — this allows randrange() to produce selections over an arbitrarily large range.
The random module also provides the SystemRandom class which uses the system function os.urandom() to
generate random numbers from sources provided by the operating system.
For security or
cryptographic uses, see the secrets module.
A 623-dimensionally equidistributed uniform pseudorandom num-
ber generator”, ACM Transactions on Modeling and Computer Simulation Vol. 8, No. 1, January pp.3–30 1998.
Complementary-Multiply-with-Carry recipe for a compatible alternative random number generator with a long period
and comparatively simple update operations.
If a is omitted or None, the current system time is used.
If randomness sources are provided by the operating
system, they are used instead of the system time (see the os.urandom() function for details on availability).
If a is an int, it is used directly.
With version 2 (the default), a str, bytes, or bytearray object gets converted to an int and all of its bits
are used.
With version 1 (provided for reproducing random sequences from older versions of Python), the algorithm for str
and bytes generates a narrower range of seeds.
Changed in version 3.2: Moved to the version 2 scheme which uses all of the bits in a string seed.
Changed in version 3.11: The seed must be one of the following types: NoneType, int, float, str, bytes,
or bytearray.
This object can be passed to setstate()
to restore the state.
This method should not be used for generating security tokens.
Use secrets.token_bytes() instead.
New in version 3.9.
This is roughly equivalent to choice(range(start, stop, step)) but supports arbitrarily large ranges
and is optimized for common cases.
The positional argument pattern matches the range() function.
Keyword arguments should not be used because they can be interpreted in unexpected ways.
For example
randrange(start=100) is interpreted as randrange(0, 100, 1).
Changed in version 3.2: randrange() is more sophisticated about producing equally distributed values.
For-
merly it used a style like int(random()*n) which could produce slightly uneven distributions.
Changed in version 3.12: Automatic conversion of non-integer types is no longer supported.
Calls such as
randrange(10.0) and randrange(Fraction(10, 1)) now raise a TypeError.
random.randint(a, b)
Return a random integer N such that a <= N <= b. Alias for randrange(a, b+1).
random.getrandbits(k)
Returns a non-negative Python integer with k random bits.
This method is supplied with the Mersenne Twister
generator and some other generators may also provide it as an optional part of the API.
When available,
getrandbits() enables randrange() to handle arbitrarily large ranges.
This method now accepts zero for k.
9.6.4 Functions for sequences
random.choice(seq)
Return a random element from the non-empty sequence seq.
If seq is empty, raises IndexError.
random.choices(population, weights=None, *, cum_weights=None, k=1)
Return a k sized list of elements chosen from the population with replacement.
If the population is empty, raises
IndexError.
If a weights sequence is specified, selections are made according to the relative weights.
Alternatively, if a
cum_weights sequence is given, the selections are made according to the cumulative weights (perhaps computed
using itertools.accumulate()).
For example, the relative weights [10, 5, 30, 5] are equivalent
to the cumulative weights [10, 15, 45, 50].
Internally, the relative weights are converted to cumulative
weights before making selections, so supplying the cumulative weights saves work.
If neither weights nor cum_weights are specified, selections are made with equal probability.
If a weights sequence
is supplied, it must be the same length as the population sequence.
It is a TypeError to specify both weights and
cum_weights.
The weights or cum_weights can use any numeric type that interoperates with the float values returned by
random() (that includes integers, floats, and fractions but excludes decimals).
Weights are assumed to be non-
negative and finite.
A ValueError is raised if all weights are zero.
For a given seed, the choices() function with equal weighting typically produces a different sequence than
repeated calls to choice().
The algorithm used by choices() uses floating point arithmetic for internal
consistency and speed.
The algorithm used by choice() defaults to integer arithmetic with repeated selections
to avoid small biases from round-off error.
Raises a ValueError if all weights are zero.
random.shuffle(x)
Shuffle the sequence x in place.
To shuffle an immutable sequence and return a new shuffled list, use sample(x, k=len(x)) instead.
Note that even for small len(x), the total number of permutations of x can quickly grow larger than the period of
most random number generators.
This implies that most permutations of a long sequence can never be generated.
For example, a sequence of length 2080 is the largest that can fit within the period of the Mersenne Twister random
number generator.
The optional parameter random.
Used for random sampling without
replacement.
Returns a new list containing elements from the population while leaving the original population unchanged.
The
resulting list is in selection order so that all sub-slices will also be valid random samples.
This allows raffle winners
(the sample) to be partitioned into grand prize and second place winners (the subslices).
Members of the population need not be hashable or unique.
If the population contains repeats, then each occurrence
is a possible selection in the sample.
Repeated elements can be specified one at a time or with the optional keyword-only counts parameter.
For example,
sample(['red', 'blue'], counts=[4, 2], k=5) is equivalent to sample(['red', 'red',
'red', 'red', 'blue', 'blue'], k=5).
To choose a sample from a range of integers, use a range() object as an argument.
This is especially fast and
space efficient for sampling from a large population: sample(range(10000000), k=60).
If the sample size is larger than the population size, a ValueError is raised.
Added the counts parameter.
Changed in version 3.11: The population must be a sequence.
Automatic conversion of sets to lists is no longer
supported.
The number of trials n should be a non-negative integer.
The probability of success p should be between 0.0 <=
p <= 1.0.
Function parameters are named after the correspond-
ing variables in the distribution’s equation, as used in common mathematical practice; most of these equations can be
found in any statistics text.
The low and high bounds default to zero and one.
The mode argument defaults to the midpoint
between the bounds, giving a symmetric distribution.
Beta distribution.
Exponential distribution.
It should be nonzero.
Returned values range from 0 to positive infinity if
lambd is positive, and from negative infinity to 0 if lambd is negative.
Changed in version 3.12: Added the default value for lambd.
random.gammavariate(alpha, beta)
Gamma distribution.
The shape and scale parameters, alpha and beta, must have
positive values.
This is slightly faster than the normalvariate() function defined below.
Multithreading note: When two threads call this function simultaneously, it is possible that they will receive the
same return value.
This can be avoided in three ways.
Changed in version 3.11: mu and sigma now have default arguments.
If you take the natural logarithm of this distribution, you’ll get a normal distribution with
mean mu and standard deviation sigma.
Changed in version 3.11: mu and sigma now have default arguments.
If kappa is equal to zero, this distribution reduces to a uniform random angle
over the range 0 to 2*pi.
Weibull distribution.
Class that implements the default pseudo-random number generator used by the random module.
Deprecated since version 3.9, removed in version 3.11: Formerly the seed could be any hashable object.
Now it is
limited to: NoneType, int, float, str, bytes, or bytearray.
Class that uses the os.urandom() function for generating random numbers from sources provided by the op-
erating system.
Not available on all systems.
Does not rely on software state, and sequences are not reproducible.
Accordingly, the seed() method has no effect and is ignored.
The getstate() and setstate() methods
raise NotImplementedError if called.
Sometimes it is useful to be able to reproduce the sequences given by a pseudo-random number generator.
By reusing a
seed value, the same sequence should be reproducible from run to run as long as multiple threads are not running.
If a new seeding method is added, then a backward compatible seeder will be offered.
The generator’s random() method will continue to produce the same sequence when the compatible seeder is
given the same seed.
See also:
Statistics for Hackers a video tutorial by Jake Vanderplas on statistical analysis using just a few fundamental concepts
including simulation, sampling, shuffling, and cross-validation.
Economics Simulation a simulation of a marketplace by Peter Norvig that shows effective use of many of the tools and
distributions provided by this module (gauss, uniform, sample, betavariate, choice, triangular, and randrange).
A Concrete Introduction to Probability (using Python) a tutorial by Peter Norvig covering the basics of probability theory,
how to write simulations, and how to perform data analysis using Python.
Order the result to match the iterable.
The default random() returns multiples of 2⁻⁵³ in the range 0.0 ≤ x < 1.0.
All such numbers are evenly spaced and are
exactly representable as Python floats.
However, many other representable floats in that interval are not possible selections.
For example, 0.05954861408025609 isn’t an integer multiple of 2⁻⁵³.
The following recipe takes a different approach.
All floats in the interval are possible selections.
The exponent comes from a geometric distribution
where exponents smaller than -53 occur half as often as the next larger exponent.
The recipe is conceptually equivalent to an algorithm that chooses from all the multiples of 2⁻¹⁰⁷⁴ in the range 0.0 ≤ x <
1.0.
All such numbers are evenly spaced, but most have to be rounded down to the nearest representable Python float.
See also:
Generating Pseudo-random Floating-Point Values a paper by Allen B. Downey describing ways to generate more fine-
grained floats than normally generated by random().
Source code: Lib/statistics.py
This module provides functions for calculating mathematical statistics of numeric (Real-valued) data.
The module is not intended to be a competitor to third-party libraries such as NumPy, SciPy, or proprietary full-featured
statistics packages aimed at professional statisticians such as Minitab, SAS and Matlab.
It is aimed at the level of graphing
and scientific calculators.
Unless explicitly noted, these functions support int, float, Decimal and Fraction.
Behaviour with other types
(whether in the numeric tower or not) is currently unsupported.
Collections with a mix of types are also undefined
and implementation-dependent.
If your input data consists of mixed types, you may be able to use map() to ensure a
consistent result, for example: map(float, input_data).
Some datasets use NaN (not a number) values to represent missing data.
Since NaNs have unusual comparison seman-
tics, they cause surprising or undefined behaviors in the statistics functions that sort data or that count occurrences.
The functions affected are median(), median_low(), median_high(), median_grouped(), mode(),
multimode(), and quantiles().
Fast, floating point arithmetic mean, with optional weighting.
Geometric mean of data.
However, for reading convenience, most of the
examples show sorted sequences.
The arithmetic mean is the sum of the data divided by the number of data points.
It is commonly called “the
average”, although it is only one of many different mathematical averages.
It is a measure of the central location of
the data.
If data is empty, StatisticsError will be raised.
Note: The mean is strongly affected by outliers and is not necessarily a typical example of the data points.
For a
more robust, although less efficient, measure of central tendency, see median().
The sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all
the possible samples, mean(sample) converges on the true mean of the entire population.
If data represents the
entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean
μ.
statistics.fmean(data, weights=None)
Convert data to floats and compute the arithmetic mean.
This runs faster than the mean() function and it always returns a float.
The data may be a sequence or iterable.
If the input dataset is empty, raises a StatisticsError.
If weights is supplied, it must be the same length as the data or a ValueError will be raised.
New in version 3.8.
Convert data to floats and compute the geometric mean.
The geometric mean indicates the central tendency or typical value of the data using the product of the values (as
opposed to the arithmetic mean which uses their sum).
Raises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value.
The data may be a sequence or iterable.
No special efforts are made to achieve exact results.
If weights is omitted or None,
then equal weighting is assumed.
The harmonic mean is the reciprocal of the arithmetic mean() of the reciprocals of the data.
For example, the
harmonic mean of three values a, b and c will be equivalent to 3/(1/a + 1/b + 1/c).
If one of the values
is zero, the result will be zero.
The harmonic mean is a type of average, a measure of the central location of the data.
It is often appropriate when
averaging ratios or rates, for example speeds.
Suppose a car travels 10 km at 40 km/hr, then another 10 km at 60 km/hr.
What is the average speed?
48.0
Suppose a car travels 40 km/hr for 5 km, and when traffic clears, speeds-up to 60 km/hr for the remaining 30 km
of the journey.
What is the average speed?
56.0
StatisticsError is raised if data is empty, any element is less than zero, or if the weighted sum isn’t positive.
The current algorithm has an early-out when it encounters a zero in the input.
This means that the subsequent
inputs are not tested for validity.
New in version 3.6.
Return the median (middle value) of numeric data, using the common “mean of middle two” method.
If data is
empty, StatisticsError is raised.
The median is a robust measure of central location and is less affected by the presence of outliers.
This is suited for when your data is discrete, and you don’t mind that the median may not be an actual data point.
If the data is ordinal (supports order operations) but not numeric (doesn’t support addition), consider using
median_low() or median_high() instead.
If data is empty, StatisticsError is raised.
The low median is always a member of the data set.
When the number of data points is odd, the middle value is
returned.
When it is even, the smaller of the two middle values is returned.
Return the high median of data.
If data is empty, StatisticsError is raised.
The high median is always a member of the data set.
When the number of data points is odd, the middle value is
returned.
When it is even, the larger of the two middle values is returned.
If data is
empty, StatisticsError is raised.
In the following example, the data are rounded, so that each value represents the midpoint of data classes, e.g. 1
is the midpoint of the class 0.5–1.5, 2 is the midpoint of 1.5–2.5, 3 is the midpoint of 2.5–3.5, etc.
With the data
given, the middle value falls somewhere in the class 3.5–4.5, and interpolation is used to estimate it:
3.7
Optional argument interval represents the class interval, and defaults to 1.
This function does not check whether the data points are at least interval apart.
Under some circumstances, median_grouped() may coerce data points to
floats.
This behaviour is likely to change in the future.
See also:
• “Statistics for the Behavioral Sciences”, Frederick J Gravetter and Larry B Wallnau (8th Edition).
The SSMEDIAN function in the Gnome Gnumeric spreadsheet, including this discussion.
The mode (when it exists) is the most
typical value and serves as a measure of central location.
If there are multiple modes with the same frequency, returns the first one encountered in the data.
If the smallest
or largest of those is desired instead, use min(multimode(data)) or max(multimode(data)).
If the
input data is empty, StatisticsError is raised.
Formerly, it
raised StatisticsError when more than one mode was found.
statistics.multimode(data)
Return a list of the most frequently occurring values in the order they were first encountered in the data.
See pvariance() for
arguments and other details.
0.986893273527251
statistics.pvariance(data, mu=None)
Return the population variance of data, a non-empty sequence or iterable of real-valued numbers.
Variance, or
second moment about the mean, is a measure of the variability (spread or dispersion) of data.
A large variance
indicates that the data is spread out; a small variance indicates it is clustered closely around the mean.
If the optional second argument mu is given, it is typically the mean of the data.
It can also be used to compute the
second moment around a point that is not the mean.
If it is missing or None (the default), the arithmetic mean is
automatically calculated.
Use this function to calculate the variance from the entire population.
To estimate the variance from a sample, the
variance() function is usually a better choice.
Raises StatisticsError if data is empty.
Note:
When called with the entire population, this gives the population variance σ².
When called on a sample
instead, this is the biased sample variance s², also known as variance with N degrees of freedom.
If you somehow know the true population mean μ, you may use this function to calculate the variance of a sample,
giving the known population mean as the second argument.
Provided the data points are a random sample of the
population, the result will be an unbiased estimate of the population variance.
statistics.stdev(data, xbar=None)
Return the sample standard deviation (the square root of the sample variance).
See variance() for arguments
and other details.
Variance, or second moment
about the mean, is a measure of the variability (spread or dispersion) of data.
A large variance indicates that the
data is spread out; a small variance indicates it is clustered closely around the mean.
If the optional second argument xbar is given, it should be the mean of data.
If it is missing or None (the default),
the mean is automatically calculated.
Use this function when your data is a sample from a population.
To calculate the variance from the entire population,
see pvariance().
Raises StatisticsError if data has fewer than two values.
Examples:
1.3720238095238095
If you have already calculated the mean of your data, you can pass it as the optional second argument xbar to avoid
recalculation:
1.3720238095238095
9.7.
This function does not attempt to verify that you have passed the actual mean as xbar.
Using arbitrary values for
xbar can lead to invalid or impossible results.
Fraction(67, 108)
Note: This is the sample variance s² with Bessel’s correction, also known as variance with N-1 degrees of freedom.
Provided that the data points are representative (e.g. independent and identically distributed), the result should be
an unbiased estimate of the true population variance.
Divide data into n continuous intervals with equal probability.
Returns a list of n - 1 cut points separating the
intervals.
Set n to 4 for quartiles (the default).
Set n to 10 for deciles.
Set n to 100 for percentiles which gives the 99 cuts
points that separate data into 100 equal sized groups.
Raises StatisticsError if n is not least 1.
The data can be any iterable containing sample data.
For meaningful results, the number of data points in data
should be larger than n. Raises StatisticsError if there are not at least two data points.
The cut points are linearly interpolated from the two nearest data points.
For example, if a cut point falls one-third
of the distance between two sample values, 100 and 112, the cut-point will evaluate to 104.
The method for computing quantiles can be varied depending on whether the data includes or excludes the lowest
and highest possible values from the population.
The default method is “exclusive” and is used for data sampled from a population that can have more extreme values
than found in the samples.
The portion of the population falling below the i-th of m sorted data points is computed
as i / (m + 1).
Given nine sample values, the method sorts them and assigns the following percentiles: 10%,
20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%.
Setting the method to “inclusive” is used for describing population data or for samples that are known to include
the most extreme values from the population.
The minimum value in data is treated as the 0th percentile and the
maximum value is treated as the 100th percentile.
The portion of the population falling below the i-th of m sorted
data points is computed as (i - 1) / (m - 1).
Given 11 sample values, the method sorts them and assigns
the following percentiles: 0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100%.
Both inputs must be of the same length (no less than two), otherwise StatisticsError is raised.
Examples:
0.75
-7.5
-7.5
New in version 3.10.
Pearson’s correlation coefficient r takes values between
-1 and +1.
It measures the strength and direction of a linear relationship.
If method is “ranked”, computes Spearman’s rank correlation coefficient for two inputs.
The data is replaced by
ranks.
Ties are averaged so that equal values receive the same rank.
The resulting coefficient measures the strength
of a monotonic relationship.
Spearman’s correlation coefficient is appropriate for ordinal data or for continuous data that doesn’t meet the linear
proportion requirement for Pearson’s correlation coefficient.
Both inputs must be of the same length (no less than two), and need not to be constant, otherwise
StatisticsError is raised.
Example with Kepler’s laws of planetary motion:
Neptune
# days
1.0
0.9882
1.0
New in version 3.10.
Changed in version 3.12: Added support for Spearman’s rank correlation coefficient.
Both inputs must be of the same length (no less than two), and the independent variable x cannot be constant;
otherwise a StatisticsError is raised.
For example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty
Python films that would have been produced by 2019 assuming that they had kept the pace.
The data is fit to a line passing through the origin.
Continuing the example from correlation(), we look to see how well a model based on major planets can
predict the orbital distances for dwarf planets:
Pluto,
Eris,
Makemake, Haumea, Ceres
# days
[5912, 10166, 6806, 6459, 414]
# actual distance in million km
[5906, 10152, 6796, 6450, 414]
New in version 3.10.
Changed in version 3.11: Added support for proportional.
It is a class that treats
the mean and standard deviation of data measurements as a single entity.
Normal distributions arise from the Central Limit Theorem and have a wide range of applications in statistics.
NormalDist(mu=0.0, sigma=1.0)
Returns a new NormalDist object where mu represents the arithmetic mean and sigma represents the standard
deviation.
If sigma is negative, raises StatisticsError.
A read-only property for the variance of a normal distribution.
Equal to the square of the standard deviation.
The data can be any iterable and should consist of values that can be converted to type float.
If data does
not contain at least two elements, raises StatisticsError because it takes at least one point to estimate
a central value and at least two points to estimate dispersion.
Returns a list of float values.
If seed is given, creates a new instance of the underlying random number generator.
This is useful for creating
reproducible results, even in a multi-threading context.
Mathematically, it is the limit of the ratio P(x <= X < x+dx) / dx as dx
approaches zero.
The relative likelihood is computed as the probability of a sample occurring in a narrow range divided by the
width of the range (hence the word “density”).
Since the likelihood is relative to other points, its value can
be greater than 1.0.
cdf(x)
Using a cumulative distribution function (cdf), compute the probability that a random variable X will be less
than or equal to x.
Mathematically, it is written P(X <= x).
Compute the inverse cumulative distribution function, also known as the quantile function or the percent-point
function.
Mathematically, it is written x : P(X <= x) = p.
9.7.
Measures the agreement between two normal probability distributions.
Returns a value between 0.0 and 1.0
giving the overlapping area for the two probability density functions.
Returns a list of (n - 1) cut
points separating the intervals.
Set n to 4 for quartiles (the default).
Set n to 10 for deciles.
Set n to 100 for percentiles which gives the 99
cuts points that separate the normal distribution into 100 equal sized groups.
Compute the Standard Score describing x in terms of the number of standard deviations above or below the
mean of the normal distribution: (x - mean) / stdev.
New in version 3.9.
Instances of NormalDist support addition, subtraction, multiplication and division by a constant.
These opera-
tions are used for translation and scaling.
Dividing a constant by an instance of NormalDist is not supported because the result wouldn’t be normally
distributed.
Since normal distributions arise from additive effects of independent variables, it is possible to add and subtract
two independent normally distributed random variables represented as instances of NormalDist.
For example:
3.1
0.5
New in version 3.8.
NormalDist Examples and Recipes
Classic probability problems
NormalDist readily solves classic probability problems.
For example, an open source conference has 750 attendees and two rooms with a 500 person capacity.
There is a talk
about Python and another about Ruby.
In previous conferences, 65% of the attendees preferred to listen to Python talks.
Assuming the population preferences haven’t changed, what is the probability that the Python room will stay within its
capacity limits?
Wikipedia has a nice example of a Naive Bayesian Classifier.
The challenge is to predict a person’s gender from mea-
surements of normally distributed features including height, weight, and foot size.
We’re given a training dataset with measurements for eight people.
The final prediction goes to the largest posterior.
This is known as the maximum a posteriori or MAP:
'female'
Kernel density estimation
It is possible to estimate a continuous probability density function from a fixed number of discrete samples.
The basic idea is to smooth the data using a kernel function such as a normal distribution, triangular distribution, or
uniform distribution.
The degree of smoothing is controlled by a single parameter, h, representing the variance of the
kernel function.
The points in xarr and yarr can be used to make a PDF plot:
9.7.
The modules described in this chapter provide functions and classes that support a functional programming style, and
general operations on callables.
The following modules are documented in this chapter:
10.1 itertools — Functions creating iterators for efficient looping
This module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML.
Each
has been recast in a form suitable for Python.
The module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination.
To-
gether, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure
Python.
For instance, SML provides a tabulation tool: tabulate(f) which produces a sequence f(0), f(1), ....
The
same effect can be achieved in Python by combining map() and count() to form map(f, count()).
These tools and their built-in counterparts also work well with the high-speed functions in the operator mod-
ule.
For example, the multiplication operator can be mapped across two vectors to form an efficient dot-product:
sum(starmap(operator.mul, zip(vec1, vec2, strict=True))).
Itertool functions
The following module functions all construct and return iterators.
Some provide streams of infinite length, so they should
only be accessed by functions or loops that truncate the stream.
If func is supplied, it should be a function of two arguments.
Elements of the input iterable may be any type that
can be accepted as arguments to func.
Usually, the number of elements output matches the input iterable.
However, if the keyword argument initial is
provided, the accumulation leads off with the initial value so that the output has one more element than the input
iterable.
Roughly equivalent to:

# accumulate([1,2,3,4,5]) --> 1 3 6 10 15
# accumulate([1,2,3,4,5], initial=100) --> 100 101 103 106 110 115
# accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120
it = iter(iterable)
total = initial
if initial is None:
try:
total = next(it)
except StopIteration:

for element in it:
total = func(total, element)

There are a number of uses for the func argument.
It can be set to min() for a running minimum, max() for a
running maximum, or operator.mul() for a running product.
Amortization tables can be built by accumu-
lating interest and applying payments:
# running product
[3, 12, 72, 144, 144, 1296, 0, 0, 0, 0]
# running maximum
[3, 4, 6, 6, 6, 9, 9, 9, 9, 9]
# Amortize a 5% loan of 1000 with 10 annual payments of 90
[1000, 960, 918, 874, 828, 779, 728, 674, 618, 559, 497]
See functools.reduce() for a similar function that returns only the final accumulated value.
New in version 3.2.
Added the optional func parameter.
Added the optional initial parameter.
10.1.
The last batch may be shorter than n.
Loops over the input iterable and accumulates data into tuples up to size n.
The input is consumed lazily, just
enough to fill a batch.
Used for treating consecutive sequences as a single sequence.
Gets chained inputs from a single iterable argument that is evaluated lazily.
Roughly equivalent to:

# chain.from_iterable(['ABC', 'DEF']) --> A B C D E F
for it in iterables:
for element in it:

itertools.combinations(iterable, r)
Return r length subsequences of elements from the input iterable.
The combination tuples are emitted in lexicographic ordering according to the order of the input iterable.
So, if
the input iterable is sorted, the output tuples will be produced in sorted order.
Elements are treated as unique based on their position, not on their value.
So if the input elements are unique, there
will be no repeated values in each combination.
The combination tuples are emitted in lexicographic ordering according to the order of the input iterable.
So, if
the input iterable is sorted, the output tuples will be produced in sorted order.
Elements are treated as unique based on their position, not on their value.
So if the input elements are unique, the
generated combinations will also be unique.
The number of items returned is (n+r-1)!
New in version 3.1.
itertools.compress(data, selectors)
Make an iterator that filters elements from data returning only those that have a corresponding element in selectors
that evaluates to True.
Stops when either the data or selectors iterables has been exhausted.
Often used as an argument to map()
to generate consecutive data points.
Also, used with zip() to add sequence numbers.
Changed in version 3.1: Added step argument and allowed non-integer arguments.
Repeats indefinitely.
Note, the iterator does not produce any output until the predicate first becomes false, so it may have a
lengthy start-up time.
Roughly equivalent to:

# dropwhile(lambda x: x<5, [1,4,6,4,1]) --> 6 4 1
iterable = iter(iterable)
for x in iterable:
if not predicate(x):

break
for x in iterable:

itertools.filterfalse(predicate, iterable)
Make an iterator that filters elements from iterable returning only those for which the predicate is false.
If predicate
is None, return the items that are false.
Roughly equivalent to:

# filterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8
if predicate is None:
predicate = bool
for x in iterable:
if not predicate(x):

itertools.groupby(iterable, key=None)
Make an iterator that returns consecutive keys and groups from the iterable.
The key is a function computing a key
value for each element.
If not specified or is None, key defaults to an identity function and returns the element
unchanged.
Generally, the iterable needs to already be sorted on the same key function.
The operation of groupby() is similar to the uniq filter in Unix.
It generates a break or new group every time
the value of the key function changes (which is why it is usually necessary to have sorted the data using the same
key function).
That behavior differs from SQL’s GROUP BY which aggregates common elements regardless of
their input order.
The returned group is itself an iterator that shares the underlying iterable with groupby().
Because the source
is shared, when the groupby() object is advanced, the previous group is no longer visible.
If start is non-zero, then elements from the iterable
are skipped until start is reached.
Afterward, elements are returned consecutively unless step is set higher than one
which results in items being skipped.
If stop is None, then iteration continues until the iterator is exhausted, if at
all; otherwise, it stops at the specified position.
If start is None, then iteration starts at zero.
If step is None, then the step defaults to one.
Unlike regular slicing, islice() does not support negative values for start, stop, or step.
Can be used to extract
related fields from data where the internal structure has been flattened (for example, a multi-line report may list a
name field on every third line).
The number of 2-tuples in the output iterator will be one fewer than the number of inputs.
It will be empty if the
input iterable has fewer than two values.
Roughly equivalent to:

# pairwise('ABCDEFG') --> AB BC CD DE EF FG
a, b = tee(iterable)
next(b, None)

New in version 3.10.
itertools.permutations(iterable, r=None)
Return successive r length permutations of elements in the iterable.
If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations
are generated.
The permutation tuples are emitted in lexicographic order according to the order of the input iterable.
So, if the
input iterable is sorted, the output tuples will be produced in sorted order.
Elements are treated as unique based on their position, not on their value.
So if the input elements are unique, there
will be no repeated values within a permutation.
Roughly equivalent to nested for-loops in a generator expression.
For example, product(A, B) returns the
same as ((x,y) for x in A for y in B).
The nested loops cycle like an odometer with the rightmost element advancing on every iteration.
This pattern
creates a lexicographic ordering so that if the input’s iterables are sorted, the product tuples are emitted in sorted
order.
To compute the product of an iterable with itself, specify the number of repetitions with the optional repeat keyword
argument.
For example, product(A, repeat=4) means the same as product(A, A, A, A).
Before product() runs, it completely consumes the input iterables, keeping pools of values in memory to gen-
erate the products.
Accordingly, it is only useful with finite inputs.
Runs indefinitely unless the times argument is specified.
Used instead of map()
when argument parameters are already grouped in tuples from a single iterable (when the data has been “pre-
zipped”).
The difference between map() and starmap() parallels the distinction between function(a,b) and
function(*c).
Roughly equivalent to:

# starmap(pow, [(2,5), (3,2), (10,3)]) --> 32 9 1000
for args in iterable:

itertools.takewhile(predicate, iterable)
Make an iterator that returns elements from the iterable as long as the predicate is true.
Once a tee() has been created, the original iterable should not be used anywhere else; otherwise, the iterable
could get advanced without the tee objects being informed.
A RuntimeError may be raised when simultaneously using iterators returned
by the same tee() call, even if the original iterable is threadsafe.
This itertool may require significant auxiliary storage (depending on how much temporary data needs to be stored).
In general, if one iterator uses most or all of the data before another iterator starts, it is faster to use list()
instead of tee().
If the iterables are of uneven length, missing
values are filled-in with fillvalue.
Iteration continues until the longest iterable is exhausted.
If one of the iterables is potentially infinite, then the zip_longest() function should be wrapped with some-
thing that limits the number of calls (for example islice() or takewhile()).
If not specified, fillvalue
defaults to None.
This section shows recipes for creating an extended toolset using the existing itertools as building blocks.
The primary purpose of the itertools recipes is educational.
The recipes show various ways of thinking about individual
tools — for example, that chain.from_iterable is related to the concept of flattening.
The recipes also give ideas
about ways that the tools can be combined — for example, how compress() and range() can work together.
The
recipes also show patterns for using itertools with the operator and collections modules as well as with the
built-in itertools such as map(), filter(), reversed(), and enumerate().
A secondary purpose of the recipes is to serve as an incubator.
The accumulate(), compress(), and
pairwise() itertools started out as recipes.
Currently, the sliding_window() and iter_index() recipes
are being tested to see whether they prove their worth.
Superior memory performance is kept by
processing elements one at a time rather than bringing the whole iterable into memory all at once.
Code volume is kept
small by linking the tools together in a functional style which helps eliminate temporary variables.
High speed is retained
by preferring “vectorized” building blocks over the use of for-loops and generators which incur interpreter overhead.
If n is None, consume entirely.
If no true value is found, returns *default*
If *pred* is not None, returns the first item
for which pred(item) is true.
Converts a call-until-exception interface to an iterator interface.
Like builtins.iter(func, sentinel) but uses an exception instead
of a sentinel to end the loop.
If *pred* is slow, consider wrapping it with functools.lru_cache().
Remember all elements ever seen.
Remember only the element just seen.
The kernel is fully consumed before the calculations begin.
The signal is consumed lazily and can be infinite.
Convolutions are mathematically commutative.
If the signal and kernel are swapped,
the output will be the same.
In general, any
callable object can be treated as a function for the purposes of this module.
The functools module defines the following functions:
@functools.cache(user_function)
Simple lightweight unbounded function cache.
Sometimes called “memoize”.
Returns the same as lru_cache(maxsize=None), creating a thin wrapper around a dictionary lookup for
the function arguments.
Because it never needs to evict old values, this is smaller and faster than lru_cache()
with a size limit.
For example:


# no previously cached result, makes 11 recursive calls

# just looks up cached value result

# makes two new recursive calls, the other 10 are cached

10.2.
The cache is threadsafe so that the wrapped function can be used in multiple threads.
This means that the underlying
data structure will remain coherent during concurrent updates.
It is possible for the wrapped function to be called more than once if another thread makes an additional call before
the initial call has been completed and cached.
New in version 3.9.
@functools.cached_property(func)
Transform a method of a class into a property whose value is computed once and then cached as a normal attribute
for the life of the instance.
Similar to property(), with the addition of caching.
Useful for expensive computed
properties of instances that are otherwise effectively immutable.
The mechanics of cached_property() are somewhat different from property().
A regular property
blocks attribute writes unless a setter is defined.
In contrast, a cached_property allows writes.
The cached_property decorator only runs on lookups and only when an attribute of the same name doesn’t exist.
When it does run, the cached_property writes to the attribute with the same name.
Subsequent attribute reads and
writes take precedence over the cached_property method and it works like a normal attribute.
The cached value can be cleared by deleting the attribute.
This allows the cached_property method to run again.
The cached_property does not prevent a possible race condition in multi-threaded usage.
The getter function could
run more than once on the same instance, with the latest run setting the cached value.
If the cached property is
idempotent or otherwise not harmful to run more than once on an instance, this is fine.
If synchronization is needed,
implement the necessary locking inside the decorated getter function or around the cached property access.
Note, this decorator interferes with the operation of PEP 412 key-sharing dictionaries.
This means that instance
dictionaries can take more space than usual.
If a mutable mapping is not available or if space-efficient key sharing is desired, an effect similar to
cached_property() can also be achieved by stacking property() on top of lru_cache().
See faq-
cache-method-calls for more details on how this differs from cached_property().
New in version 3.8.
Changed in version 3.12: Prior to Python 3.12, cached_property included an undocumented lock to ensure
that in multi-threaded usage the getter function was guaranteed to run only once per instance.
However, the lock
was per-property, not per-instance, which could result in unacceptably high lock contention.
In Python 3.12+ this
locking is removed.
Transform an old-style comparison function to a key function.
This function is primarily used as a transition tool for programs being converted from Python
2 which supported the use of comparison functions.
A comparison function is any callable that accepts two arguments, compares them, and returns a negative number
for less-than, zero for equality, or a positive number for greater-than.
A key function is a callable that accepts one
argument and returns another value to be used as the sort key.
New in version 3.2.
@functools.lru_cache(user_function)
@functools.lru_cache(maxsize=128, typed=False)
Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls.
It can save
time when an expensive or I/O bound function is periodically called with the same arguments.
The cache is threadsafe so that the wrapped function can be used in multiple threads.
This means that the underlying
data structure will remain coherent during concurrent updates.
It is possible for the wrapped function to be called more than once if another thread makes an additional call before
the initial call has been completed and cached.
Since a dictionary is used to cache results, the positional and keyword arguments to the function must be hashable.
Distinct argument patterns may be considered to be distinct calls with separate cache entries.
For example, f(a=1,
b=2) and f(b=2, a=1) differ in their keyword argument order and may have two separate cache entries.
If user_function is specified, it must be a callable.
If maxsize is set to None, the LRU feature is disabled and the cache can grow without bound.
If typed is set to true, function arguments of different types will be cached separately.
If typed is false, the imple-
mentation will usually regard them as equivalent calls and only cache a single result.
Note, type specificity applies only to the function’s immediate arguments rather than their contents.
The scalar
arguments, Decimal(42) and Fraction(42) are be treated as distinct calls with distinct results.
In contrast,
the tuple arguments ('answer', Decimal(42)) and ('answer', Fraction(42)) are treated as
equivalent.
The wrapped function is instrumented with a cache_parameters() function that returns a new dict showing
the values for maxsize and typed.
This is for information purposes only.
Mutating the values has no effect.
To help measure the effectiveness of the cache and tune the maxsize parameter, the wrapped function is instrumented
with a cache_info() function that returns a named tuple showing hits, misses, maxsize and currsize.
The decorator also provides a cache_clear() function for clearing or invalidating the cache.
The original underlying function is accessible through the __wrapped__ attribute.
This is useful for introspec-
tion, for bypassing the cache, or for rewrapping the function with a different cache.
10.2.
The cache keeps references to the arguments and return values until they age out of the cache or until the cache is
cleared.
If a method is cached, the self instance argument is included in the cache.
See faq-cache-method-calls
An LRU (least recently used) cache works best when the most recent calls are the best predictors of upcoming calls
(for example, the most popular articles on a news server tend to change each day).
The cache’s size limit assures
that the cache does not grow without bound on long-running processes such as web servers.
In general, the LRU cache should only be used when you want to reuse previously computed values.
Accordingly,
it doesn’t make sense to cache functions with side-effects, functions that need to create distinct mutable objects on
each call (such as generators and async functions), or impure functions such as time() or random().
Example of an LRU cache for static web content:
@lru_cache(maxsize=32)

resource = f'https://peps.python.org/pep-{num:04d}'
try:
with urllib.request.urlopen(resource) as s:

except urllib.error.
Added the typed option.
Changed in version 3.8: Added the user_function option.
Added the function cache_parameters()

Given a class defining one or more rich comparison ordering methods, this class decorator supplies the rest.
The class must define one of __lt__(), __le__(), __gt__(), or __ge__().
In addition, the class should
supply an __eq__() method.
Note: While this decorator makes it easy to create well behaved totally ordered types, it does come at the cost of
slower execution and more complex stack traces for the derived comparison methods.
If performance benchmark-
ing indicates this is a bottleneck for a given application, implementing all six rich comparison methods instead is
likely to provide an easy speed boost.
Note: This decorator makes no attempt to override methods that have been declared in the class or its superclasses.
Meaning that if a superclass defines a comparison operator, total_ordering will not implement it again, even if the
original method is abstract.
New in version 3.2.
Changed in version 3.4: Returning NotImplemented from the underlying comparison function for unrecognised
types is now supported.
If more arguments are supplied to the call, they are appended to args.
If additional
keyword arguments are supplied, they extend and override keywords.
The partial() is used for partial function application which “freezes” some portion of a function’s arguments
and/or keywords resulting in a new object with a simplified signature.
For example, partial() can be used to
create a callable that behaves like the int() function where the base argument defaults to two:
(continues on next page)
10.2.
When func is a descriptor (such as a normal Python function, classmethod(), staticmethod(),
abstractmethod() or another instance of partialmethod), calls to __get__ are delegated to the un-
derlying descriptor, and an appropriate partial object returned as the result.
When func is a non-descriptor callable, an appropriate bound method is created dynamically.
This behaves like a
normal Python function when used as a method: the self argument will be inserted as the first positional argument,
even before the args and keywords supplied to the partialmethod constructor.
Example:


self._alive = False







self._alive = bool(state)

set_alive = partialmethod(set_state, True)

set_dead = partialmethod(set_state, False)

False
True
New in version 3.4.
functools.reduce(function, iterable[, initializer])
Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the
iterable to a single value.
For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
((((1+2)+3)+4)+5).
The left argument, x, is the accumulated value and the right argument, y, is the update
value from the iterable.
If the optional initializer is present, it is placed before the items of the iterable in the
calculation, and serves as a default when the iterable is empty.
If initializer is not given and iterable contains only
one item, the first item is returned.
To define a generic function, decorate it with the @singledispatch decorator.
UnionType and typing.
The register() attribute returns the undecorated function.
False
When called, the generic function dispatches on the type of the first argument:
Hello, world.
Let me just say, test.
Strength in numbers, eh? 42
Enumerate this:
0 spam
1 spam
2 eggs
3 spam
Nothing.
The original function decorated with @singledispatch is registered for the base
object type, which means it is used if no better implementation is found.
Decimal'>, <class 'list'>,
<class 'float'>])
<function fun_num at 0x1035a2840>
<function fun at 0x103fe0000>
New in version 3.4.
Changed in version 3.7: The register() attribute now supports using type annotations.
The register() attribute now supports types.
UnionType and typing.
Union
as type annotations.
To define a generic method, decorate it with the @singledispatchmethod decorator.
When defining a
function using @singledispatchmethod, note that the dispatch happens on the type of the first non-self or
non-cls argument:
class Negator:






@singledispatchmethod supports nesting with other decorators such as @classmethod.
Note that to
allow for dispatcher.register, singledispatchmethod must be the outer most decorator.
Here is
the Negator class with the neg methods bound to the class, rather than an instance of the class:
class Negator:






(continues on next page)
10.2.
New in version 3.8.
functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS,
updated=WRAPPER_UPDATES)
Update a wrapper function to look like the wrapped function.
The optional arguments are tuples to specify which
attributes of the original function are assigned directly to the matching attributes on the wrapper function and which
attributes of the wrapper function are updated with the corresponding attributes from the original function.
The
default values for these arguments are the module level constants WRAPPER_ASSIGNMENTS (which assigns to
the wrapper function’s __module__, __name__, __qualname__, __annotations__ and __doc__,
the documentation string) and WRAPPER_UPDATES (which updates the wrapper function’s __dict__, i.e. the
instance dictionary).
To allow access to the original function for introspection and other purposes (e.g. bypassing a caching decorator
such as lru_cache()), this function automatically adds a __wrapped__ attribute to the wrapper that refers
to the function being wrapped.
The main intended use for this function is in decorator functions which wrap the decorated function and return
the wrapper.
If the wrapper function is not updated, the metadata of the returned function will reflect the wrapper
definition rather than the original function definition, which is typically less than helpful.
Any attributes named in assigned or
updated that are missing from the object being wrapped are ignored (i.e. this function will not attempt to set them
on the wrapper function).
AttributeError is still raised if the wrapper function itself is missing any attributes
named in updated.
New in version 3.2: Automatic addition of the __wrapped__ attribute.
Copying of the __annotations__ attribute by default.
Missing attributes no longer trigger an AttributeError.
The __wrapped__ attribute now always refers to the wrapped function, even if that
function defined a __wrapped__ attribute.
This is a convenience function for invoking update_wrapper() as a function decorator when defin-
ing a wrapper function.
It is equivalent to partial(update_wrapper, wrapped=wrapped,
assigned=assigned, updated=updated).
Calling decorated function
Called example function
'example'
'Docstring'
Without the use of this decorator factory, the name of the example function would have been 'wrapper', and
the docstring of the original example() would have been lost.
They have three read-only attributes:
partial.func
A callable object or function.
Calls to the partial object will be forwarded to func with new arguments and
keywords.
There
are some important differences.
For instance, the __name__ and __doc__ attributes are not created automatically.
Also, partial objects defined in classes behave like static methods and do not transform into bound methods during
instance attribute look-up.
For
example, operator.add(x, y) is equivalent to the expression x+y.
Many function names are those used for special
methods, without the double underscores.
For backward compatibility, many of these have a variant with the double
underscores kept.
The variants without the double underscores are preferred for clarity.
The functions fall into categories that perform object comparisons, logical operations, mathematical operations and se-
quence operations.
See comparisons for more information about rich
comparisons.
The logical operations are also generally applicable to all objects, and support truth tests, identity tests, and boolean
operations:
operator.not_(obj)
operator.__not__(obj)
Return the outcome of not obj.
Return True if obj is true, and False otherwise.
This is equivalent to using the bool constructor.
The mathematical and bitwise operations are the most numerous:
operator.abs(obj)
operator.__abs__(obj)
Return the absolute value of obj.
Equivalent to a.__index__().
The result always has exact type int.
Previously, the result could have been an instance
of a subclass of int.
This is equivalent to ~obj.
This is also known as “true” division.
Note the reversed operands.
First try to return its actual length, then an estimate using object.
__length_hint__(), and finally return the default value.
New in version 3.4.
New in version 3.11.
The operator module also defines tools for generalized attribute and item lookups.
These are useful for making fast
field extractors as arguments for map(), sorted(), itertools.groupby(), or other functions that expect a
function argument.
operator.attrgetter(attr)
operator.attrgetter(*attrs)
Return a callable object that fetches attr from its operand.
If more than one attribute is requested, returns a tuple
of attributes.
The attribute names can also contain dots.
For example:
• After f = attrgetter('name'), the call f(b) returns b.name.
Equivalent to:

raise TypeError('attribute name must be a string')
if len(items) == 1:
attr = items[0]

else:



obj = getattr(obj, name)

operator.itemgetter(item)
operator.itemgetter(*items)
Return a callable object that fetches item from its operand using the operand’s __getitem__() method.
If
multiple items are specified, returns a tuple of lookup values.
After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]).
Equivalent to:

item = items[0]

else:


The items can be any type accepted by the operand’s __getitem__() method.
Dictionaries accept any hashable
value.
Lists, tuples, and strings accept an index or a slice:
'B'
('B', 'D', 'F')
'CDEFG'
'captain'
Example of using itemgetter() to retrieve specific fields from a tuple record:
10.3.
If additional arguments and/or keyword argu-
ments are given, they will be given to the method as well.
For example:
• After f = methodcaller('name'), the call f(b) returns b.name().
This table shows how abstract operations correspond to operator symbols in the Python syntax and the functions in the
operator module.
In-place Operators
Many operations have an “in-place” version.
Listed below are functions providing a more primitive access to in-place
operators than the usual syntax does; for example, the statement x += y is equivalent to x = operator.iadd(x,
y).
In those examples, note that when an in-place method is called, the computation and assignment are performed in two
separate steps.
The in-place functions listed below only do the first step, calling the in-place method.
The second step,
assignment, is not handled.
The modules described in this chapter deal with disk files and directories.
For example, there are modules for reading
the properties of files, manipulating paths in a portable way, and creating temporary files.
The full list of modules in this
chapter is:
11.1 pathlib — Object-oriented filesystem paths
New in version 3.4.
Source code: Lib/pathlib.py
This module offers classes representing filesystem paths with semantics appropriate for different operating systems.
Path
classes are divided between pure paths, which provide purely computational operations without I/O, and concrete paths,
which inherit from pure paths but also provide I/O operations.
It instantiates a concrete path for the platform the code is running on.
Pure paths are useful in some special cases; for example:
1.
If you want to manipulate Windows paths on a Unix machine (or vice versa).
You cannot instantiate a
WindowsPath when running on Unix, but you can instantiate PureWindowsPath.
You want to make sure that your code only manipulates paths without actually accessing the OS.
In this case,
instantiating one of the pure classes may be useful since those simply don’t have any OS-accessing operations.
See also:
PEP 428: The pathlib module – object-oriented filesystem paths.
See also:
For low-level path manipulation on strings, you can also use the os.path module.
There are three ways to
access these classes, which we also call flavours:
class pathlib.
A generic class that represents the system’s path flavour (instantiating it creates either a PurePosixPath or a
PureWindowsPath):
# Running on a Unix machine
PurePosixPath('setup.py')
Each element of pathsegments can be either a string representing a path segment, or an object implementing the
os.
PurePosixPath('foo/bar')
PurePosixPath('foo/../bar')
(a
naïve
approach
would
make
PurePosixPath('foo/../bar')
equivalent
to
PurePosixPath('bar'), which is wrong if foo is a symbolic link to another directory)
11.1.
PathLike interface, allowing them to be used anywhere the interface is
accepted.
Changed in version 3.6: Added support for the os.
PathLike interface.
A subclass of PurePath, this path flavour represents non-Windows filesystem paths:
PurePosixPath('/etc')
pathsegments is specified similarly to PurePath.
A subclass of PurePath, this path flavour represents Windows filesystem paths, including UNC paths:
PureWindowsPath('c:/Program Files')
PureWindowsPath('//server/share/file')
pathsegments is specified similarly to PurePath.
Regardless of the system you’re running on, you can instantiate all of these classes, since they don’t provide any operation
that does system calls.
General properties
Paths are immutable and hashable.
Paths of a same flavour are comparable and orderable.
If the argument is an absolute path, the previous
path is ignored.
A path object can be used anywhere an object implementing os.
The string representation of a path is the raw filesystem path itself (in native form, e.g. with backslashes under Windows),
which you can pass to any function taking a file path as a string:
'/etc'
'c:\\Program Files'
Similarly, calling bytes on a path gives the raw filesystem path as a bytes object, as encoded by os.fsencode():
b'/etc'
Note: Calling bytes is only recommended under Unix.
Under Windows, the unicode form is the canonical represen-
tation of filesystem paths.
PurePath.parts
A tuple giving access to the path’s various components:
('/', 'usr', 'bin', 'python3')
(continues on next page)
11.1.
The parents sequence now supports slices and negative index values.
If you want to walk an arbitrary filesystem path upwards, it is recommended to first call Path.resolve() so
as to resolve symlinks and eliminate ".." components.
PurePath.name
A string representing the final path component, excluding the drive and root, if any:
'setup.py'
11.1.
A path is considered absolute if it has both a root and (if the flavour
allows) a drive:
True
False
True
False
False
True
PurePath.is_relative_to(other)
Return whether or not this path is relative to the other path.
True
False
New in version 3.9.
Deprecated since version 3.12, will be removed in version 3.14: Passing additional arguments is deprecated; if
supplied, they are joined with other.
PurePath.is_reserved()
With PureWindowsPath, return True if the path is considered reserved under Windows, False otherwise.
With PurePosixPath, False is always returned.
PurePosixPath('/etc/init.d/apache2')
PureWindowsPath('c:/Program Files')
PurePath.match(pattern, *, case_sensitive=None)
Match this path against the provided glob-style pattern.
Return True if matching is successful, False otherwise.
True
As with other methods, case-sensitivity follows platform defaults:
False
True
Set case_sensitive to True or False to override this behaviour.
The case_sensitive parameter was added.
PurePath.relative_to(other, walk_up=False)
Compute a version of this path relative to the path represented by other.
ValueError: '/etc/passwd' is not in the subpath of '/usr' OR one path is relative␣
�→and the other is absolute.
When walk_up is False (the default), the path must start with other.
When the argument is True, .. entries may
be added to form the relative path.
ValueError: '/etc/passwd' is not on the same drive as 'foo' OR one path is␣
�→relative and the other is absolute.
Warning:
This function is part of PurePath and works with strings.
It does not check or access the
underlying file structure.
This can impact the walk_up option as it assumes that no symlinks are present in the
path; call resolve() first if necessary to resolve symlinks.
Changed in version 3.12: The walk_up parameter was added (old behavior is the same as walk_up=False).
Return a new path with the name changed.
ValueError: PureWindowsPath('c:/') has an empty name
PurePath.with_stem(stem)
Return a new path with the stem changed.
ValueError: PureWindowsPath('c:/') has an empty name
New in version 3.9.
PurePath.with_suffix(suffix)
Return a new path with the suffix changed.
If the original path doesn’t have a suffix, the new suffix is appended
instead.
If the suffix is an empty string, the original suffix is removed:
11.1.
This method is called whenever
a derivative path is created, such as from parent and relative_to().
Subclasses may override this method
to pass information to derivative paths, for example:
from pathlib import PurePosixPath
class MyPath(PurePosixPath):

self.session_id = session_id

etc = MyPath('/etc', session_id=42)
hosts = etc / 'hosts'
print(hosts.session_id)
# 42
New in version 3.12.
In addition to operations provided by the latter, they also provide
methods to do system calls on path objects.
A subclass of PurePath, this class represents concrete paths of the system’s path flavour (instantiating it creates
either a PosixPath or a WindowsPath):
PosixPath('setup.py')
pathsegments is specified similarly to PurePath.
A subclass of Path and PurePosixPath, this class represents concrete non-Windows filesystem paths:
PosixPath('/etc')
pathsegments is specified similarly to PurePath.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "pathlib.py", line 798, in __new__
% (cls.__name__,))
NotImplementedError: cannot instantiate 'WindowsPath' on your system
Methods
Concrete paths provide the following methods in addition to pure paths methods.
Many of these methods can raise an
OSError if a system call fails (for example because the path doesn’t exist).
Changed
in
version
3.8:
exists(),
is_dir(),
is_file(),
is_mount(),
is_symlink(),
is_block_device(), is_char_device(), is_fifo(), is_socket() now return False instead
of raising an exception for paths that contain characters unrepresentable at the OS level.
classmethod Path.cwd()
Return a new path object representing the current directory (as returned by os.getcwd()):
PosixPath('/home/antoine/pathlib')
classmethod Path.home()
Return a new path object representing the user’s home directory (as returned by os.path.expanduser()
with ~ construct).
If the home directory can’t be resolved, RuntimeError is raised.
New in version 3.5.
Path.stat(*, follow_symlinks=True)
Return a os.stat_result object containing information about this path, like os.stat().
The result is
looked up at each call to this method.
This method normally follows symlinks; to stat a symlink add the argument follow_symlinks=False, or
use lstat().
The follow_symlinks parameter was added.
Path.chmod(mode, *, follow_symlinks=True)
Change the file mode and permissions, like os.chmod().
This method normally follows symlinks.
Some Unix flavours support changing permissions on the symlink itself;
on these platforms you may add the argument follow_symlinks=False, or use lchmod().


Changed in version 3.10: The follow_symlinks parameter was added.
Path.exists(*, follow_symlinks=True)
Return True if the path points to an existing file or directory.
This
method
normally
follows
symlinks;
to
check
if
a
symlink
exists,
add
the
argument
follow_symlinks=False.
True
True
True
False
Changed in version 3.12: The follow_symlinks parameter was added.
Path.expanduser()
Return a new path with expanded ~ and ~user constructs, as returned by os.path.expanduser().
If a
home directory can’t be resolved, RuntimeError is raised.
PosixPath('/home/eric/films/Monty Python')
New in version 3.5.
Patterns are the same as for fnmatch, with the addition of “**” which means “this directory and all subdirectories,
recursively”.
By default, or when the case_sensitive keyword-only argument is set to None, this method matches paths us-
ing platform-specific casing rules: typically, case-sensitive on POSIX, and case-insensitive on Windows.
Set
case_sensitive to True or False to override this behaviour.
Note: Using the “**” pattern in large directory trees may consume an inordinate amount of time.
Raises an auditing event pathlib.Path.glob with arguments self, pattern.
Changed in version 3.11: Return only directories if pattern ends with a pathname components separator (sep or
altsep).
The case_sensitive parameter was added.
Path.group()
Return the name of the group owning the file.
KeyError is raised if the file’s gid isn’t found in the system
database.
Path.is_dir()
Return True if the path points to a directory (or a symbolic link pointing to a directory), False if it points to
another kind of file.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
Path.is_file()
Return True if the path points to a regular file (or a symbolic link pointing to a regular file), False if it points to
another kind of file.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
Path.is_junction()
Return True if the path points to a junction, and False for any other type of file.
Currently only Windows
supports junctions.
New in version 3.12.
Path.is_mount()
Return True if the path is a mount point: a point in a file system where a different file system has been mounted.
On POSIX, the function checks whether path’s parent, path/.., is on a different device than path, or whether
path/.. and path point to the same i-node on the same device — this should detect mount points for all Unix
and POSIX variants.
On Windows, a mount point is considered to be a drive letter root (e.g. c:\), a UNC share
(e.g. \\server\share), or a mounted filesystem directory.
New in version 3.7.
Changed in version 3.12: Windows support was added.
Path.is_symlink()
Return True if the path points to a symbolic link, False otherwise.
False is also returned if the path doesn’t exist; other errors (such as permission errors) are propagated.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
Path.is_fifo()
Return True if the path points to a FIFO (or a symbolic link pointing to a FIFO), False if it points to another
kind of file.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
Path.is_block_device()
Return True if the path points to a block device (or a symbolic link pointing to a block device), False if it points
to another kind of file.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
Path.is_char_device()
Return True if the path points to a character device (or a symbolic link pointing to a character device), False if
it points to another kind of file.
False is also returned if the path doesn’t exist or is a broken symlink; other errors (such as permission errors) are
propagated.
The children are yielded in arbitrary order, and the special entries '.' and '..' are not included.
If a file is
removed from or added to the directory after creating the iterator, whether a path object for that file be included is
unspecified.
Path.walk(top_down=True, on_error=None, follow_symlinks=False)
Generate the file names in a directory tree by walking the tree either top-down or bottom-up.
To get a full path (which begins with self) to a file or directory in dirpath, do dirpath / name.
Whether or not the lists are sorted is file system-dependent.
If the optional argument top_down is true (which is the default), the triple for a directory is generated before the
triples for any of its subdirectories (directories are walked top-down).
No matter the value of
top_down, the list of subdirectories is retrieved before the triples for the directory and its subdirectories are walked.
When top_down is true, the caller can modify the dirnames list in-place (for example, using del or slice assign-
ment), and Path.walk() will only recurse into the subdirectories whose names remain in dirnames.
This can be
used to prune the search, or to impose a specific order of visiting, or even to inform Path.walk() about directo-
ries the caller creates or renames before it resumes Path.walk() again.
Modifying dirnames when top_down is
false has no effect on the behavior of Path.walk() since the directories in dirnames have already been generated
by the time dirnames is yielded to the caller.
By default, errors from os.scandir() are ignored.
If the optional argument on_error is specified, it should
be a callable; it will be called with one argument, an OSError instance.
The callable can handle the error to
continue the walk or re-raise it to stop the walk.
Note that the filename is available as the filename attribute of
the exception object.
By default, Path.walk() does not follow symbolic links, and instead adds them to the filenames list.
Set fol-
low_symlinks to true to resolve symlinks and place them in dirnames and filenames as appropriate for their targets,
and consequently visit directories pointed to by symlinks (where supported).
Note:
Be aware that setting follow_symlinks to true can lead to infinite recursion if a link points to a parent
directory of itself.
Path.walk() does not keep track of the directories it has already visited.
Note:
Path.walk() assumes the directories it walks are not modified during execution.
For example, if a
directory from dirnames has been replaced with a symlink and follow_symlinks is false, Path.walk() will still
try to descend into it.
To prevent such behavior, remove directories from dirnames as appropriate.
Note: Unlike os.walk(), Path.walk() lists symlinks to directories in filenames if follow_symlinks is false.
This next example is a simple implementation of shutil.rmtree().
Walking the tree bottom-up is essential
as rmdir() doesn’t allow deleting a directory before it is empty:
# Delete everything reachable from the directory "top".
This is dangerous!
For example, if top == Path('/'),
# it could delete all of your files.
Like Path.chmod() but, if the path points to a symbolic link, the symbolic link’s mode is changed rather than
its target’s.
Path.lstat()
Like Path.stat() but, if the path points to a symbolic link, return the symbolic link’s information rather than
its target’s.
Path.mkdir(mode=0o777, parents=False, exist_ok=False)
Create a new directory at this given path.
If mode is given, it is combined with the process’ umask value to
determine the file mode and access flags.
If the path already exists, FileExistsError is raised.
If parents is true, any missing parents of this path are created as needed; they are created with the default permissions
without taking mode into account (mimicking the POSIX mkdir -p command).
If parents is false (the default), a missing parent raises FileNotFoundError.
If exist_ok is false (the default), FileExistsError is raised if the target directory already exists.
If exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p
command), but only if the last path component is not an existing non-directory file.
The exist_ok parameter was added.
Path.open(mode=’r’, buffering=- 1, encoding=None, errors=None, newline=None)
Open the file pointed to by the path, like the built-in open() function does:

f.readline()

'#!/usr/bin/env python3\n'
Path.owner()
Return the name of the user owning the file.
KeyError is raised if the file’s uid isn’t found in the system database.
Path.read_bytes()
Return the binary contents of the pointed-to file as a bytes object:

b'Binary file contents'
New in version 3.5.
The file is opened and then closed.
The optional parameters have the same meaning as in open().
Rename this file or directory to the given target, and return a new Path instance pointing to target.
On Unix, if
target exists and is a file, it will be replaced silently if the user has permission.
On Windows, if target exists,
FileExistsError will be raised.
The target path may be absolute or relative.
Relative paths are interpreted relative to the current working directory,
not the directory of the Path object.
It is implemented in terms of os.rename() and gives the same guarantees.
Rename this file or directory to the given target, and return a new Path instance pointing to target.
If target points
to an existing file or empty directory, it will be unconditionally replaced.
The target path may be absolute or relative.
Relative paths are interpreted relative to the current working directory,
not the directory of the Path object.
Changed in version 3.8: Added return value, return the new Path instance.
Path.absolute()
Make the path absolute, without normalization or resolving symlinks.
Returns a new path object:
PosixPath('tests')
PosixPath('/home/antoine/pathlib/tests')
11.1.
If the path doesn’t exist and strict is True, FileNotFoundError is raised.
If strict is False, the path is
resolved as far as possible and any remainder is appended without checking whether it exists.
If an infinite loop is
encountered along the resolution path, RuntimeError is raised.
The strict parameter was added (pre-3.6 behavior is strict).
Path.rglob(pattern, *, case_sensitive=None)
Glob the given relative pattern recursively.
By default, or when the case_sensitive keyword-only argument is set to None, this method matches paths us-
ing platform-specific casing rules: typically, case-sensitive on POSIX, and case-insensitive on Windows.
Set
case_sensitive to True or False to override this behaviour.
Raises an auditing event pathlib.Path.rglob with arguments self, pattern.
Changed in version 3.11: Return only directories if pattern ends with a pathname components separator (sep or
altsep).
The case_sensitive parameter was added.
Path.rmdir()
Remove this directory.
The directory must be empty.
Path.samefile(other_path)
Return whether this path points to the same file as other_path, which can be either a Path object, or a string.
The
semantics are similar to os.path.samefile() and os.path.samestat().
An OSError can be raised if either file cannot be accessed for some reason.
Under Windows, target_is_directory must be true (default False) if the
link’s target is a directory.
Under POSIX, target_is_directory’s value is ignored.
Make this path a hard link to the same file as target.
Note: The order of arguments (link, target) is the reverse of os.link()’s.
New in version 3.10.
Path.touch(mode=0o666, exist_ok=True)
Create a file at this given path.
If mode is given, it is combined with the process’ umask value to determine the file
mode and access flags.
If the file already exists, the function succeeds if exist_ok is true (and its modification time
is updated to the current time), otherwise FileExistsError is raised.
Path.unlink(missing_ok=False)
Remove this file or symbolic link.
If the path points to a directory, use Path.rmdir() instead.
If missing_ok is false (the default), FileNotFoundError is raised if the path does not exist.
If missing_ok is true, FileNotFoundError exceptions will be ignored (same behavior as the POSIX rm -f
command).
Changed in version 3.8: The missing_ok parameter was added.
Path.write_bytes(data)
Open the file pointed to in bytes mode, write data to it, and close the file:

b'Binary file contents'
An existing file of the same name is overwritten.
New in version 3.5.
Path.write_text(data, encoding=None, errors=None, newline=None)
Open the file pointed to in text mode, write data to it, and close the file:
11.1.
The optional parameters have the same meaning as in open().
New in version 3.5.
The newline parameter was added.
Note: Not all pairs of functions/methods below are equivalent.
Some of them, despite having some overlapping use-cases,
have different semantics.
They include os.path.abspath() and Path.absolute(), os.path.relpath()
and PurePath.relative_to().
This module implements some useful functions on pathnames.
To read or write files see open(), and for accessing the
filesystem see the os module.
The path parameters can be passed as strings, or bytes, or any object implementing the
os.
PathLike protocol.
Unlike a Unix shell, Python does not do any automatic path expansions.
Functions such as expanduser() and
expandvars() can be invoked explicitly when an application desires shell-like path expansion.
See also:
The pathlib module offers high-level path objects.
Note: All of these functions accept either only bytes or only string objects as their parameters.
The result is an object of
the same type, if a path or file name is returned.
Note: Since different operating systems have different path name conventions, there are several versions of this module
in the standard library.
The os.path module is always the path module suitable for the operating system Python is
running on, and therefore usable for local paths.
However, you can also import and use the individual modules if you
want to manipulate a path that is always in one of the different formats.
They all have the same interface:
• posixpath for UNIX-style paths
• ntpath for Windows paths
Changed in version 3.8: exists(), lexists(), isdir(), isfile(), islink(), and ismount() now return
False instead of raising an exception for paths that contain characters or bytes unrepresentable at the OS level.
os.path.abspath(path)
Return a normalized absolutized version of the pathname path.
On most platforms, this is equivalent to calling the
function normpath() as follows: normpath(join(os.getcwd(), path)).
Accepts a path-like object.
Return the base name of pathname path.
This is the second element of the pair returned by passing path to the
function split().
Note that the result of this function is different from the Unix basename program; where
basename for '/foo/bar/' returns 'bar', the basename() function returns an empty string ('').
Return the longest common sub-path of each pathname in the sequence paths.
Raise ValueError if paths
contain both absolute and relative pathnames, the paths are on the different drives or if paths is empty.
Unlike
commonprefix(), this returns a valid path.
Availability: Unix, Windows.
New in version 3.5.
Accepts a sequence of path-like objects.
Note: This function may return invalid paths because it works a character at a time.
To obtain a valid path, see
commonpath().
Return the directory name of pathname path.
This is the first element of the pair returned by passing path to the
function split().
Accepts a path-like object.
Returns False for broken symbolic
links.
On some platforms, this function may return False if permission is not granted to execute os.stat()
on the requested file, even if the path physically exists.
Changed in version 3.3: path can now be an integer: True is returned if it is an open file descriptor, False
otherwise.
Accepts a path-like object.
Returns True for broken symbolic links.
Equivalent to exists()
on platforms lacking os.lstat().
On Unix and Windows, return the argument with an initial component of ~ or ~user replaced by that user’s home
directory.
On Unix, an initial ~ is replaced by the environment variable HOME if it is set; otherwise the current user’s home
directory is looked up in the password directory through the built-in module pwd.
An initial ~user is looked up
directly in the password directory.
On Windows, USERPROFILE will be used if set, otherwise a combination of HOMEPATH and HOMEDRIVE will
be used.
An initial ~user is handled by checking that the last directory component of the current user’s home
directory matches USERNAME, and replacing it if so.
If the expansion fails or if the path does not begin with a tilde, the path is returned unchanged.
Accepts a path-like object.
No longer uses HOME on Windows.
os.path.expandvars(path)
Return the argument with environment variables expanded.
Substrings of the form $name or ${name} are
replaced by the value of environment variable name.
Malformed variable names and references to non-existing
variables are left unchanged.
Return the time of last access of path.
The return value is a floating point number giving the number of seconds
since the epoch (see the time module).
Raise OSError if the file does not exist or is inaccessible.
The return value is a floating point number giving the number of
seconds since the epoch (see the time module).
Raise OSError if the file does not exist or is inaccessible.
Accepts a path-like object.
os.path.getctime(path)
Return the system’s ctime which, on some systems (like Unix) is the time of the last metadata change, and, on
others (like Windows), is the creation time for path.
The return value is a number giving the number of seconds
since the epoch (see the time module).
Raise OSError if the file does not exist or is inaccessible.
Accepts a path-like object.
Raise OSError if the file does not exist or is inaccessible.
Accepts a path-like object.
os.path.isabs(path)
Return True if path is an absolute pathname.
On Unix, that means it begins with a slash, on Windows that it
begins with a (back)slash after chopping off a potential drive letter.
Accepts a path-like object.
os.path.isfile(path)
Return True if path is an existing regular file.
This follows symbolic links, so both islink() and
isfile() can be true for the same path.
Accepts a path-like object.
Return True if path is an existing directory.
This follows symbolic links, so both islink() and isdir()
can be true for the same path.
Accepts a path-like object.
Always return False if junctions
are not supported on the current platform.
New in version 3.12.
os.path.islink(path)
Return True if path refers to an existing directory entry that is a symbolic link.
Always False if symbolic
links are not supported by the Python runtime.
Accepts a path-like object.
On POSIX, the function checks whether path’s parent, path/.., is on a different device than path, or
whether path/.. and path point to the same i-node on the same device — this should detect mount points for
all Unix and POSIX variants.
It is not able to reliably detect bind mounts on the same filesystem.
On Windows,
11.2.
New in version 3.4: Support for detecting non-root mount points on Windows.
Return True if pathname path is located on a Windows Dev Drive.
A Dev Drive is optimized for developer
scenarios, and offers faster performance for reading and writing files.
It is recommended for use for source code,
temporary build directories, package caches, and other IO-intensive operations.
May raise an error for an invalid path, for example, one without a recognizable drive, but returns False on
platforms that do not support Dev Drives.
See the Windows documentation for information on enabling and creating
Dev Drives.
Availability: Windows.
New in version 3.12.
The return value is the concatenation of path and all members of
*paths, with exactly one directory separator following each non-empty part, except the last.
That is, the result will
only end in a separator if the last part is either empty or ends in a separator.
If a segment is an absolute path (which
on Windows requires both a drive and a root), then all previous segments are ignored and joining continues from
the absolute path segment.
On Windows, the drive is not reset when a rooted path segment (e.g., r'\foo') is encountered.
If a segment is
on a different drive or is an absolute path, all previous segments are ignored and the drive is reset.
Note that since
there is a current directory for each drive, os.path.join("c:", "foo") represents a path relative to the
current directory on drive C: (c:foo), not c:\foo.
Accepts a path-like object for path and paths.
os.path.normcase(path)
Normalize the case of a pathname.
On Windows, convert all characters in the pathname to lowercase, and also
convert forward slashes to backward slashes.
On other operating systems, return the path unchanged.
B.
This string manipulation may change the meaning of a
path that contains symbolic links.
On Windows, it converts forward slashes to backward slashes.
To
normalize case, use normcase().
Note:
On POSIX systems, in accordance with IEEE Std 1003.1 2013 Edition; 4.13 Pathname Resolution, if
a pathname begins with exactly two slashes, the first component following the leading characters may
be interpreted in an implementation-defined manner, although more than two leading characters shall
be treated as a single character.
Accepts a path-like object.
If a path doesn’t exist or a symlink loop is encountered, and strict is True, OSError is raised.
If strict is False,
the path is resolved as far as possible and any remainder is appended without checking whether it exists.
Note: This function emulates the operating system’s procedure for making a path canonical, which differs slightly
between Windows and UNIX with respect to how links and subsequent path components interact.
Operating system APIs make paths canonical as needed, so it’s not normally necessary to call this function.
Accepts a path-like object.
Symbolic links and junctions are now resolved on Windows.
The strict parameter was added.
This is a
path computation: the filesystem is not accessed to confirm the existence or nature of path or start.
On Windows,
ValueError is raised when path and start are on different drives.
Availability: Unix, Windows.
Changed in version 3.6: Accepts a path-like object.
This is determined by the device
number and i-node number and raises an exception if an os.stat() call on either pathname fails.
Availability: Unix, Windows.
Changed in version 3.2: Added Windows support.
Windows now uses the same implementation as all other platforms.
Accepts a path-like object.
Availability: Unix, Windows.
Changed in version 3.2: Added Windows support.
Accepts a path-like object.
os.path.samestat(stat1, stat2)
Return True if the stat tuples stat1 and stat2 refer to the same file.
These structures may have been returned by
os.fstat(), os.lstat(), or os.stat().
This function implements the underlying comparison used by
samefile() and sameopenfile().
Availability: Unix, Windows.
Changed in version 3.4: Added Windows support.
Split the pathname path into a pair, (head, tail) where tail is the last pathname component and head is
everything leading up to that.
The tail part will never contain a slash; if path ends in a slash, tail will be empty.
If there is no slash in path, head will be empty.
If path is empty, both head and tail are empty.
Trailing slashes
are stripped from head unless it is the root (one or more slashes only).
In all cases, join(head, tail)
11.2.
Also see the functions dirname() and
basename().
Split the pathname path into a pair (drive, tail) where drive is either a mount point or the empty string.
On
systems which do not use drive specifications, drive will always be the empty string.
In all cases, drive + tail
will be the same as path.
On Windows, splits a pathname into drive/UNC sharepoint and relative path.
Split the pathname path into a 3-item tuple (drive, root, tail) where drive is a device name or mount
point, root is a string of separators after the drive, and tail is everything after the root.
Any of these items may be
the empty string.
In all cases, drive + root + tail will be the same as path.
On POSIX systems, drive is always empty.
For example:
('', '/', 'home/sam')
('', '//', 'home/sam')
('', '/', '//home/sam')
On Windows, drive may be empty, a drive-letter name, a UNC share, or a device name.
The root may be empty,
a forward slash, or a backward slash.
For example:
('C:', '/', 'Users/Sam')
('//Server/Share', '/', 'Users/Sam')
New in version 3.12.
os.path.splitext(path)
Split the pathname path into a pair (root, ext) such that root + ext == path, and the extension, ext,
is empty or begins with a period and contains at most one period.
If the path contains an extension, then ext will be set to this extension, including the leading period.
Note that
previous periods will be ignored:
('foo.bar', '.exe')
('/foo/bar', '.exe')
Leading periods of the last component of the path are considered to be part of the root:
('.cshrc', '')
('/foo/....jpg', '')
Changed in version 3.6: Accepts a path-like object.
If you
just want to read or write one file see open().
This iterates over the lines of all files listed in sys.argv[1:], defaulting to sys.stdin if the list is empty.
If a
filename is '-', it is also replaced by sys.stdin and the optional arguments mode and openhook are ignored.
To
specify an alternative list of filenames, pass it as the first argument to input().
A single file name is also allowed.
All files are opened in text mode by default, but you can override this by specifying the mode parameter in the call to
input() or FileInput.
If an I/O error occurs during opening or reading a file, OSError is raised.
Changed in version 3.3: IOError used to be raised; it is now an alias of OSError.
If sys.stdin is used more than once, the second and further use will return no lines, except perhaps for interactive
use, or if it has been explicitly reset (e.g. using sys.stdin.seek(0)).
Empty files are opened and immediately closed; the only time their presence in the list of filenames is noticeable at all is
when the last file opened is empty.
Lines are returned with any newlines intact, which means that the last line in a file may not have one.
You can control how files are opened by providing an opening hook via the openhook parameter to fileinput.
input() or FileInput().
The hook must be a function that takes two arguments, filename and mode, and returns
an accordingly opened file-like object.
If encoding and/or errors are specified, they will be passed to the hook as additional
keyword arguments.
This module provides a hook_compressed() to support compressed files.
The following function is the primary interface of this module:
11.3.
The instance will be used as global state for the functions of this
module, and is also returned to use during iteration.
The parameters to this function will be passed along to the
constructor of the FileInput class.
The FileInput instance can be used as a context manager in the with statement.
In this example, input is
closed after the with statement is exited, even if an exception occurs:
with fileinput.input(files=('spam.txt', 'eggs.txt'), encoding="utf-8") as f:
for line in f:
process(line)
Changed in version 3.2: Can be used as a context manager.
The keyword parameters mode and openhook are now keyword-only.
The keyword-only parameter encoding and errors are added.
The following functions use the global state created by fileinput.input(); if there is no active state,
RuntimeError is raised.
Before the first line has been read, returns None.
fileinput.fileno()
Return the integer “file descriptor” for the current file.
When no file is opened (before the first line and between
files), returns -1.
Before the first line has been read, returns
0.
After the last line of the last file has been read, returns the line number of that line.
Before the first line has been read, returns 0.
After the last line of the
last file has been read, returns the line number of that line within the file.
The filename is not changed until after the first line of the
next file has been read.
Before the first line has been read, this function has no effect; it cannot be used to skip the
first file.
After the last line of the last file has been read, this function has no effect.
fileinput.close()
Close the sequence.
The class which implements the sequence behavior provided by the module is available for subclassing as well:
class fileinput.
FileInput(files=None, inplace=False, backup=”, *, mode=’r’, openhook=None,
encoding=None, errors=None)
Class
FileInput
is
the
implementation;
its
methods
filename(),
fileno(),
lineno(),
filelineno(), isfirstline(), isstdin(), nextfile() and close() correspond to the
functions of the same name in the module.
The sequence must be accessed in strictly sequential order; random access and readline()
cannot be mixed.
With mode you can specify which file mode will be passed to open().
It must be one of 'r' and 'rb'.
The openhook, when given, must be a function that takes two arguments, filename and mode, and returns an ac-
cordingly opened file-like object.
You cannot use inplace and openhook together.
You can specify encoding and errors that is passed to open() or openhook.
A FileInput instance can be used as a context manager in the with statement.
In this example, input is closed
after the with statement is exited, even if an exception occurs:
with FileInput(files=('spam.txt', 'eggs.txt')) as input:
process(input)
Changed in version 3.2: Can be used as a context manager.
The keyword parameter mode and openhook are now keyword-only.
The keyword-only parameter encoding and errors are added.
The 'rU' and 'U' modes and the __getitem__() method have been removed.
Optional in-place filtering: if the keyword argument inplace=True is passed to fileinput.input() or to the
FileInput constructor, the file is moved to a backup file and standard output is directed to the input file (if a file of
the same name as the backup file already exists, it will be replaced silently).
This makes it possible to write a filter that
rewrites its input file in place.
If the backup parameter is given (typically as backup='.<some extension>'), it
specifies the extension for the backup file, and the backup file remains around; by default, the extension is '.bak' and
it is deleted when the output file is closed.
In-place filtering is disabled when standard input is read.
The two following opening hooks are provided by this module:
fileinput.hook_compressed(filename, mode, *, encoding=None, errors=None)
Transparently opens files compressed with gzip and bzip2 (recognized by the extensions '.gz' and '.bz2')
using the gzip and bz2 modules.
If the filename extension is not '.gz' or '.bz2', the file is opened normally
(ie, using open() without any decompression).
The encoding and errors values are passed to io.
TextIOWrapper for compressed files and open for normal
files.
Usage
example:
fi = fileinput.
The keyword-only parameter encoding and errors are added.
Usage
example:
fi = fileinput.
This function is deprecated since fileinput.input() and FileInput now
have encoding and errors parameters.
Lib/stat.py
The stat module defines constants and functions for interpreting the results of os.stat(), os.fstat() and os.
lstat() (if they exist).
For complete details about the stat(), fstat() and lstat() calls, consult the docu-
mentation for your system.
Changed in version 3.4: The stat module is backed by a C implementation.
The stat module defines the following functions to test for specific file types:
stat.
Return non-zero if the mode is from a directory.
Return non-zero if the mode is from a character special device file.
Return non-zero if the mode is from a block special device file.
Return non-zero if the mode is from a regular file.
S_ISFIFO(mode)
Return non-zero if the mode is from a FIFO (named pipe).
S_ISLNK(mode)
Return non-zero if the mode is from a symbolic link.
Return non-zero if the mode is from a socket.
S_ISDOOR(mode)
Return non-zero if the mode is from a door.
New in version 3.4.
stat.
Return non-zero if the mode is from an event port.
New in version 3.4.
stat.
Return non-zero if the mode is from a whiteout.
New in version 3.4.
Two additional functions are defined for more general manipulation of the file’s mode:
stat.
S_IMODE(mode)
Return the portion of the file’s mode that can be set by os.chmod()—that is, the file’s permission bits, plus the
sticky bit, set-group-id, and set-user-id bits (on systems that support them).
S_IFMT(mode)
Return the portion of the file’s mode that describes the file type (used by the S_IS*() functions above).
These are also useful when checking for information about a file that isn’t handled by os.path, like the tests for
block and character devices.
An additional utility function is provided to convert a file’s mode in a human readable string:
stat.filemode(mode)
Convert a file’s mode to a string of the form ‘-rwxrwxrwx’.
New in version 3.3.
The function supports S_IFDOOR, S_IFPORT and S_IFWHT.
All the variables below are simply symbolic indexes into the 10-tuple returned by os.stat(), os.fstat() or os.
lstat().
stat.
ST_MODE
Inode protection mode.
stat.
ST_INO
Inode number.
ST_DEV
Device inode resides on.
stat.
ST_NLINK
Number of links to the inode.
ST_UID
User id of the owner.
ST_GID
Group id of the owner.
ST_SIZE
Size in bytes of a plain file; amount of data waiting on some special files.
stat.
ST_ATIME
Time of last access.
ST_MTIME
Time of last modification.
ST_CTIME
The “ctime” as reported by the operating system.
On some systems (like Unix) is the time of the last metadata
change, and, on others (like Windows), is the creation time (see platform documentation for details).
The interpretation of “file size” changes according to the file type.
For plain files this is the size of the file in bytes.
For
FIFOs and sockets under most flavors of Unix (including Linux in particular), the “size” is the number of bytes waiting
to be read at the time of the call to os.stat(), os.fstat(), or os.lstat(); this can sometimes be useful,
especially for polling one of these special files after a non-blocking open.
The meaning of the size field for other character
and block devices varies more, depending on the implementation of the underlying system call.
The variables below define the flags used in the ST_MODE field.
Use of the functions above is more portable than use of the first set of flags:
stat.
S_IFSOCK
Socket.
S_IFLNK
Symbolic link.
S_IFREG
Regular file.
S_IFBLK
Block device.
stat.
S_IFDIR
Directory.
stat.
S_IFCHR
Character device.
stat.
S_IFIFO
FIFO.
stat.
S_IFDOOR
Door.
New in version 3.4.
S_IFPORT
Event port.
S_IFWHT
Whiteout.
New in version 3.4.
Note:
S_IFDOOR, S_IFPORT or S_IFWHT are defined as 0 when the platform does not have support for the file
types.
The following flags can also be used in the mode argument of os.chmod():
stat.
S_ISUID
Set UID bit.
S_ISGID
Set-group-ID bit.
This bit has several special uses.
For a directory it indicates that BSD semantics is to be used
for that directory: files created there inherit their group ID from the directory, not from the effective group ID
of the creating process, and directories created there will also get the S_ISGID bit set.
For a file that does not
have the group execution bit (S_IXGRP) set, the set-group-ID bit indicates mandatory file/record locking (see also
S_ENFMT).
S_ISVTX
Sticky bit.
When this bit is set on a directory it means that a file in that directory can be renamed or deleted only
by the owner of the file, by the owner of the directory, or by a privileged process.
S_IRWXU
Mask for file owner permissions.
S_IRUSR
Owner has read permission.
stat.
S_IWUSR
Owner has write permission.
stat.
S_IXUSR
Owner has execute permission.
stat.
S_IRWXG
Mask for group permissions.
S_IRGRP
Group has read permission.
stat.
S_IWGRP
Group has write permission.
S_IXGRP
Group has execute permission.
stat.
S_IRWXO
Mask for permissions for others (not in group).
Others have read permission.
stat.
S_IWOTH
Others have write permission.
S_IXOTH
Others have execute permission.
stat.
S_ENFMT
System V file locking enforcement.
This flag is shared with S_ISGID: file/record locking is enforced on files that
do not have the group execution bit (S_IXGRP) set.
stat.
S_IREAD
Unix V7 synonym for S_IRUSR.
stat.
S_IWRITE
Unix V7 synonym for S_IWUSR.
stat.
S_IEXEC
Unix V7 synonym for S_IXUSR.
The following flags can be used in the flags argument of os.chflags():
stat.
UF_NODUMP
Do not dump the file.
stat.
The file may not be changed.
The file may only be appended to.
stat.
The directory is opaque when viewed through a union stack.
The file may not be renamed or deleted.
stat.
The file is stored compressed (macOS 10.6+).
UF_HIDDEN
The file should not be displayed in a GUI (macOS 10.5+).
The file may be archived.
The file may not be changed.
The file may only be appended to.
stat.
The file may not be renamed or deleted.
stat.
The file is a snapshot file.
See the *BSD or macOS systems man page chflags(2) for more information.
On Windows, the following file attribute constants are available for use when testing bits in the st_file_attributes
member returned by os.stat().
See the Windows API documentation for more detail on the meaning of these con-
stants.
FILE_ATTRIBUTE_ARCHIVE
stat.
FILE_ATTRIBUTE_COMPRESSED
stat.
FILE_ATTRIBUTE_DEVICE
stat.
FILE_ATTRIBUTE_DIRECTORY
stat.
FILE_ATTRIBUTE_ENCRYPTED
stat.
FILE_ATTRIBUTE_HIDDEN
stat.
FILE_ATTRIBUTE_INTEGRITY_STREAM
stat.
FILE_ATTRIBUTE_NORMAL
stat.
FILE_ATTRIBUTE_NOT_CONTENT_INDEXED
stat.
FILE_ATTRIBUTE_NO_SCRUB_DATA
stat.
FILE_ATTRIBUTE_OFFLINE
stat.
FILE_ATTRIBUTE_READONLY
stat.
FILE_ATTRIBUTE_REPARSE_POINT
stat.
FILE_ATTRIBUTE_SPARSE_FILE
stat.
FILE_ATTRIBUTE_SYSTEM
stat.
FILE_ATTRIBUTE_TEMPORARY
stat.
FILE_ATTRIBUTE_VIRTUAL
New in version 3.5.
On Windows, the following constants are available for comparing against the st_reparse_tag member returned by
os.lstat().
These are well-known constants, but are not an exhaustive list.
stat.
IO_REPARSE_TAG_SYMLINK
stat.
IO_REPARSE_TAG_MOUNT_POINT
stat.
IO_REPARSE_TAG_APPEXECLINK
New in version 3.8.
11.5 filecmp — File and Directory Comparisons
Source code: Lib/filecmp.py
The filecmp module defines functions to compare files and directories, with various optional time/correctness trade-
offs.
For comparing files, see also the difflib module.
The filecmp module defines the following functions:
filecmp.cmp(f1, f2, shallow=True)
Compare the files named f1 and f2, returning True if they seem equal, False otherwise.
If shallow is true and the os.stat() signatures (file type, size, and modification time) of both files are identical,
the files are taken to be equal.
Otherwise, the files are treated as different if their sizes or contents differ.
Note that no external programs are called from this function, giving it portability and efficiency.
This function uses a cache for past comparisons and the results, with cache entries invalidated if the os.stat()
information for the file changes.
The entire cache may be cleared using clear_cache().
Returns three lists of file names: match, mismatch, errors.
Files
are listed in errors if they don’t exist in one of the directories, the user lacks permission to read them or if the
comparison could not be done for some other reason.
The shallow parameter has the same meaning and default value as for filecmp.cmp().
For example, cmpfiles('a', 'b', ['c', 'd/e']) will compare a/c with b/c and a/d/e with b/
d/e. 'c' and 'd/e' will each be in one of the three returned lists.
This may be useful if a file is compared so quickly after it is modified that it is within the
mtime resolution of the underlying filesystem.
New in version 3.4.
The dircmp class
class filecmp.dircmp(a, b, ignore=None, hide=None)
Construct a new directory comparison object, to compare the directories a and b. ignore is a list of names to ignore,
and defaults to filecmp.
DEFAULT_IGNORES.
The dircmp class compares files by doing shallow comparisons as described for filecmp.cmp().
The dircmp class provides the following methods:
report()
Print (to sys.stdout) a comparison between a and b.
report_partial_closure()
Print a comparison between a and b and common immediate subdirectories.
Print a comparison between a and b and common subdirectories (recursively).
The dircmp class offers a number of interesting attributes that may be used to get various bits of information
about the directory trees being compared.
Previously entries were always dircmp instances.
Now entries are the same type
as self, if self is a subclass of dircmp.
New in version 3.4.
List of directories ignored by dircmp by default.
Here is a simplified example of using the subdirs attribute to search recursively through two directories to show
common different files:

for name in dcmp.diff_files:

print("diff_file %s found in %s and %s" % (name, dcmp.left,

dcmp.right))

for sub_dcmp in dcmp.subdirs.values():

print_diff_files(sub_dcmp)

11.5.
Lib/tempfile.py
This module creates temporary files and directories.
It works on all supported platforms.
TemporaryFile,
NamedTemporaryFile, TemporaryDirectory, and SpooledTemporaryFile are high-level interfaces
which provide automatic cleanup and can be used as context managers.
All the user-callable functions and constructors take additional arguments which allow direct control over the location
and name of temporary files and directories.
Files names used by this module include a string of random characters
which allows those files to be securely created in shared temporary directories.
To maintain backward compatibility, the
argument order is somewhat odd; it is recommended to use keyword arguments for clarity.
The module defines the following user-callable items:
tempfile.
TemporaryFile(mode=’w+b’, buffering=- 1, encoding=None, newline=None, suffix=None,
prefix=None, dir=None, *, errors=None)
Return a file-like object that can be used as a temporary storage area.
The file is created securely, using the same
rules as mkstemp().
It will be destroyed as soon as it is closed (including an implicit close when the object is
garbage collected).
Under Unix, the directory entry for the file is either not created at all or is removed immediately
after the file is created.
Other platforms do not support this; your code should not rely on a temporary file created
using this function having or not having a visible name in the file system.
The resulting object can be used as a context manager (see Examples).
On completion of the context or destruction
of the file object the temporary file will be removed from the filesystem.
The mode parameter defaults to 'w+b' so that the file created can be read and written without being closed.
Binary
mode is used so that it behaves consistently on all platforms without regard for the data that is stored.
The dir, prefix and suffix parameters have the same meaning and defaults as with mkstemp().
The returned object is a true file object on POSIX platforms.
On other platforms, it is a file-like object whose file
attribute is the underlying true file object.
The os.O_TMPFILE flag is used if it is available and works (Linux-specific, requires Linux kernel 3.11 or later).
On platforms that are neither Posix nor Cygwin, TemporaryFile is an alias for NamedTemporaryFile.
Raises an auditing event tempfile.mkstemp with argument fullpath.
The os.O_TMPFILE flag is now used if available.
Changed in version 3.8: Added errors parameter.
This function returns a file that is guaranteed to have a visible name in the file system.
The returned object is always a file-like object whose file attribute is the underlying true file object.
This file-like
object can be used in a with statement, just like a normal file.
The name of the temporary file can be retrieved
from the name attribute of the returned file-like object.
On Unix, unlike with the TemporaryFile(), the
directory entry does not get unlinked immediately after the file creation.
If delete is true (the default) and delete_on_close is true (the default), the file is deleted as soon as it is closed.
If
delete is true and delete_on_close is false, the file is deleted on context manager exit only, or else when the file-like
object is finalized.
Deletion is not always guaranteed in this case (see object.__del__()).
If delete is false,
the value of delete_on_close is ignored.
Therefore to use the name of the temporary file to reopen the file after closing it, either make sure not to delete the
file upon closure (set the delete parameter to be false) or, in case the temporary file is created in a with statement,
set the delete_on_close parameter to be false.
The latter approach is recommended as it provides assistance in
automatic cleaning of the temporary file upon the context manager exit.
On POSIX the file can always be opened again.
Note, that in this case the additional opens that do not share
delete access (e.g. created via builtin open()) must be closed before exiting the context manager, else
the os.unlink() call on context manager exit will fail with a PermissionError.
On Windows, if delete_on_close is false, and the file is created in a directory for which the user lacks delete access,
then the os.unlink() call on exit of the context manager will fail with a PermissionError.
This cannot
happen when delete_on_close is true because delete access is requested by the open, which fails immediately if the
requested access is not granted.
On POSIX (only), a process that is terminated abruptly with SIGKILL cannot automatically delete any NamedTem-
poraryFiles it created.
Raises an auditing event tempfile.mkstemp with argument fullpath.
Changed in version 3.8: Added errors parameter.
Changed in version 3.12: Added delete_on_close parameter.
This class operates exactly as TemporaryFile() does, except that data is spooled in memory until the file size
exceeds max_size, or until the file’s fileno() method is called, at which point the contents are written to disk
and operation proceeds as with TemporaryFile().
The resulting file has one additional method, rollover(), which causes the file to roll over to an on-disk file
regardless of its size.
The returned object is a file-like object whose _file attribute is either an io.BytesIO or io.
TextIOWrapper object (depending on whether binary or text mode was specified) or a true file object, de-
pending on whether rollover() has been called.
This file-like object can be used in a with statement, just
like a normal file.
Changed in version 3.3: the truncate method now accepts a size argument.
Changed in version 3.8: Added errors parameter.
Changed in version 3.11: Fully implements the io.BufferedIOBase and io.TextIOBase abstract base
classes (depending on whether binary or text mode was specified).
TemporaryDirectory(suffix=None, prefix=None, dir=None, ignore_cleanup_errors=False,
*, delete=True)
11.6.
This class securely creates a temporary directory using the same rules as mkdtemp().
The resulting object can be
used as a context manager (see Examples).
On completion of the context or destruction of the temporary directory
object, the newly created temporary directory and all its contents are removed from the filesystem.
The directory name can be retrieved from the name attribute of the returned object.
When the returned object is
used as a context manager, the name will be assigned to the target of the as clause in the with statement, if there
is one.
The directory can be explicitly cleaned up by calling the cleanup() method.
If ignore_cleanup_errors is true,
any unhandled exceptions during explicit or implicit cleanup (such as a PermissionError removing open files
on Windows) will be ignored, and the remaining removable items deleted on a “best-effort” basis.
Otherwise, errors
will be raised in whatever context cleanup occurs (the cleanup() call, exiting the context manager, when the
object is garbage-collected or during interpreter shutdown).
The delete parameter can be used to disable cleanup of the directory tree upon exiting the context.
While it may
seem unusual for a context manager to disable the action taken when exiting the context, it can be useful during
debugging or when you need your cleanup behavior to be conditional based on other logic.
Raises an auditing event tempfile.mkdtemp with argument fullpath.
New in version 3.2.
Changed in version 3.10: Added ignore_cleanup_errors parameter.
Changed in version 3.12: Added the delete parameter.
There are no race conditions in the file’s creation,
assuming that the platform properly implements the os.O_EXCL flag for os.open().
The file is readable and
writable only by the creating user ID.
If the platform uses permission bits to indicate whether a file is executable,
the file is executable by no one.
The file descriptor is not inherited by child processes.
Unlike TemporaryFile(), the user of mkstemp() is responsible for deleting the temporary file when done
with it.
If suffix is not None, the file name will end with that suffix, otherwise there will be no suffix.
If prefix is not None, the file name will begin with that prefix; otherwise, a default prefix is used.
If dir is not None, the file will be created in that directory; otherwise, a default directory is used.
The default
directory is chosen from a platform-dependent list, but the user of the application can control the directory location
by setting the TMPDIR, TEMP or TMP environment variables.
There is thus no guarantee that the generated filename
will have any nice properties, such as not requiring quoting when passed to external commands via os.popen().
If any of suffix, prefix, and dir are not None, they must be the same type.
If they are bytes, the returned name will be
bytes instead of str.
If you want to force a bytes return value with otherwise default behavior, pass suffix=b''.
If text is specified and true, the file is opened in text mode.
Raises an auditing event tempfile.mkstemp with argument fullpath.
Changed in version 3.5: suffix, prefix, and dir may now be supplied in bytes in order to obtain a bytes return value.
Prior to this, only str was allowed.
The dir parameter now accepts a path-like object.
There are no race conditions in the directory’s
creation.
The directory is readable, writable, and searchable only by the creating user ID.
The user of mkdtemp() is responsible for deleting the temporary directory and its contents when done with it.
The prefix, suffix, and dir arguments are the same as for mkstemp().
Raises an auditing event tempfile.mkdtemp with argument fullpath.
Changed in version 3.5: suffix, prefix, and dir may now be supplied in bytes in order to obtain a bytes return value.
Prior to this, only str was allowed.
The dir parameter now accepts a path-like object.
Changed in version 3.12: mkdtemp() now always returns an absolute path, even if dir is relative.
This defines the default value for the dir argument to all
functions in this module.
Python searches a standard list of directories to find one which the calling user can create files in.
The list is:
1.
The directory named by the TMPDIR environment variable.
The directory named by the TEMP environment variable.
The directory named by the TMP environment variable.
A platform-specific location:
• On Windows, the directories C:\TEMP, C:\TMP, \TEMP, and \TMP, in that order.
On all other platforms, the directories /tmp, /var/tmp, and /usr/tmp, in that order.
As a last resort, the current working directory.
The result of this search is cached, see the description of tempdir below.
Changed in version 3.10: Always returns a str.
Same as gettempdir() but the return value is in bytes.
New in version 3.5.
tempfile.gettempprefix()
Return the filename prefix used to create temporary files.
This does not contain the directory component.
New in version 3.5.
The module uses a global variable to store the name of the directory used for temporary files returned by
gettempdir().
It can be set directly to override the selection process, but this is discouraged.
All functions in
this module take a dir argument which can be used to specify the directory.
This is the recommended approach that does
not surprise other unsuspecting code by changing global API behavior.
It cannot be a path-like object.
If tempdir is None (the default) at any call to any of the above functions except gettempprefix() it is
initialized following the algorithm described in gettempdir().
Note: Beware that if you set tempdir to a bytes value, there is a nasty side effect: The global default return type
of mkstemp() and mkdtemp() changes to bytes when no explicit prefix, suffix, or dir arguments of
type str are supplied.
Please do not write code expecting or depending on this.
This awkward behavior is maintained
for compatibility with the historical implementation.
Unfortunately this is not secure, because a different process may create a file with this name in the
time between the call to mktemp() and the subsequent attempt to create the file by the first process.
The solution is to
combine the two steps and create the file immediately.
This approach is used by mkstemp() and the other functions
described above.
Return an absolute pathname of a file that did not exist at the time the call is made.
The prefix, suffix, and dir
arguments are similar to those of mkstemp(), except that bytes file names, suffix=None and prefix=None
are not supported.
Warning:
Use of this function may introduce a security hole in your program.
By the time you get around
to doing anything with the file name it returns, someone else may have beaten you to the punch.
Lib/glob.py
The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell,
although results are returned in arbitrary order.
No tilde expansion is done, but *, ?, and character ranges expressed with
[] will be correctly matched.
This is done by using the os.scandir() and fnmatch.fnmatch() functions in
concert, and not by actually invoking a subshell.
Note that files beginning with a dot (.) can only be matched by patterns that also start with a dot, unlike fnmatch.
For a literal match, wrap the meta-characters in brackets.
For example, '[?]' matches the character '?'.
See also:
The pathlib module offers high-level path objects.
glob.glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)
Return a possibly empty list of path names that match pathname, which must be a string containing a path specifi-
cation.
Broken symlinks are included in the results (as in the
shell).
Whether or not the results are sorted depends on the file system.
If a file that satisfies conditions is removed
or added during the call of this function, whether a path name for that file will be included is unspecified.
11.7.
If root_dir is not None, it should be a path-like object specifying the root directory for searching.
It has the same
effect on glob() as changing the current directory before calling it.
If pathname is relative, the result will contain
paths relative to root_dir.
This function can support paths relative to directory descriptors with the dir_fd parameter.
If recursive is true, the pattern “**” will match any files and zero or more directories, subdirectories and symbolic
links to directories.
If the pattern is followed by an os.sep or os.altsep then files will not match.
If include_hidden is true, “**” pattern will match hidden directories.
Raises an auditing event glob.glob with arguments pathname, recursive.
Raises an auditing event glob.glob/2 with arguments pathname, recursive, root_dir, dir_fd.
Note: Using the “**” pattern in large directory trees may consume an inordinate amount of time.
Changed in version 3.5: Support for recursive globs using “**”.
Changed in version 3.10: Added the root_dir and dir_fd parameters.
Changed in version 3.11: Added the include_hidden parameter.
glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)
Return an iterator which yields the same values as glob() without actually storing them all simultaneously.
Raises an auditing event glob.glob with arguments pathname, recursive.
Raises an auditing event glob.glob/2 with arguments pathname, recursive, root_dir, dir_fd.
Changed in version 3.5: Support for recursive globs using “**”.
Changed in version 3.10: Added the root_dir and dir_fd parameters.
Changed in version 3.11: Added the include_hidden parameter.
glob.escape(pathname)
Escape all special characters ('?', '*' and '[').
This is useful if you want to match an arbitrary literal string that
may have special characters in it.
Special characters in drive/UNC sharepoints are not escaped, e.g. on Windows
escape('//?/c:/Quo vadis?.txt') returns '//?/c:/Quo vadis[?].txt'.
New in version 3.4.
For example, consider a directory containing the following files: 1.gif, 2.txt, card.gif and a subdirectory sub
which contains only the file 3.txt.
Notice how any leading components of
the path are preserved.
For example, '[?]' matches the character '?'.
Note that the filename separator ('/' on Unix) is not special to this module.
See module glob for pathname expansion
(glob uses filter() to match pathname segments).
Similarly, filenames starting with a period are not special for this
module, and are matched by the * and ? patterns.
Also note that functools.lru_cache() with the maxsize of 32768 is used to cache the compiled regex patterns
in the following functions: fnmatch(), fnmatchcase(), filter().
Both parameters are
case-normalized using os.path.normcase().
This example will print all file names in the current directory with the extension .txt:
import fnmatch
import os
for file in os.listdir('.'):
if fnmatch.fnmatch(file, '*.txt'):
print(file)
fnmatch.fnmatchcase(filename, pattern)
Test whether filename matches pattern, returning True or False; the comparison is case-sensitive and does not
apply os.path.normcase().
fnmatch.filter(names, pattern)
Construct a list from those elements of the iterable names that match pattern.
It is the same as [n for n in
names if fnmatch(n, pattern)], but implemented more efficiently.
Example:
'(?s:.*\\.txt)\\Z'
<re.
Match object; span=(0, 10), match='foobar.txt'>
See also:
Module glob Unix shell-style path expansion.
This is used by the traceback module
to retrieve source lines for inclusion in the formatted traceback.
The tokenize.open() function is used to open files.
This function uses tokenize.detect_encoding() to
get the encoding of the file; in the absence of an encoding token, the file encoding defaults to UTF-8.
The linecache module defines the following functions:
linecache.getline(filename, lineno, module_globals=None)
Get line lineno from file named filename.
This function will never raise an exception — it will return '' on errors
(the terminating newline character will be included for lines that are found).
If a file named filename is not found, the function first checks for a PEP 302 __loader__ in module_globals.
If there is such a loader and it defines a get_source method, then that determines the source lines (if
get_source() returns None, then '' is returned).
Finally, if filename is a relative filename, it is looked
up relative to the entries in the module search path, sys.path.
Use this function if you no longer need lines from files previously read using getline().
Use this function if files in the cache may have changed on disk, and you require the
updated version.
If filename is omitted, it will check all the entries in the cache.
linecache.lazycache(filename, module_globals)
Capture enough detail about a non-file-based module to permit getting its lines later via getline() even if
module_globals is None in the later call.
This avoids doing I/O until a line is actually needed, without having to
carry the module globals around indefinitely.
New in version 3.5.
In particular, functions are
provided which support file copying and removal.
For operations on individual files, see also the os module.
Warning: Even the higher-level file copying functions (shutil.copy(), shutil.copy2()) cannot copy all
file metadata.
On POSIX platforms, this means that file owner and group are lost as well as ACLs.
On Mac OS, the resource fork
and other metadata are not used.
This means that resources will be lost and file type and creator codes will not be
correct.
On Windows, file owners, ACLs and alternate data streams are not copied.
The integer length, if given, is the buffer
size.
In particular, a negative length value means to copy the data without looping over the source data in chunks; by
default the data is read in chunks to avoid uncontrolled memory consumption.
Note that if the current file position
of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied.
If src
and dst specify the same file, SameFileError is raised.
The destination location must be writable; otherwise, an OSError exception will be raised.
If dst already exists,
it will be replaced.
Special files such as character or block devices and pipes cannot be copied with this function.
If follow_symlinks is false and src is a symbolic link, a new symbolic link will be created instead of copying the file
src points to.
Raises an auditing event shutil.copyfile with arguments src, dst.
Changed in version 3.3: IOError used to be raised instead of OSError.
Added follow_symlinks argument.
Now returns dst.
Changed in version 3.4: Raise SameFileError instead of Error.
Since the former is a subclass of the latter,
this change is backward compatible.
Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more
efficiently.
See Platform-dependent efficient copy operations section.
This exception is raised if source and destination in copyfile() are the same file.
New in version 3.4.
shutil.copymode(src, dst, *, follow_symlinks=True)
Copy the permission bits from src to dst.
The file contents, owner, and group are unaffected.
If follow_symlinks is false, and both src and dst are symbolic links,
copymode() will attempt to modify the mode of dst itself (rather than the file it points to).
This functionality is
not available on every platform; please see copystat() for more information.
If copymode() cannot modify
symbolic links on the local platform, and it is asked to do so, it will do nothing and return.
Raises an auditing event shutil.copymode with arguments src, dst.
Changed in version 3.3: Added follow_symlinks argument.
On Linux,
copystat() also copies the “extended attributes” where possible.
The file contents, owner, and group are unaf-
fected.
If follow_symlinks is false, and src and dst both refer to symbolic links, copystat() will operate on the symbolic
links themselves rather than the files the symbolic links refer to—reading the information from the src symbolic
link, and writing the information to the dst symbolic link.
Note: Not all platforms provide the ability to examine and modify symbolic links.
Python itself can tell you what
functionality is locally available.
If os.chmod in os.supports_follow_symlinks is True, copystat() can modify the per-
mission bits of a symbolic link.
If os.utime in os.supports_follow_symlinks is True, copystat() can modify the last
access and modification times of a symbolic link.
If os.chflags in os.supports_follow_symlinks is True, copystat() can modify the
flags of a symbolic link.
On platforms where some or all of this functionality is unavailable, when asked to modify a symbolic link,
copystat() will copy everything it can.
Please see os.supports_follow_symlinks for more information.
Raises an auditing event shutil.copystat with arguments src, dst.
Changed in version 3.3: Added follow_symlinks argument and support for Linux extended attributes.
If dst specifies a
directory, the file will be copied into dst using the base filename from src.
If dst specifies a file that already exists,
it will be replaced.
Returns the path to the newly created file.
If follow_symlinks is false, and src is a symbolic link, dst will be created as a symbolic link.
If follow_symlinks is
true and src is a symbolic link, dst will be a copy of the file src refers to.
Other metadata, like the file’s
creation and modification times, is not preserved.
To preserve all file metadata from the original, use copy2()
instead.
Raises an auditing event shutil.copyfile with arguments src, dst.
Now returns path to the newly created file.
Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more
efficiently.
See Platform-dependent efficient copy operations section.
When follow_symlinks is false, and src is a symbolic link, copy2() attempts to copy all metadata from the src
symbolic link to the newly created dst symbolic link.
However, this functionality is not available on all platforms.
On platforms where some or all of this functionality is unavailable, copy2() will preserve all the metadata it can;
copy2() never raises an exception because it cannot preserve file metadata.
Please see copystat() for more information about
platform support for modifying symbolic link metadata.
Raises an auditing event shutil.copyfile with arguments src, dst.
Raises an auditing event shutil.copystat with arguments src, dst.
Changed in version 3.3: Added follow_symlinks argument, try to copy extended file system attributes too (currently
Linux only).
Now returns path to the newly created file.
Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more
efficiently.
This factory function creates a function that can be used as a callable for copytree()’s ignore argument, ignoring
files and directories that match one of the glob-style patterns provided.
Recursively copy an entire directory tree rooted at src to a directory named dst and return the destination directory.
All intermediate directories needed to contain dst will also be created by default.
Permissions and times of directories are copied with copystat(), individual files are copied using copy2().
If symlinks is true, symbolic links in the source tree are represented as symbolic links in the new tree and the
metadata of the original links will be copied as far as the platform allows; if false or omitted, the contents and
metadata of the linked files are copied to the new tree.
When symlinks is false, if the file pointed by the symlink doesn’t exist, an exception will be added in the list of errors
raised in an Error exception at the end of the copy process.
You can set the optional ignore_dangling_symlinks
flag to true if you want to silence this exception.
Notice that this option has no effect on platforms that don’t support
os.symlink().
If ignore is given, it must be a callable that will receive as its arguments the directory being visited by copytree(),
and a list of its contents, as returned by os.listdir().
Since copytree() is called recursively, the ignore
callable will be called once for each directory that is copied.
The callable must return a sequence of directory and
file names relative to the current directory (i.e. a subset of the items in its second argument); these names will then
be ignored in the copy process.
If exception(s) occur, an Error is raised with a list of reasons.
If copy_function is given, it must be a callable that will be used to copy each file.
It will be called with the source
path and the destination path as arguments.
By default, copy2() is used, but any function that supports the same
signature (like copy()) can be used.
If dirs_exist_ok is false (the default) and dst already exists, a FileExistsError is raised.
If dirs_exist_ok is
true, the copying operation will continue if it encounters existing directories, and files within the dst tree will be
overwritten by corresponding files from the src tree.
Changed in version 3.3: Copy metadata when symlinks is false.
Now returns dst.
Changed in version 3.2: Added the copy_function argument to be able to provide a custom copy function.
Added
the ignore_dangling_symlinks argument to silence dangling symlinks errors when symlinks is false.
Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more
efficiently.
See Platform-dependent efficient copy operations section.
The dirs_exist_ok parameter.
shutil.rmtree(path, ignore_errors=False, onerror=None, *, onexc=None, dir_fd=None)
Delete an entire directory tree; path must point to a directory (but not a symbolic link to a directory).
If ignore_errors
is true, errors resulting from failed removals will be ignored; if false or omitted, such errors are handled by calling
a handler specified by onexc or onerror or, if both are omitted, exceptions are propagated to the caller.
This function can support paths relative to directory descriptors.
Note: On platforms that support the necessary fd-based functions a symlink attack resistant version of rmtree()
is used by default.
On other platforms, the rmtree() implementation is susceptible to a symlink attack: given
proper timing and circumstances, attackers can manipulate symlinks on the filesystem to delete files they wouldn’t be
able to access otherwise.
Applications can use the rmtree.avoids_symlink_attacks function attribute
to determine which case applies.
If onexc is provided, it must be a callable that accepts three parameters: function, path, and excinfo.
The first parameter, function, is the function which raised the exception; it depends on the platform and implemen-
tation.
The second parameter, path, will be the path name passed to function.
The third parameter, excinfo, is the
exception that was raised.
Exceptions raised by onexc will not be caught.
The deprecated onerror is similar to onexc, except that the third parameter it receives is the tuple returned from
sys.exc_info().
Raises an auditing event shutil.rmtree with arguments path, dir_fd.
Changed in version 3.3: Added a symlink attack resistant version that is used automatically if platform supports
fd-based functions.
Changed in version 3.8: On Windows, will no longer delete the contents of a directory junction before removing
the junction.
The dir_fd parameter.
Changed in version 3.12: Added the onexc parameter, deprecated onerror.
Currently this is only true for platforms supporting fd-based directory access functions.
New in version 3.3.
shutil.move(src, dst, copy_function=copy2)
Recursively move a file or directory (src) to another location (dst) and return the destination.
If the destination is an existing directory, then src is moved inside that directory.
If the destination already exists
but is not a directory, it may be overwritten depending on os.rename() semantics.
If the destination is on the current filesystem, then os.rename() is used.
Otherwise, src is copied to dst using
copy_function and then removed.
If copy_function is given, it must be a callable that takes two arguments src and dst, and will be used to copy src
to dst if os.rename() cannot be used.
If the source is a directory, copytree() is called, passing it the
copy_function.
The default copy_function is copy2().
Using copy() as the copy_function allows the move to
succeed when it is not possible to also copy the metadata, at the expense of not copying any of the metadata.
Raises an auditing event shutil.move with arguments src, dst.
Added explicit symlink handling for foreign filesystems, thus adapting it to the behavior
of GNU’s mv.
Now returns dst.
Added the copy_function keyword argument.
Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more
efficiently.
See Platform-dependent efficient copy operations section.
Accepts a path-like object for both src and dst.
shutil.disk_usage(path)
Return disk usage statistics about the given path as a named tuple with the attributes total, used and free, which are
the amount of total, used and free space, in bytes.
Note: On Unix filesystems, path must point to a path within a mounted filesystem partition.
On those platforms,
CPython doesn’t attempt to retrieve disk usage information from non-mounted filesystems.
New in version 3.3.
Changed in version 3.8: On Windows, path can now be a file or directory.
Availability: Unix, Windows.
shutil.chown(path, user=None, group=None)
Change owner user and/or group of the given path.
At least one argument is required.
See also os.chown(), the underlying function.
Raises an auditing event shutil.chown with arguments path, user, group.
Availability: Unix.
New in version 3.3.
If no cmd would be called, return
None.
mode is a permission mask passed to os.access(), by default determining if the file exists and executable.
When no path is specified, the results of os.environ() are used, returning either the “PATH” value or a fallback
of os.defpath.
On Windows, the current directory is prepended to the path if mode does not include os.X_OK.
When the mode
does include os.X_OK, the Windows API NeedCurrentDirectoryForExePathW will be consulted to
determine if the current directory should be prepended to path.
To avoid consulting the current working directory
for executables: set the environment variable NoDefaultCurrentDirectoryInExePath.
Also on Windows, the PATHEXT variable is used to resolve commands that may not already include an extension.
For example, if you call shutil.which("python"), which() will search PATHEXT to know that it should
look for python.exe within the path directories.
For example, on Windows:
11.10.
This is also applied when cmd is a path that contains a directory component:
>> shutil.which("C:\\Python33\\python")
'C:\\Python33\\python.
EXE'
New in version 3.3.
The bytes type is now accepted.
If cmd type is bytes, the result type is also bytes.
Changed in version 3.12: On Windows, the current directory is no longer prepended to the search path if mode
includes os.X_OK and WinAPI NeedCurrentDirectoryForExePathW(cmd) is false, else the current
directory is prepended even if it is already in the search path; PATHEXT is used now even when cmd includes a
directory component or ends with an extension that is in PATHEXT; and filenames that have no extension can now
be found.
Changed in version 3.12.1: On Windows, if mode includes os.X_OK, executables with an extension in PATHEXT
will be preferred over executables without a matching extension.
This brings behavior closer to that of Python 3.11.
exception shutil.
Error
This exception collects exceptions that are raised during a multi-file operation.
For copytree(), the exception
argument is a list of 3-tuples (srcname, dstname, exception).
Platform-dependent efficient copy operations
Starting from Python 3.8, all functions involving a file copy (copyfile(), copy(), copy2(), copytree(), and
move()) may use platform-specific “fast-copy” syscalls in order to copy the file more efficiently (see bpo-33671).
On macOS fcopyfile is used to copy the file content (not metadata).
On Linux os.sendfile() is used.
On Windows shutil.copyfile() uses a bigger default buffer size (1 MiB instead of 64 KiB) and a
memoryview()-based variant of shutil.copyfileobj() is used.
If the fast-copy operation fails and no data was written in the destination file then shutil will silently fallback on using less
efficient copyfileobj() function internally.
Changed in version 3.8.
This will copy everything except .pyc files and files or directories whose name starts with tmp.
It
uses the onexc callback to clear the readonly bit and reattempt the remove.
Any subsequent failure will propagate.
Changed in version 3.5: Added support for the xztar format.
High-level utilities to create and read compressed and archived files are also provided.
They rely on the zipfile and
tarfile modules.
See Archiving example with base_dir for how
to use base_dir and root_dir together.
If dry_run is true, no archive is created, but the operations that would be executed are logged to logger.
By default, uses the current owner and group.
Logger.
The verbose argument is unused and deprecated.
Note:
This
function
is
not
thread-safe
when
custom
archivers
registered
with
register_archive_format() do not support the root_dir argument.
In this case it temporarily
changes the current working directory of the process to root_dir to perform archiving.
The modern pax (POSIX.1-2001) format is now used instead of the legacy GNU format
for archives created with format="tar".
Each element of the returned sequence is a tuple (name,
description).
By default shutil provides these formats:
• zip: ZIP file (if the zlib module is available).
Uses POSIX.1-2001 pax format for new archives.
• gztar: gzip’ed tar-file (if the zlib module is available).
You can register new formats or provide your own archiver for any existing formats,
by using
register_archive_format().
The callable will receive the base_name of the file to
create, followed by the base_dir (which defaults to os.curdir) to start archiving from.
Further arguments are
passed as keyword arguments: owner, group, dry_run and logger (as passed in make_archive()).
If function has the custom attribute function.supports_root_dir set to True, the root_dir argument is
passed as a keyword argument.
Otherwise the current working directory of the process is temporarily changed to
root_dir before calling function.
In this case make_archive() is not thread-safe.
If given, extra_args is a sequence of (name, value) pairs that will be used as extra keywords arguments when
the archiver callable is used.
Defaults to an empty
string.
Changed in version 3.12: Added support for functions supporting the root_dir argument.
Or any other format registered
with register_unpack_format().
If not provided, unpack_archive() will use the archive file name
extension and see if an unpacker was registered for that extension.
In case none is found, a ValueError is raised.
The keyword-only filter argument is passed to the underlying unpacking function.
For zip files, filter is not accepted.
For tar files, it is recommended to set it to 'data', unless using features specific to tar and UNIX-like filesystems.
The 'data' filter will become the default for tar files in Python 3.14.
Raises an auditing event shutil.unpack_archive with arguments filename, extract_dir, format.
Warning: Never extract archives from untrusted sources without prior inspection.
It is possible that files are
created outside of the path specified in the extract_dir argument, e.g. members that have absolute filenames
starting with “/” or filenames with two dots “..”.
Accepts a path-like object for filename and extract_dir.
Changed in version 3.12: Added the filter argument.
The callable will receive:
• the path of the archive, as a positional argument;
• the directory the archive must be extracted to, as a positional argument;
• possibly a filter keyword argument, if it was given to unpack_archive();
• additional keyword arguments, specified by extra_args as a sequence of (name, value) tuples.
Unregister an unpack format.
Each element of the returned sequence is a tuple (name,
extensions, description).
By default shutil provides these formats:
• zip: ZIP file (unpacking compressed files works only if the corresponding module is available).
You can register new formats or provide your own unpacker for any existing formats,
by using
register_unpack_format().
Archiving example with base_dir
In this example, similar to the one above, we show how to use make_archive(), but this time with the usage of
base_dir.
We now have the following directory structure:
$ tree tmp
tmp
└── root
└── structure
├── content
└── please_add.txt
└── do_not_add.txt
In the final archive, please_add.txt should be included, but do_not_add.txt should not.
Get the size of the terminal window.
For each of the two dimensions, the environment variable, COLUMNS and LINES respectively, is checked.
If the
variable is defined and the value is a positive integer, it is used.
When COLUMNS or LINES is not defined, which is the common case, the terminal connected to sys.
__stdout__ is queried by invoking os.get_terminal_size().
If the terminal size cannot be successfully queried, either because the system doesn’t support querying, or because
we are not connected to a terminal, the value given in fallback parameter is used.
The value returned is a named tuple of type os.terminal_size.
See also: The Single UNIX Specification, Version 2, Other Environment Variables.
New in version 3.3.
Changed in version 3.11: The fallback values are also used if os.get_terminal_size() returns zeroes.
See also:
Module os Operating system interfaces, including functions to work with files at a lower level than Python file objects.
Module io Python’s built-in I/O library, including both abstract classes and some concrete classes such as file I/O.
Built-in function open() The standard way to open files for reading and writing with Python.
File and Directory Access
CHAPTER
TWELVE
DATA PERSISTENCE
The modules described in this chapter support storing Python data in a persistent form on disk.
The pickle and
marshal modules can turn many Python data types into a stream of bytes and then recreate the objects from the bytes.
The various DBM-related modules support a family of hash-based file formats that store a mapping of strings to other
strings.
The list of modules described in this chapter is:
12.1 pickle — Python object serialization
Source code: Lib/pickle.py
The pickle module implements binary protocols for serializing and de-serializing a Python object structure.
Pickling (and
unpickling) is alternatively known as “serialization”, “marshalling,”1 or “flattening”; however, to avoid confusion, the
terms used here are “pickling” and “unpickling”.
Warning: The pickle module is not secure.
Only unpickle data you trust.
It is possible to construct malicious pickle data which will execute arbitrary code during unpickling.
Never unpickle
data that could have come from an untrusted source, or that could have been tampered with.
Consider signing data with hmac if you need to ensure that it has not been tampered with.
Safer serialization formats such as json may be more appropriate if you are processing untrusted data.
See Com-
parison with json.
Relationship to other Python modules
Comparison with marshal
Python has a more primitive serialization module called marshal, but in general pickle should always be the preferred
way to serialize Python objects.
This has implications both for recursive objects and object sharing.
Recursive objects are objects that contain
references to themselves.
These are not handled by marshal, and in fact, attempting to marshal recursive objects
will crash your Python interpreter.
Object sharing happens when there are multiple references to the same object in
different places in the object hierarchy being serialized.
Shared objects remain shared, which can be very important for mutable
objects.
The marshal serialization format is not guaranteed to be portable across Python versions.
Because its primary
job in life is to support .pyc files, the Python implementers reserve the right to change the serialization format
in non-backwards compatible ways should the need arise.
The pickle serialization format is guaranteed to be
backwards compatible across Python releases provided a compatible pickle protocol is chosen and pickling and
unpickling code deals with Python 2 to Python 3 type differences if your data is crossing that unique breaking
change language boundary.
Comparison with json
There are fundamental differences between the pickle protocols and JSON (JavaScript Object Notation):
• JSON is a text serialization format (it outputs unicode text, although most of the time it is then encoded to utf-8),
while pickle is a binary serialization format;
• JSON is human-readable, while pickle is not;
• JSON is interoperable and widely used outside of the Python ecosystem, while pickle is Python-specific;
• JSON, by default, can only represent a subset of the Python built-in types, and no custom classes; pickle can
represent an extremely large number of Python types (many of them automatically, by clever usage of Python’s
introspection facilities; complex cases can be tackled by implementing specific object APIs);
• Unlike pickle, deserializing untrusted JSON does not in itself create an arbitrary code execution vulnerability.
See also:
The json module: a standard library module allowing JSON serialization and deserialization.
This has the advantage that there are no restrictions imposed by
external standards such as JSON or XDR (which can’t represent pointer sharing); however it means that non-Python
programs may not be able to reconstruct pickled Python objects.
By default, the pickle data format uses a relatively compact binary representation.
If you need optimal size character-
istics, you can efficiently compress pickled data.
The module pickletools contains tools for analyzing data streams generated by pickle.
There are currently 6 different protocols which can be used for pickling.
The higher the protocol used, the more recent
the version of Python needed to read the pickle produced.
Protocol version 0 is the original “human-readable” protocol and is backwards compatible with earlier versions of
Python.
Protocol version 2 was introduced in Python 2.3.
It provides much more efficient pickling of new-style classes.
Protocol version 3 was added in Python 3.0.
It has explicit support for bytes objects and cannot be unpickled by
Python 2.x.
Protocol version 4 was added in Python 3.4.
It adds support for very large objects, pickling more kinds of objects,
and some data format optimizations.
It is the default protocol starting with Python 3.8.
Protocol version 5 was added in Python 3.8.
It adds support for out-of-band data and speedup for in-band data.
Refer to PEP 574 for information about improvements brought by protocol 5.
Note: Serialization is a more primitive notion than persistence; although pickle reads and writes file objects, it does
not handle the issue of naming persistent objects, nor the (even more complicated) issue of concurrent access to persistent
objects.
The pickle module can transform a complex object into a byte stream and it can transform the byte stream into
an object with the same internal structure.
Perhaps the most obvious thing to do with these byte streams is to write them
onto a file, but it is also conceivable to send them across a network or store them in a database.
The shelve module
provides a simple interface to pickle and unpickle objects on DBM-style database files.
Similarly, to de-serialize a data stream, you
call the loads() function.
However, if you want more control over serialization and de-serialization, you can create a
Pickler or an Unpickler object, respectively.
The pickle module provides the following constants:
pickle.
An integer, the highest protocol version available.
This value can be passed as a protocol value to functions dump()
and dumps() as well as the Pickler constructor.
An integer, the default protocol version used for pickling.
May be less than HIGHEST_PROTOCOL.
Currently the
default protocol is 4, first introduced in Python 3.4 and incompatible with previous versions.
The default protocol is 4.
The pickle module provides the following functions to make the pickling process more convenient:
pickle.dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None)
Write the pickled representation of the object obj to the open file object file.
This is equivalent to Pickler(file,
protocol).dump(obj).
Arguments file, protocol, fix_imports and buffer_callback have the same meaning as in the Pickler constructor.
Changed in version 3.8: The buffer_callback argument was added.
Arguments protocol, fix_imports and buffer_callback have the same meaning as in the Pickler constructor.
Changed in version 3.8: The buffer_callback argument was added.
This is equivalent to Unpickler(file).load().
The protocol version of the pickle is detected automatically, so no protocol argument is needed.
Bytes past the
pickled representation of the object are ignored.
Arguments file, fix_imports, encoding, errors, strict and buffers have the same meaning as in the Unpickler
constructor.
The buffers argument was added.
The protocol version of the pickle is detected automatically, so no protocol argument is needed.
Bytes past the
pickled representation of the object are ignored.
Arguments fix_imports, encoding, errors, strict and buffers have the same meaning as in the Unpickler construc-
tor.
The buffers argument was added.
The pickle module defines three exceptions:
exception pickle.
PickleError
Common base class for the other pickling exceptions.
It inherits Exception.
PicklingError
Error raised when an unpicklable object is encountered by Pickler.
It inherits PickleError.
Refer to What can be pickled and unpickled?
UnpicklingError
Error raised when there is a problem unpickling an object, such as a data corruption or a security violation.
It
inherits PickleError.
Note that other exceptions may also be raised during unpickling, including (but not necessarily limited to) Attribu-
teError, EOFError, ImportError, and IndexError.
The pickle module exports three classes, Pickler, Unpickler and PickleBuffer:
class pickle.
This takes a binary file for writing a pickle data stream.
The optional protocol argument, an integer, tells the pickler to use the given protocol; supported protocols are 0 to
HIGHEST_PROTOCOL.
If not specified, the default is DEFAULT_PROTOCOL.
If a negative number is specified,
HIGHEST_PROTOCOL is selected.
The file argument must have a write() method that accepts a single bytes argument.
It can thus be an on-disk file
opened for binary writing, an io.BytesIO instance, or any other custom object that meets this interface.
If fix_imports is true and protocol is less than 3, pickle will try to map the new Python 3 names to the old module
names used in Python 2, so that the pickle data stream is readable with Python 2.
If buffer_callback is None (the default), buffer views are serialized into file as part of the pickle stream.
If buffer_callback is not None, then it can be called any number of times with a buffer view.
If the callback returns
a false value (such as None), the given buffer is out-of-band; otherwise the buffer is serialized in-band, i.e. inside
the pickle stream.
Do nothing by default.
This exists so a subclass can override it.
If persistent_id() returns None, obj is pickled as usual.
Any other value causes Pickler to
emit the returned value as a persistent ID for obj.
The meaning of this persistent ID should be defined
by Unpickler.persistent_load().
Note that the value returned by persistent_id() cannot
itself have a persistent ID.
See Persistence of External Objects for details and examples of uses.
It is a mapping whose keys are classes and whose values are reduction functions.
A
reduction function takes a single argument of the associated class and should conform to the same interface
as a __reduce__() method.
By default, a pickler object will not have a dispatch_table attribute, and it will instead use the global
dispatch table managed by the copyreg module.
However, to customize the pickling for a specific pickler
object one can set the dispatch_table attribute to a dict-like object.
Alternatively, if a subclass of
Pickler has a dispatch_table attribute then this will be used as the default dispatch table for instances
of that class.
See Dispatch Tables for usage examples.
New in version 3.3.
This method has priority over any reducer
in the dispatch_table.
It should conform to the same interface as a __reduce__() method, and
can optionally return NotImplemented to fallback on dispatch_table-registered reducers to pickle
obj.
For a detailed example, see Custom Reduction for Types, Functions, and Other Objects.
New in version 3.8.
Enable fast mode if set to a true value.
The fast mode disables the usage of memo, therefore
speeding the pickling process by not generating superfluous PUT opcodes.
It should not be used with self-
referential objects, doing otherwise will cause Pickler to recurse infinitely.
Use pickletools.optimize() if you need more compact pickles.
This takes a binary file for reading a pickle data stream.
The protocol version of the pickle is detected automatically, so no protocol argument is needed.
The argument file must have three methods, a read() method that takes an integer argument, a readinto() method
that takes a buffer argument and a readline() method that requires no arguments, as in the io.BufferedIOBase
interface.
Thus file can be an on-disk file opened for binary reading, an io.BytesIO object, or any other custom
object that meets this interface.
The optional arguments fix_imports, encoding and errors are used to control compatibility support for pickle stream
generated by Python 2.
If fix_imports is true, pickle will try to map the old Python 2 names to the new names used
in Python 3.
The encoding and errors tell pickle how to decode 8-bit string instances pickled by Python 2; these
12.1.
The encoding can be ‘bytes’ to read these 8-bit string instances as bytes
objects.
Using encoding='latin1' is required for unpickling NumPy arrays and instances of datetime,
date and time pickled by Python 2.
If buffers is None (the default), then all data necessary for deserialization must be contained in the pickle stream.
This means that the buffer_callback argument was None when a Pickler was instantiated (or when dump() or
dumps() was called).
If buffers is not None, it should be an iterable of buffer-enabled objects that is consumed each time the pickle stream
references an out-of-band buffer view.
Such buffers have been given in order to the buffer_callback of a Pickler
object.
The buffers argument was added.
Bytes past the pickled representation of the object are
ignored.
If defined, persistent_load() should return the object specified by the persistent ID pid.
If an invalid
persistent ID is encountered, an UnpicklingError should be raised.
See Persistence of External Objects for details and examples of uses.
Import module if necessary and return the object called name from it, where the module and name arguments
are str objects.
Note, unlike its name suggests, find_class() is also used for finding functions.
Subclasses may override this to gain control over what type of objects and how they can be loaded, potentially
reducing security risks.
Refer to Restricting Globals for details.
Raises an auditing event pickle.find_class with arguments module, name.
A wrapper for a buffer representing picklable data.
PickleBuffer is itself a buffer provider, therefore it is possible to pass it to other APIs expecting a buffer-
providing object, such as memoryview.
PickleBuffer objects can only be serialized using pickle protocol 5 or higher.
They are eligible for out-of-band
serialization.
New in version 3.8.
The returned object is a one-dimensional,
C-contiguous memoryview with format B (unsigned bytes).
BufferError is raised if the buffer is neither
C- nor Fortran-contiguous.
release()
Release the underlying buffer exposed by the PickleBuffer object.
What can be pickled and unpickled?
The following types can be pickled:
• built-in constants (None, True, False, Ellipsis, and NotImplemented);
• integers, floating-point numbers, complex numbers;
• strings, bytes, bytearrays;
• tuples, lists, sets, and dictionaries containing only picklable objects;
• functions (built-in and user-defined) accessible from the top level of a module (using def, not lambda);
• classes accessible from the top level of a module;
• instances of such classes whose the result of calling __getstate__() is picklable (see section Pickling Class
Instances for details).
Attempts to pickle unpicklable objects will raise the PicklingError exception; when this happens, an unspecified
number of bytes may have already been written to the underlying file.
Trying to pickle a highly recursive data structure
may exceed the maximum recursion depth, a RecursionError will be raised in this case.
You can carefully raise this
limit with sys.setrecursionlimit().
This means that only
the function name is pickled, along with the name of the containing module and classes.
Neither the function’s code, nor
any of its function attributes are pickled.
Thus the defining module must be importable in the unpickling environment,
and the module must contain the named object, otherwise an exception will be raised.3
Similarly, classes are pickled by fully qualified name, so the same restrictions in the unpickling environment apply.
These restrictions are why picklable functions and classes must be defined at the top level of a module.
Similarly, when class instances are pickled, their class’s code and data are not pickled along with them.
Only the instance
data are pickled.
This is done on purpose, so you can fix bugs in a class or add methods to the class and still load objects
that were created with an earlier version of the class.
If you plan to have long-lived objects that will see many versions
of a class, it may be worthwhile to put a version number in the objects so that suitable conversions can be made by the
class’s __setstate__() method.
In this section, we describe the general mechanisms available to you to define, customize, and control how class instances
are pickled and unpickled.
In most cases, no additional code is needed to make instances picklable.
By default, pickle will retrieve the class and the
attributes of an instance via introspection.
When a class instance is unpickled, its __init__() method is usually not
invoked.
The default behaviour first creates an uninitialized instance and then restores the saved attributes.
This is why lambda functions cannot be pickled: all lambda functions share the same name: <lambda>.
The exception raised will likely be an ImportError or an AttributeError but it could be something else.
In protocols 2 and newer, classes that implements the __getnewargs_ex__() method can dictate the values
passed to the __new__() method upon unpickling.
The method must return a pair (args, kwargs) where
args is a tuple of positional arguments and kwargs a dictionary of named arguments for constructing the object.
Those will be passed to the __new__() method upon unpickling.
You should implement this method if the __new__() method of your class requires keyword-only arguments.
Otherwise, it is recommended for compatibility to implement __getnewargs__().
This method serves a similar purpose as __getnewargs_ex__(), but supports only positional arguments.
It
must return a tuple of arguments args which will be passed to the __new__() method upon unpickling.
__getnewargs__() will not be called if __getnewargs_ex__() is defined.
Changed
in
version
3.6:
Before
Python
3.6,
__getnewargs__()
was
called
instead
of
__getnewargs_ex__() in protocols 2 and 3.
object.__getstate__()
Classes can further influence how their instances are pickled by overriding the method __getstate__().
It is
called and the returned object is pickled as the contents for the instance, instead of a default state.
For a class that has an instance __dict__ and __slots__, the default state is a tuple consisting of two
dictionaries: self.__dict__, and a dictionary mapping slot names to slot values.
Only slots that have a
value are included in the latter.
Changed in version 3.11: Added the default implementation of the __getstate__() method in the object
class.
object.__setstate__(state)
Upon unpickling, if the class defines __setstate__(), it is called with the unpickled state.
In that case, there
is no requirement for the state object to be a dictionary.
Otherwise, the pickled state must be a dictionary and its
items are assigned to the new instance’s dictionary.
If __getstate__() returns a false value, the __setstate__() method will not be called upon
unpickling.
In case those methods rely on some internal invariant being true, the type should
implement __new__() to establish such an invariant, as __init__() is not called when unpickling an instance.
As we shall see, pickle does not use directly the methods described above.
In fact, these methods are part of the copy
protocol which implements the __reduce__() special method.
The copy protocol provides a unified interface for
retrieving the data necessary for pickling and copying objects.4
Although powerful, implementing __reduce__() directly in your classes is error prone.
For this reason, class designers
should use the high-level interface (i.e., __getnewargs_ex__(), __getstate__() and __setstate__())
whenever possible.
The interface is currently defined as follows.
The __reduce__() method takes no argument and shall return
either a string or preferably a tuple (the returned object is often referred to as the “reduce value”).
If a string is returned, the string should be interpreted as the name of a global variable.
It should be the object’s local
name relative to its module; the pickle module searches the module namespace to determine the object’s module.
This behaviour is typically useful for singletons.
When a tuple is returned, it must be between two and six items long.
Optional items can either be omitted, or None
can be provided as their value.
A callable object that will be called to create the initial version of the object.
A tuple of arguments for the callable object.
An empty tuple must be given if the callable does not accept
any argument.
Optionally, the object’s state, which will be passed to the object’s __setstate__() method as previously
described.
Optionally, an iterator (and not a sequence) yielding successive items.
These items will be appended to the
object either using obj.append(item) or, in batch, using obj.extend(list_of_items).
This
is primarily used for list subclasses, but may be used by other classes as long as they have append() and
extend() methods with the appropriate signature.
Optionally, an iterator (not a sequence) yielding successive key-value pairs.
These items will be stored to the
object using obj[key] = value.
This is primarily used for dictionary subclasses, but may be used by
other classes as long as they implement __setitem__().
Optionally, a callable with a (obj, state) signature.
This callable allows the user to programmatically
control the state-updating behavior of a specific object, instead of using obj’s static __setstate__()
method.
If not None, this callable will have priority over obj’s __setstate__().
The optional sixth tuple item, (obj, state), was added.
object.__reduce_ex__(protocol)
Alternatively, a __reduce_ex__() method may be defined.
The only difference is this method should take
a single integer argument, the protocol version.
When defined, pickle will prefer it over the __reduce__()
method.
In addition, __reduce__() automatically becomes a synonym for the extended version.
The main use
for this method is to provide backwards-compatible reduce values for older Python releases.
The copy module uses this protocol for shallow and deep copying operations.
Such objects are referenced by a persistent ID, which should be either a string of alphanumeric
characters (for protocol 0)5 or just an arbitrary object (for any newer protocol).
The resolution of such persistent IDs is not defined by the pickle module; it will delegate this resolution to the user-
defined methods on the pickler and unpickler, persistent_id() and persistent_load() respectively.
To pickle objects that have an external persistent ID, the pickler must have a custom persistent_id() method that
takes an object as an argument and returns either None or the persistent ID for that object.
When None is returned, the
pickler simply pickles the object as normal.
When a persistent ID string is returned, the pickler will pickle that object,
along with a marker so that the unpickler will recognize it as a persistent ID.
To unpickle external objects, the unpickler must have a custom persistent_load() method that takes a persistent
ID object and returns the referenced object.
Here is a comprehensive example presenting how persistent ID can be used to pickle external objects by reference.
MemoRecord = namedtuple("MemoRecord", "key, task")
class DBPickler(pickle.
Pickler):

# Instead of pickling MemoRecord as a regular class instance, we emit a
# persistent ID.
If obj does not have a persistent ID, return None.
This means obj
# needs to be pickled as usual.

class DBUnpickler(pickle.
This method is invoked whenever a persistent ID is encountered.
The limitation on alphanumeric characters is due to the fact that persistent IDs in protocol 0 are delimited by the newline character.
Therefore if
any kind of newline characters occurs in persistent IDs, the resulting pickled data will become unreadable.
UnpicklingError("unsupported persistent object")

import pprint
# Initialize and populate our database.
The global dispatch table managed by the copyreg module is available as copyreg.dispatch_table.
Therefore,
one may choose to use a modified copy of copyreg.dispatch_table as a private dispatch table.
For example
f = io.BytesIO()
p = pickle.
Pickler(f)
p.dispatch_table = copyreg.dispatch_table.copy()
p.dispatch_table[SomeClass] = reduce_SomeClass
creates an instance of pickle.
Pickler with a private dispatch table which handles the SomeClass class specially.
On the other hand, the
code
copyreg.pickle(SomeClass, reduce_SomeClass)
f = io.BytesIO()
p = pickle.
Pickler(f)
modifies the global dispatch table shared by all users of the copyreg module.
Handling Stateful Objects
Here’s an example that shows how to modify pickling behavior for a class.
The TextReader class opens a text file,
and returns the line number and line contents each time its readline() method is called.
If a TextReader instance
is pickled, all attributes except the file object member are saved.
When the instance is unpickled, the file is reopened,
and reading resumes from the last location.
The __setstate__() and __getstate__() methods are used to
implement this behavior.
Always use the dict.copy()
# method to avoid modifying the original state.
Remove the unpicklable entries.
To do so, we need to
# reopen it and read from it until the line count is restored.
Sometimes, dispatch_table may not be flexible enough.
In particular we may want to customize pickling based on
another criterion than the object’s type, or we may want to customize the pickling of functions and classes.
For those cases, it is possible to subclass from the Pickler class and implement a reducer_override()
method.
This method can return an arbitrary reduction tuple (see __reduce__()).
It can alternatively return
NotImplemented to fallback to the traditional behavior.
If both the dispatch_table and reducer_override() are defined, then reducer_override() method
takes priority.
Note: For performance reasons, reducer_override() may not be called for the following objects: None, True,
False, and exact instances of int, float, bytes, str, dict, set, frozenset, list and tuple.
Here is a simple example where we allow pickling and reconstructing a given class:
import io
import pickle
(continues on next page)
12.1.
In some contexts, the pickle module is used to transfer massive amounts of data.
Therefore, it can be important to
minimize the number of memory copies, to preserve performance and resource consumption.
However, normal operation
of the pickle module, as it transforms a graph-like structure of objects into a sequential stream of bytes, intrinsically
involves copying data to and from the pickle stream.
This constraint can be eschewed if both the provider (the implementation of the object types to be transferred) and the
consumer (the implementation of the communications system) support the out-of-band transfer facilities provided by
pickle protocol 5 and higher.
Provider API
The large data objects to be pickled must implement a __reduce_ex__() method specialized for protocol 5 and
higher, which returns a PickleBuffer instance (instead of e.g. a bytes object) for any large data.
A PickleBuffer object signals that the underlying buffer is eligible for out-of-band data transfer.
Those objects
remain compatible with normal usage of the pickle module.
However, consumers can also opt-in to tell pickle that
they will handle those buffers by themselves.
On the sending side, it needs to pass a buffer_callback argument to Pickler (or to the dump() or dumps() function),
which will be called with each PickleBuffer generated while pickling the object graph.
Buffers accumulated by the
buffer_callback will not see their data copied into the pickle stream, only a cheap marker will be inserted.
On the receiving side, it needs to pass a buffers argument to Unpickler (or to the load() or loads() function),
which is an iterable of the buffers which were passed to buffer_callback.
That iterable should produce buffers in the same
order as they were passed to buffer_callback.
Those buffers will provide the data expected by the reconstructors of the
objects whose pickling produced the original PickleBuffer objects.
Between the sending side and the receiving side, the communications system is free to implement its own transfer mecha-
nism for out-of-band buffers.
Potential optimizations include the use of shared memory or datatype-dependent compres-
sion.
The reconstructor (the _reconstruct class method) returns the buffer’s providing object if it has the right type.
This
is an easy way to simulate zero-copy behaviour on this toy example.
But if we pass a buffer_callback and then give back the accumulated buffers when unserializing, we are able to get back
the original object:
12.1.
True: no copy was made
This example is limited by the fact that bytearray allocates its own memory: you cannot create a bytearray
instance that is backed by another object’s memory.
However, third-party datatypes such as NumPy arrays do not have
this limitation, and allow use of zero-copy pickling (or making as few copies as possible) when transferring between
distinct processes or systems.
Restricting Globals
By default, unpickling will import any class or function that it finds in the pickle data.
For many applications, this behaviour
is unacceptable as it permits the unpickler to import and invoke arbitrary code.
Although this example is inoffensive, it is not difficult to imagine one that could damage your system.
For this reason, you may want to control what gets unpickled by customizing Unpickler.find_class().
Unlike
its name suggests, Unpickler.find_class() is called whenever a global (i.e., a class or a function) is requested.
Thus it is possible to either completely forbid globals or restrict them to a safe subset.
Here is an example of an unpickler allowing only few safe classes from the builtins module to be loaded:
import builtins
import io
import pickle
safe_builtins = {
'range',
'complex',
'set',
'frozenset',
'slice',
}
class RestrictedUnpickler(pickle.
Unpickler):

# Only allow safe classes from builtins.
Traceback (most recent call last):

pickle.
UnpicklingError: global 'builtins.eval' is forbidden
As our examples shows, you have to be careful with what you allow to be unpickled.
Therefore if security is a concern,
you may want to consider alternatives such as the marshalling API in xmlrpc.client or third-party solutions.
Performance
Recent versions of the pickle protocol (from protocol 2 and upwards) feature efficient binary encodings for several common
features and built-in types.
Also, the pickle module has a transparent optimizer written in C.
12.1.10 Examples
For the simplest code, use the dump() and load() functions.
The following example reads the resulting pickled data.
Module pickletools Tools for working with and analyzing pickled data.
Module shelve Indexed databases of objects; uses pickle.
Module copy Shallow and deep object copying.
Module marshal High-performance serialization of built-in types.
The pickle and copy
modules use those functions when pickling/copying those objects.
The module provides configuration information about
object constructors which are not classes.
Such constructors may be factory functions or class instances.
If object is not callable (and hence not valid as a constructor), raises
TypeError.
copyreg.pickle(type, function, constructor_ob=None)
Declares that function should be used as a “reduction” function for objects of type type.
See the dispatch_table for more details on the
interface of function.
The constructor_ob parameter is a legacy feature and is now ignored, but if passed it must be a callable.
Note that the dispatch_table attribute of a pickler object or subclass of pickle.
Pickler can also be
used for declaring reduction functions.
The difference with “dbm” databases is that the values (not the keys!) in
a shelf can be essentially arbitrary Python objects — anything that the pickle module can handle.
This includes most
class instances, recursive data types, and objects containing lots of shared sub-objects.
The keys are ordinary strings.
The filename specified is the base filename for the underlying database.
As a side-
effect, an extension may be added to the filename and more than one file may be created.
By default, the underlying
database file is opened for reading and writing.
The optional flag parameter has the same interpretation as the flag
parameter of dbm.open().
By default, pickles created with pickle.
DEFAULT_PROTOCOL are used to serialize values.
The version of the
pickle protocol can be specified with the protocol parameter.
Because of Python semantics, a shelf cannot know when a mutable persistent-dictionary entry is modified.
By de-
fault modified objects are written only when assigned to the shelf (see Example).
If the optional writeback parameter
is set to True, all entries accessed are also cached in memory, and written back on sync() and close(); this
can make it handier to mutate mutable entries in the persistent dictionary, but, if many entries are accessed, it can
consume vast amounts of memory for the cache, and it can make the close operation very slow since all accessed
entries are written back (there is no way to determine which accessed entries are mutable, nor which ones were
actually mutated).
Changed in version 3.10: pickle.
DEFAULT_PROTOCOL is now used as the default pickle protocol.
Accepts path-like object for filename.
Because the shelve module is backed by pickle, it is insecure to load a shelf from an untrusted
source.
Like with pickle, loading a shelf can execute arbitrary code.
Shelf objects support most of methods and operations supported by dictionaries (except copying, constructors and oper-
ators | and |=).
This eases the transition from dictionary based scripts to those requiring persistent storage.
Two additional methods are supported:
Shelf.sync()
Write back all entries in the cache if the shelf was opened with writeback set to True.
Also empty the cache and
synchronize the persistent dictionary on disk, if feasible.
This is called automatically when the shelf is closed with
close().
Shelf.close()
Synchronize and close the persistent dict object.
Operations on a closed shelf will fail with a ValueError.
See also:
Persistent dictionary recipe with widely supported storage formats and having the speed of native dictionaries.
12.3.
The choice of which database package will be used (such as dbm.ndbm or dbm.gnu) depends on which interface
is available.
Therefore it is not safe to open the database directly using dbm.
The database is also (unfortunately)
subject to the limitations of dbm, if it is used — this means that (the pickled representation of) the objects stored
in the database should be fairly small, and in rare cases key collisions may cause the database to refuse updates.
The shelve module does not support concurrent read/write access to shelved objects.
When a program has a shelf open for writing, no other program should have it open for
reading or writing.
Unix file locking can be used to solve this, but this differs across Unix versions and requires
knowledge about the database implementation used.
class shelve.
A subclass of collections.abc.
MutableMapping which stores pickled values in the dict object.
By default, pickles created with pickle.
DEFAULT_PROTOCOL are used to serialize values.
The version of the
pickle protocol can be specified with the protocol parameter.
See the pickle documentation for a discussion of
the pickle protocols.
If the writeback parameter is True, the object will hold a cache of all entries accessed and write them back to
the dict at sync and close times.
This allows natural operations on mutable entries, but can consume much more
memory and make sync and close take a long time.
The keyencoding parameter is the encoding used to encode keys before they are used with the underlying dict.
A Shelf object can also be used as a context manager, in which case it will be automatically closed when the
with block ends.
Added the keyencoding parameter; previously, keys were always encoded in UTF-8.
Changed in version 3.4: Added context manager support.
Changed in version 3.10: pickle.
DEFAULT_PROTOCOL is now used as the default pickle protocol.
A subclass of Shelf which exposes first(), next(), previous(), last() and set_location()
which are available in the third-party bsddb module from pybsddb but not in other database modules.
The dict
object passed to the constructor must support those methods.
This is generally accomplished by calling one of
bsddb.hashopen(), bsddb.btopen() or bsddb.rnopen().
The optional protocol, writeback, and
keyencoding parameters have the same interpretation as for the Shelf class.
A subclass of Shelf which accepts a filename instead of a dict-like object.
The underlying file will be opened
using dbm.open().
By default, the file will be created and opened for both read and write.
The optional flag
parameter has the same interpretation as for the open() function.
The optional protocol and writeback parameters
have the same interpretation as for the Shelf class.
Module pickle Object serialization used by shelve.
The format is specific to Python,
but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a
Sun, and read it back there).
Details of the format are undocumented on purpose; it may change between Python versions
(although it rarely does).1
This is not a general “persistence” module.
For general persistence and transfer of Python objects through RPC calls, see
the modules pickle and shelve.
The marshal module exists mainly to support reading and writing the “pseudo-
compiled” code for Python modules of .pyc files.
Therefore, the Python maintainers reserve the right to modify the
marshal format in backward incompatible ways should the need arise.
If you’re serializing and de-serializing Python
objects, use the pickle module instead – the performance is comparable, version independence is guaranteed, and
pickle supports a substantially wider range of objects than marshal.
Warning:
The marshal module is not intended to be secure against erroneous or maliciously constructed data.
Never unmarshal data received from an untrusted or unauthenticated source.
Not all Python object types are supported; in general, only objects whose value is independent from a particular invocation
of Python can be written and read by this module.
The name of this module stems from a bit of terminology used by the designers of Modula-3 (amongst others), who use the term “marshalling”
for shipping of data around in a self-contained form.
Strictly speaking, “to marshal” means to convert some data from internal to external form (in an
RPC buffer for instance) and “unmarshalling” for the reverse process.
The singletons None, Ellipsis and StopIteration can also be marshalled and
unmarshalled.
For format version lower than 3, recursive lists, sets and dictionaries cannot be written (see below).
There are functions that read/write files as well as functions operating on bytes-like objects.
The module defines these functions:
marshal.dump(value, file[, version])
Write the value on the open file.
The value must be a supported type.
The file must be a writeable binary file.
If the value has (or contains an object that has) an unsupported type, a ValueError exception is raised — but
garbage data will also be written to the file.
The object will not be properly read back by load().
The version argument indicates the data format that dump should use (see below).
Raises an auditing event marshal.dumps with arguments value, version.
marshal.load(file)
Read one value from the open file and return it.
If no valid value is read (e.g. because the data has a different
Python version’s incompatible marshal format), raise EOFError, ValueError or TypeError.
The file must
be a readable binary file.
Raises an auditing event marshal.load with no arguments.
Note: If an object containing an unsupported type was marshalled with dump(), load() will substitute None
for the unmarshallable type.
This call used to raise a code.__new__ audit event for each code object.
Now it raises
a single marshal.load event for the entire load operation.
The value must be a supported
type.
Raise a ValueError exception if value has (or contains an object that has) an unsupported type.
The version argument indicates the data format that dumps should use (see below).
Convert the bytes-like object to a value.
If no valid value is found, raise EOFError, ValueError or
TypeError.
Extra bytes in the input are ignored.
Raises an auditing event marshal.loads with argument bytes.
This call used to raise a code.__new__ audit event for each code object.
Now it raises
a single marshal.loads event for the entire load operation.
In addition, the following constants are defined:
marshal.version
Indicates the format that the module uses.
Version 0 is the historical format, version 1 shares interned strings
and version 2 uses a binary format for floating point numbers.
Version 3 adds support for object instancing and
recursion.
If none of these modules is
installed, the slow-but-simple implementation in module dbm.dumb will be used.
A tuple containing the exceptions that can be raised by each of the supported modules, with a unique exception
also named dbm.error as the first item — the latter is used when dbm.error is raised.
This function attempts to guess which of the several simple database modules available — dbm.gnu, dbm.ndbm
or dbm.dumb — should be used to open a given file.
Returns one of the following values: None if the file can’t be opened because it’s unreadable or doesn’t exist; the
empty string ('') if the file’s format can’t be guessed; or a string containing the required module name, such as
'dbm.ndbm' or 'dbm.gnu'.
Accepts path-like object for filename.
If the database file already exists, the whichdb() function is used to determine its type and the appropriate
module is used; if it does not exist, the first module listed above that can be imported is used.
It defaults
to octal 0o666 (and will be modified by the prevailing umask).
The object returned by open() supports the same basic functionality as dictionaries; keys and their corresponding values
can be stored, retrieved, and deleted, and the in operator and the keys() method are available, as well as get() and
setdefault().
Changed in version 3.2: get() and setdefault() are now available in all database modules.
Changed in version 3.8: Deleting a key from a read-only database raises database module specific error instead of
KeyError.
Accepts path-like object for file.
Key and values are always stored as bytes.
This means that when strings are used they are implicitly converted to the
default encoding before being stored.
These objects also support being used in a with statement, which will automatically close them when done.
Changed in version 3.4: Added native support for the context management protocol to the objects returned by open().
The following example records some hostnames and a corresponding title, and then prints out the contents of the database:
12.5.
See also:
Module shelve Persistence module which stores non-string data.
The individual submodules are described in the following sections.
This module is quite similar to the dbm module, but uses the GNU library gdbm instead to provide some additional
functionality.
Please note that the file formats created by dbm.gnu and dbm.ndbm are incompatible.
The dbm.gnu module provides an interface to the GNU DBM library.
Printing a gdbm object doesn’t
print the keys and values, and the items() and values() methods are not supported.
KeyError is raised for general mapping errors like
specifying an incorrect key.
The filename argument is the name of the database file.
The following additional characters may be appended to the flag to control how the database is opened:
Value
Meaning
'f'
Open the database in fast mode.
Writes to the database will not be synchronized.
This will cause changes to the database to be immediately written to the file.
Not all flags are valid for all versions of gdbm.
The module constant open_flags is a string of supported flag
characters.
The exception error is raised if an invalid flag is specified.
The optional mode argument is the Unix mode of the file, used only when the database has to be created.
It defaults
to octal 0o666.
It’s possible to loop over every key in the database using this method and the nextkey() method.
The
traversal is ordered by gdbm’s internal hash values, and won’t be sorted by the key values.
Returns the key that follows key in the traversal.
If you have carried out a lot of deletions and would like to shrink the space used by the gdbm file, this routine
will reorganize the database.
When the database has been opened in fast mode, this method forces any unwritten data to be written to the
disk.
Dbm objects behave like mappings (dictio-
naries), except that keys and values are always stored as bytes.
Printing a dbm object doesn’t print the keys and values,
and the items() and values() methods are not supported.
This module can be used with the “classic” ndbm interface or the GNU GDBM compatibility interface.
On Unix, the
configure script will attempt to locate the appropriate header file to simplify building this module.
KeyError is raised for general mapping errors like
specifying an incorrect key.
The filename argument is the name of the database file (without
the .dir or .pag extensions).
The optional flag argument must be one of these values:
Value
Meaning
'r'
Open existing database for reading only (default)
'w'
Open existing database for reading and writing
'c'
Open database for reading and writing, creating it if it doesn’t exist
'n'
Always create a new, empty database, open for reading and writing
The optional mode argument is the Unix mode of the file, used only when the database has to be created.
It defaults
to octal 0o666 (and will be modified by the prevailing umask).
Accepts path-like object for filename.
ndbm.close()
Close the ndbm database.
The dbm.dumb module is not written for speed and is not nearly as heavily used as the other database
modules.
The dbm.dumb module provides a persistent dictionary-like interface which is written entirely in Python.
Unlike other
modules such as dbm.gnu no external library is required.
As with other persistent mappings, the keys and values are
always stored as bytes.
The module defines the following:
exception dbm.dumb.error
Raised on dbm.dumb-specific errors, such as I/O errors.
KeyError is raised for general mapping errors like
specifying an incorrect key.
The filename argument is the basename of the database
file (without any specific extensions).
It defaults
to octal 0o666 (and will be modified by the prevailing umask).
Warning:
It is possible to crash the Python interpreter when loading a database with a sufficiently
large/complex entry due to stack depth limitations in Python’s AST compiler.
Changed in version 3.5: open() always creates a new database when the flag has the value 'n'.
A database opened with flags 'r' is now read-only.
Opening with flags 'r' and 'w' no
longer creates a database if it does not exist.
Accepts path-like object for filename.
In addition to the methods provided by the collections.abc.
MutableMapping class, dumbdbm objects
provide the following methods:
dumbdbm.sync()
Synchronize the on-disk directory and data files.
This method is called by the Shelve.sync() method.
Some
applications can use SQLite for internal data storage.
It’s also possible to prototype an application using SQLite and then
port the code to a larger database such as PostgreSQL or Oracle.
The sqlite3 module was written by Gerhard Häring.
It provides an SQL interface compliant with the DB-API 2.0
specification described by PEP 249, and requires SQLite 3.7.15 or newer.
Tutorial teaches how to use the sqlite3 module.
The SQLite web page; the documentation describes the syntax and the available data types for
the supported SQL dialect.
PEP 249 - Database API Specification 2.0 PEP written by Marc-André Lemburg.
In this tutorial, you will create a database of Monty Python movies using basic sqlite3 functionality.
It assumes a
fundamental understanding of database concepts, including cursors and transactions.
First, we need to create a new database and open a database connection to allow sqlite3 to work with it.
The returned Connection object con represents the connection to the on-disk database.
In order to execute SQL statements and fetch results from SQL queries, we will need to use a database cursor.
Now that we’ve got a database connection and a cursor, we can create a database table movie with columns for title,
release year, and review score.
For simplicity, we can just use column names in the table declaration – thanks to the
flexible typing feature of SQLite, specifying the data types is optional.
We can verify that the new table has been created by querying the sqlite_master table built-in to SQLite, which
should now contain an entry for the movie table definition (see The Schema Table for details).
We can see that the table has been created, as the query returns a tuple containing the table’s name.
The INSERT statement implicitly opens a transaction, which needs to be committed before changes are saved in the
database (see Transaction control for details).
Call con.commit() on the connection object to commit the transaction:
con.commit()
We can verify that the data was inserted correctly by executing a SELECT query.
Notice that ?
Always use placeholders instead of string formatting to
bind Python values to SQL statements, to avoid SQL injection attacks (see How to use placeholders to bind values in SQL
queries for more details).
Each row is a two-item tuple of (year, title), matching the columns selected in the query.
Finally, verify that the database has been written to disk by calling con.close() to close the existing connection,
opening a new one, creating a new cursor, then querying the database:
The highest scoring Monty Python movie is 'Monty Python and the Holy Grail', released␣
�→in 1975
You’ve now created an SQLite database using the sqlite3 module, inserted data and retrieved values from it in multiple
ways.
See also:
• How-to guides for further reading:
– How to use placeholders to bind values in SQL queries
– How to adapt custom Python types to SQLite values
– How to convert SQLite values to custom Python types
– How to use the connection context manager
– How to create and use row factories
12.6.
Parameters
• database (path-like object) – The path to the database file to be opened.
You can pass
":memory:" to create an SQLite database existing only in memory, and open a connection
to it.
If another connection opens a transaction
to modify a table, that table will be locked until the transaction is committed.
Default five
seconds.
Set it to any combination (using |, bitwise or) of
PARSE_DECLTYPES and PARSE_COLNAMES to enable this.
Column names takes prece-
dence over declared types if both flags are set.
Types cannot be detected for generated fields
(for example max(data)), even when the detect_types parameter is set; str will be returned
instead.
By default (0), type detection is disabled.
See Connection.isolation_level and Transaction control via the isolation_level
attribute for more information.
Can be "DEFERRED" (default), "EXCLUSIVE" or
"IMMEDIATE"; or None to disable opening transactions implicitly.
Has no effect unless
Connection.autocommit is set to LEGACY_TRANSACTION_CONTROL (the default).
If True (default), ProgrammingError will be raised
if the database connection is used by a thread other than the one that created it.
If False, the
connection may be accessed in multiple threads; write operations may need to be serialized by
the user to avoid data corruption.
See threadsafety for more information.
By default, 128 statements.
If set to True, database is interpreted as a URI (Uniform Resource Identifier)
with a file path and an optional query string.
The scheme part must be "file:", and the path
can be relative or absolute.
The query string allows passing parameters to SQLite, enabling
various How to work with SQLite URIs.
See
Connection.autocommit and Transaction control via the autocommit attribute for more
information.
The
default will change to False in a future Python release.
Raises an auditing event sqlite3.connect/handle with argument connection_handle.
The uri parameter.
Changed in version 3.7: database can now also be a path-like object, not only a string.
The sqlite3.connect/handle auditing event.
The autocommit parameter.
Return True if the string statement appears to contain one or more complete SQL statements.
No syntactic
verification or parsing of any kind is performed, other than checking that there are no unclosed string literals and
the statement is terminated by a semicolon.
For example:
True
False
This function may be useful during command-line input to determine if the entered text seems to form a complete
SQL statement, or if additional input is needed before calling execute().
See runsource() in Lib/sqlite3/__main__.py for real-world use.
By default you will not get any tracebacks in user-defined functions, aggre-
gates, converters, authorizer callbacks etc.
If you want to debug them, you can call this function with flag set to
True.
Afterwards, you will get tracebacks from callbacks on sys.stderr.
The adapter is called with a Python
object of type type as its sole argument, and must return a value of a type that SQLite natively understands.
The converter is invoked for all SQLite values of type typename; it is passed a bytes object and should return an
12.6.
Consult the parameter detect_types of connect() for information regarding
how type detection works.
Module constants
sqlite3.LEGACY_TRANSACTION_CONTROL
Set autocommit to this constant to select old style (pre-Python 3.12) transaction control behaviour.
See Trans-
action control via the isolation_level attribute for more information.
The type name must be wrapped in
square brackets ([]).
SELECT p as "p [point]" FROM test;
!
The types are declared when the database table is created.
For example:
CREATE TABLE test(
i integer primary key,
!
This flag may be combined with PARSE_COLNAMES using the | (bitwise or) operator.
The column should be treated as a NULL value (SQLITE_IGNORE)
sqlite3.apilevel
String constant stating the supported DB-API level.
Required by the DB-API.
Hard-coded to "2.0".
Required by
the DB-API.
Hard-coded to "qmark".
Note: The named DB-API parameter style is also supported.
This
attribute is set based on the default threading mode the underlying SQLite library is compiled with.
The SQLite
threading modes are:
1.
Single-thread: In this mode, all mutexes are disabled and SQLite is unsafe to use in more than a single thread
at once.
Multi-thread: In this mode, SQLite can be safely used by multiple threads provided that no single database
connection is used simultaneously in two or more threads.
Serialized: In serialized mode, SQLite can be safely used by multiple threads with no restriction.
The mappings from SQLite threading modes to DB-API 2.0 threadsafety levels are as follows:
SQLite
threading
mode
thread-
safety
SQLITE_THREADSAFEDB-API 2.0 meaning
single-thread


Threads may not share the module
multi-thread


Threads may share the module, but not con-
nections
serialized


Threads may share the module, connections
and cursors
Changed in version 3.11: Set threadsafety dynamically instead of hard-coding it to 1.
sqlite3.version
Version number of this module as a string.
This is not the version of the SQLite library.
This constant used to reflect the version number
of the pysqlite package, a third-party library which used to upstream changes to sqlite3.
Today, it carries
no meaning or practical value.
This is not the version of the SQLite library.
This constant used to reflect the version number
of the pysqlite package, a third-party library which used to upstream changes to sqlite3.
Today, it carries
no meaning or practical value.
The availability of these constants varies depending on the version of SQLite Python was compiled with.
New in version 3.12.
See also:
https://www.sqlite.org/c3ref/c_dbconfig_defensive.html SQLite docs: Database Connection Configuration
Options
Connection objects
class sqlite3.Connection
Each open SQLite database is represented by a Connection object, which is created using sqlite3.
Their main purpose is creating Cursor objects, and Transaction control.
See also:
• How to use connection shortcut methods
• How to use the connection context manager
An SQLite database connection has the following attributes and methods:
cursor(factory=Cursor)
Create and return a Cursor object.
The cursor method accepts a single optional parameter factory.
If
supplied, this must be a callable returning an instance of Cursor or its subclasses.
Parameters
• table (str) – The name of the table where the blob is located.
• column (str) – The name of the column where the blob is located.
• row (str) – The name of the row where the blob is located.
Defaults to False.
• name (str) – The name of the database where the blob is located.
Defaults to "main".
Raises OperationalError – When trying to open a blob in a WITHOUT ROWID table.
Return type Blob
Note: The blob size cannot be changed using the Blob class.
If autocommit is True, or there is no open transaction,
this method does nothing.
If autocommit is False, a new transaction is implicitly opened if a pending
transaction was committed by this method.
If autocommit is True, or there is no open transaction,
this method does nothing.
If autocommit is False, a new transaction is implicitly opened if a pending
transaction was rolled back by this method.
If autocommit is False, any pending transaction is implicitly rolled
back.
If autocommit is True or LEGACY_TRANSACTION_CONTROL, no implicit transaction control
is executed.
Make sure to commit() before closing to avoid losing pending changes.
Return the new
cursor object.
Return
the new cursor object.
Return the new
cursor object.
Parameters
• name (str) – The name of the SQL function.
The number of arguments the SQL function can accept.
If -1, it may take
any number of arguments.
The
callable must return a type natively supported by SQLite.
Set to None to remove an existing
SQL function.
Raises NotSupportedError – If deterministic is used with SQLite versions older than 3.8.3.
The deterministic parameter.
Example:



print(row)
('acbd18db4cc2f85cedef654fccc4a4d8',)
12.6.
Parameters
• name (str) – The name of the SQL aggregate function.
The number of arguments the SQL aggregate function can accept.
If -1, it
may take any number of arguments.
The number of arguments that the step() method must accept is controlled by n_arg.
Set to None to remove an existing SQL aggregate function.
Parameters
• name (str) – The name of the SQL aggregate window function to create or remove.
The number of arguments the SQL aggregate window function can
accept.
If -1, it may take any number of arguments.
The number of arguments that the step() and value() methods must accept is con-
trolled by num_params.
Set to None to remove an existing SQL aggregate window function.
Raises NotSupportedError – If used with a version of SQLite older than 3.25.0, which does
not support aggregate window functions.
New in version 3.11.
Any clean-up actions should be placed here.
The collation name can contain any Unicode character.
Earlier, only ASCII char-
acters were allowed.
Register callable authorizer_callback to be invoked for each attempt to access a column of a table in the
database.
The callback should return one of SQLITE_OK, SQLITE_DENY, or SQLITE_IGNORE to signal
how access to the column should be handled by the underlying SQLite library.
The first argument to the callback signifies what kind of operation is to be authorized.
The second and third
argument will be arguments or None depending on the first argument.
The 5th argument is the name of the inner-most trigger or view
that is responsible for the access attempt or None if this access attempt is directly from input SQL code.
Please consult the SQLite documentation about the possible values for the first argument and the meaning
of the second and third argument depending on the first one.
All necessary constants are available in the
sqlite3 module.
Passing None as authorizer_callback will disable the authorizer.
Register callable progress_handler to be invoked for every n instructions of the SQLite virtual machine.
This
is useful if you want to get called from SQLite during long-running operations, for example to update a GUI.
If you want to clear any previously installed progress handler, call the method with None for progress_handler.
Returning a non-zero value from the handler function will terminate the currently executing query and cause
it to raise a DatabaseError exception.
The only argument passed to the callback is the statement (as str) that is being executed.
The return value
of the callback is ignored.
Note that the backend does not only run statements passed to the Cursor.
execute() methods.
Other sources include the transaction management of the sqlite3 module and the
execution of triggers defined in the current database.
Passing None as trace_callback will disable the trace callback.
Note: Exceptions raised in the trace callback are not propagated.
As a development and debugging aid, use
enable_callback_tracebacks() to enable printing tracebacks from exceptions raised in the trace
callback.
New in version 3.3.
SQLite extensions can define new functions, aggregates or whole new virtual table
implementations.
One well-known extension is the fulltext-search extension distributed with SQLite.
Note: The sqlite3 module is not built with loadable extension support by default, because some platforms
(notably macOS) have SQLite libraries which are compiled without this feature.
To get loadable extension
support, you must pass the --enable-loadable-sqlite-extensions option to configure.
Raises an auditing event sqlite3.enable_load_extension with arguments connection,
enabled.
New in version 3.2.
Enable
extension
loading
with
enable_load_extension() before calling this method.
Parameters
• path (str) – The path to the SQLite extension.
If None (the default), SQLite will
come up with an entry point name of its own; see the SQLite docs Loading an Extension for
details.
Raises an auditing event sqlite3.load_extension with arguments connection, path.
New in version 3.2.
Changed in version 3.10: Added the sqlite3.load_extension auditing event.
The entrypoint parameter.
iterdump()
Return an iterator to dump the database as SQL source code.
Useful when saving an in-memory database for
later restoration.
Similar to the .dump command in the sqlite3 shell.
Example:
# Convert file example.db to SQL dump file dump.sql
con = sqlite3.connect('example.db')
with open('dump.sql', 'w') as f:
for line in con.iterdump():
f.write('%s\n' % line)
con.close()
See also:
How to handle non-UTF-8 text encodings
backup(target, *, pages=- 1, progress=None, name=’main’, sleep=0.250)
Create a backup of an SQLite database.
Works even if the database is being accessed by other clients or concurrently by the same connection.
The database connection to save the backup to.
• pages (int) – The number of pages to copy at a time.
If equal to or less than 0, the entire
database is copied in a single step.
Defaults to -1.
• progress (callback | None) – If set to a callable, it is invoked with three integer arguments
for every backup iteration: the status of the last iteration, the remaining number of pages still
to be copied, and the total number of pages.
The number of seconds to sleep between successive attempts to back up
remaining pages.
Example 2, copy an existing database into a transient copy:
src = sqlite3.connect('example.db')
dst = sqlite3.connect(':memory:')
src.backup(dst)
New in version 3.7.
Get a connection runtime limit.
Parameters category (int) – The SQLite limit category to be queried.
Return type int
Raises ProgrammingError – If category is not recognised by the underlying SQLite library.
Example, query the maximum length of an SQL statement for Connection con (the default is
1000000000):

New in version 3.11.
Attempts to increase a limit above its hard upper bound are silently truncated
to the hard upper bound.
Regardless of whether or not the limit was changed, the prior value of the limit is
returned.
Parameters
• category (int) – The SQLite limit category to be set.
• limit (int) – The value of the new limit.
If negative, the current limit is unchanged.
Return type int
Raises ProgrammingError – If category is not recognised by the underlying SQLite library.
Query a boolean connection configuration option.
Parameters op (int) – A SQLITE_DBCONFIG code.
Return type bool
New in version 3.12.
False if
it should be disabled.
New in version 3.12.
serialize(*, name=’main’)
Serialize a database into a bytes object.
For an ordinary on-disk database file, the serialization is just a copy
of the disk file.
For an in-memory database or a “temp” database, the serialization is the same sequence of
bytes which would be written to disk if that database were backed up to disk.
The database name to be serialized.
Defaults to "main".
Return type bytes
Note: This method is only available if the underlying SQLite library has the serialize API.
New in version 3.11.
This method causes the database connection
to disconnect from database name, and reopen name as an in-memory database based on the serialization
contained in data.
The database name to deserialize into.
Defaults to "main".
Raises
• OperationalError – If the database connection is currently involved in a read transac-
tion or a backup operation.
New in version 3.11.
False: Select PEP 249-compliant transaction behaviour, implying that sqlite3 ensures a transaction
is always open.
Use commit() and rollback() to close transactions.
This is the recommended value of autocommit.
True: Use SQLite’s autocommit mode.
See isolation_level for more details.
This is currently the default value of autocommit.
Changing autocommit to False will open a new transaction, and changing it to True will commit any
pending transaction.
See Transaction control via the autocommit attribute for more details.
Note:
The
isolation_level
attribute
has
no
effect
unless
autocommit
is
LEGACY_TRANSACTION_CONTROL.
New in version 3.12.
True if a transaction is active (there are uncommitted changes), False otherwise.
New in version 3.2.
If set to None, transactions are never implic-
itly opened.
If set to one of "DEFERRED", "IMMEDIATE", or "EXCLUSIVE", corresponding to the
underlying SQLite transaction behaviour, implicit transaction management is performed.
If not overridden by the isolation_level parameter of connect(), the default is "", which is an alias for
"DEFERRED".
Note:
Using
autocommit
to
control
transaction
handling
is
recommended
over
using
isolation_level.
Assigning to this attribute
does not affect the row_factory of existing cursors belonging to this connection, only new ones.
Is None
by default, meaning each row is returned as a tuple.
See How to create and use row factories for more details.
A callable that accepts a bytes parameter and returns a text representation of it.
The callable is invoked for
SQLite values with the TEXT data type.
By default, this attribute is set to str.
See How to handle non-UTF-8 text encodings for more details.
Cursor objects
A Cursor object represents a database cursor which is used to execute SQL statements, and manage the
context of a fetch operation.
Cursors are created using Connection.cursor(), or by using any of the
connection shortcut methods.
Cursor objects are iterators, meaning that if you execute() a SELECT query, you can simply iterate over
the cursor to fetch the resulting rows:
for row in cur.execute("SELECT t FROM data"):
print(row)
class sqlite3.Cursor
A Cursor instance has the following attributes and methods.
Parameters
• sql (str) – A single SQL statement.
• parameters (dict | sequence) – Python values to bind to placeholders in sql.
A dict
if named placeholders are used.
A sequence if unnamed placeholders are used.
See How to
use placeholders to bind values in SQL queries.
Raises ProgrammingError – If sql contains more than one SQL statement.
DeprecationWarning is emitted if
named placeholders are used and parameters is a sequence instead of a dict.
Starting with Python 3.14,
ProgrammingError will be raised instead.
Use executescript() to execute multiple SQL statements.
SQL
statement sql.
Uses the same implicit transaction handling as execute().
Parameters
• sql (str) – A single SQL DML statement.
• parameters (iterable) – An iterable of parameters to bind with the placeholders in sql.
See How to use placeholders to bind values in SQL queries.
Note: Any resulting rows are discarded, including DML statements with RETURNING clauses.
DeprecationWarning is emitted if
named placeholders are used and the items in parameters are sequences instead of dicts.
Starting with
Python 3.14, ProgrammingError will be raised instead.
If the autocommit is LEGACY_TRANSACTION_CONTROL
and there is a pending transaction, an implicit COMMIT statement is executed first.
No other implicit trans-
action control is performed; any transaction control must be added to sql_script.
If row_factory is None, return the next row query result set as a tuple.
Else, pass it to the row factory
and return its result.
Return None if no more data is available.
Return an empty list if no more rows are available.
The number of rows to fetch per call is specified by the size parameter.
If size is not given, arraysize
determines the number of rows to be fetched.
If fewer than size rows are available, as many rows as are
available are returned.
Note there are performance considerations involved with the size parameter.
For optimal performance, it is
usually best to use the arraysize attribute.
If the size parameter is used, then it is best for it to retain the same
value from one fetchmany() call to the next.
Return an empty list if no rows are available.
Note
that the arraysize attribute can affect the performance of this operation.
The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if
any operation is attempted with the cursor.
Does nothing in sqlite3.
Does nothing in sqlite3.
The default value is 1
which means a single row would be fetched per call.
A Cursor
object created by calling con.cursor() will have a connection attribute that refers to con:
True
description
Read-only attribute that provides the column names of the last query.
To remain compatible with the Python
DB API, it returns a 7-tuple for each column where the last six items of each tuple are None.
It is set for SELECT statements without any matching rows as well.
It is only updated after suc-
cessful INSERT or REPLACE statements using the execute() method.
For other statements, after
executemany() or executescript(), or if the insertion failed, the value of lastrowid is left
unchanged.
The initial value of lastrowid is None.
Note: Inserts into WITHOUT ROWID tables are not recorded.
Changed in version 3.6: Added support for the REPLACE statement.
It is
only updated by the execute() and executemany() methods, after the statement has run to comple-
tion.
This means that any resulting rows must be fetched in order for rowcount to be updated.
If None, a row is represented as a tuple.
Can be set to the included sqlite3.Row; or a callable that accepts two arguments, a Cursor object and
the tuple of row values, and returns a custom object representing an SQLite row.
Defaults to what Connection.row_factory was set to when the Cursor was created.
Assigning to
this attribute does not affect Connection.row_factory of the parent connection.
See How to create and use row factories for more details.
It supports iteration,
equality testing, len(), and mapping access by column name and index.
Two Row objects compare equal if they have identical column names and values.
See How to create and use row factories for more details.
keys()
Return a list of column names as strings.
Immediately after a query, it is the first member of each
tuple in Cursor.description.
Changed in version 3.5: Added support of slicing.
Blob objects
New in version 3.11.
Call len(blob) to get the
size (number of bytes) of the blob.
Use indices and slices for direct access to the blob data.
Use the Blob as a context manager to ensure that the blob handle is closed after use.
The blob will be unusable from this point onward.
An Error (or subclass) exception will be raised if any
further operation is attempted with the blob.
If the end of the blob is reached, the
data up to EOF (End of File) will be returned.
Write data to the blob at the current offset.
This function cannot change the blob length.
Writing beyond the
end of the blob will raise ValueError.
SEEK_SET, / )
Set the current access position of the blob to offset.
Other values for origin are os.
SEEK_CUR (seek relative to the current position) and os.
SEEK_END (seek relative to the blob’s end).
PrepareProtocol objects
class sqlite3.PrepareProtocol
The PrepareProtocol type’s single purpose is to act as a PEP 246 style adaption protocol for objects that can adapt
themselves to native SQLite types.
Exceptions
The exception hierarchy is defined by the DB-API 2.0 (PEP 249).
Warning is a subclass of Exception.
Use this to catch all errors with one single except statement.
Error is a subclass of Exception.
If the exception originated from within the SQLite library, the following two attributes are added to the exception:
sqlite_errorcode
The numeric error code from the SQLite API
New in version 3.11.
In other words, if this exception is raised, it probably
indicates a bug in the sqlite3 module.
InterfaceError is a subclass of Error.
This serves as the base exception for several types of
database errors.
It is only raised implicitly through the specialised subclasses.
DatabaseError is a subclass of
Error.
DataError is a subclass of DatabaseError.
For example, the database path is not found, or a transaction could not be processed.
OperationalError is a subclass of DatabaseError.
It is a
subclass of DatabaseError.
If this is raised, it may indicate that there is a problem
with the runtime SQLite library.
InternalError is a subclass of DatabaseError.
ProgrammingError is a subclass of
DatabaseError.
For exam-
ple, setting deterministic to True in create_function(), if the underlying SQLite library does not support
deterministic functions.
NotSupportedError is a subclass of DatabaseError.
SQLite and Python types
SQLite natively supports the following types: NULL, INTEGER, REAL, TEXT, BLOB.
The following Python types can thus be sent to SQLite without any problem:
Python type
SQLite type
None
NULL
int
INTEGER
float
REAL
str
TEXT
bytes
BLOB
This is how SQLite types are converted to Python types by default:
SQLite type
Python type
NULL
None
INTEGER
int
REAL
float
TEXT
depends on text_factory, str by default
BLOB
bytes
The type system of the sqlite3 module is extensible in two ways: you can store additional Python types in an SQLite
database via object adapters, and you can let the sqlite3 module convert SQLite types to Python types via converters.
Note:
The default adapters and converters are deprecated as of Python 3.12.
Instead, use the Adapter and converter
recipes and tailor them to your needs.
The deprecated default adapters and converters consist of:
• An adapter for datetime.date objects to strings in ISO 8601 format.
An adapter for datetime.datetime objects to strings in ISO 8601 format.
A converter for declared “date” types to datetime.date objects.
A converter for declared “timestamp” types to datetime.datetime objects.
Fractional parts will be truncated
to 6 digits (microsecond precision).
Note: The default “timestamp” converter ignores UTC offsets in the database and always returns a naive datetime.
To preserve UTC offsets in timestamps, either leave converters disabled, or register an offset-aware
converter with register_converter().
New in version 3.12.
However, beware of using Python’s string operations
to assemble queries, as they are vulnerable to SQL injection attacks.
To insert a variable into a query string, use a placeholder in the string,
and substitute the actual values into the query by providing them as a tuple of values to the second argument of the
cursor’s execute() method.
An SQL statement may use one of two kinds of placeholders: question marks (qmark style) or named placeholders (named
style).
For the qmark style, parameters must be a sequence whose length must match the number of placeholders, or a
ProgrammingError is raised.
For the named style, parameters must be an instance of a dict (or a subclass), which
must contain keys for all named parameters; any extra items are ignored.
Note: PEP 249 numeric placeholders are not supported.
If used, they will be interpreted as named placeholders.
How to adapt custom Python types to SQLite values
SQLite supports only a limited set of data types natively.
To store custom Python types in SQLite databases, adapt them
to one of the Python types SQLite natively understands.
There are two ways to adapt Python objects to SQLite types: letting your object adapt itself, or using an adapter callable.
The latter will take precedence above the former.
For a library that exports a custom type, it may make sense to enable
that type to adapt itself.
As an application developer, it may make more sense to take direct control by registering custom
adapter functions.
How to write adaptable objects
Suppose we have a Point class that represents a pair of coordinates, x and y, in a Cartesian coordinate system.
The
coordinate pair will be stored as a text string in the database, using a semicolon to separate the coordinates.
This can be
implemented by adding a __conform__(self, protocol) method which returns the adapted value.
The object
passed to protocol will be of type PrepareProtocol.
The other possibility is to create a function that converts the Python object to an SQLite-compatible type.
This function
can then be registered using register_adapter().
To be able to convert from SQLite values
to custom Python types, we use converters.
Let’s go back to the Point class.
We stored the x and y coordinates separated via semicolons as strings in SQLite.
First, we’ll define a converter function that accepts the string as a parameter and constructs a Point object from it.
Note: Converter functions are always passed a bytes object, no matter the underlying SQLite data type.
We now need to tell sqlite3 when it should convert a given SQLite value.
This is done when connecting to a database,
using the detect_types parameter of connect().
Column names
take precedence over declared types.
Instead, the Cursor objects are created implicitly and these shortcut methods return the cursor objects.
This way, you
can execute a SELECT statement and iterate over it directly using only a single call on the Connection object.
If the body of the with statement finishes without exceptions, the transaction
is committed.
If this commit fails, or if the body of the with statement raises an uncaught exception, the transaction is
rolled back.
If autocommit is False, a new transaction is implicitly opened after committing or rolling back.
If there is no open transaction upon leaving the body of the with statement, or if autocommit is True, the context
manager does nothing.
Note: The context manager neither implicitly opens a new transaction nor closes the connection.
If you need a closing
context manager, consider using contextlib.closing().
Traceback (most recent call last):
OperationalError: attempt to write a readonly database
• Do not implicitly create a new database file if it does not already exist; will raise OperationalError if unable
to create a new file:
Traceback (most recent call last):
OperationalError: unable to open database file
• Create a shared named in-memory database:
db = "file:mem1?mode=memory&cache=shared"
con1 = sqlite3.connect(db, uri=True)
(continues on next page)
12.6.
INTO shared VALUES(28)")
res = con2.execute("SELECT data FROM shared")
assert res.fetchone() == (28,)
More information about this feature, including a list of parameters, can be found in the SQLite URI documentation.
How to create and use row factories
By default, sqlite3 represents each row as a tuple.
If a tuple does not suit your needs, you can use the sqlite3.
Row class or a custom row_factory.
While row_factory exists as an attribute both on the Cursor and the Connection, it is recommended to set
Connection.row_factory, so all cursors created from the connection will use the same row factory.
Row provides indexed and case-insensitive named access to columns, with minimal memory overhead and performance
impact over a tuple.
The FROM clause can be omitted in the SELECT statement, as in the above example.
In such cases, SQLite
returns a single row with columns defined by expressions, e.g. literals, with the given aliases expr AS alias.
How to handle non-UTF-8 text encodings
By default, sqlite3 uses str to adapt SQLite values with the TEXT data type.
This works well for UTF-8 encoded
text, but it might fail for other encodings and invalid UTF-8.
You can use a custom text_factory to handle such
cases.
Because of SQLite’s flexible typing, it is not uncommon to encounter table columns with the TEXT data type containing
non-UTF-8 encodings, or even arbitrary data.
To demonstrate, let’s assume we have a database with ISO-8859-2 (Latin-2)
encoded text, for example a table of Czech-English dictionary entries.
Note: The sqlite3 module API does not support strings containing surrogates.
See also:
unicode-howto
12.6.
Transaction control via the autocommit attribute is recommended, while Transaction control via the isolation_level attribute
retains the pre-Python 3.12 behaviour.
Transaction control via the autocommit attribute
The recommended way of controlling transaction behaviour is through the Connection.autocommit attribute,
which should preferably be set using the autocommit parameter of connect().
It is suggested to set autocommit to False, which implies PEP 249-compliant transaction control.
This means:
• sqlite3 ensures that a transaction is always open, so connect(), Connection.commit(), and
Connection.rollback() will implicitly open a new transaction (immediately after closing the pending one,
for the latter two).
An implicit rollback is performed if the database is close()-ed with pending changes.
Set autocommit to True to enable SQLite’s autocommit mode.
In this mode, Connection.commit() and
Connection.rollback() have no effect.
Note that SQLite’s autocommit mode is distinct from the PEP 249-
compliant Connection.autocommit attribute; use Connection.in_transaction to query the low-level
SQLite autocommit mode.
Set autocommit to LEGACY_TRANSACTION_CONTROL to leave transaction control behaviour to the Connection.
See Transaction control via the isolation_level attribute for more information.
Transaction control via the isolation_level attribute
Note: The recommended way of controlling transactions is via the autocommit attribute.
See Transaction control via
the autocommit attribute.
If Connection.autocommit is set to LEGACY_TRANSACTION_CONTROL (the default), transaction behaviour is
controlled using the Connection.isolation_level attribute.
Otherwise, isolation_level has no effect.
If the connection attribute isolation_level is not None, new transactions are implicitly opened before
execute() and executemany() executes INSERT, UPDATE, DELETE, or REPLACE statements; for other state-
ments, no implicit transaction handling is performed.
Use the commit() and rollback() methods to respectively
commit and roll back pending transactions.
You can choose the underlying SQLite transaction behaviour — that is,
whether and what type of BEGIN statements sqlite3 implicitly executes – via the isolation_level attribute.
If isolation_level is set to None, no transactions are implicitly opened at all.
This leaves the underlying SQLite
library in autocommit mode, but also allows the user to perform their own transaction handling using explicit SQL state-
ments.
The underlying SQLite library autocommit mode can be queried using the in_transaction attribute.
The executescript() method implicitly commits any pending transaction before execution of the given SQL script,
regardless of the value of isolation_level.
This is no
longer the case.
The modules described in this chapter support data compression with the zlib, gzip, bzip2 and lzma algorithms, and the
creation of ZIP- and tar-format archives.
See also Archiving operations provided by the shutil module.
The zlib library has its own home page at https://www.zlib.net.
There are known incompatibilities
between the Python module and versions of the zlib library earlier than 1.1.3; 1.1.3 has a security vulnerability, so we
recommend using 1.1.4 or later.
This documentation doesn’t attempt to
cover all of the permutations; consult the zlib manual at http://www.zlib.net/manual.html for authoritative information.
Exception raised on compression and decompression errors.
The result is an unsigned 32-bit integer.
If value is present, it is used as the
starting value of the checksum; otherwise, a default value of 1 is used.
Passing in value allows computing a running
checksum over the concatenation of several inputs.
The algorithm is not cryptographically strong, and should not
be used for authentication or digital signatures.
Since the algorithm is designed for use as a checksum algorithm, it
is not suitable for use as a general hash algorithm.
The result is always unsigned.
The default value is -1 (Z_DEFAULT_COMPRESSION).
Z_DEFAULT_COMPRESSION represents a default
compromise between speed and compression (currently equivalent to level 6).
The wbits argument controls the size of the history buffer (or the “window size”) used when compressing data,
and whether a header and trailer is included in the output.
The base-two logarithm of the window size, which therefore ranges between 512 and 32768.
Larger values produce better compression at the expense of greater memory usage.
The resulting output will
include a zlib-specific header and trailer.
Uses the absolute value of wbits as the window size logarithm, while producing a raw output
stream with no header or trailing checksum.
Uses the low 4 bits of the value as the window size logarithm, while including
a basic gzip header and trailing checksum in the output.
Raises the error exception if any error occurs.
Changed in version 3.6: level can now be used as a keyword parameter.
The wbits parameter is now available to set window bits and compression type.
The default value is -1 (Z_DEFAULT_COMPRESSION).
Z_DEFAULT_COMPRESSION represents a default compromise between speed and compression (currently
equivalent to level 6).
Currently, the only supported value is DEFLATED.
The wbits parameter controls the size of the history buffer (or the “window size”), and what header and trailer
format will be used.
It has the same meaning as described for compress().
The memLevel argument controls the amount of memory used for the internal compression state.
Valid values
range from 1 to 9.
Higher values use more memory, but are faster and produce smaller output.
Possible values are Z_DEFAULT_STRATEGY, Z_FILTERED,
Z_HUFFMAN_ONLY, Z_RLE (zlib 1.2.0.1) and Z_FIXED (zlib 1.2.2.2).
This is a sequence of bytes (such as a bytes object) containing
subsequences that are expected to occur frequently in the data that is to be compressed.
Those subsequences that
are expected to be most common should come at the end of the dictionary.
Added the zdict parameter and keyword argument support.
The result is an unsigned 32-bit integer.
If
value is present, it is used as the starting value of the checksum; otherwise, a default value of 0 is used.
Passing
in value allows computing a running checksum over the concatenation of several inputs.
The algorithm is not
cryptographically strong, and should not be used for authentication or digital signatures.
Since the algorithm is
designed for use as a checksum algorithm, it is not suitable for use as a general hash algorithm.
The result is always unsigned.
The wbits parameter
depends on the format of data, and is discussed further below.
If bufsize is given, it is used as the initial size of the
output buffer.
Raises the error exception if any error occurs.
The wbits parameter controls the size of the history buffer (or “window size”), and what header and trailer format
is expected.
The base-two logarithm of the window size.
The input must include a zlib header and trailer.
Uses the absolute value of wbits as the window size logarithm.
The input must be a raw stream
with no header or trailer.
Uses the low 4 bits of the value as the window size logarithm.
The input must
include a gzip header and trailer.
Uses the low 4 bits of the value as the window size logarithm, and automatically
accepts either the zlib or gzip format.
When decompressing a stream, the window size must not be smaller than the size originally used to compress the
stream; using a too-small value may result in an error exception.
The default wbits value corresponds to the
largest window size and requires a zlib header and trailer to be included.
bufsize is the initial size of the buffer used to hold decompressed data.
If more space is required, the buffer size
will be increased as needed, so you don’t have to get this value exactly right; tuning it will only save a few calls to
malloc().
Changed in version 3.6: wbits and bufsize can be used as keyword arguments.
The wbits parameter controls the size of the history buffer (or the “window size”), and what header and trailer
format is expected.
It has the same meaning as described for decompress().
The zdict parameter specifies a predefined compression dictionary.
If provided, this must be the same dictionary
as was used by the compressor that produced the data that is to be decompressed.
Note: If zdict is a mutable object (such as a bytearray), you must not modify its contents between the call to
decompressobj() and the first call to the decompressor’s decompress() method.
Added the zdict parameter.
Compression objects support the following methods:
Compress.compress(data)
Compress data, returning a bytes object containing compressed data for at least part of the data in data.
This data
should be concatenated to the output produced by any preceding calls to the compress() method.
Some input
may be kept in internal buffers for later processing.
All pending input is processed, and a bytes object containing the remaining compressed output is returned.
Except Z_FINISH, all constants allow com-
pressing further bytestrings of data, while Z_FINISH finishes the compressed stream and prevents compressing
any more data.
After calling flush() with mode set to Z_FINISH, the compress() method cannot be called
again; the only realistic action is to delete the object.
Compress.copy()
Returns a copy of the compression object.
This can be used to efficiently compress a set of data that share a common
initial prefix.
Changed in version 3.8: Added copy.copy() and copy.deepcopy() support to compression objects.
Decompression objects support the following methods and attributes:
Decompress.unused_data
A bytes object which contains any bytes past the end of the compressed data.
That is, this remains b"" until the
13.1.
If the whole bytestring turned out to contain compressed data,
this is b"", an empty bytes object.
A bytes object that contains any data that was not consumed by the last decompress() call because it exceeded
the limit for the uncompressed data buffer.
This data has not yet been seen by the zlib machinery, so you must
feed it (possibly with further data concatenated to it) back to a subsequent decompress() method call in order
to get correct output.
Decompress.eof
A boolean indicating whether the end of the compressed data stream has been reached.
This makes it possible to distinguish between a properly formed compressed stream, and an incomplete or truncated
one.
New in version 3.3.
Decompress.decompress(data, max_length=0)
Decompress data, returning a bytes object containing the uncompressed data corresponding to at least part of
the data in string.
This data should be concatenated to the output produced by any preceding calls to the
decompress() method.
Some of the input data may be preserved in internal buffers for later processing.
If the optional parameter max_length is non-zero then the return value will be no longer than max_length.
This may
mean that not all of the compressed input can be processed; and unconsumed data will be stored in the attribute
unconsumed_tail.
This bytestring must be passed to a subsequent call to decompress() if decompression
is to continue.
If max_length is zero then the whole input is decompressed, and unconsumed_tail is empty.
Changed in version 3.6: max_length can be used as a keyword argument.
All pending input is processed, and a bytes object containing the remaining uncompressed output is returned.
After
calling flush(), the decompress() method cannot be called again; the only realistic action is to delete the
object.
The optional parameter length sets the initial size of the output buffer.
Decompress.copy()
Returns a copy of the decompression object.
This can be used to save the state of the decompressor midway through
the data stream in order to speed up random seeks into the stream at a future point.
Changed in version 3.8: Added copy.copy() and copy.deepcopy() support to decompression objects.
The version string of the zlib library that was used for building the module.
This may be different from the zlib
library actually used at runtime, which is available as ZLIB_RUNTIME_VERSION.
zlib.
The version string of the zlib library actually loaded by the interpreter.
New in version 3.3.
See also:
Module gzip Reading and writing gzip-format files.
The zlib manual explains the semantics and usage of the library’s many functions.
The data compression is provided by the zlib module.
The gzip module provides the GzipFile class, as well as the open(), compress() and decompress() conve-
nience functions.
The GzipFile class reads and writes gzip-format files, automatically compressing or decompressing
the data so that it looks like an ordinary file object.
Note that additional file formats which can be decompressed by the gzip and gunzip programs, such as those produced
by compress and pack, are not supported by this module.
The module defines the following items:
gzip.open(filename, mode=’rb’, compresslevel=9, encoding=None, errors=None, newline=None)
Open a gzip-compressed file in binary or text mode, returning a file object.
The filename argument can be an actual filename (a str or bytes object), or an existing file object to read from
or write to.
The mode argument can be any of 'r', 'rb', 'a', 'ab', 'w', 'wb', 'x' or 'xb' for binary mode, or 'rt',
'at', 'wt', or 'xt' for text mode.
The default is 'rb'.
The compresslevel argument is an integer from 0 to 9, as for the GzipFile constructor.
For binary mode, this function is equivalent to the GzipFile constructor: GzipFile(filename, mode,
compresslevel).
In this case, the encoding, errors and newline arguments must not be provided.
For text mode, a GzipFile object is created, and wrapped in an io.
TextIOWrapper instance with the
specified encoding, error handling behavior, and line ending(s).
Changed in version 3.3: Added support for filename being a file object, support for text mode, and the encoding,
errors and newline arguments.
Changed in version 3.4: Added support for the 'x', 'xb' and 'xt' modes.
Accepts a path-like object.
BadGzipFile
An exception raised for invalid gzip files.
It inherits OSError.
EOFError and zlib.error can also be raised
for invalid gzip files.
New in version 3.8.
class gzip.
GzipFile(filename=None, mode=None, compresslevel=9, fileobj=None, mtime=None)
Constructor for the GzipFile class, which simulates most of the methods of a file object, with the exception of
the truncate() method.
At least one of fileobj and filename must be given a non-trivial value.
The new class instance is based on fileobj, which can be a regular file, an io.BytesIO object, or any other object
which simulates a file.
It defaults to None, in which case filename is opened to provide a file object.
When fileobj is not None, the filename argument is only used to be included in the gzip file header, which may
include the original filename of the uncompressed file.
It defaults to the filename of fileobj, if discernible; otherwise,
it defaults to the empty string, and in this case the original filename is not included in the header.
The mode argument can be any of 'r', 'rb', 'a', 'ab', 'w', 'wb', 'x', or 'xb', depending on whether
the file will be read or written.
The default is the mode of fileobj if discernible; otherwise, the default is 'rb'.
In
future Python releases the mode of fileobj will not be used.
It is better to always specify mode for writing.
Note that the file is always opened in binary mode.
To open a compressed file in text mode, use open() (or wrap
your GzipFile with an io.TextIOWrapper).
The compresslevel argument is an integer from 0 to 9 controlling the level of compression; 1 is fastest and produces
the least compression, and 9 is slowest and produces the most compression.
The default is 9.
The mtime argument is an optional numeric timestamp to be written to the last modification time field in the stream
when compressing.
It should only be provided in compression mode.
If omitted or None, the current time is used.
See the mtime attribute for more details.
Calling a GzipFile object’s close() method does not close fileobj, since you might wish to append more
material after the compressed data.
GzipFile supports the io.BufferedIOBase interface, including iteration and the with statement.
Only
the truncate() method isn’t implemented.
GzipFile also provides the following method and attribute:
peek(n)
Read n uncompressed bytes without advancing the file position.
At most one single read on the compressed
stream is done to satisfy the call.
The number of bytes returned may be more or less than requested.
Note: While calling peek() does not change the file position of the GzipFile, it may change the position
of the underlying file object (e.g. if the GzipFile was constructed with the fileobj parameter).
New in version 3.2.
The initial value before reading any headers is None.
All gzip compressed streams are required to contain this timestamp field.
Some programs, such as gunzip,
make use of the timestamp.
The format is the same as the return value of time.time() and the
st_mtime attribute of the object returned by os.stat().
Equivalent to the output of os.fspath() on the
original input path, with no other normalization, resolution or expansion.
Changed in version 3.1: Support for the with statement was added, along with the mtime constructor argument
and mtime attribute.
Changed in version 3.2: Support for zero-padded and unseekable files was added.
Changed in version 3.3: The io.BufferedIOBase.read1() method is now implemented.
Changed in version 3.4: Added support for the 'x' and 'xb' modes.
Changed in version 3.5: Added support for writing arbitrary bytes-like objects.
The read() method now accepts
an argument of None.
Accepts a path-like object.
Changed in version 3.12: Remove the filename attribute, use the name attribute instead.
When mtime is set to 0, this function is equivalent to
zlib.compress() with wbits set to 31.
The zlib function is faster.
New in version 3.2.
Changed in version 3.8: Added the mtime parameter for reproducible output.
Changed in version 3.11: Speed is improved by compressing all data at once instead of in a streamed fashion.
Calls
with mtime set to 0 are delegated to zlib.compress() for better speed.
Decompress the data, returning a bytes object containing the uncompressed data.
This function is capable of
decompressing multi-member gzip data (multiple gzip blocks concatenated together).
When the data is certain to
contain only one member the zlib.decompress() function with wbits set to 31 is faster.
New in version 3.2.
Changed in version 3.11: Speed is improved by decompressing members at once in memory instead of in a streamed
fashion.
The basic data compression module needed to support the gzip file format.
The gzip module provides a simple command line interface to compress or decompress files.
Once executed the gzip module keeps the input file(s).
Add a new command line interface with a usage.
By default, when you will execute the CLI, the
default compression level is 6.
Command line options
file
If file is not specified, read from sys.stdin.
The bz2 module contains:
• The open() function and BZ2File class for reading and writing compressed files.
The compress() and decompress() functions for one-shot (de)compression.
13.3.1 (De)compression of files
bz2.open(filename, mode=’rb’, compresslevel=9, encoding=None, errors=None, newline=None)
Open a bzip2-compressed file in binary or text mode, returning a file object.
As with the constructor for BZ2File, the filename argument can be an actual filename (a str or bytes object),
or an existing file object to read from or write to.
The mode argument can be any of 'r', 'rb', 'w', 'wb', 'x', 'xb', 'a' or 'ab' for binary mode, or 'rt',
'wt', 'xt', or 'at' for text mode.
The default is 'rb'.
The compresslevel argument is an integer from 1 to 9, as for the BZ2File constructor.
In this case, the encoding, errors and newline arguments must not be
provided.
For text mode, a BZ2File object is created, and wrapped in an io.
TextIOWrapper instance with the specified
encoding, error handling behavior, and line ending(s).
New in version 3.3.
The 'x' (exclusive creation) mode was added.
Accepts a path-like object.
If filename is a str or bytes object, open the named file directly.
Otherwise, filename should be a file object,
which will be used to read or write the compressed data.
The mode argument can be either 'r' for reading (default), 'w' for overwriting, 'x' for exclusive creation, or
'a' for appending.
These can equivalently be given as 'rb', 'wb', 'xb' and 'ab' respectively.
If filename is a file object (rather than an actual file name), a mode of 'w' does not truncate the file, and is instead
equivalent to 'a'.
If mode is 'w' or 'a', compresslevel can be an integer between 1 and 9 specifying the level of compression: 1
produces the least compression, and 9 (default) produces the most compression.
If mode is 'r', the input file may be the concatenation of multiple compressed streams.
BZ2File provides all of the members specified by the io.BufferedIOBase, except for detach() and
truncate().
Iteration and the with statement are supported.
Return buffered data without advancing the file position.
At least one byte of data will be returned (unless at
EOF).
The exact number of bytes returned is unspecified.
Note: While calling peek() does not change the file position of the BZ2File, it may change the position
of the underlying file object (e.g. if the BZ2File was constructed by passing a file object for filename).
New in version 3.3.
New in version 3.3.
New in version 3.3.
New in version 3.3.
New in version 3.3.
Reads up to a buffer’s worth of data if size is negative.
Returns b'' if the file is at EOF.
New in version 3.3.
New in version 3.3.
Changed in version 3.1: Support for the with statement was added.
Changed in version 3.3: Support was added for filename being a file object instead of an actual filename.
The 'a' (append) mode was added, along with support for reading multi-stream files.
The 'x' (exclusive creation) mode was added.
The read() method now accepts an argument of None.
Accepts a path-like object.
The buffering parameter has been removed.
It was ignored and deprecated since Python
3.0.
Pass an open file object to control how the file is opened.
The compresslevel parameter became keyword-only.
This class is thread unsafe in the face of multiple simultaneous readers or writers, just
like its equivalent classes in gzip and lzma have always been.
This object may be used to compress data incrementally.
For one-shot compres-
sion, use the compress() function instead.
compresslevel, if given, must be an integer between 1 and 9.
The default is 9.
compress(data)
Provide data to the compressor object.
Returns a chunk of compressed data if possible, or an empty byte
string otherwise.
When you have finished providing data to the compressor, call the flush() method to finish the compression
process.
Returns the compressed data left in internal buffers.
The compressor object may not be used after this method has been called.
This object may be used to decompress data incrementally.
For one-shot
compression, use the decompress() function instead.
Note:
This class does not transparently handle inputs containing multiple compressed streams, unlike
decompress() and BZ2File.
If you need to decompress a multi-stream input with BZ2Decompressor,
you must use a new decompressor for each stream.
decompress(data, max_length=- 1)
Decompress data (a bytes-like object), returning uncompressed data as bytes.
Some of data may be buffered
internally, for use in later calls to decompress().
The returned data should be concatenated with the
output of any previous calls to decompress().
If max_length is nonnegative, returns at most max_length bytes of decompressed data.
If this limit is reached
and further output can be produced, the needs_input attribute will be set to False.
In this case, the
next call to decompress() may provide data as b'' to obtain more of the output.
If all of the input data was decompressed and returned (either because this was less than max_length bytes,
or because max_length was negative), the needs_input attribute will be set to True.
Attempting to decompress data after the end of stream is reached raises an EOFError.
Any data found after
the end of the stream is ignored and saved in the unused_data attribute.
Added the max_length parameter.
New in version 3.3.
If this attribute is accessed before the end of the stream has been reached, its value will be b''.
New in version 3.5.
Compress data, a bytes-like object.
The default is 9.
For incremental compression, use a BZ2Compressor instead.
If data is the concatenation of multiple compressed streams, decompress all of the streams.
For incremental decompression, use a BZ2Decompressor instead.
Changed in version 3.3: Support for multi-stream inputs was added.
Donec rhoncus quis sapien sit amet molestie.
Nam rutrum pretium placerat.
In interdum laoreet mi, sit amet ultrices purus
... pulvinar a.
Aliquam pharetra lacus non risus vehicula rutrum.
Maecenas aliquam leo
... felis.
Curabitur lacinia mi ornare consectetur vestibulum.
Call this once you have
The example above uses a very “nonrandom” stream of data (a stream of b"z" chunks).
Random data tends to compress
poorly, while ordered, repetitive data usually yields a high compression ratio.
Donec rhoncus quis sapien sit amet molestie.
Nam rutrum pretium placerat.
In interdum laoreet mi, sit amet ultrices purus
... pulvinar a.
Aliquam pharetra lacus non risus vehicula rutrum.
Maecenas aliquam leo
... felis.
Curabitur lacinia mi ornare consectetur vestibulum.
Source code: Lib/lzma.py
This module provides classes and convenience functions for compressing and decompressing data using the LZMA com-
pression algorithm.
Also included is a file interface supporting the .xz and legacy .lzma file formats used by the xz
utility, as well as raw compressed streams.
The interface provided by this module is very similar to that of the bz2 module.
Note that LZMAFile and bz2.
BZ2File are not thread-safe, so if you need to use a single LZMAFile instance from multiple threads, it is necessary
to protect it with a lock.
This exception is raised when an error occurs during compression or decompression, or while initializing the com-
pressor/decompressor state.
Reading and writing compressed files
lzma.open(filename, mode=’rb’, *, format=None, check=- 1, preset=None, filters=None, encoding=None,
errors=None, newline=None)
Open an LZMA-compressed file in binary or text mode, returning a file object.
The filename argument can be either an actual file name (given as a str, bytes or path-like object), in which
case the named file is opened, or it can be an existing file object to read from or write to.
The mode argument can be any of "r", "rb", "w", "wb", "x", "xb", "a" or "ab" for binary mode, or "rt",
"wt", "xt", or "at" for text mode.
The default is "rb".
When opening a file for reading, the format and filters arguments have the same meanings as for
LZMADecompressor.
In this case, the check and preset arguments should not be used.
When opening a file for writing, the format, check, preset and filters arguments have the same meanings as for
LZMACompressor.
For binary mode, this function is equivalent to the LZMAFile constructor: LZMAFile(filename, mode,
...).
In this case, the encoding, errors and newline arguments must not be provided.
For text mode, a LZMAFile object is created, and wrapped in an io.
TextIOWrapper instance with the
specified encoding, error handling behavior, and line ending(s).
Changed in version 3.4: Added support for the "x", "xb" and "xt" modes.
Accepts a path-like object.
LZMAFile(filename=None, mode=’r’, *, format=None, check=- 1, preset=None, filters=None)
Open an LZMA-compressed file in binary mode.
An LZMAFile can wrap an already-open file object, or operate directly on a named file.
The filename argument
specifies either the file object to wrap, or the name of the file to open (as a str, bytes or path-like object).
When
wrapping an existing file object, the wrapped file will not be closed when the LZMAFile is closed.
The mode argument can be either "r" for reading (default), "w" for overwriting, "x" for exclusive creation, or
"a" for appending.
These can equivalently be given as "rb", "wb", "xb" and "ab" respectively.
If filename is a file object (rather than an actual file name), a mode of "w" does not truncate the file, and is instead
equivalent to "a".
When opening a file for reading, the input file may be the concatenation of multiple separate compressed streams.
These are transparently decoded as a single logical stream.
When opening a file for reading, the format and filters arguments have the same meanings as for
LZMADecompressor.
In this case, the check and preset arguments should not be used.
When opening a file for writing, the format, check, preset and filters arguments have the same meanings as for
LZMACompressor.
LZMAFile supports all the members specified by io.BufferedIOBase, except for detach() and
truncate().
Iteration and the with statement are supported.
The following method is also provided:
peek(size=- 1)
Return buffered data without advancing the file position.
At least one byte of data will be returned, unless
EOF has been reached.
The exact number of bytes returned is unspecified (the size argument is ignored).
Note: While calling peek() does not change the file position of the LZMAFile, it may change the position
of the underlying file object (e.g. if the LZMAFile was constructed by passing a file object for filename).
The read() method now accepts an argument of None.
Accepts a path-like object.
Compressing and decompressing data in memory
class lzma.
LZMACompressor(format=FORMAT_XZ, check=- 1, preset=None, filters=None)
Create a compressor object, which can be used to compress data incrementally.
For a more convenient way of compressing a single chunk of data, see compress().
The format argument specifies what container format should be used.
The .xz container format.
This is the default format.
This format is more limited than .xz – it does
not support integrity checks or multiple filters.
This format specifier does not sup-
port integrity checks, and requires that you always specify a custom filter chain (for both compres-
sion and decompression).
Additionally, data compressed in this manner cannot be decompressed using
FORMAT_AUTO (see LZMADecompressor).
The check argument specifies the type of integrity check to include in the compressed data.
This check is used
when decompressing, to ensure that the data has not been corrupted.
Possible values are:
• CHECK_NONE: No integrity check.
This is the default (and the only acceptable value) for FORMAT_ALONE
and FORMAT_RAW.
CHECK_CRC64: 64-bit Cyclic Redundancy Check.
CHECK_SHA256: 256-bit Secure Hash Algorithm.
If the specified check is not supported, an LZMAError is raised.
The compression settings can be specified either as a preset compression level (with the preset argument), or in
detail as a custom filter chain (with the filters argument).
The preset argument (if provided) should be an integer between 0 and 9 (inclusive), optionally OR-ed with the con-
stant PRESET_EXTREME.
If neither preset nor filters are given, the default behavior is to use PRESET_DEFAULT
(preset level 6).
Higher presets produce smaller output, but make the compression process slower.
Note: In addition to being more CPU-intensive, compression with higher presets also requires much more memory
(and produces output that needs more memory to decompress).
With preset 9 for example, the overhead for an
LZMACompressor object can be as high as 800 MiB.
For this reason, it is generally best to stick with the default
preset.
The filters argument (if provided) should be a filter chain specifier.
Compress data (a bytes object), returning a bytes object containing compressed data for at least part of
the input.
Some of data may be buffered internally, for use in later calls to compress() and flush().
The returned data should be concatenated with the output of any previous calls to compress().
The compressor cannot be used after this method has been called.
LZMADecompressor(format=FORMAT_AUTO, memlimit=None, filters=None)
Create a decompressor object, which can be used to decompress data incrementally.
For a more convenient way of decompressing an entire compressed stream at once, see decompress().
The format argument specifies the container format that should be used.
The default is FORMAT_AUTO, which
can decompress both .xz and .lzma files.
Other possible values are FORMAT_XZ, FORMAT_ALONE, and
FORMAT_RAW.
The memlimit argument specifies a limit (in bytes) on the amount of memory that the decompressor can use.
When
this argument is used, decompression will fail with an LZMAError if it is not possible to decompress the input
within the given memory limit.
The filters argument specifies the filter chain that was used to create the stream being decompressed.
This argument
is required if format is FORMAT_RAW, but should not be used for other formats.
See Specifying custom filter chains
for more information about filter chains.
Note:
This class does not transparently handle inputs containing multiple compressed streams, unlike
decompress() and LZMAFile.
To decompress a multi-stream input with LZMADecompressor, you must
13.4.
Some of data may be buffered
internally, for use in later calls to decompress().
The returned data should be concatenated with the
output of any previous calls to decompress().
If max_length is nonnegative, returns at most max_length bytes of decompressed data.
If this limit is reached
and further output can be produced, the needs_input attribute will be set to False.
In this case, the
next call to decompress() may provide data as b'' to obtain more of the output.
If all of the input data was decompressed and returned (either because this was less than max_length bytes,
or because max_length was negative), the needs_input attribute will be set to True.
Attempting to decompress data after the end of stream is reached raises an EOFError.
Any data found after
the end of the stream is ignored and saved in the unused_data attribute.
Added the max_length parameter.
This may be CHECK_UNKNOWN until enough of the
input has been decoded to determine what integrity check it uses.
Before the end of the stream is reached, this will be b"".
New in version 3.5.
See LZMACompressor above for a description of the format, check, preset and filters arguments.
If data is the concatenation of multiple distinct compressed streams, decompress all of these streams, and return
the concatenation of the results.
See LZMADecompressor above for a description of the format, memlimit and filters arguments.
CHECK_NONE and CHECK_CRC32 are always supported.
Specifying custom filter chains
A filter chain specifier is a sequence of dictionaries, where each dictionary contains the ID and options for a single filter.
Each dictionary must contain the key "id", and may contain additional keys to specify filter-dependent options.
Valid
filter IDs are as follows:
• Compression filters:
– FILTER_LZMA1 (for use with FORMAT_ALONE)
– FILTER_LZMA2 (for use with FORMAT_XZ and FORMAT_RAW)
• Delta filter:
– FILTER_DELTA
• Branch-Call-Jump (BCJ) filters:
– FILTER_X86
– FILTER_IA64
– FILTER_ARM
– FILTER_ARMTHUMB
– FILTER_POWERPC
– FILTER_SPARC
A filter chain can consist of up to 4 filters, and cannot be empty.
The last filter in the chain must be a compression filter,
and any other filters must be delta or BCJ filters.
Compression filters support the following options (specified as additional entries in the dictionary representing the filter):
• preset: A compression preset to use as a source of default values for options that are not specified explicitly.
This should be between 4 KiB and 1.5 GiB (inclusive).
The sum lc + lp must be at most 4.
This should be 273 or less.
The delta filter stores the differences between bytes, producing more repetitive input for the compressor in certain cir-
cumstances.
It supports one option, dist.
This indicates the distance between bytes to be subtracted.
The default is 1,
i.e. take the differences between adjacent bytes.
The BCJ filters are intended to be applied to machine code.
They convert relative branches, calls and jumps in the code
to use absolute addressing, with the aim of increasing the redundancy that can be exploited by the compressor.
These
filters support one option, start_offset.
This specifies the address that should be mapped to the beginning of the
input data.
The default is 0.
Incremental compression:
import lzma
lzc = lzma.
FILTER_DELTA, "dist": 5},
{"id": lzma.
FILTER_LZMA2, "preset": 7 | lzma.
Lib/zipfile/
The ZIP file format is a common archive and compression standard.
This module provides tools to create, read, write,
append, and list a ZIP file.
Any advanced use of this module will require an understanding of the format, as defined in
PKZIP Application Note.
This module does not currently handle multi-disk ZIP files.
It can handle ZIP files that use the ZIP64 extensions (that
is ZIP files that are more than 4 GiB in size).
It supports decryption of encrypted files in ZIP archives, but it currently
cannot create an encrypted file.
Decryption is extremely slow as it is implemented in native Python rather than C.
The module defines the following items:
exception zipfile.
The error raised for bad ZIP files.
New in version 3.2.
exception zipfile.
BadZipfile
Alias of BadZipFile, for compatibility with older Python versions.
Deprecated since version 3.2.
exception zipfile.
LargeZipFile
The error raised when a ZIP file would require ZIP64 functionality but that has not been enabled.
ZipFile
The class for reading and writing ZIP files.
See section ZipFile Objects for constructor details.
Path
Class that implements a subset of the interface provided by pathlib.
Path, including the full importlib.
resources.abc.
Traversable interface.
New in version 3.8.
PyZipFile
Class for creating ZIP archives containing Python libraries.
Class used to represent information about a member of an archive.
Instances of this class are returned by the
getinfo() and infolist() methods of ZipFile objects.
Most users of the zipfile module will not
need to create these, but only use those created by this module.
Returns True if filename is a valid ZIP file based on its magic number, otherwise returns False.
Changed in version 3.1: Support for file and file-like objects.
The numeric constant for an uncompressed archive member.
ZIP_DEFLATED
The numeric constant for the usual ZIP compression method.
This requires the zlib module.
zipfile.
The numeric constant for the BZIP2 compression method.
This requires the bz2 module.
New in version 3.3.
The numeric constant for the LZMA compression method.
This requires the lzma module.
New in version 3.3.
Note: The ZIP file format specification has included support for bzip2 compression since 2001, and for LZMA
compression since 2006.
However, some tools (including older Python releases) do not support these compression
methods, and may either refuse to process the ZIP file altogether, or fail to extract individual files.
See also:
PKZIP Application Note Documentation on the ZIP file format by Phil Katz, the creator of the format and algorithms
used.
Info-ZIP Home Page Information about the Info-ZIP project’s ZIP archive programs and development libraries.
ZipFile Objects
class zipfile.
ZipFile(file, mode=’r’, compression=ZIP_STORED, allowZip64=True, compresslevel=None, *,
strict_timestamps=True, metadata_encoding=None)
Open a ZIP file, where file can be a path to a file (a string), a file-like object or a path-like object.
The mode parameter should be 'r' to read an existing file, 'w' to truncate and write a new file, 'a' to append
to an existing file, or 'x' to exclusively create and write a new file.
If mode is 'x' and file refers to an existing
file, a FileExistsError will be raised.
If mode is 'a' and file refers to an existing ZIP file, then additional
files are added to it.
If file does not refer to a ZIP file, then a new ZIP archive is appended to the file.
This is meant
for adding a ZIP archive to another file (such as python.exe).
If mode is 'a' and the file does not exist at all,
it is created.
If mode is 'r' or 'a', the file should be seekable.
If ZIP_DEFLATED, ZIP_BZIP2 or ZIP_LZMA is specified but the corresponding module (zlib,
bz2 or lzma) is not available, RuntimeError is raised.
The default is ZIP_STORED.
If allowZip64 is True (the default) zipfile will create ZIP files that use the ZIP64 extensions when the zipfile
is larger than 4 GiB. If it is false zipfile will raise an exception when the ZIP file would require ZIP64
extensions.
The compresslevel parameter controls the compression level to use when writing files to the archive.
When using
ZIP_STORED or ZIP_LZMA it has no effect.
When using ZIP_BZIP2 integers 1 through 9 are accepted (see bz2 for more
information).
The strict_timestamps argument, when set to False, allows to zip files older than 1980-01-01 at the cost of setting
the timestamp to 1980-01-01.
Similar behavior occurs with files newer than 2107-12-31, the timestamp is also set
to the limit.
When mode is 'r', metadata_encoding may be set to the name of a codec, which will be used to decode metadata
such as the names of members and ZIP comments.
If the file is created with mode 'w', 'x' or 'a' and then closed without adding any files to the archive, the
appropriate ZIP structures for an empty archive will be written to the file.
ZipFile is also a context manager and therefore supports the with statement.
Note: metadata_encoding is an instance-wide setting for the ZipFile.
It is not currently possible to set this on a
per-member basis.
This attribute is a workaround for legacy implementations which produce archives with names in the current locale
encoding or code page (mostly on Windows).
According to the .ZIP standard, the encoding of metadata may be
specified to be either IBM code page (default) or UTF-8 by a flag in the archive header.
That flag takes precedence
over metadata_encoding, which is a Python-specific extension.
New in version 3.2: Added the ability to use ZipFile as a context manager.
Changed in version 3.3: Added support for bzip2 and lzma compression.
ZIP64 extensions are enabled by default.
Changed in version 3.5: Added support for writing to unseekable streams.
Added support for the 'x' mode.
Changed in version 3.6: Previously, a plain RuntimeError was raised for unrecognized compression values.
Changed in version 3.6.2: The file parameter accepts a path-like object.
Add the compresslevel parameter.
New in version 3.8: The strict_timestamps keyword-only argument
Changed in version 3.11: Added support for specifying member name encoding for reading metadata in the zipfile’s
directory and file headers.
ZipFile.close()
Close the archive file.
Return a ZipInfo object with information about the archive member name.
Calling getinfo() for a name
not currently contained in the archive will raise a KeyError.
ZipFile.infolist()
Return a list containing a ZipInfo object for each member of the archive.
The objects are in the same order as
their entries in the actual ZIP file on disk if an existing archive was opened.
ZipFile.namelist()
Return a list of archive members by name.
ZipFile.open(name, mode=’r’, pwd=None, *, force_zip64=False)
Access a member of the archive as a binary file-like object.
The mode parameter, if included, must be 'r' (the default) or 'w'.
These objects can operate
independently of the ZipFile.
With mode='w', a writable file handle is returned, which supports the write() method.
While a writable file
handle is open, attempting to read or write other files in the ZIP file will raise a ValueError.
When writing a file, if the file size is not known in advance but may exceed 2 GiB, pass force_zip64=True
to ensure that the header format is capable of supporting large files.
If the file size is known in advance, construct
a ZipInfo object with file_size set, and use that as the name parameter.
Note: The open(), read() and extract() methods can take a filename or a ZipInfo object.
You will
appreciate this when trying to read a ZIP file that contains members with duplicate names.
Removed support of mode='U'.
Use io.
TextIOWrapper for reading compressed
text files in universal newlines mode.
Changed in version 3.6: ZipFile.open() can now be used to write files into the archive with the mode='w'
option.
Calling open() on a closed ZipFile will raise a ValueError.
Previously, a
RuntimeError was raised.
ZipFile.extract(member, path=None, pwd=None)
Extract a member from the archive to the current working directory; member must be its full name or a ZipInfo
object.
Its file information is extracted as accurately as possible.
Returns the normalized path created (a directory or new file).
Note: If a member filename is an absolute path, a drive/UNC sharepoint and leading (back)slashes will be stripped,
e.g.: ///foo/bar becomes foo/bar on Unix, and C:\foo\bar becomes foo\bar on Windows.
And all
".." components in a member filename will be removed, e.g.: ../../foo../../ba..
Calling extract() on a closed ZipFile will raise a ValueError.
Previously, a
RuntimeError was raised.
Changed in version 3.6.2: The path parameter accepts a path-like object.
Warning: Never extract archives from untrusted sources without prior inspection.
It is possible that files are
created outside of path, e.g. members that have absolute filenames starting with "/" or filenames with two
dots "..".
This module attempts to prevent that.
See extract() note.
Calling extractall() on a closed ZipFile will raise a ValueError.
Previously, a
RuntimeError was raised.
Changed in version 3.6.2: The path parameter accepts a path-like object.
ZipFile.read(name, pwd=None)
Return the bytes of the file name in the archive.
The archive must be open for read or append.
Calling read() on a ZipFile
that uses a compression method other than ZIP_STORED, ZIP_DEFLATED, ZIP_BZIP2 or ZIP_LZMA will
raise a NotImplementedError.
An error will also be raised if the corresponding compression module is not
available.
Calling read() on a closed ZipFile will raise a ValueError.
Previously, a
RuntimeError was raised.
ZipFile.testzip()
Read all the files in the archive and check their CRC’s and file headers.
Return the name of the first bad file, or
else return None.
Calling testzip() on a closed ZipFile will raise a ValueError.
Previously, a
RuntimeError was raised.
ZipFile.write(filename, arcname=None, compress_type=None, compresslevel=None)
Write the file named filename to the archive, giving it the archive name arcname (by default, this will be the same
as filename, but without a drive letter and with leading path separators removed).
If given, compress_type overrides
the value given for the compression parameter to the constructor for the new entry.
Similarly, compresslevel will
override the constructor if given.
The archive must be open with mode 'w', 'x' or 'a'.
Note: The ZIP file standard historically did not specify a metadata encoding, but strongly recommended CP437
(the original IBM PC encoding) for interoperability.
Recent versions allow use of UTF-8 (only).
In this module,
UTF-8 will automatically be used to write the member names if they contain any non-ASCII characters.
It is not
possible to write member names in any encoding other than ASCII or UTF-8.
Note: Archive names should be relative to the archive root, that is, they should not start with a path separator.
Note:
If arcname (or filename, if arcname is not given) contains a null byte, the name of the file in the
archive will be truncated at the null byte.
Note: A leading slash in the filename may lead to the archive being impossible to open in some zip programs on
Windows systems.
Calling write() on a ZipFile created with mode 'r' or a closed ZipFile will raise a
ValueError.
Previously, a RuntimeError was raised.
The contents is data, which may be either a str or a bytes instance; if it is a str,
it is encoded as UTF-8 first.
If it’s an instance, at least the filename, date, and time must be given.
If it’s a name, the date and time is
set to the current date and time.
The archive must be opened with mode 'w', 'x' or 'a'.
If given, compress_type overrides the value given for the compression parameter to the constructor for the new entry,
or in the zinfo_or_arcname (if that is a ZipInfo instance).
When passing a ZipInfo instance as the zinfo_or_arcname parameter, the compression method used
will be that specified in the compress_type member of the given ZipInfo instance.
By default, the ZipInfo
constructor sets this member to ZIP_STORED.
The compress_type argument.
Calling writestr() on a ZipFile created with mode 'r' or a closed ZipFile will raise
a ValueError.
Previously, a RuntimeError was raised.
ZipFile.mkdir(zinfo_or_directory, mode=511)
Create a directory inside the archive.
If zinfo_or_directory is a string, a directory is created inside the archive with
the mode that is specified in the mode argument.
If, however, zinfo_or_directory is a ZipInfo instance then the
mode argument is ignored.
The archive must be opened with mode 'w', 'x' or 'a'.
New in version 3.11.
The following data attributes are also available:
ZipFile.filename
Name of the ZIP file.
The level of debug output to use.
This may be set from 0 (the default, no output) to 3 (the most output).
Debugging
information is written to sys.stdout.
ZipFile.comment
The comment associated with the ZIP file as a bytes object.
If assigning a comment to a ZipFile instance
created with mode 'w', 'x' or 'a', it should be no longer than 65535 bytes.
Comments longer than this will be
truncated.
Path(root, at=”)
Construct a Path object from a root zipfile (which may be a ZipFile instance or file suitable for passing to
the ZipFile constructor).
Defaults to the empty string,
indicating the root.
Path objects expose the following features of pathlib.
Path objects:
Path objects are traversable using the / operator or joinpath.
Invoke ZipFile.open() on the current path.
Allows opening for read or write, text or binary through supported
modes: ‘r’, ‘w’, ‘rb’, ‘wb’.
Positional and keyword arguments are passed through to io.
TextIOWrapper when
opened as text and ignored otherwise.
Changed in version 3.9: Added support for text and binary modes for open.
Default mode is now text.
Changed in version 3.11.2: The encoding parameter can be supplied as a positional argument without causing
a TypeError.
As it could in 3.9.
TextIOWrapper arguments, encoding included, as keywords.
Path.iterdir()
Enumerate the children of the current directory.
Path.is_dir()
Return True if the current context references a directory.
Path.is_file()
Return True if the current context references a file.
Path.exists()
Return True if the current context references a file or directory in the zip file.
Path.suffix
The file extension of the final component.
New in version 3.11: Added Path.suffix property.
The final path component, without its suffix.
New in version 3.11: Added Path.stem property.
Path.suffixes
A list of the path’s file extensions.
New in version 3.11: Added Path.suffixes property.
Path.read_text(*, **)
Read the current file as unicode text.
Positional and keyword arguments are passed through to io.
TextIOWrapper (except buffer, which is implied by the context).
Changed in version 3.11.2: The encoding parameter can be supplied as a positional argument without causing
a TypeError.
As it could in 3.9.
TextIOWrapper arguments, encoding included, as keywords.
Path.read_bytes()
Read the current file as bytes.
Path.joinpath(*other)
Return a new Path object with each of the other arguments joined.
The following are equivalent:
Changed in version 3.10: Prior to 3.10, joinpath was undocumented and accepted exactly one parameter.
The zipp project provides backports of the latest path object functionality to older Pythons.
Use zipp.
Path in place
of zipfile.
Path for early access to changes.
PyZipFile Objects
The PyZipFile constructor takes the same parameters as the ZipFile constructor, and one additional parameter,
optimize.
PyZipFile(file, mode=’r’, compression=ZIP_STORED, allowZip64=True, optimize=- 1)
New in version 3.2: The optimize parameter.
ZIP64 extensions are enabled by default.
Instances have one method in addition to those of ZipFile objects:
writepy(pathname, basename=”, filterfunc=None)
Search for files *.py and add the corresponding file to the archive.
If the optimize parameter to PyZipFile was not given or -1, the corresponding file is a *.pyc file, com-
piling if necessary.
If the optimize parameter to PyZipFile was 0, 1 or 2, only files with that optimization level (see
compile()) are added to the archive, compiling if necessary.
If pathname is a file, the filename must end with .py, and just the (corresponding *.pyc) file is added at
the top level (no path information).
If pathname is a file that does not end with .py, a RuntimeError will
be raised.
If it is a directory, and the directory is not a package directory, then all the files *.pyc are added
at the top level.
If the directory is a package directory, then all *.pyc are added under the package name as
a file path, and if any subdirectories are package directories, all of these are added recursively in sorted order.
It will be passed each path (including
each individual full file path) before it is added to the archive.
If filterfunc returns a false value, the path will
not be added, and if it is a directory its contents will be ignored.
The filterfunc parameter.
Changed in version 3.6.2: The pathname parameter accepts a path-like object.
Recursion sorts directory entries.
ZipInfo Objects
Instances of the ZipInfo class are returned by the getinfo() and infolist() methods of ZipFile objects.
Each object stores information about a single member of the ZIP archive.
There is one classmethod to make a ZipInfo instance for a filesystem file:
classmethod ZipInfo.from_file(filename, arcname=None, *, strict_timestamps=True)
Construct a ZipInfo instance for a file on the filesystem, in preparation for adding it to a zip file.
If arcname is specified, it is used as the name within the archive.
If arcname is not specified, the name will be the
same as filename, but with any drive letter and leading path separators removed.
The strict_timestamps argument, when set to False, allows to zip files older than 1980-01-01 at the cost of setting
the timestamp to 1980-01-01.
Similar behavior occurs with files newer than 2107-12-31, the timestamp is also set
to the limit.
New in version 3.6.
Changed in version 3.6.2: The filename parameter accepts a path-like object.
New in version 3.8: The strict_timestamps keyword-only argument
Instances have the following methods and attributes:
ZipInfo.is_dir()
Return True if this archive member is a directory.
This uses the entry’s name: directories should always end with /.
New in version 3.6.
ZipInfo.filename
Name of the file in the archive.
ZipInfo.date_time
The time and date of the last modification to the archive member.
Note: The ZIP file format does not support timestamps before 1980.
ZipInfo.compress_type
Type of compression for the archive member.
ZipInfo.comment
Comment for the individual archive member as a bytes object.
The PKZIP Application Note contains some comments on the internal structure of the data
contained in this bytes object.
ZipInfo.create_system
System which created ZIP archive.
ZipInfo.create_version
PKZIP version which created ZIP archive.
ZipInfo.extract_version
PKZIP version needed to extract archive.
ZipInfo.reserved
Must be zero.
ZipInfo.flag_bits
ZIP flag bits.
ZipInfo.volume
Volume number of file header.
ZipInfo.internal_attr
Internal attributes.
ZipInfo.external_attr
External file attributes.
ZipInfo.header_offset
Byte offset to the file header.
ZipInfo.
CRC
CRC-32 of the uncompressed file.
ZipInfo.compress_size
Size of the compressed data.
ZipInfo.file_size
Size of the uncompressed file.
Specify encoding of member names for -l, -e and -t.
New in version 3.11.
Decompression pitfalls
The extraction in zipfile module might fail due to some pitfalls listed below.
From file itself
Decompression may fail due to incorrect password / CRC checksum / ZIP format or unsupported compression method /
decryption.
File System limitations
Exceeding limitations on different file systems can cause decompression failed.
Such as allowable characters in the di-
rectory entries, length of the file name, length of the pathname, size of a single file, and number of files, etc.
Resources limitations
The lack of memory or disk volume would lead to decompression failed.
For example, decompression bombs (aka ZIP
bomb) apply to zipfile library that can cause disk volume exhaustion.
Default behaviors of extraction
Not knowing the default extraction behaviors can cause unexpected decompression results.
For example, when extracting
the same archive twice, it overwrites files without asking.
Use the zipfile module to read or write .zip files, or the higher-level functions in shutil.
Some facts and figures:
• reads and writes gzip, bz2 and lzma compressed archives if the respective modules are available.
Changed in version 3.3: Added support for lzma compression.
Changed in version 3.12: Archives are extracted using a filter, which makes it possible to either limit surprising/dangerous
features, or to acknowledge that they are expected and the archive is fully trusted.
By default, archives are fully trusted,
but this default is deprecated and slated to change in Python 3.14.
For detailed information on TarFile objects and the keyword
arguments that are allowed, see TarFile Objects.
Raise a FileExistsError exception
if it already exists.
Raise a FileExistsError exception if it already
exists.
Raise a FileExistsError exception if it already
exists.
Raise a FileExistsError exception if it already
exists.
The file is created if it does not exist.
Note that 'a:gz', 'a:bz2' or 'a:xz' is not possible.
If mode is not suitable to open a certain (compressed)
file for reading, ReadError is raised.
Use mode 'r' to avoid this.
If a compression method is not supported,
CompressionError is raised.
If fileobj is specified, it is used as an alternative to a file object opened in binary mode for name.
It is supposed to
be at position 0.
For modes 'w:gz', 'x:gz', 'w|gz', 'w:bz2', 'x:bz2', 'w|bz2', tarfile.open() accepts the
keyword argument compresslevel (default 9) to specify the compression level of the file.
For modes 'w:xz' and 'x:xz', tarfile.open() accepts the keyword argument preset to specify the com-
pression level of the file.
For special purposes, there is a second format for mode: 'filemode|[compression]'.
tarfile.
No random seeking will
be done on the file.
If given, fileobj may be any object that has a read() or write() method (depending on the
mode).
Use this variant in combination with e.g.
sys.stdin, a socket file object or a tape device.
However, such a TarFile object is limited in that it does not
allow random access, see Examples.
The currently possible modes:
Mode
Action
'r|*'
Open a stream of tar blocks for reading with transparent compression.
The 'x' (exclusive creation) mode was added.
The name parameter accepts a path-like object.
The compresslevel keyword argument also works for streams.
TarFile
Class for reading and writing tar archives.
Do not use this class directly: use tarfile.open() instead.
See
TarFile Objects.
tarfile.is_tarfile(name)
Return True if name is a tar archive file, that the tarfile module can read.
Support for file and file-like objects.
The tarfile module defines the following exceptions:
exception tarfile.
TarError
Base class for all tarfile exceptions.
ReadError
Is raised when a tar archive is opened, that either cannot be handled by the tarfile module or is somehow
invalid.
CompressionError
Is raised when a compression method is not supported or when the data cannot be decoded properly.
StreamError
Is raised for the limitations that are typical for stream-like TarFile objects.
ExtractError
Is raised for non-fatal errors when using TarFile.extract(), but only if TarFile.errorlevel== 2.
exception tarfile.
HeaderError
Is raised by TarInfo.frombuf() if the buffer it gets is invalid.
FilterError
Base class for members refused by filters.
AbsolutePathError
Raised to refuse extracting a member with an absolute path.
OutsideDestinationError
Raised to refuse extracting a member outside the destination directory.
SpecialFileError
Raised to refuse extracting a special file (e.g. a device or pipe).
AbsoluteLinkError
Raised to refuse extracting a symbolic link with an absolute path.
LinkOutsideDestinationError
Raised to refuse extracting a symbolic link pointing outside the destination directory.
ENCODING
The
default
character
encoding:
'utf-8'
on
Windows,
the
value
returned
by
sys.
Each of the following constants defines a tar archive format that the tarfile module is able to create.
See section
Supported tar formats for details.
USTAR_FORMAT
POSIX.1-1988 (ustar) format.
GNU_FORMAT
GNU tar format.
PAX_FORMAT
POSIX.1-2001 (pax) format.
The default format for creating archives.
This is currently PAX_FORMAT.
The default format for new archives was changed to PAX_FORMAT from GNU_FORMAT.
See also:
Module zipfile Documentation of the zipfile standard module.
Archiving operations Documentation of the higher-level archiving facilities provided by the standard shutil module.
GNU tar manual, Basic Tar Format Documentation for tar archive files, including GNU tar extensions.
TarFile Objects
The TarFile object provides an interface to a tar archive.
A tar archive is a sequence of blocks.
An archive member
(a stored file) is made up of a header block followed by data blocks.
It is possible to store a file in a tar archive several
times.
Each archive member is represented by a TarInfo object, see TarInfo Objects for details.
A TarFile object can be used as a context manager in a with statement.
It will automatically be closed when the
block is completed.
Please note that in the event of an exception an archive opened for writing will not be finalized; only
the internally used file object will be closed.
See the Examples section for a use case.
New in version 3.2: Added support for the context management protocol.
class tarfile.
All following arguments are optional and can be accessed as instance attributes as well.
It can be omitted if fileobj is given.
In this
case, the file object’s name attribute is used if it exists.
If fileobj is given, it is used for reading or writing data.
If it can be determined, mode is overridden by fileobj’s
mode.
Note: fileobj is not closed, when TarFile is closed.
It must be one of the constants USTAR_FORMAT, GNU_FORMAT
or PAX_FORMAT that are defined at module level.
When reading, format will be automatically detected, even if
different formats are present in a single archive.
The tarinfo argument can be used to replace the default TarInfo class with a different one.
If dereference is False, add symbolic and hard links to the archive.
If it is True, add the content of the target
files to the archive.
This has no effect on systems that do not support symbolic links.
If ignore_zeros is False, treat an empty block as the end of the archive.
If it is True, skip empty (and invalid)
blocks and try to get as many members as possible.
This is only useful for reading concatenated or damaged
archives.
The messages are written to sys.
stderr.
The encoding and errors arguments define the character encoding to be used for reading or writing the archive and
how conversion errors are going to be handled.
The default settings will work for most users.
See section Unicode
issues for in-depth information.
The pax_headers argument is an optional dictionary of strings which will be added as a pax global header if format
is PAX_FORMAT.
Changed in version 3.2: Use 'surrogateescape' as the default for the errors argument.
The 'x' (exclusive creation) mode was added.
The name parameter accepts a path-like object.
Alternative constructor.
The tarfile.open() function is actually a shortcut to this classmethod.
TarFile.getmember(name)
Return a TarInfo object for member name.
If name can not be found in the archive, KeyError is raised.
Note: If a member occurs more than once in the archive, its last occurrence is assumed to be the most up-to-date
version.
TarFile.getmembers()
Return the members of the archive as a list of TarInfo objects.
The list has the same order as the members in
the archive.
TarFile.getnames()
Return the members as a list of their names.
It has the same order as the list returned by getmembers().
TarFile.list(verbose=True, *, members=None)
Print a table of contents to sys.stdout.
If verbose is False, only the names of the members are printed.
If it
is True, output similar to that of ls -l is produced.
If optional members is given, it must be a subset of the list
returned by getmembers().
Added the members parameter.
TarFile.next()
Return the next member of the archive as a TarInfo object, when TarFile is opened for reading.
Return
None if there is no more available.
If optional members is
given, it must be a subset of the list returned by getmembers().
Directory information like owner, modification
time and permissions are set after all members have been extracted.
This is done to work around two problems: A
directory’s modification time is reset each time a file is created in it.
And, if a directory’s permissions do not allow
writing, extracting files to it will fail.
If numeric_owner is True, the uid and gid numbers from the tarfile are used to set the owner/group for the extracted
files.
Otherwise, the named values from the tarfile are used.
The filter argument specifies how members are modified or rejected before extraction.
See Extraction filters for
details.
It is recommended to set this explicitly depending on which tar features you need to support.
Warning: Never extract archives from untrusted sources without prior inspection.
It is possible that files are
created outside of path, e.g. members that have absolute filenames starting with "/" or filenames with two
dots "..".
Set filter='data' to prevent the most dangerous security issues, and read the Extraction filters section for
details.
Added the numeric_owner parameter.
The path parameter accepts a path-like object.
Changed in version 3.12: Added the filter parameter.
TarFile.extract(member, path=”, set_attrs=True, *, numeric_owner=False, filter=None)
Extract a member from the archive to the current working directory, using its full name.
Its file information is
extracted as accurately as possible.
You can specify a different
directory using path.
File attributes (owner, mtime, mode) are set unless set_attrs
is false.
The numeric_owner and filter arguments are the same as for extractall().
Note: The extract() method does not take care of several extraction issues.
In most cases you should consider
using the extractall() method.
Warning: See the warning for extractall().
Set filter='data' to prevent the most dangerous security issues, and read the Extraction filters section for
details.
Changed in version 3.2: Added the set_attrs parameter.
Added the numeric_owner parameter.
The path parameter accepts a path-like object.
Changed in version 3.12: Added the filter parameter.
Extract a member from the archive as a file object.
If member
is a regular file or a link, an io.
BufferedReader object is returned.
For all other existing members, None is
returned.
If member does not appear in the archive, KeyError is raised.
Return an io.
BufferedReader object.
If errorlevel is 0, errors are ignored when using TarFile.extract() and TarFile.extractall().
Nevertheless, they appear as error messages in the debug output when debug is greater than 0.
If 1 (the default),
all fatal errors are raised as OSError or FilterError exceptions.
If 2, all non-fatal errors are raised as
TarError exceptions as well.
Some exceptions, e.g. ones caused by wrong argument types or data corruption, are always raised.
Custom extraction filters should raise FilterError for fatal errors and ExtractError for non-fatal ones.
Note that when an exception is raised, the archive may be partially extracted.
It is the user’s responsibility to clean
up.
TarFile.extraction_filter
New in version 3.12.
The extraction filter used as a default for the filter argument of extract() and extractall().
The attribute may be None or a callable.
String names are not allowed for this attribute, unlike the filter argument
to extract().
If extraction_filter is None (the default), calling an extraction method without a filter argument will raise
a DeprecationWarning, and fall back to the fully_trusted filter, whose dangerous behavior matches
previous versions of Python.
In Python 3.14+, leaving extraction_filter=None will cause extraction methods to use the data filter by
default.
The attribute may be set on instances or overridden in subclasses.
It also is possible to set it on the TarFile class
itself to set a global default, although, since it affects all uses of tarfile, it is best practice to only do so in top-level
applications or site configuration.
To set a global default this way, a filter function needs to be wrapped
in staticmethod() to prevent injection of a self argument.
TarFile.add(name, arcname=None, recursive=True, *, filter=None)
Add the file name to the archive.
If given,
arcname specifies an alternative name for the file in the archive.
Directories are added recursively by default.
This
can be avoided by setting recursive to False.
Recursion adds entries in sorted order.
If filter is given, it should be
a function that takes a TarInfo object argument and returns the changed TarInfo object.
If it instead returns
None the TarInfo object will be excluded from the archive.
See Examples for an example.
Added the filter parameter.
Changed in version 3.7: Recursion adds entries in sorted order.
TarFile.addfile(tarinfo, fileobj=None)
Add the TarInfo object tarinfo to the archive.
If fileobj is given, it should be a binary file, and tarinfo.
You can create TarInfo objects directly, or by using
gettarinfo().
TarFile.gettarinfo(name=None, arcname=None, fileobj=None)
Create a TarInfo object from the result of os.stat() or equivalent on an existing file.
The file is either named
by name, or specified as a file object fileobj with a file descriptor.
If given, arcname
specifies an alternative name for the file in the archive, otherwise, the name is taken from fileobj’s name attribute,
or the name argument.
The name should be a text string.
You can modify some of the TarInfo’s attributes before you add it using addfile().
If the file object is not
an ordinary file object positioned at the beginning of the file, attributes such as size may need modifying.
This
is the case for objects such as GzipFile.
The name may also be modified, in which case arcname could be a
dummy string.
The name parameter accepts a path-like object.
In write mode, two finishing zero blocks are appended to the archive.
TarFile.pax_headers
A dictionary containing key-value pairs of pax global headers.
TarInfo Objects
A TarInfo object represents one member in a TarFile.
Aside from storing all required attributes of a file (like file
type, size, time, permissions, owner etc.), it provides some useful methods to determine its type.
It does not contain the
file’s data itself.
TarInfo objects are returned by TarFile’s methods getmember(), getmembers() and gettarinfo().
Modifying the objects returned by getmember() or getmembers() will affect all subsequent operations on the
archive.
For cases where this is unwanted, you can use copy.copy() or call the replace() method to create a
modified copy in one step.
Several attributes can be set to None to indicate that a piece of metadata is unused or unknown.
The extract() or extractall() methods will ignore the corresponding metadata, leaving it set to a default.
TarInfo(name=”)
Create a TarInfo object.
Raises HeaderError if the buffer is invalid.
TarInfo.tobuf(format=DEFAULT_FORMAT, encoding=ENCODING, errors=’surrogateescape’)
Create a string buffer from a TarInfo object.
For information on the arguments see the constructor of the
TarFile class.
Changed in version 3.2: Use 'surrogateescape' as the default for the errors argument.
A TarInfo object has the following public data attributes:
TarInfo.name: str
Name of the archive member.
TarInfo.size: int
Size in bytes.
TarInfo.mtime: int | float
Time of last modification in seconds since the epoch, as in os.stat_result.st_mtime.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
TarInfo.type
File type. type is usually one of these constants: REGTYPE, AREGTYPE, LNKTYPE, SYMTYPE, DIRTYPE,
FIFOTYPE, CONTTYPE, CHRTYPE, BLKTYPE, GNUTYPE_SPARSE.
To determine the type of a TarInfo
object more conveniently, use the is*() methods below.
TarInfo.linkname: str
Name of the target file name, which is only present in TarInfo objects of type LNKTYPE and SYMTYPE.
For symbolic links (SYMTYPE), the linkname is relative to the directory that contains the link.
For hard links
(LNKTYPE), the linkname is relative to the root of the archive.
TarInfo.uid: int
User ID of the user who originally stored this member.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
TarInfo.gid: int
Group ID of the user who originally stored this member.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
TarInfo.uname: str
User name.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
TarInfo.gname: str
Group name.
Changed in version 3.12: Can be set to None for extract() and extractall(), causing extraction to skip
applying this attribute.
TarInfo.pax_headers: dict
A dictionary containing key-value pairs of an associated pax extended header.
TarInfo.replace(name=..., mtime=..., mode=..., linkname=...,
uid=..., gid=..., uname=..., gname=...,
deep=True)
New in version 3.12.
Return a new copy of the TarInfo object with the given attributes changed.
By default, a deep copy is made.
If deep is false, the copy is shallow, i.e. pax_headers and any custom attributes
are shared with the original TarInfo object.
TarInfo.isreg()
Same as isfile().
TarInfo.isdir()
Return True if it is a directory.
TarInfo.issym()
Return True if it is a symbolic link.
TarInfo.islnk()
Return True if it is a hard link.
TarInfo.ischr()
Return True if it is a character device.
TarInfo.isblk()
Return True if it is a block device.
TarInfo.isfifo()
Return True if it is a FIFO.
TarInfo.isdev()
Return True if it is one of character device, block device or FIFO.
The tar format is designed to capture all details of a UNIX-like filesystem, which makes it very powerful.
Unfortunately,
the features make it easy to create tar files that have unintended – and possibly malicious – effects when extracted.
For ex-
ample, extracting a tar file can overwrite arbitrary files in various ways (e.g. by using absolute paths, .. path components,
or symlinks that affect later members).
In most cases, the full functionality is not needed.
Therefore, tarfile supports extraction filters: a mechanism to limit
functionality, and thus mitigate some of the security issues.
See also:
PEP 706 Contains further motivation and rationale behind the design.
The filter argument to TarFile.extract() or extractall() can be:
• the string 'fully_trusted': Honor all metadata as specified in the archive.
Should be used if the user trusts
the archive completely, or implements their own complex verification.
Honor most tar-specific features (i.e. features of UNIX-like filesystems), but block features that
are very likely to be surprising or malicious.
See tar_filter() for details.
Ignore or block most features specific to UNIX-like filesystems.
Intended for extracting cross-
platform data archives.
See data_filter() for details.
If that is also None (the default), raise a DeprecationWarning, and fall back to the 'fully_trusted'
filter, whose dangerous behavior matches previous versions of Python.
In Python 3.14, the 'data' filter will become the default instead.
It’s possible to switch earlier; see TarFile.
extraction_filter.
The callable is called just before each member is extracted, so it can take the current state of the disk into account.
It can:
– return a TarInfo object which will be used instead of the metadata in the archive, or
– return None, in which case the member will be skipped, or
– raise an exception to abort the operation or skip the member, depending on errorlevel.
Note that when
extraction is aborted, extractall() may leave the archive partially extracted.
It does not attempt to clean
up.
Default named filters
The pre-defined, named filters are available as functions, so they can be reused in custom filters:
tarfile.fully_trusted_filter(member, path)
Return member unchanged.
This implements the 'fully_trusted' filter.
This raises AbsolutePathError.
This raises OutsideDestinationError.
Return the modified TarInfo member.
In addition to what tar_filter does:
• Refuse to extract links (hard or soft) that link to absolute paths, or ones that link outside the destination.
This raises AbsoluteLinkError or LinkOutsideDestinationError.
Note that such files are refused even on platforms that do not support symbolic links.
This raises SpecialFileError.
For other files (directories), set mode to None, so that extraction methods skip applying permission bits.
• Set user and group info (uid, gid, uname, gname) to None, so that extraction methods skip setting it.
Return the modified TarInfo member.
This will
abort the extraction if TarFile.errorlevel is 1 or more.
With errorlevel=0 the error will be logged and the
member will be skipped, but extraction will continue.
Hints for further verification
Even with filter='data', tarfile is not suited for extracting untrusted files without prior inspection.
Among other
issues, the pre-defined filters do not prevent denial-of-service attacks.
Users should do additional checks.
Here is an incomplete list of things to consider:
• Extract to a new temporary directory to prevent e.g. exploiting pre-existing links, and to make it easier
to clean up after a failed extraction.
Limit the number of extracted files, total size of extracted data, filename length (including symlink length), and
size of individual files.
Tar files may contain multiple versions of the same file.
Later ones are expected to overwrite any earlier ones.
This
feature is crucial to allow updating tape archives, but can be abused maliciously.
Supporting older Python versions
Extraction filters were added to Python 3.12, but may be backported to older versions as security updates.
To check
whether the feature is available, use e.g. hasattr(tarfile, 'data_filter') rather than checking the Python
version.
The following examples show how to support Python versions with and without the feature.
Note that setting
extraction_filter will affect any subsequent operations.
The tarfile module provides a simple command-line interface to interact with tar archives.
See Extraction filters for details.
Only string names are accepted (that is,
fully_trusted, tar, and data).
It supports filenames up to a length of at best 256 characters
and linknames up to 100 characters.
The maximum file size is 8 GiB.
This is an old and limited but widely supported
format.
The GNU tar format (GNU_FORMAT).
It supports long filenames and linknames, files bigger than 8 GiB and sparse
files.
It is the de facto standard on GNU/Linux systems.
It is the most flexible format with virtually no limits.
It supports
long filenames and linknames, large files and stores pathnames in a portable way.
Modern tar implementations,
including GNU tar, bsdtar/libarchive and star, fully support extended pax features; some old or unmaintained
libraries may not, but should treat pax archives as if they were in the universally supported ustar format.
It is the
current default format for new archives.
It extends the existing ustar format with extra headers for information that cannot be stored otherwise.
There are
two flavours of pax headers: Extended headers only affect the subsequent file header, global headers are valid for
the complete archive and affect all following files.
All the data in a pax header is encoded in UTF-8 for portability
reasons.
There are some more variants of the tar format which can be read, but not created:
• The ancient V7 format.
This is the first tar format from Unix Seventh Edition, storing only regular files and direc-
tories.
Names must not be longer than 100 characters, there is no user/group name information.
Some archives
have miscalculated header checksums in case of fields with non-ASCII characters.
The SunOS tar extended format.
This format is a variant of the POSIX.1-2001 pax format, but is not compatible.
Nowadays tar archives are commonly used for file distribution and exchanging archives over networks.
One
problem of the original format (which is the basis of all other formats) is that there is no concept of supporting different
character encodings.
For example, an ordinary tar archive created on a UTF-8 system cannot be read correctly on a
Latin-1 system if it contains non-ASCII characters.
Textual metadata (like filenames, linknames, user/group names) will
appear damaged.
Unfortunately, there is no way to autodetect the encoding of an archive.
The pax format was designed
to solve this problem.
It stores non-ASCII metadata using the universal character encoding UTF-8.
The details of character conversion in tarfile are controlled by the encoding and errors keyword arguments of the
TarFile class.
The default value is sys.
Depending on whether the archive is read or written,
the metadata must be either decoded or encoded.
If encoding is not set appropriately, this conversion may fail.
The errors argument defines how characters are treated that cannot be converted.
Possible values are listed in section
Error Handlers.
The default scheme is 'surrogateescape' which Python also uses for its file system calls, see File
Names, Command Line Arguments, and Environment Variables.
For PAX_FORMAT archives (the default), encoding is generally not needed because all the metadata is stored using UTF-
8.
The modules described in this chapter parse various miscellaneous file formats that aren’t markup languages and are not
related to e-mail.
CSV format was used for many years prior to attempts to describe the format in a standardized way in RFC
4180.
The lack of a well-defined standard means that subtle differences often exist in the data produced and consumed
by different applications.
These differences can make it annoying to process CSV files from multiple sources.
Still, while
the delimiters and quoting characters vary, the overall format is similar enough that it is possible to write a single module
which can efficiently manipulate such data, hiding the details of reading and writing the data from the programmer.
The csv module implements classes to read and write tabular data in CSV format.
It allows programmers to say, “write
this data in the format preferred by Excel,” or “read data from this file which was generated by Excel,” without knowing
the precise details of the CSV format used by Excel.
Programmers can also describe the CSV formats understood by
other applications or define their own special-purpose CSV formats.
The csv module’s reader and writer objects read and write sequences.
Programmers can also read and write data
in dictionary form using the DictReader and DictWriter classes.
See also:
PEP 305 - CSV File API The Python Enhancement Proposal which proposed this addition to Python.
If csvfile is a file object, it should be opened with newline=''.1 An optional dialect parameter can
be given which is used to define a set of parameters specific to a particular CSV dialect.
It may be an instance of
a subclass of the Dialect class or one of the strings returned by the list_dialects() function.
The other
1 If newline='' is not specified, newlines embedded inside quoted fields will not be interpreted correctly, and on platforms that use \r\n
linendings on write an extra \r will be added.
It should always be safe to specify newline='', since the csv module does its own (universal) newline
handling.
For full details about the dialect and formatting parameters, see section Dialects and Formatting Parameters.
Each row read from the csv file is returned as a list of strings.
No automatic data type conversion is performed
unless the QUOTE_NONNUMERIC format option is specified (in which case unquoted fields are transformed into
floats).
A short usage example:

spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')

for row in spamreader:

print(', '.join(row))
Spam, Spam, Spam, Spam, Spam, Baked Beans
Spam, Lovely Spam, Wonderful Spam
csv.writer(csvfile, dialect=’excel’, **fmtparams)
Return a writer object responsible for converting the user’s data into delimited strings on the given file-like ob-
ject.
If csvfile is a file object, it should be opened with
newline=''Page 607, 1.
An optional dialect parameter can be given which is used to define a set of parameters
specific to a particular CSV dialect.
It may be an instance of a subclass of the Dialect class or one of the strings
returned by the list_dialects() function.
The other optional fmtparams keyword arguments can be given
to override individual formatting parameters in the current dialect.
For full details about dialects and formatting
parameters, see the Dialects and Formatting Parameters section.
To make it as easy as possible to interface with
modules which implement the DB API, the value None is written as the empty string.
While this isn’t a reversible
transformation, it makes it easier to dump SQL NULL data values to CSV files without preprocessing the data re-
turned from a cursor.fetch* call.
All other non-string data are stringified with str() before being written.
The dialect can be specified either by passing a sub-class of
Dialect, or by fmtparams keyword arguments, or both, with keyword arguments overriding parameters of the
dialect.
For full details about dialects and formatting parameters, see section Dialects and Formatting Parameters.
Delete the dialect associated with name from the dialect registry.
An Error is raised if name is not a registered
dialect name.
An Error is raised if name is not a registered dialect name.
This function
returns an immutable Dialect.
If new_limit is given, this becomes the new limit.
The fieldnames parameter is a sequence.
If fieldnames is omitted, the values in the first row of file f will be used as
the fieldnames.
Regardless of how the fieldnames are determined, the dictionary preserves their original ordering.
If a row has more fields than fieldnames, the remaining data is put in a list and stored with the fieldname specified
by restkey (which defaults to None).
If a non-blank row has fewer fields than fieldnames, the missing values are
filled-in with the value of restval (which defaults to None).
All other optional or keyword arguments are passed to the underlying reader instance.
If the argument passed to fieldnames is an iterator, it will be coerced to a list.
Changed in version 3.6: Returned rows are now of type OrderedDict.
Changed in version 3.8: Returned rows are now of type dict.
A short usage example:

reader = csv.DictReader(csvfile)

for row in reader:

print(row['first_name'], row['last_name'])

Eric Idle
John Cleese
{'first_name': 'John', 'last_name': 'Cleese'}
class csv.DictWriter(f, fieldnames, restval=”, extrasaction=’raise’, dialect=’excel’, *args, **kwds)
Create an object which operates like a regular writer but maps dictionaries onto output rows.
The fieldnames param-
eter is a sequence of keys that identify the order in which values in the dictionary passed to the writerow()
method are written to file f.
The optional restval parameter specifies the value to be written if the dictionary is
missing a key in fieldnames.
If the dictionary passed to the writerow() method contains a key not found in
fieldnames, the optional extrasaction parameter indicates what action to take.
If it is set to 'raise', the default
value, a ValueError is raised.
If it is set to 'ignore', extra values in the dictionary are ignored.
Any other
optional or keyword arguments are passed to the underlying writer instance.
Note that unlike the DictReader class, the fieldnames parameter of the DictWriter class is not optional.
If the argument passed to fieldnames is an iterator, it will be coerced to a list.
DictWriter(csvfile, fieldnames=fieldnames)
writer.writeheader()
writer.writerow({'first_name': 'Baked', 'last_name': 'Beans'})
writer.writerow({'first_name': 'Lovely', 'last_name': 'Spam'})
writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})
14.1.
Dialect
The Dialect class is a container class whose attributes contain information for how to handle doublequotes,
whitespace, delimiters, etc.
Due to the lack of a strict CSV specification, different applications produce subtly
different CSV data.
Dialect instances define how reader and writer instances behave.
It is registered with the dialect name
'excel'.
The excel_tab class defines the usual properties of an Excel-generated TAB-delimited file.
It is registered with
the dialect name 'excel-tab'.
It is registered with the dialect name 'unix'.
New in version 3.2.
Sniffer
The Sniffer class is used to deduce the format of a CSV file.
The Sniffer class provides two methods:
sniff(sample, delimiters=None)
Analyze the given sample and return a Dialect subclass reflecting the parameters found.
If the optional
delimiters parameter is given, it is interpreted as a string containing possible valid delimiter characters.
Inspecting each column, one of two key criteria will be considered to estimate if
the sample contains a header:
• the second through n-th rows contain numeric values
• the second through n-th rows contain strings where at least one value’s length differs from that of the
putative header of that column.
Twenty rows after the first row are sampled; if more than half of columns + rows meet the criteria, True is
returned.
Note: This method is a rough heuristic and may produce both false positives and negatives.
Instructs writer objects to quote all fields.
Instructs reader objects to convert all non-quoted fields to type float.
When the current delimiter occurs in output data it is preceded by
the current escapechar character.
If escapechar is not set, the writer will raise Error if any characters that require
escaping are encountered.
Instructs reader objects to perform no special processing of quote characters.
This is similar to QUOTE_ALL, except that if a
field value is None an empty (unquoted) string is written.
Instructs reader objects to interpret an empty (unquoted) field as None and to otherwise behave as QUOTE_ALL.
This is similar to
QUOTE_NONNUMERIC, except that if a field value is None an empty (unquoted) string is written.
Instructs reader objects to interpret an empty (unquoted) string as None and to otherwise behave as
QUOTE_NONNUMERIC.
The csv module defines the following exception:
exception csv.
Error
Raised by any of the functions when an error is detected.
A dialect is a subclass of the Dialect class having a set of specific methods and a single validate()
method.
When creating reader or writer objects, the programmer can specify a string or a subclass of the Dialect
class as the dialect parameter.
In addition to, or instead of, the dialect parameter, the programmer can also specify
individual formatting parameters, which have the same names as the attributes defined below for the Dialect class.
Dialects support the following attributes:
Dialect.delimiter
A one-character string used to separate fields.
It defaults to ','.
When True, the char-
acter is doubled.
When False, the escapechar is used as a prefix to the quotechar.
It defaults to True.
On output, if doublequote is False and no escapechar is set, Error is raised if a quotechar is found in a field.
Dialect.escapechar
A one-character string used by the writer to escape the delimiter if quoting is set to QUOTE_NONE and the quotechar
if doublequote is False.
On reading, the escapechar removes any special meaning from the following character.
It defaults to None, which disables escaping.
Changed in version 3.11: An empty escapechar is not allowed.
The string used to terminate lines produced by the writer.
It defaults to '\r\n'.
Note: The reader is hard-coded to recognise either '\r' or '\n' as end-of-line, and ignores lineterminator.
This behavior may change in the future.
Dialect.quotechar
A one-character string used to quote fields containing special characters, such as the delimiter or quotechar, or
which contain new-line characters.
It defaults to '"'.
Changed in version 3.11: An empty quotechar is not allowed.
Dialect.quoting
Controls when quotes should be generated by the writer and recognised by the reader.
It can take on any of the
QUOTE_* constants (see section Module Contents) and defaults to QUOTE_MINIMAL.
Dialect.skipinitialspace
When True, spaces immediately following the delimiter are ignored.
The default is False.
Dialect.strict
When True, raise exception Error on bad CSV input.
The default is False.
Usually you should call this as
next(reader).
Reader objects have the following public attributes:
csvreader.dialect
A read-only description of the dialect in use by the parser.
The number of lines read from the source iterator.
This is not the same as the number of records returned, as
records can span multiple lines.
Writer Objects
Writer objects (DictWriter instances and objects returned by the writer() function) have the following public
methods.
A row must be an iterable of strings or numbers for Writer objects and a dictionary mapping fieldnames to
strings or numbers (by passing them through str() first) for DictWriter objects.
Note that complex numbers are
written out surrounded by parens.
This may cause some problems for other programs which read CSV files (assuming
they support complex numbers at all).
Return the return
value of the call to the write method of the underlying file object.
Changed in version 3.5: Added support of arbitrary iterables.
csvwriter.writerows(rows)
Write all elements in rows (an iterable of row objects as described above) to the writer’s file object, formatted
according to the current dialect.
Writer objects have the following public attribute:
csvwriter.dialect
A read-only description of the dialect in use by the writer.
DictWriter objects have the following public method:
DictWriter.writeheader()
Write a row with the field names (as specified in the constructor) to the writer’s file object, formatted according to
the current dialect.
Return the return value of the csvwriter.writerow() call used internally.
New in version 3.2.
Changed in version 3.8:
writeheader() now also returns the value returned by the csvwriter.
The corresponding simplest possible writing example is:
14.1.
Since open() is used to open a CSV file for reading, the file will by default be decoded into unicode using the system
default encoding (see locale.getencoding()).
The same applies to writing in something other than the system default encoding: specify the encoding argument when
opening the output file.
And while the module doesn’t directly support parsing strings, it can easily be done:
import csv
for row in csv.reader(['one,two,three']):
print(row)
14.2 configparser — Configuration file parser
Source code: Lib/configparser.py
This module provides the ConfigParser class which implements a basic configuration language which provides a
structure similar to what’s found in Microsoft Windows INI files.
You can use this to write Python programs which can
be customized by end users easily.
Note: This library does not interpret or write the value-type prefixes used in the Windows Registry extended version of
INI syntax.
It is specifically designed to be
an improved version of INI.
Module shlex Support for creating Unix shell-like mini-languages which can also be used for application configuration
files.
Module json The json module implements a subset of JavaScript syntax which is sometimes used for configuration,
but does not support comments.
Essentially, the file consists of sections, each of which
contains keys with values.
As you can see, we can treat a config parser much like a dictionary.
There are differences, outlined later, but the behavior
is very close to what you would expect from a dictionary.
Now that we have created and saved a configuration file, let’s read it back and explore the data it holds.
The only bit of magic involves the DEFAULT section which
provides default values for all other sections1.
Note also that keys in sections are case-insensitive and stored in lowercase1.
It is possible to read several configurations into a single ConfigParser, where the most recently added configuration
has the highest priority.
Any conflicting keys are taken from the more recent configuration while the previously existing
keys are retained.
This means
that if you need other datatypes, you should convert on your own:

9.0
Since this task is so common, config parsers provide a range of handy getter methods to handle integers, floats
and booleans.
The last one is the most interesting because simply passing the value to bool() would do no
good since bool('False') is still True.
This is why config parsers also provide getboolean().
This
method is case-insensitive and recognizes Boolean values from 'yes'/'no', 'on'/'off', 'true'/'false' and
'1'/'0'Page 616, 1.
For example:
False
True
True
Apart from getboolean(), config parsers also provide equivalent getint() and getfloat() methods.
You can
register your own converters and customize the provided ones.
Page 616, 1
14.2.3 Fallback Values
As with a dictionary, you can use a section’s get() method to provide fallback values:
'50022'
'9'
'3des-cbc'
Please note that default values have precedence over fallback values.
For instance, in our example the
'CompressionLevel' key was specified only in the 'DEFAULT' section.
A configuration file consists of sections, each led by a [section] header, followed by key/value entries separated by a
specific string (= or : by defaultPage 616, 1).
By default, section names are case sensitive but keys are notPage 616, 1.
Leading
and trailing whitespace is removed from keys and values.
Values can be omitted if the parser is configured to allow
itPage 616, 1, in which case the key/value delimiter may also be left out.
Values can also span multiple lines, as long as they
are indented deeper than the first line of the value.
Depending on the parser’s mode, blank lines may be treated as parts
of multiline values or ignored.
By default, a valid section name can be any string that does not contain ‘\n’ or ‘]’.
To change this, see ConfigParser.
SECTCRE.
Configuration files may include comments, prefixed by specific characters (# and ; by defaultPage 616, 1).
Comments may
appear on their own on an otherwise empty line, possibly indented.
True
does_that_mean_anything_special = False
purpose = formatting for readability
multiline_values = are
handled just fine as
long as they are indented
deeper than the first line
of a value
# Did I mention we can indent comments, too?
14.2.5 Interpolation of values
On top of the core functionality, ConfigParser supports interpolation.
This means values can be preprocessed before
returning them from get() calls.
BasicInterpolation
The default implementation used by ConfigParser.
It enables values to contain format strings which refer to
other values in the same section, or values in the special default sectionPage 616, 1.
Additional default values can be
provided on initialization.
All interpolations are done on demand so keys used in the chain of references do not
have to be specified in any specific order in the configuration file.
With interpolation set to None, the parser would simply return %(my_dir)s/Pictures as the value
of my_pictures and %(home_dir)s/lumberjack as the value of my_dir.
ExtendedInterpolation
An alternative handler for interpolation which implements a more advanced syntax, used for instance in zc.
buildout.
Extended interpolation is using ${section:option} to denote a value from a foreign section.
Interpolation can span multiple levels.
For convenience, if the section: part is omitted, interpolation defaults
to the current section (and possibly the default values from the special section).
Mapping protocol access is a generic name for functionality that enables using custom objects as if they were dictionaries.
In case of configparser, the mapping interface implementation is using the parser['section']['option']
notation.
parser['section'] in particular returns a proxy for the section’s data in the parser.
This means that the values are
not copied but they are taken from the original parser on demand.
What’s even more important is that when values are
changed on a section proxy, they are actually mutated in the original parser.
The mapping interface is complete and
adheres to the MutableMapping ABC.
However, there are a few differences that should be taken into account:
• By default, all keys in sections are accessible in a case-insensitive mannerPage 616, 1.
E.g. for option in
parser["section"] yields only optionxform’ed option key names.
This means lowercased keys by de-
fault.
All sections include DEFAULTSECT values as well which means that .clear() on a section may not leave the
section visibly empty.
This is because default values cannot be deleted from the section (because technically they
are not there).
If they are overridden in the section, deleting causes the default value to be visible again.
Trying to
delete a default value causes a KeyError.
Note how-
ever that the section-level get() methods are compatible both with the mapping protocol and the classic config-
parser API.
• parser.items() is compatible with the mapping protocol (returns a list of section_name, section_proxy
pairs including the DEFAULTSECT).
However, this method can also be invoked with arguments: parser.
items(section, raw, vars).
The latter call returns a list of option, value pairs for a specified section,
with all interpolations expanded (unless raw=True is provided).
The mapping protocol is implemented on top of the existing legacy API so that subclasses overriding the original interface
still should have mappings working as expected.
The default functionality is mainly dictated by historical
background and it’s very likely that you will want to customize some of the features.
The most common way to change the way a specific config parser works is to use the __init__() options:
• defaults, default value: None
This option accepts a dictionary of key-value pairs which will be initially put in the DEFAULT section.
This
makes for an elegant way to support concise configuration files that don’t specify values which are the same as the
documented default.
Hint: if you want to specify default values for a specific section, use read_dict() before you read the actual
file.
With the standard dictionary, every section is stored in the order they were added to the parser.
Same goes
for options within sections.
An alternative dictionary type can be used for example to sort sections and options on write-back.
Please note: there are ways to add a set of key-value pairs in a single operation.
When you use a regular dictionary
in those operations, the order of the keys will be ordered.
Some configuration files are known to include settings without values, but which otherwise conform to the syntax
supported by configparser.
The first occurrence of a delimiting sub-
string on a line is considered a delimiter.
This means values (but not keys) can contain the delimiters.
See also the space_around_delimiters argument to ConfigParser.write().
By default inline comments are disabled and '#'
and ';' are used as prefixes for whole line comments.
Changed
in
version
3.2:
In
previous
versions
of
configparser
behaviour
matched
comment_prefixes=('#',';') and inline_comment_prefixes=(';',).
Please note that config parsers don’t support escaping of comment prefixes so using inline_comment_prefixes may
prevent users from specifying option values with characters used as comment prefixes.
When in doubt, avoid setting
inline_comment_prefixes.
True
When set to True, the parser will not allow for any section or option duplicates while reading from a single source
(using read_file(), read_string() or read_dict()).
It is recommended to use strict parsers in new
applications.
Changed in version 3.2: In previous versions of configparser behaviour matched strict=False.
By default parsers also let empty lines to be parts of values.
At the same time, keys can be arbitrarily indented
themselves to improve readability.
In consequence, when configuration files get big and complex, it is easy for the
user to lose track of the file structure.
Take for instance:
[Section]
key = multiline
value with a gotcha
this = is still a part of the multiline value of 'key'
This can be especially problematic for the user to see if she’s using a proportional font to edit the file.
That is why
when your application does not need values with empty lines, you should consider disallowing them.
This will make
14.2.
In the example above, it would produce two keys, key and this.
The convention of allowing a special section of default values for other sections or interpolation purposes is a
powerful concept of this library, letting users create complex declarative configurations.
This section is normally
called "DEFAULT" but this can be customized to point to any other valid section name.
Some typical values
include: "general" or "common".
The name provided is used for recognizing default sections when reading
from any source and is used when writing configuration back to a file.
Its current value can be retrieved using the
parser_instance.default_section attribute and may be modified at runtime (i.e. to convert files from
one format to another).
BasicInterpolation
Interpolation behaviour may be customized by providing a custom handler through the interpolation argument.
None can be used to turn off interpolation completely, ExtendedInterpolation() provides a more
advanced variant inspired by zc.buildout.
More on the subject in the dedicated documentation section.
RawConfigParser has a default value of None.
• converters, default value: not set
Config parsers provide option value getters that perform type conversion.
By default getint(), getfloat(),
and getboolean() are implemented.
Should other getters be desirable, users may define them in a
subclass or pass a dictionary where each key is a name of the converter and each value is a callable
implementing said conversion.
For instance, passing {'decimal': decimal.
Decimal} would add
getdecimal() on both the parser object and all section proxies.
In other words, it will be pos-
sible to write both parser_instance.getdecimal('section', 'key', fallback=0) and
parser_instance['section'].getdecimal('key', 0).
If the converter needs to access the state of the parser, it can be implemented as a method on a config parser subclass.
If the name of this method starts with get, it will be available on all section proxies, in the dict-compatible form
(see the getdecimal() example above).
More advanced customization may be achieved by overriding default values of these parser attributes.
The defaults are
defined on the classes, so they may be overridden by subclasses or by attribute assignment.
ConfigParser.
BOOLEAN_STATES
By default when using getboolean(), config parsers consider the following values True: '1', 'yes',
'true', 'on' and the following values False: '0', 'no', 'false', 'off'.
You can override this by
specifying a custom dictionary of strings and their Boolean outcomes.
This method transforms option names on every read, get, or set operation.
The default converts the name to
lowercase.
This also means that when a configuration file gets written, all keys will be lowercase.
Override this
method if that’s unsuitable.
This should be an idempotent
function: if the name is already in canonical form, it should be returned unchanged.
ConfigParser.
SECTCRE
A compiled regular expression used to parse section headers.
The default matches [section] to the name
"section".
Whitespace is considered part of the section name, thus [ larch ] will be read as a section of
name " larch ".
Override this attribute if that’s unsuitable.
Legacy API Examples
Mainly because of backwards compatibility concerns, configparser provides also a legacy API with explicit
get/set methods.
While there are valid use cases for the methods outlined below, mapping protocol access is pre-
ferred for new projects.
The legacy API is at times more advanced, low-level and downright counterintuitive.
An example of writing to a configuration file:
import configparser
config = configparser.
RawConfigParser()
# Please note that using RawConfigParser's set functions, you can assign
# non-string values to keys internally, but will receive an error when
# attempting to write to a file or when you get it in non-raw mode.
Setting
# values using the mapping protocol or ConfigParser's set() does not allow
# such assignments to take place.
An example of reading the configuration file again:
import configparser
config = configparser.
RawConfigParser()
config.read('example.cfg')
# getfloat() raises an exception if the value is not a float
# getint() and getboolean() also do this for their respective types
a_float = config.getfloat('Section1', 'a_float')
an_int = config.getint('Section1', 'an_int')
print(a_float + an_int)
# Notice that the next output does not interpolate '%(bar)s' or '%(baz)s'.
ConfigParser()
cfg.read('example.cfg')
# Set the optional *raw* argument of get() to True if you wish to disable
# interpolation in a single get operation.
They are used in interpolation if an option used is not defined
elsewhere.
ConfigParser Objects
class configparser.
ConfigParser(defaults=None, dict_type=dict, allow_no_value=False, delimiters=(’=’,
’:’), comment_prefixes=(’#’, ’;’), inline_comment_prefixes=None,
strict=True, empty_lines_in_values=True,
default_section=configparser.
The main configuration parser.
When defaults is given, it is initialized into the dictionary of intrinsic defaults.
When dict_type is given, it will be used to create the dictionary objects for the list of sections, for the options within
a section, and for the default values.
When delimiters is given, it is used as the set of substrings that divide keys from values.
When comment_prefixes
is given, it will be used as the set of substrings that prefix comments in otherwise empty lines.
Comments can be
indented.
When inline_comment_prefixes is given, it will be used as the set of substrings that prefix comments in
non-empty lines.
When strict is True (the default), the parser won’t allow for any section or option duplicates while reading from a
single source (file, string or dictionary), raising DuplicateSectionError or DuplicateOptionError.
When empty_lines_in_values is False (default: True), each empty line marks the end of an option.
When allow_no_value is True (default:
False), options without values are accepted; the value held for these is None and they are serialized without the
trailing delimiter.
When default_section is given, it specifies the name for the special section holding default values for other sections
and interpolation purposes (normally named "DEFAULT").
This value can be retrieved and changed at runtime
using the default_section instance attribute.
This won’t re-evaluate an already parsed config file, but will be
used when writing parsed settings to a new config file.
Interpolation behaviour may be customized by providing a custom handler through the interpolation argument.
None can be used to turn off interpolation completely, ExtendedInterpolation() provides a more ad-
vanced variant inspired by zc.buildout.
More on the subject in the dedicated documentation section.
All option names used in interpolation will be passed through the optionxform() method just like any other
option name reference.
For example, using the default implementation of optionxform() (which converts
option names to lower case), the values foo %(bar)s and foo %(BAR)s are equivalent.
When converters is given, it should be a dictionary where each key represents the name of a type converter and
each value is a callable implementing the conversion from string to the desired datatype.
Every converter gets its
own corresponding get*() method on the parser object and section proxies.
Changed in version 3.1: The default dict_type is collections.
OrderedDict.
Changed in version 3.2:
allow_no_value, delimiters, comment_prefixes, strict, empty_lines_in_values, de-
fault_section and interpolation were added.
The converters argument was added.
Changed in version 3.7: The defaults argument is read with read_dict(), providing consistent behavior across
the parser: non-string keys and values are implicitly converted to strings.
The default dict_type is dict, since it now preserves insertion order.
If a section by the given name already exists,
DuplicateSectionError is raised.
If the default section name is passed, ValueError is raised.
The name of the section must be a string; if not, TypeError is raised.
Changed in version 3.2: Non-string section names raise TypeError.
has_section(section)
Indicates whether the named section is present in the configuration.
The default section is not acknowledged.
Return a list of options available in the specified section.
If the given section exists, and contains the given option, return True; otherwise return False.
If the
specified section is None or an empty string, DEFAULT is assumed.
If filenames is a string, a bytes object or a path-like object, it is treated as a single filename.
If a file named
in filenames cannot be opened, that file will be ignored.
If none of the named files exist, the ConfigParser instance will contain an empty dataset.
An application
which requires initial values to be loaded from a file should load the required file or files using read_file()
before calling read() for any optional files:
import configparser, os
config = configparser.
The encoding parameter.
Previously, all files were read using the default encoding for
open().
The filenames parameter accepts a path-like object.
The filenames parameter accepts a bytes object.
Optional argument source specifies the name of the file being read.
If not given and f has a name attribute,
that is used for source; the default is '<???>'.
New in version 3.2: Replaces readfp().
Parse configuration data from a string.
Optional argument source specifies a context-specific name of the string passed.
If not given, '<string>'
is used.
This should commonly be a filesystem path or a URL.
New in version 3.2.
Load configuration from any object that provides a dict-like items() method.
Keys are section names,
values are dictionaries with keys and values that should be present in the section.
If the used dictionary type
preserves order, sections and their keys will be added in order.
Values are automatically converted to strings.
Optional argument source specifies a context-specific name of the dictionary passed.
If not given, <dict>
is used.
This method can be used to copy state between parsers.
New in version 3.2.
If vars is provided, it must be a dictionary.
The option is looked
up in vars (if provided), section, and in DEFAULTSECT in that order.
If the key is not found and fallback is
provided, it is used as a fallback value.
None can be provided as a fallback value.
All the '%' interpolations are expanded in the return values, unless the raw argument is true.
Values for
interpolation keys are looked up in the same manner as the option.
Arguments raw, vars and fallback are keyword only to protect users from trying to
use the third argument as the fallback fallback (especially when using the mapping protocol).
A convenience method which coerces the option in the specified section to an integer.
See get() for expla-
nation of raw, vars and fallback.
A convenience method which coerces the option in the specified section to a floating point number.
See
get() for explanation of raw, vars and fallback.
A convenience method which coerces the option in the specified section to a Boolean value.
Note that the
accepted values for the option are '1', 'yes', 'true', and 'on', which cause this method to return
True, and '0', 'no', 'false', and 'off', which cause it to return False.
These string values are
checked in a case-insensitive manner.
Any other value will cause it to raise ValueError.
See get() for
explanation of raw, vars and fallback.
When section is not given, return a list of section_name, section_proxy pairs, including DEFAULTSECT.
Otherwise, return a list of name, value pairs for the options in the given section.
Optional arguments have the
same meaning as for the get() method.
Items present in vars no longer appear in the result.
The previous behaviour mixed
actual parser options with variables provided for interpolation.
If the given section exists, set the given option to the specified value; otherwise raise NoSectionError.
option and value must be strings; if not, TypeError is raised.
This representation can be parsed by a future read() call.
If space_around_delimiters
is true, delimiters between keys and values are surrounded by spaces.
Note: Comments in the original configuration file are not preserved when writing the configuration back.
What is
considered a comment, depends on the given values for comment_prefix and inline_comment_prefix.
remove_option(section, option)
Remove the specified option from the specified section.
If the section does not exist,
raise
NoSectionError.
If the option existed to be removed, return True; otherwise return False.
remove_section(section)
Remove the specified section from the configuration.
If the section in fact existed, return True.
Transforms the option name option as found in an input file or as passed in by client code to the form that
should be used in the internal structures.
The default implementation returns a lower-case version of option;
subclasses may override this or client code can set an attribute of this name on instances to affect this behavior.
You don’t need to subclass the parser to use this method, you can also set it on an instance, to a function that
takes a string argument and returns a string.
The maximum depth for recursive interpolation for get() when the raw parameter is false.
This is relevant only
when the default interpolation is used.
RawConfigParser(defaults=None, dict_type=dict, allow_no_value=False, *,
delimiters=(’=’, ’:’), comment_prefixes=(’#’, ’;’),
inline_comment_prefixes=None, strict=True,
empty_lines_in_values=True,
default_section=configparser.
Legacy variant of the ConfigParser.
It has interpolation disabled by default and allows for non-string sec-
tion names, option names, and values via its unsafe add_section and set methods, as well as the legacy
defaults= keyword argument handling.
The default dict_type is dict, since it now preserves insertion order.
Note: Consider using ConfigParser instead which checks types of the values to be stored internally.
If you
don’t want interpolation, you can use ConfigParser(interpolation=None).
add_section(section)
Add a section named section to the instance.
If a section by the given name already exists,
DuplicateSectionError is raised.
If the default section name is passed, ValueError is raised.
Type of section is not checked which lets users create non-string named sections.
This behaviour is unsup-
ported and may cause internal errors.
If the given section exists, set the given option to the specified value; otherwise raise NoSectionError.
While it is possible to use RawConfigParser (or ConfigParser with raw parameters set to true) for
internal storage of non-string values, full functionality (including interpolation and output to files) can only
be achieved using string values.
This method lets users assign non-string values to keys internally.
This behaviour is unsupported and will
cause errors when attempting to write to a file or get it in non-raw mode.
Use the mapping protocol API
which does not allow such assignments to take place.
Error
Base class for all other configparser exceptions.
NoSectionError
Exception raised when a specified section is not found.
exception configparser.
DuplicateSectionError
Exception raised if add_section() is called with the name of a section that is already present or in strict parsers
when a section if found more than once in a single input file, string or dictionary.
New in version 3.2: Optional source and lineno attributes and arguments to __init__() were added.
DuplicateOptionError
Exception raised by strict parsers if a single option appears twice during reading from a single file, string or dictio-
nary.
This catches misspellings and case sensitivity-related errors, e.g. a dictionary may have two keys representing
the same case-insensitive configuration key.
NoOptionError
Exception raised when a specified option is not found in the specified section.
exception configparser.
InterpolationError
Base class for exceptions raised when problems occur performing string interpolation.
InterpolationDepthError
Exception raised when string interpolation cannot be completed because the number of iterations exceeds
MAX_INTERPOLATION_DEPTH.
Subclass of InterpolationError.
Exception raised when an option referenced from a value does not exist.
Subclass of InterpolationError.
InterpolationSyntaxError
Exception raised when the source text into which substitutions are made does not conform to the required syntax.
Subclass of InterpolationError.
Exception raised when attempting to parse a file which has no section headers.
ParsingError
Exception raised when errors occur attempting to parse a file.
The filename attribute and __init__() constructor argument were removed.
They have
been available using the name source since 3.2.
14.3 tomllib — Parse TOML files
New in version 3.11.
Lib/tomllib
This module provides an interface for parsing TOML (Tom’s Obvious Minimal Language, https://toml.io).
This module
does not support writing TOML.
See also:
The Tomli-W package is a TOML writer that can be used in conjunction with this module, providing a write API familiar
to users of the standard library marshal and pickle modules.
See also:
The TOML Kit package is a style-preserving TOML library with both read and write capability.
It is a recommended
replacement for this module for editing already existing TOML files.
The first argument should be a readable and binary file object.
Return a dict.
Convert TOML
types to Python using this conversion table.
parse_float will be called with the string of every TOML float to be decoded.
By default, this is equivalent to
float(num_str).
This can be used to use another datatype or parser for TOML floats (e.g. decimal.
Decimal).
The callable must not return a dict or a list, else a ValueError is raised.
A TOMLDecodeError will be raised on an invalid TOML document.
Load TOML from a str object.
Return a dict.
Convert TOML types to Python using this conversion table.
The
parse_float argument has the same meaning as in load().
A TOMLDecodeError will be raised on an invalid TOML document.
The following exceptions are available:
exception tomllib.TOMLDecodeError
Subclass of ValueError.
Examples
Parsing a TOML file:
import tomllib
with open("pyproject.toml", "rb") as f:
data = tomllib.load(f)
Parsing a TOML string:
import tomllib
toml_str = """
python-version = "3.11.0"
python-implementation = "CPython"
"""
data = tomllib.loads(toml_str)
14.3.2 Conversion Table
TOML
Python
table
dict
string
str
integer
int
float
float (configurable with parse_float)
boolean
bool
offset date-time
datetime.datetime (tzinfo attribute set to an instance of datetime.timezone)
local date-time
datetime.datetime (tzinfo attribute set to None)
local date
datetime.date
local time
datetime.time
array
list
14.3.
A netrc instance or subclass instance encapsulates data from a netrc file.
The initialization argument, if present,
specifies the file to parse.
If no argument is given, the file .netrc in the user’s home directory – as determined
by os.path.expanduser() – will be read.
Otherwise, a FileNotFoundError exception will be raised.
Parse errors will raise NetrcParseError with diagnostic information including the file name, line number, and
terminating token.
If no argument is specified on a POSIX system, the presence of passwords in the .netrc file
will raise a NetrcParseError if the file ownership or permissions are insecure (owned by a user other than
the user running the process, or accessible for read or write by any other user).
This implements security behavior
equivalent to that of ftp and other programs that use .netrc.
Added the POSIX permission check.
Changed in version 3.7: os.path.expanduser() is used to find the location of the .netrc file when file is
not passed as argument.
Changed in version 3.10: netrc try UTF-8 encoding before using locale specific encoding.
The entry in the netrc
file no longer needs to contain all tokens.
The missing tokens’ value default to an empty string.
All the tokens and
their values now can contain arbitrary characters, like whitespace and non-ASCII characters.
If the login name is
anonymous, it won’t trigger the security check.
NetrcParseError
Exception raised by the netrc class when syntactical errors are encountered in source text.
Instances of this
exception provide three interesting attributes:
msg
Textual explanation of the error.
If the netrc file did not contain
an entry for the given host, return the tuple associated with the ‘default’ entry.
Dump the class data as a string in the format of a netrc file.
The ‘default’ entry, if any, is
represented as a pseudo-host by that name.
This module supports both binary and XML plist files.
The property list (.plist) file format is a simple serialization supporting basic object types, like dictionaries, lists,
numbers and strings.
Usually the top level object is a dictionary.
To write out and to parse a plist file, use the dump() and load() functions.
To work with plist data in bytes objects, use dumps() and loads().
Values can be strings, integers, floats, booleans, tuples, lists, dictionaries (but only with string keys), bytes, bytearray
or datetime.datetime objects.
Changed in version 3.4: New API, old API deprecated.
Support for binary format plists added.
Old API removed.
See also:
PList manual page Apple’s documentation of the file format.
This module defines the following functions:
plistlib.load(fp, *, fmt=None, dict_type=dict)
Read a plist file.
Return the unpacked root object (which usually is
a dictionary).
None: Autodetect the file format
• FMT_XML: XML file format
• FMT_BINARY: Binary plist format
The dict_type is the type used for dictionaries that are read from the plist file.
XML data for the FMT_XML format is parsed using the Expat parser from xml.parsers.expat – see its
documentation for possible exceptions on ill-formed XML.
Unknown elements will simply be ignored by the plist
parser.
The parser for the binary format raises InvalidFileException when the file cannot be parsed.
New in version 3.4.
See load() for an explanation of the keyword arguments.
New in version 3.4.
Fp should be a writable, binary file object.
The fmt argument specifies the format of the plist file and can be one of the following values:
• FMT_XML: XML formatted plist file
• FMT_BINARY: Binary formatted plist file
When sort_keys is true (the default) the keys for dictionaries will be written to the plist in sorted order, otherwise
they will be written in the iteration order of the dictionary.
When skipkeys is false (the default) the function raises TypeError when a key of a dictionary is not a string,
otherwise such keys are skipped.
A TypeError will be raised if the object is of an unsupported type or a container that contains objects of un-
supported types.
An OverflowError will be raised for integer values that cannot be represented in (binary) plist files.
New in version 3.4.
plistlib.dumps(value, *, fmt=FMT_XML, sort_keys=True, skipkeys=False)
Return value as a plist-formatted bytes object.
See the documentation for dump() for an explanation of the
keyword arguments of this function.
New in version 3.4.
The following classes are available:
class plistlib.
Wraps an int.
This is used when reading or writing NSKeyedArchiver encoded data, which contains UID (see
PList manual).
It has one attribute, data, which can be used to retrieve the int value of the UID.
New in version 3.8.
The XML format for plist files.
New in version 3.4.
plistlib.
Parsing a plist:
import plistlib
plist = b"""<plist version="1.0">
<dict>
<key>foo</key>
<string>bar</string>
</dict>
</plist>"""
pl = plistlib.loads(plist)
print(pl["foo"])
14.5.
The modules described in this chapter implement various algorithms of a cryptographic nature.
They are available at the
discretion of the installation.
On Unix systems, the crypt module may also be available.
Here’s an overview:
15.1 hashlib — Secure hashes and message digests
Source code: Lib/hashlib.py
This module implements a common interface to many different secure hash and message digest algorithms.
Included are
the FIPS secure hash algorithms SHA1, SHA224, SHA256, SHA384, SHA512, (defined in the FIPS 180-4 standard),
the SHA-3 series (defined in the FIPS 202 standard) as well as RSA’s MD5 algorithm (defined in internet RFC 1321).
The terms “secure hash” and “message digest” are interchangeable.
Older algorithms were called message digests.
The
modern term is secure hash.
Note: If you want the adler32 or crc32 hash functions, they are available in the zlib module.
All return a hash object with the same simple interface.
For example: use sha256() to create a SHA-256 hash object.
You can now feed this object with bytes-like objects
(normally bytes) using the update method.
At any point you can ask it for the digest of the concatenation of the data
fed to it so far using the digest() or hexdigest() methods.
To allow multithreading, the Python GIL is released while computing a hash supplied more than 2047 bytes of data at
once in its constructor or .update method.
Constructors for hash algorithms that are always present in this module are sha1(), sha224(), sha256(),
sha384(),
sha512(),
sha3_224(),
sha3_256(),
sha3_384(),
sha3_512(),
shake_128(),
shake_256(), blake2b(), and blake2s().
These correspond to algorithms_guaranteed.
Additional algorithms may also be available if your Python distribution’s hashlib was linked against a build of OpenSSL
that provides others.
Others are not guaranteed available on all installations and will only be accessible by name via
new().
See algorithms_available.
Warning:
Some algorithms have known hash collision weaknesses (including MD5 and SHA1).
Refer to Attacks
on cryptographic hash algorithms and the hashlib-seealso section at the end of this document.
New in version 3.6: blake2b() and blake2s() were added.
All hashlib constructors take
a keyword-only argument usedforsecurity with default value True.
A false value allows the use of insecure and blocked
hashing algorithms in restricted environments.
False indicates that the hashing algorithm is not used in a security
context, e.g. as a non-cryptographic one-way compression function.
Changed in version 3.9: Hashlib now uses SHA3 and SHAKE from OpenSSL if it provides it.
Changed in version 3.12: For any of the MD5, SHA1, SHA2, or SHA3 algorithms that the linked OpenSSL does not
provide we fall back to a verified implementation from the HACL* project.
It also exists to
allow access to the above listed hashes as well as any other algorithms that your OpenSSL library may offer.
Note that ‘md5’ is in this list despite some upstream vendors offering an odd “FIPS compliant” Python build that
excludes it.
New in version 3.2.
These
names will be recognized when passed to new().
The
same algorithm may appear multiple times in this set under different names (thanks to OpenSSL).
New in version 3.2.
Hash Objects
The following values are provided as constant attributes of the hash objects returned by the constructors:
hash.digest_size
The size of the resulting hash in bytes.
A hash object has the following attributes:
hash.name
The canonical name of this hash, always lowercase and always suitable as a parameter to new() to create another
hash of this type.
Changed in version 3.4: The name attribute has been present in CPython since its inception, but until Python 3.4
was not formally specified, so may not exist on some platforms.
A hash object has the following methods:
hash.update(data)
Update the hash object with the bytes-like object.
Repeated calls are equivalent to a single call with the concatenation
of all the arguments: m.update(a); m.update(b) is equivalent to m.update(a+b).
This is a bytes object of size digest_size
which may contain bytes in the whole range from 0 to 255.
hash.hexdigest()
Like digest() except the digest is returned as a string object of double length, containing only hexadecimal
digits.
This may be used to exchange the value safely in email or other non-binary environments.
This can be used to efficiently compute the digests of data sharing a
common initial substring.
The shake_128() and shake_256() algorithms provide variable length digests with length_in_bits//2 up to 128
or 256 bits of security.
As such, their digest methods require a length.
Maximum length is not limited by the SHAKE
algorithm.
This is a bytes object of size length which
may contain bytes in the whole range from 0 to 255.
shake.hexdigest(length)
Like digest() except the digest is returned as a string object of double length, containing only hexadecimal
digits.
This may be used to exchange the value in email or other non-binary environments.
File hashing
The hashlib module provides a helper function for efficient hashing of a file or file-like object.
It accepts file objects from builtin open(),
BytesIO instances, SocketIO objects from socket.socket.makefile(), and similar.
The function may
bypass Python’s I/O and use the file descriptor from fileno() directly.
It is up to the caller to close fileobj.
Key derivation
Key derivation and key stretching algorithms are designed for secure password hashing.
Naive algorithms such as
sha1(password) are not resistant against brute-force attacks.
A good password hashing function must be tunable,
slow, and include a salt.
The function provides PKCS#5 password-based key derivation function 2.
It uses HMAC as pseudorandom func-
tion.
The string hash_name is the desired name of the hash digest algorithm for HMAC, e.g. ‘sha1’ or ‘sha256’.
Applications and libraries should limit password to a sensible length
(e.g. 1024).
The number of iterations should be chosen based on the hash algorithm and computing power.
As of 2022, hundreds
of thousands of iterations of SHA-256 are suggested.
For rationale as to why and how to choose what is best for
your application, read Appendix A.2.2 of NIST-SP-800-132.
The answers on the stackexchange pbkdf2 iterations
question explain in detail.
If dklen is None then the digest size of the hash algorithm hash_name is
used, e.g. 64 for SHA-512.
# Application specific, read above.
New in version 3.4.
Changed in version 3.12: Function now only available when Python is built with OpenSSL.
The slow pure Python
implementation has been removed.
The function provides scrypt password-based key derivation function as defined in RFC 7914.
Applications and libraries should limit password to a sensible length
(e.g. 1024).
New in version 3.6.
BLAKE2 supports keyed mode (a faster and simpler replacement for HMAC), salted hashing, personalization, and
tree hashing.
Hash objects from this module follow the API of standard library’s hashlib objects.
These functions return the corresponding hash objects for calculating BLAKE2b or BLAKE2s.
They optionally take these
general parameters:
• data: initial chunk of data to hash, which must be bytes-like object.
It can be passed only as positional argument.
BLAKE2b




BLAKE2s




Note: BLAKE2 specification defines constant lengths for salt and personalization parameters, however, for convenience,
this implementation accepts byte strings of any size up to the specified length.
If the length of the parameter is less than
specified, it is padded with zeros, thus, for example, b'salt' and b'salt\x00' is the same value.
These sizes are available as module constants described below.
See section 2.10 in BLAKE2 specification for comprehensive review of tree hashing.
Constants
blake2b.SALT_SIZE
blake2s.SALT_SIZE
Salt length (maximum length accepted by constructors).
Examples
Simple hashing
To calculate hash of some data, you should first construct a hash object by calling the appropriate constructor function
(blake2b() or blake2s()), then update it with the data by calling update() on the object, and, finally, get the
digest out of the object by calling digest() (or hexdigest() for hex-encoded string).
For example,
to replace SHA-1 with BLAKE2b without changing the size of output, we can tell BLAKE2b to produce 20-byte digests:
'd24f26cf8de66472d58d4e1b1774b4c9158b1f4c'


Hash objects with different digest sizes have completely different outputs (shorter hashes are not prefixes of longer hashes);
BLAKE2b and BLAKE2s produce different outputs even if the output length is the same:
'6fa1d8fcfd719046d762'
'eb6ec15daf9546254f0809'
'1bf21a98c78a1c376ae9'
'567004bf96e4a25773ebf4'
Keyed hashing
Keyed hashing can be used for authentication as a faster and simpler replacement for Hash-based message authentication
code (HMAC).
BLAKE2 can be securely used in prefix-MAC mode thanks to the indifferentiability property inherited
from BLAKE.
Randomized hashing is useful for
protecting against collision attacks on the hash function used in digital signatures.
Randomized hashing is designed for situations where one party, the message preparer, generates all or part of
a message to be signed by a second party, the message signer.
If the message preparer is able to find crypto-
graphic hash function collisions (i.e., two messages producing the same hash value), then they might prepare
meaningful versions of the message that would produce the same hash value and digital signature, but with
different results (e.g., transferring $1,000,000 to an account, rather than $10).
Cryptographic hash functions
have been designed with collision resistance as a major goal, but the current concentration on attacking cryp-
tographic hash functions may result in a given cryptographic hash function providing less collision resistance
than expected.
Randomized hashing offers the signer additional protection by reducing the likelihood that
a preparer can generate two or more messages that ultimately yield the same hash value during the digital
signature generation process — even if it is practical to find collisions for the hash function.
However, the use
of randomized hashing may reduce the amount of security provided by a digital signature when all portions
of the message are prepared by the signer.
In BLAKE2 the salt is processed as a one-time input to the hash function during initialization, rather than as an input to
each compression function.
Warning: Salted hashing (or just hashing) with BLAKE2 or any other general-purpose cryptographic hash function,
such as SHA-256, is not suitable for hashing passwords.
See BLAKE2 FAQ for more information.
Sometimes it is useful to force hash function to produce different digests for the same input for different purposes.
Quoting
the authors of the Skein hash function:
We recommend that all application designers seriously consider doing this; we have seen many protocols
where a hash that is computed in one part of the protocol can be used in an entirely different part because
two hash computations were done on similar or related data, and the attacker can force the application to
make the hash inputs the same.
Personalizing each hash function used in the protocol summarily stops this
type of attack.
It uses core algorithm from ChaCha cipher designed by Daniel J. Bernstein.
The stdlib implementation is based on pyblake2 module.
It was written by Dmitry Chestnykh based on C implementation
written by Samuel Neves.
The documentation was copied from pyblake2 and written by Dmitry Chestnykh.
The C code was partly rewritten for Python by Christian Heimes.
The following public domain dedication applies for both C hash function implementation, extension code, and this doc-
umentation:
To the extent possible under law, the author(s) have dedicated all copyright and related and neighboring rights
to this software to the public domain worldwide.
This software is distributed without any warranty.
You should have received a copy of the CC0 Public Domain Dedication along with this software.
Alexandr Sokolovskiy
See also:
Module hmac A module to generate message authentication codes using hashes.
Module base64 Another way to encode binary hashes for non-binary environments.
The FIPS 202 publication on the SHA-3 Standard.
If msg is present, the method
call update(msg) is made.
It may be any name suitable to hashlib.new().
Despite its argument position, it is required.
Parameter key can be a bytes or bytearray object.
Parameter msg can be of any type
supported by hashlib.
Parameter digestmod can be the name of a hash algorithm.
Deprecated since version 3.4, removed in version 3.8: MD5 as implicit default digest for digestmod is deprecated.
The digestmod parameter is now required.
Return digest of msg for given secret key and digest.
The function is equivalent to HMAC(key, msg,
digest).digest(), but uses an optimized C or inline implementation, which is faster for messages that fit
into memory.
The parameters key, msg, and digest have the same meaning as in new().
CPython implementation detail, the optimized C implementation is only used when digest is a string and name of
a digest algorithm, which is supported by OpenSSL.
New in version 3.7.
Update the hmac object with msg.
Repeated calls are equivalent to a single call with the concatenation of all the
arguments: m.update(a); m.update(b) is equivalent to m.update(a + b).
Parameter msg can be of any type supported by hashlib.
This bytes object will be the same length
as the digest_size of the digest given to the constructor.
It may contain non-ASCII bytes, including NUL bytes.
Warning:
When comparing the output of digest() to an externally supplied digest during a verification
routine, it is recommended to use the compare_digest() function instead of the == operator to reduce
the vulnerability to timing attacks.
HMAC.hexdigest()
Like digest() except the digest is returned as a string twice the length containing only hexadecimal digits.
This
may be used to exchange the value safely in email or other non-binary environments.
Warning: When comparing the output of hexdigest() to an externally supplied digest during a verification
routine, it is recommended to use the compare_digest() function instead of the == operator to reduce
the vulnerability to timing attacks.
HMAC.copy()
Return a copy (“clone”) of the hmac object.
This can be used to efficiently compute the digests of strings that share
a common initial substring.
A hash object has the following attributes:
HMAC.digest_size
The size of the resulting HMAC digest in bytes.
HMAC.block_size
The internal block size of the hash algorithm in bytes.
New in version 3.4.
HMAC.name
The canonical name of this HMAC, always lowercase, e.g. hmac-md5.
New in version 3.4.
The undocumented attributes HMAC.digest_cons, HMAC.inner, and HMAC.
This module also provides the following helper function:
hmac.compare_digest(a, b)
Return a == b.
This function uses an approach designed to prevent timing analysis by avoiding content-based
short circuiting behaviour, making it appropriate for cryptography.
Note: If a and b are of different lengths, or if an error occurs, a timing attack could theoretically reveal information
about the types and lengths of a and b—but not their values.
New in version 3.3.
The function uses OpenSSL’s CRYPTO_memcmp() internally when available.
See also:
Module hashlib The Python module providing secure hash functions.
Source code: Lib/secrets.py
The secrets module is used for generating cryptographically strong random numbers suitable for managing data such
as passwords, account authentication, security tokens, and related secrets.
In particular, secrets should be used in preference to the default pseudo-random number generator in the random
module, which is designed for modelling and simulation, not security or cryptography.
See also:
PEP 506
15.3.1 Random numbers
The secrets module provides access to the most secure source of randomness that your operating system provides.
SystemRandom
A class for generating random numbers using the highest-quality sources provided by the operating system.
See
random.
SystemRandom for additional details.
secrets.choice(sequence)
Return a randomly chosen element from a non-empty sequence.
If nbytes is None or not supplied, a reasonable
default is used.
The string has nbytes random bytes, each byte converted to two hex
digits.
If nbytes is None or not supplied, a reasonable default is used.
The text is Base64 encoded, so on average
each byte results in approximately 1.3 characters.
If nbytes is None or not supplied, a reasonable default is used.
Unfortunately, what is considered
sufficient will necessarily increase as computers get more powerful and able to make more guesses in a shorter period.
As of 2015, it is believed that 32 bytes (256 bits) of randomness is sufficient for the typical use-case expected for the
secrets module.
For those who want to manage their own token length, you can explicitly specify how much randomness is used for
tokens by giving an int argument to the various token_* functions.
That argument is taken as the number of bytes of
randomness to use.
Otherwise, if no argument is provided, or if the argument is None, the token_* functions will use a reasonable default
instead.
Note: That default is subject to change at any time, including during maintenance releases.
Other functions
secrets.compare_digest(a, b)
Return True if strings or bytes-like objects a and b are equal, otherwise False, using a “constant-time compare”
to reduce the risk of timing attacks.
See hmac.compare_digest() for additional details.
This section shows recipes and best practices for using secrets to manage a basic level of security.
Note: Applications should not store passwords in a recoverable format, whether plain text or encrypted.
They should be
salted and hashed using a cryptographically strong one-way (irreversible) hash function.
The modules described in this chapter provide interfaces to operating system features that are available on (almost) all
operating systems, such as files and a clock.
The interfaces are generally modeled after the Unix or C interfaces, but they
are available on most other systems as well.
Lib/os.py
This module provides a portable way of using operating system dependent functionality.
If you just want to read or write
a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines
in all the files on the command line see the fileinput module.
For creating temporary files and directories see the
tempfile module, and for high-level file and directory handling see the shutil module.
The design of all built-in operating system dependent modules of Python is such that as long as the same functionality
is available, it uses the same interface; for example, the function os.stat(path) returns stat information about
path in the same format (which happens to have originated with the POSIX interface).
Extensions peculiar to a particular operating system are also available through the os module, but using them is of
course a threat to portability.
All functions accepting path or file names accept both bytes and string objects, and result in an object of the same
type, if a path or file name is returned.
On WebAssembly platforms wasm32-emscripten and wasm32-wasi, large parts of the os module are not
available or behave differently.
API related to processes (e.g. fork(), execve()), signals (e.g. kill(),
wait()), and resources (e.g. nice()) are not available.
Others like getuid() and getpid() are emulated
or stubs.
Note:
All functions in this module raise OSError (or subclasses thereof) in the case of invalid or inaccessible file
names and paths, or other arguments that have the correct type, but are not accepted by the operating system.
The name of the operating system dependent module imported.
The following names have currently been regis-
tered: 'posix', 'nt', 'java'.
See also:
sys.platform has a finer granularity.
The platform module provides detailed checks for the system’s identity.
File Names, Command Line Arguments, and Environment Variables
In Python, file names, command line arguments, and environment variables are represented using the string type.
On
some systems, decoding these strings to and from bytes is necessary before passing them to the operating system.
Python
uses the filesystem encoding and error handler to perform this conversion (see sys.getfilesystemencoding()).
The filesystem encoding and error handler are configured at Python startup by the PyConfig_Read() function: see
filesystem_encoding and filesystem_errors members of PyConfig.
Changed in version 3.1: On some systems, conversion using the file system encoding may fail.
In this case, Python uses
the surrogateescape encoding error handler, which means that undecodable bytes are replaced by a Unicode character
U+DCxx on decoding, and these are again translated to the original byte on encoding.
The file system encoding must guarantee to successfully decode all bytes below 128.
If the file system encoding fails to
provide this guarantee, API functions can raise UnicodeError.
See also the locale encoding.
Python UTF-8 Mode
New in version 3.7: See PEP 540 for more details.
The Python UTF-8 Mode ignores the locale encoding and forces the usage of the UTF-8 encoding:
• Use UTF-8 as the filesystem encoding.
On Unix, os.device_encoding() returns 'utf-8' rather than the device encoding.
Note that the standard stream settings in UTF-8 mode can be overridden by PYTHONIOENCODING (just as they can be
in the default locale-aware mode).
As a consequence of the changes in those lower level APIs, other higher level APIs also exhibit different default behaviours:
• Command line arguments, environment variables and filenames are decoded to text using the UTF-8 encoding.
However, they still use the
strict error handler by default so that attempting to open a binary file in text mode is likely to raise an exception
rather than producing nonsense data.
The Python UTF-8 Mode is enabled if the LC_CTYPE locale is C or POSIX at Python startup (see the
PyConfig_Read() function).
It can be enabled or disabled using the -X utf8 command line option and the PYTHONUTF8 environment variable.
If the PYTHONUTF8 environment variable is not set at all, then the interpreter defaults to using the current locale settings,
unless the current locale is identified as a legacy ASCII-based locale (as described for PYTHONCOERCECLOCALE), and
locale coercion is either disabled or fails.
In such legacy locales, the interpreter will default to enabling UTF-8 mode
unless explicitly instructed not to do so.
The Python UTF-8 Mode can only be enabled at the Python startup.
Its value can be read from sys.flags.
See also the UTF-8 mode on Windows and the filesystem encoding and error handler.
See also:
PEP 686 Python 3.15 will make Python UTF-8 Mode default.
Availability: Unix, not Emscripten, not WASI.
For exam-
ple, environ['HOME'] is the pathname of your home directory (on some platforms), and is equivalent to
getenv("HOME") in C.
This mapping is captured the first time the os module is imported, typically during Python startup as part of
processing site.py.
Changes to the environment made after this time are not reflected in os.environ, except
for changes made by modifying os.environ directly.
This mapping may be used to modify the environment as well as query the environment.
On Unix, keys and values use sys.getfilesystemencoding() and 'surrogateescape' error han-
dler.
Use environb if you would like to use a different encoding.
On Windows, the keys are converted to uppercase.
This also applies when getting, setting, or deleting an item.
For
example, environ['monty'] = 'python' maps the key 'MONTY' to the value 'python'.
Note: Calling putenv() directly does not change os.environ, so it’s better to modify os.environ.
Note: On some platforms, including FreeBSD and macOS, setting environ may cause memory leaks.
Refer to
the system documentation for putenv().
You can delete items in this mapping to unset environment variables.
Changed in version 3.9: Updated to support PEP 584’s merge (|) and update (|=) operators.
New in version 3.2.
These functions are described in Files and Directories.
New in version 3.2.
Support added to accept objects implementing the os.
PathLike interface.
New in version 3.2.
Support added to accept objects implementing the os.
PathLike interface.
If str or bytes is passed in, it is returned unchanged.
Otherwise __fspath__() is called and its value is
returned as long as it is a str or bytes object.
In all other cases, TypeError is raised.
New in version 3.6.
PathLike
An abstract base class for objects representing a file system path, e.g. pathlib.
PurePath.
New in version 3.6.
The method should only return a str or bytes object, with the preference being for str.
os.getenv(key, default=None)
Return the value of the environment variable key as a string if it exists, or default if it doesn’t.
Note
that since getenv() uses os.environ, the mapping of getenv() is similarly also captured on import, and
the function may not reflect future environment changes.
On
Unix,
keys
and
values
are
decoded
with
sys.getfilesystemencoding()
and
'surrogateescape' error handler.
Use os.getenvb() if you would like to use a different encod-
ing.
Note that since getenvb() uses os.environb, the mapping of getenvb() is similarly also captured on
import, and the function may not reflect future environment changes.
Availability: Unix.
New in version 3.2.
By default,
when env is None, environ is used.
New in version 3.2.
This corresponds to the “set id” bit on the file being executed
in the current process.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix.
The function is a stub on Emscripten and WASI, see WebAssembly platforms for more information.
If group is not in the list, it is included; typically, group is specified as
the group ID field from the password record for user, because that group ID will otherwise be potentially omitted.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
Availability: Unix, not Emscripten, not WASI.
Note:
On macOS, getgroups() behavior differs somewhat from other Unix platforms.
If the Python in-
terpreter was built with a deployment target of 10.5 or earlier, getgroups() returns the list of effective
group ids associated with the current user process; this list is limited to a system-defined number of entries,
typically 16, and may be modified by calls to setgroups() if suitably privileged.
If built with a deploy-
ment target greater than 10.5, getgroups() returns the current group access list for the user associated
with the effective user id of the process; the group access list may change over the lifetime of the process, it
is not affected by calls to setgroups(), and its length is not limited to 16.
The deployment target value,
MACOSX_DEPLOYMENT_TARGET, can be obtained with sysconfig.get_config_var().
For most purposes, it is more
useful to use getpass.getuser() since the latter checks the environment variables LOGNAME or USERNAME
to find out who the user is, and falls back to pwd.getpwuid(os.getuid())[0] to get the login name of
the current real user id.
Availability: Unix, Windows, not Emscripten, not WASI.
If pid is 0, the process group id of the current
process is returned.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
The function is a stub on Emscripten and WASI, see WebAssembly platforms for more information.
When the parent process has exited, on Unix the id returned is the one of the init
process (1), on Windows it is still the same id, which may be already reused by another process.
Availability: Unix, Windows, not Emscripten, not WASI.
Changed in version 3.2: Added support for Windows.
The value which is one of PRIO_PROCESS, PRIO_PGRP, or PRIO_USER,
and who is interpreted relative to which (a process identifier for PRIO_PROCESS, process group identifier for
PRIO_PGRP, and a user ID for PRIO_USER).
A zero value for who denotes (respectively) the calling process,
the process group of the calling process, or the real user ID of the calling process.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
PRIO_PROCESS
os.
PRIO_PGRP
os.PRIO_USER
Parameters for the getpriority() and setpriority() functions.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
PRIO_DARWIN_PROCESS
os.
PRIO_DARWIN_BG
os.PRIO_DARWIN_NONUI
Parameters for the getpriority() and setpriority() functions.
Availability: Unix, not Emscripten, not WASI.
New in version 3.2.
Availability: Unix, not Emscripten, not WASI.
New in version 3.2.
The function is a stub on Emscripten and WASI, see WebAssembly platforms for more information.
New in version 3.2.
Such changes to the environment affect subprocesses
started with os.system(), popen() or fork() and execv().
Assignments to items in os.environ are automatically translated into corresponding calls to putenv(); how-
ever, calls to putenv() don’t update os.environ, so it is actually preferable to assign to items of os.
This also applies to getenv() and getenvb(), which respectively use os.environ and os.
environb in their implementations.
Note: On some platforms, including FreeBSD and macOS, setting environ may cause memory leaks.
Refer to
the system documentation for putenv().
Raises an auditing event os.putenv with arguments key, value.
The function is now always available.
Availability: Unix, not Emscripten, not WASI.
This operation is typically available only to the superuser.
Availability: Unix, not Emscripten, not WASI.
Note: On macOS, the length of groups may not exceed the system-defined maximum number of effective group
ids, typically 16.
See the documentation for getgroups() for cases where it may not return the same group list
set by calling setgroups().
See the setns(2) and namespaces(7) man pages
for more details.
If fd refers to a /proc/pid/ns/ link, setns() reassociates the calling thread with the namespace associated
with that link, and nstype may be set to one of the CLONE_NEW* constants to impose constraints on the operation
(0 means no constraints).
Since Linux 5.8, fd may refer to a PID file descriptor obtained from pidfd_open().
In this case, setns()
reassociates the calling thread into one or more of the same namespaces as the thread referred to by fd.
The caller’s memberships in
unspecified namespaces are left unchanged.
New in version 3.12.
See also:
The unshare() function.
See
the Unix manual for the semantics.
Availability: Unix, not Emscripten, not WASI.
os.setpgid(pid, pgrp, /)
Call the system call setpgid() to set the process group id of the process with id pid to the process group with
id pgrp.
See the Unix manual for the semantics.
Availability: Unix, not Emscripten, not WASI.
The value which is one of PRIO_PROCESS, PRIO_PGRP, or PRIO_USER,
and who is interpreted relative to which (a process identifier for PRIO_PROCESS, process group identifier for
PRIO_PGRP, and a user ID for PRIO_USER).
A zero value for who denotes (respectively) the calling process,
the process group of the calling process, or the real user ID of the calling process.
The default priority is 0; lower priorities cause more favorable scheduling.
New in version 3.3.
New in version 3.2.
New in version 3.2.
See the Unix manual for the semantics.
Availability: Unix, not Emscripten, not WASI.
See the Unix manual for the semantics.
Availability: Unix, not Emscripten, not WASI.
os.setuid(uid, /)
Set the current process’s user id.
Availability: Unix, not Emscripten, not WASI.
os.strerror(code, /)
Return the error message corresponding to the error code in code.
On platforms where strerror() returns
NULL when given an unknown error number, ValueError is raised.
False on Windows).
New in version 3.2.
The function is a stub on Emscripten and WASI, see WebAssembly platforms for more information.
The return value is an object with five attributes:
• sysname - operating system name
• nodename - name of machine on network (implementation-defined)
16.1.
Some systems truncate nodename to 8 characters or to the leading component; a better way to get the hostname
is socket.gethostname() or even socket.gethostbyaddr(socket.gethostname()).
Unset (delete) the environment variable named key.
Such changes to the environment affect subprocesses started
with os.system(), popen() or fork() and execv().
Deletion of items in os.environ is automatically translated into a corresponding call to unsetenv(); how-
ever, calls to unsetenv() don’t update os.environ, so it is actually preferable to delete items of os.
Raises an auditing event os.unsetenv with argument key.
The function is now always available and is also available on Windows.
See the
unshare(2) man page for more details.
The flags argument is a bit mask, combining zero or more of the
CLONE_* constants, that specifies which parts of the execution context should be unshared from their existing
associations and moved to a new namespace.
If the flags argument is 0, no changes are made to the calling process’s
execution context.
Availability: Linux >= 2.6.16.
New in version 3.12.
See also:
The setns() function.
Flags to the unshare() function, if the implementation supports them.
See unshare(2) in the Linux manual for
their exact effect and availability.
File Object Creation
These functions create new file objects.
This is an alias of the open() built-in function
and accepts the same arguments.
File Descriptor Operations
These functions operate on I/O streams referenced using file descriptors.
File descriptors are small integers corresponding to a file that has been opened by the current process.
For example,
standard input is usually file descriptor 0, standard output is 1, and standard error is 2.
Further files opened by a process
will then be assigned 3, 4, 5, and so forth.
The name “file descriptor” is slightly deceptive; on Unix platforms, sockets and
pipes are also referenced by file descriptors.
The fileno() method can be used to obtain the file descriptor associated with a file object when required.
Note that
using the file descriptor directly will bypass the file object methods, ignoring aspects such as internal buffering of data.
Note:
This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.
To close a “file object” returned by the built-in function open() or by popen() or
fdopen(), use its close() method.
os.closerange(fd_low, fd_high, /)
Close all file descriptors from fd_low (inclusive) to fd_high (exclusive), ignoring errors.
Equivalent to (but much
faster than):
for fd in range(fd_low, fd_high):
try:
os.close(fd)
except OSError:

Copy count bytes from file descriptor src, starting from offset offset_src, to file descriptor dst, starting from offset
offset_dst.
If offset_src is None, then src is read from the current position; respectively for offset_dst.
In Linux kernel older than 5.3, the files pointed by src and dst must reside in the same filesystem, otherwise an
OSError is raised with errno set to errno.
EXDEV.
This copy is done without the additional cost of transferring data from the kernel to user space and then back into
the kernel.
Additionally, some filesystems could implement extra optimizations, such as the use of reflinks (i.e.,
two or more inodes that share pointers to the same copy-on-write disk blocks; supported file systems include btrfs
and XFS) and server-side copy (in the case of NFS).
The function copies bytes between two file descriptors.
Text options, like the encoding and the line ending, are
ignored.
The return value is the amount of bytes copied.
This could be less than the amount requested.
Note:
On Linux, os.copy_file_range() should not be used for copying a range of a pseudo file from a
special filesystem like procfs and sysfs.
It will always copy no bytes and return 0 as if the file was empty because
of a known Linux kernel issue.
Availability: Linux >= 4.5 with glibc >= 2.27.
New in version 3.8.
On Unix, if the Python UTF-8 Mode is enabled, return 'UTF-8' rather than the device encoding.
Changed in version 3.10: On Unix, the function now implements the Python UTF-8 Mode.
os.dup(fd, /)
Return a duplicate of file descriptor fd.
The new file descriptor is non-inheritable.
On Windows, when duplicating a standard stream (0: stdin, 1: stdout, 2: stderr), the new file descriptor is inheri-
table.
Availability: not WASI.
The new file descriptor is now non-inheritable.
Return fd2.
The new file descriptor is
inheritable by default or non-inheritable if inheritable is False.
Availability: not WASI.
Add the optional inheritable parameter.
Return fd2 on success.
Previously, None was always returned.
See the docs for chmod() for possible values of
mode.
As of Python 3.3, this is equivalent to os.chmod(fd, mode).
Availability: Unix.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
To leave one of the ids unchanged,
set it to -1.
See chown().
As of Python 3.3, this is equivalent to os.chown(fd, uid, gid).
Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.
Availability: Unix.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
Does not force update of metadata.
Some platforms define additional names as well.
The names known to
the host operating system are given in the pathconf_names dictionary.
For configuration variables not included
in that mapping, passing an integer for name is also accepted.
If name is a string and is not known, ValueError is raised.
If a specific value for name is not supported by the
host system, even if it is included in pathconf_names, an OSError is raised with errno.
EINVAL for the
error number.
As of Python 3.3, this is equivalent to os.pathconf(fd, name).
Availability: Unix.
os.fstat(fd)
Get the status of the file descriptor fd.
Return a stat_result object.
As of Python 3.3, this is equivalent to os.stat(fd).
See also:
The stat() function.
As of Python 3.3, this is equivalent to os.statvfs(fd).
Availability: Unix.
On Unix, this calls the native fsync() function; on Windows,
the MS _commit() function.
If you’re starting with a buffered Python file object f, first do f.flush(), and then do os.fsync(f.
fileno()), to ensure that all internal buffers associated with f are written to disk.
Availability: Unix, Windows.
As of Python 3.3, this
is equivalent to os.truncate(fd, length).
Raises an auditing event os.truncate with arguments fd, length.
False if the O_NONBLOCK flag is set, True if the flag is cleared.
See also set_blocking() and socket.socket.setblocking().
Availability: Unix, Windows.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
On Windows, this function is limited to pipes.
Changed in version 3.12: Added support for pipes on Windows.
Raises an auditing event os.lockf with arguments fd, cmd, len.
Availability: Unix.
New in version 3.3.
F_ULOCK
os.F_TEST
Flags that specify what action lockf() will take.
Availability: Unix.
New in version 3.3.
Make the calling process a session leader;
make the tty the controlling tty, the stdin, the stdout, and the stderr of the calling process; close fd.
Availability: Unix, not Emscripten, not WASI.
New in version 3.11.
Valid values for whence are:
• SEEK_SET or 0 – set pos relative to the beginning of the file
• SEEK_CUR or 1 – set pos relative to the current file position
• SEEK_END or 2 – set pos relative to the end of the file
• SEEK_HOLE – set pos to the next data location, relative to pos
• SEEK_DATA – set pos to the next data hole, relative to pos
Changed in version 3.3: Add support for SEEK_HOLE and SEEK_DATA.
SEEK_SET
os.
SEEK_CUR
os.
SEEK_END
Parameters to the lseek() function and the seek() method on file-like objects, for whence to adjust the file
position indicator.
SEEK_SET Adjust the file position relative to the beginning of the file.
SEEK_CUR Adjust the file position relative to the current file position.
SEEK_END Adjust the file position relative to the end of the file.
Their values are 0, 1, and 2, respectively.
Adjust the file offset to the next location containing data, relative to the seek position.
Adjust the file offset to the next location containing a hole, relative to the seek position.
A hole is
defined as a sequence of zeros.
Note: These operations only make sense for filesystems that support them.
Availability: Linux >= 3.1, macOS, Unix
New in version 3.3.
When computing
mode, the current umask value is first masked out.
Return the file descriptor for the newly opened file.
The new
file descriptor is non-inheritable.
For a description of the flag and mode values, see the C run-time documentation; flag constants (like O_RDONLY
and O_WRONLY) are defined in the os module.
In particular, on Windows adding O_BINARY is needed to open
files in binary mode.
This function can support paths relative to directory descriptors with the dir_fd parameter.
Raises an auditing event open with arguments path, mode, flags.
The new file descriptor is now non-inheritable.
Note:
This function is intended for low-level I/O. For normal usage, use the built-in function open(), which
returns a file object with read() and write() methods (and many more).
To wrap a file descriptor in a file
object, use fdopen().
The dir_fd argument.
If the system call is interrupted and the signal handler does not raise an exception, the
function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Accepts a path-like object.
The following constants are options for the flags parameter to the open() function.
They can be combined using the
bitwise OR operator |.
Some of them are not available on all platforms.
For descriptions of their availability and use,
consult the open(2) manual page on Unix or the MSDN on Windows.
O_RDONLY
os.
O_WRONLY
os.
O_RDWR
os.
O_APPEND
os.O_CREAT
os.O_EXCL
16.1.
O_TRUNC
The above constants are available on Unix and Windows.
The above constants are only available on Unix.
Changed in version 3.3: Add O_CLOEXEC constant.
O_BINARY
os.O_NOINHERIT
os.O_SHORT_LIVED
os.
The above constants are only available on Windows.
The above constants are only available on macOS.
Add O_EVTONLY, O_FSYNC, O_SYMLINK and O_NOFOLLOW_ANY constants.
O_NOFOLLOW
os.O_NOATIME
os.O_PATH
os.O_TMPFILE
os.
O_SHLOCK
os.O_EXLOCK
The above constants are extensions and not present if they are not defined by the C library.
Add O_PATH on systems that support it.
Add O_TMPFILE, only available on Linux
Kernel 3.11 or newer.
os.openpty()
Open a new pseudo-terminal pair.
Return a pair of file descriptors (master, slave) for the pty and the tty,
respectively.
The new file descriptors are non-inheritable.
For a (slightly) more portable approach, use the pty
module.
Availability: Unix, not Emscripten, not WASI.
The new file descriptors are now non-inheritable.
Return a pair of file descriptors (r, w) usable for reading and writing, respectively.
The new file
descriptor is non-inheritable.
Availability: Unix, Windows.
Changed in version 3.4: The new file descriptors are now non-inheritable.
os.pipe2(flags, /)
Create a pipe with flags set atomically.
Return a pair of file descriptors (r, w) usable for reading and writing, respec-
tively.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
Availability: Unix, not Emscripten.
New in version 3.3.
The advice applies to the region of the file specified by fd starting at offset and continuing for len
bytes.
New in version 3.3.
POSIX_FADV_NORMAL
os.
Availability: Unix.
New in version 3.3.
Return a bytestring containing the bytes read.
If the end of the file referred to by fd has been reached, an empty
bytes object is returned.
Availability: Unix.
New in version 3.3.
Transfer data into each buffer until it is full and then move on to the next buffer in the sequence to hold
the rest of the data.
The flags argument contains a bitwise OR of zero or more of the following flags:
• RWF_HIPRI
• RWF_NOWAIT
Return the total number of bytes actually read which can be less than the total capacity of all the objects.
The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can
be used.
Combine the functionality of os.readv() and os.pread().
Availability: Linux >= 2.6.30, FreeBSD >= 6.0, OpenBSD >= 2.7, AIX >= 7.1.
Using flags requires Linux >= 4.6.
New in version 3.7.
RWF_NOWAIT
Do not wait for data which is not immediately available.
If this flag is specified, the system call will return instantly
if it would have to read data from the backing storage or wait for a lock.
If some data was successfully read, it will return the number of bytes read.
If no bytes were read, it will return -1
and set errno to errno.
EAGAIN.
Availability: Linux >= 4.14.
New in version 3.7.
High priority read/write.
Allows block-based filesystems to use polling of the device, which provides lower latency,
but may use additional resources.
Currently, on Linux, this feature is usable only on a file descriptor opened using the O_DIRECT flag.
Availability: Linux >= 4.6.
New in version 3.7.
Return the number of bytes actually written.
Availability: Unix.
New in version 3.3.
Buffers are processed in array order.
Entire contents of the first buffer is written
before proceeding to the second, and so on.
The flags argument contains a bitwise OR of zero or more of the following flags:
• RWF_DSYNC
• RWF_SYNC
• RWF_APPEND
Return the total number of bytes actually written.
The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can
be used.
Availability: Linux >= 2.6.30, FreeBSD >= 6.0, OpenBSD >= 2.7, AIX >= 7.1.
Using flags requires Linux >= 4.6.
New in version 3.7.
RWF_DSYNC
Provide a per-write equivalent of the O_DSYNC os.open() flag.
This flag effect applies only to the data range
written by the system call.
Availability: Linux >= 4.7.
New in version 3.7.
RWF_SYNC
Provide a per-write equivalent of the O_SYNC os.open() flag.
This flag effect applies only to the data range
written by the system call.
Availability: Linux >= 4.7.
New in version 3.7.
RWF_APPEND
Provide a per-write equivalent of the O_APPEND os.open() flag.
This flag is meaningful only for os.
pwritev(), and its effect applies only to the data range written by the system call.
The offset argument does not
affect the write operation; the data is always appended to the end of the file.
However, if the offset argument is -1,
the current file offset is updated.
Availability: Linux >= 4.16.
New in version 3.10.
Return a bytestring containing the bytes read.
If the end of the file referred to by fd has been reached, an empty
bytes object is returned.
Note:
This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.
To read a “file object” returned by the built-in function open() or by popen() or
fdopen(), or sys.stdin, use its read() or readline() methods.
If the system call is interrupted and the signal handler does not raise an exception, the
function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Return the number of bytes
sent.
When EOF is reached return 0.
The first function notation is supported by all platforms that define sendfile().
On Linux, if offset is given as None, the bytes are read from the current position of in_fd and the position of in_fd
is updated.
The second case may be used on macOS and FreeBSD where headers and trailers are arbitrary sequences of buffers
that are written before and after the data from in_fd is written.
It returns the same as the first case.
All platforms support sockets as out_fd file descriptor, and some platforms allow other types (e.g. regular file, pipe)
as well.
Cross-platform applications should not use headers, trailers and flags arguments.
Availability: Unix, not Emscripten, not WASI.
Note: For a higher-level wrapper of sendfile(), see socket.socket.sendfile().
New in version 3.3.
Parameters out and in was renamed to out_fd and in_fd.
os.
SF_NODISKIO
os.SF_MNOWAIT
os.
SF_SYNC
Parameters to the sendfile() function, if the implementation supports them.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
SF_NOCACHE
Parameter to the sendfile() function, if the implementation supports it.
The data won’t be cached in the virtual
memory and will be freed afterwards.
Availability: Unix, not Emscripten, not WASI.
New in version 3.11.
Set the O_NONBLOCK flag if blocking is False, clear the
flag otherwise.
See also get_blocking() and socket.socket.setblocking().
Availability: Unix, Windows.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
On Windows, this function is limited to pipes.
New in version 3.5.
Changed in version 3.12: Added support for pipes on Windows.
os.splice(src, dst, count, offset_src=None, offset_dst=None)
Transfer count bytes from file descriptor src, starting from offset offset_src, to file descriptor dst, starting from offset
offset_dst.
At least one of the file descriptors must refer to a pipe.
If offset_src is None, then src is read from the
current position; respectively for offset_dst.
The offset associated to the file descriptor that refers to a pipe must be
None.
The files pointed by src and dst must reside in the same filesystem, otherwise an OSError is raised with
errno set to errno.
EXDEV.
This copy is done without the additional cost of transferring data from the kernel to user space and then back into
the kernel.
Additionally, some filesystems could implement extra optimizations.
The copy is done as if both files
are opened as binary.
Upon successful completion, returns the number of bytes spliced to or from the pipe.
A return value of 0 means
end of input.
If src refers to a pipe, then this means that there was no data to transfer, and it would not make sense
to block because there are no writers connected to the write end of the pipe.
SPLICE_F_MOVE
os.SPLICE_F_NONBLOCK
os.
SPLICE_F_MORE
New in version 3.10.
Transfer data into each buffer
until it is full and then move on to the next buffer in the sequence to hold the rest of the data.
Return the total number of bytes actually read which can be less than the total capacity of all the objects.
The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can
be used.
Availability: Unix.
New in version 3.3.
Availability: Unix, not WASI.
If fd is not associated with a
terminal device, an exception is raised.
Availability: Unix.
os.write(fd, str, /)
Write the bytestring in str to file descriptor fd.
Return the number of bytes actually written.
Note:
This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.
To write a “file object” returned by the built-in function open() or by popen() or
fdopen(), or sys.stdout or sys.stderr, use its write() method.
If the system call is interrupted and the signal handler does not raise an exception, the
function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Buffers are
processed in array order.
Entire contents of the first buffer is written before proceeding to the second, and so on.
Returns the total number of bytes actually written.
The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can
be used.
Availability: Unix.
New in version 3.3.
Querying the size of a terminal
New in version 3.3.
The optional argument fd (default STDOUT_FILENO, or standard output) specifies which file descriptor should
be queried.
If the file descriptor is not connected to a terminal, an OSError is raised.
Availability: Unix, Windows.
Inheritance of File Descriptors
New in version 3.4.
A file descriptor has an “inheritable” flag which indicates if the file descriptor can be inherited by child processes.
Since
Python 3.4, file descriptors created by Python are non-inheritable by default.
On UNIX, non-inheritable file descriptors are closed in child processes at the execution of a new program, other file
descriptors are inherited.
On Windows, non-inheritable handles and file descriptors are closed in child processes, except for standard streams (file
descriptors 0, 1 and 2: stdin, stdout and stderr), which are always inherited.
Using spawn* functions, all inheritable
handles and all inheritable file descriptors are inherited.
Using the subprocess module, all file descriptors except
standard streams are closed, and inheritable handles are only inherited if the close_fds parameter is False.
On WebAssembly platforms wasm32-emscripten and wasm32-wasi, the file descriptor cannot be modified.
Availability: Windows.
Files and Directories
On some Unix platforms, many of these functions support one or more of these features:
• specifying a file descriptor: Normally the path argument provided to functions in the os module must be a string
specifying a file path.
However, some functions now alternatively accept an open file descriptor for their path
argument.
The function will then operate on the file referred to by the descriptor.
You can check whether or not path can be specified as a file descriptor for a particular function on your platform
using os.supports_fd.
If this functionality is unavailable, using it will raise a NotImplementedError.
If the function also supports dir_fd or follow_symlinks arguments, it’s an error to specify one of those when sup-
plying path as a file descriptor.
If the path is absolute,
dir_fd is ignored.
You can check whether or not dir_fd is supported for a particular function on your platform using os.
If it’s unavailable, using it will raise a NotImplementedError.
You can check whether or not follow_symlinks is supported for a particular function on your platform using os.
supports_follow_symlinks.
If it’s unavailable, using it will raise a NotImplementedError.
Note that most operations will use the effective uid/gid, therefore this
routine can be used in a suid/sgid environment to test if the invoking user has the specified access to path.
Return True if access is allowed, False if not.
See the Unix man page access(2)
for more information.
This function can support specifying paths relative to directory descriptors and not following symlinks.
If effective_ids is True, access() will perform its access checks using the effective uid/gid instead of the real
uid/gid.
If it is unavailable, using it will raise a NotImplementedError.
Note: Using access() to check if a user is authorized to e.g. open a file before actually doing so using open()
creates a security hole, because the user might exploit the short time interval between checking and opening the
file to manipulate it.
It’s preferable to use EAFP techniques.
For example:
16.1.
Note: I/O operations may fail even when access() indicates that they would succeed, particularly for operations
on network filesystems which may have permissions semantics beyond the usual POSIX permission-bit model.
Added the dir_fd, effective_ids, and follow_symlinks parameters.
Accepts a path-like object.
This function can support specifying a file descriptor.
The descriptor must refer to an opened directory, not an open
file.
This function can raise OSError and subclasses such as FileNotFoundError, PermissionError, and
NotADirectoryError.
Raises an auditing event os.chdir with argument path.
New in version 3.3: Added support for specifying path as a file descriptor on some platforms.
Accepts a path-like object.
UF_NODUMP
• stat.
UF_IMMUTABLE
• stat.
UF_APPEND
• stat.
UF_OPAQUE
• stat.
UF_COMPRESSED
• stat.
UF_HIDDEN
• stat.
SF_ARCHIVED
• stat.
SF_IMMUTABLE
• stat.
SF_APPEND
• stat.
SF_NOUNLINK
• stat.
This function can support not following symlinks.
Raises an auditing event os.chflags with arguments path, flags.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3: The follow_symlinks argument.
Accepts a path-like object.
S_ISUID
• stat.
S_ISGID
• stat.
S_ENFMT
• stat.
S_ISVTX
• stat.
S_IREAD
• stat.
S_IWRITE
• stat.
S_IEXEC
• stat.
S_IRWXU
• stat.
S_IRUSR
• stat.
S_IWUSR
• stat.
S_IXUSR
• stat.
S_IRWXG
• stat.
S_IRGRP
• stat.
S_IWGRP
• stat.
S_IXGRP
• stat.
S_IRWXO
• stat.
S_IROTH
• stat.
S_IWOTH
• stat.
S_IXOTH
This function can support specifying a file descriptor, paths relative to directory descriptors and not following sym-
links.
Although Windows supports chmod(), you can only set the file’s read-only flag with it (via the stat.
S_IWRITE and stat.
S_IREAD constants or a corresponding integer value).
All other bits are ignored.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd and follow_symlinks
arguments.
Accepts a path-like object.
To leave one of the ids unchanged, set it to -1.
This function can support specifying a file descriptor, paths relative to directory descriptors and not following sym-
links.
See shutil.chown() for a higher-level function that accepts names in addition to numeric ids.
Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.
Availability: Unix.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd and follow_symlinks
arguments.
Supports a path-like object.
Availability: Unix, not Emscripten, not WASI.
Accepts a path-like object.
The descriptor must
refer to an opened directory, not an open file.
As of Python 3.3, this is equivalent to os.chdir(fd).
Raises an auditing event os.chdir with argument path.
Availability: Unix.
os.getcwd()
Return a string representing the current working directory.
Changed in version 3.8: The function now uses the UTF-8 encoding on Windows, rather than the ANSI code page:
see PEP 529 for the rationale.
The function is no longer deprecated on Windows.
As of Python 3.3,
this is equivalent to os.chflags(path, flags, follow_symlinks=False).
Raises an auditing event os.chflags with arguments path, flags.
Availability: Unix, not Emscripten, not WASI.
Accepts a path-like object.
If path is a symlink, this affects the symlink rather than the target.
See the docs for chmod() for possible values of mode.
As of Python 3.3, this is equivalent to os.chmod(path,
mode, follow_symlinks=False).
Availability: Unix.
Changed in version 3.6: Accepts a path-like object.
This function will not follow symbolic links.
As of Python 3.3, this is equivalent to os.chown(path, uid, gid, follow_symlinks=False).
Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.
Availability: Unix.
Changed in version 3.6: Accepts a path-like object.
os.link(src, dst, *, src_dir_fd=None, dst_dir_fd=None, follow_symlinks=True)
Create a hard link pointing to src named dst.
This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors,
and not following symlinks.
Raises an auditing event os.link with arguments src, dst, src_dir_fd, dst_dir_fd.
Availability: Unix, Windows, not Emscripten.
Changed in version 3.2: Added Windows support.
New in version 3.3: Added the src_dir_fd, dst_dir_fd, and follow_symlinks arguments.
Return a list containing the names of the entries in the directory given by path.
If a file is removed from
or added to the directory during the call of this function, whether a name for that file be included is unspecified.
If path is of type bytes (directly or indirectly through the PathLike interface),
the filenames returned will also be of type bytes; in all other circumstances, they will be of type str.
This function can also support specifying a file descriptor; the file descriptor must refer to a directory.
Raises an auditing event os.listdir with argument path.
Note: To encode str filenames to bytes, use fsencode().
See also:
The scandir() function returns directory entries along with file attribute information, giving better performance
for many common use cases.
The path parameter became optional.
New in version 3.3: Added support for specifying path as an open file descriptor.
Accepts a path-like object.
A drive name typically looks like 'C:\\'.
Not every drive name will be associated with a volume, and some
may be inaccessible for a variety of reasons, including permissions, network connectivity or missing media.
This
function does not test for access.
May raise OSError if an error occurs collecting the drive names.
Raises an auditing event os.listdrives with no arguments.
Availability: Windows
New in version 3.12.
os.listmounts(volume)
Return a list containing the mount points for a volume on a Windows system.
Volumes may be
mounted in multiple locations or not at all.
In the latter case, the list will be empty.
Mount points that are not
associated with a volume will not be returned by this function.
The mount points return by this function will be absolute paths, and may be longer than the drive name.
Raises OSError if the volume is not recognized or if an error occurs collecting the paths.
Raises an auditing event os.listmounts with argument volume.
Availability: Windows
New in version 3.12.
Volumes
are
typically
represented
as
a
GUID
path
that
looks
like
\\?\
Volume{xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx}\.
Files can usually be accessed through
a GUID path, permissions allowing.
However, users are generally not familiar with them, and so the recommended
use of this function is to retrieve mount points using os.listmounts().
May raise OSError if an error occurs collecting the volumes.
Raises an auditing event os.listvolumes with no arguments.
Availability: Windows
New in version 3.12.
Similar to stat(), but does not follow
symbolic links.
Return a stat_result object.
On platforms that do not support symbolic links, this is an alias for stat().
As
of
Python
3.3,
this
is
equivalent
to
os.stat(path, dir_fd=dir_fd,
follow_symlinks=False).
This function can also support paths relative to directory descriptors.
See also:
The stat() function.
Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.
Added the dir_fd parameter.
Accepts a path-like object.
Changed in version 3.8: On Windows, now opens reparse points that represent another path (name surrogates),
including symbolic links and directory junctions.
Other kinds of reparse points are resolved by the operating
system as for stat().
If the directory already exists, FileExistsError is raised.
If a parent directory in the path does not exist,
FileNotFoundError is raised.
On some systems, mode is ignored.
Where it is used, the current umask value is first masked out.
If bits other than
the last 9 (i.e. the last 3 digits of the octal representation of the mode) are set, their meaning is platform-dependent.
On some platforms, they are ignored and you should call chmod() explicitly to set them.
This function can also support paths relative to directory descriptors.
It is also possible to create temporary directories; see the tempfile module’s tempfile.mkdtemp() func-
tion.
Raises an auditing event os.mkdir with arguments path, mode, dir_fd.
The dir_fd argument.
Accepts a path-like object.
Like mkdir(), but makes all intermediate-level directories needed to
contain the leaf directory.
The mode parameter is passed to mkdir() for creating the leaf directory; see the mkdir() description for how it
is interpreted.
To set the file permission bits of any newly created parent directories you can set the umask before
invoking makedirs().
The file permission bits of existing parent directories are not changed.
If exist_ok is False (the default), a FileExistsError is raised if the target directory already exists.
Note: makedirs() will become confused if the path elements to create include pardir (eg. “..” on UNIX
systems).
This function handles UNC paths correctly.
Raises an auditing event os.mkdir with arguments path, mode, dir_fd.
The exist_ok parameter.
Changed in version 3.4.1: Before Python 3.4.1, if exist_ok was True and the directory existed, makedirs()
would still raise an error if mode did not match the mode of the existing directory.
Since this behavior was impos-
sible to implement safely, it was removed in Python 3.4.1.
See bpo-21082.
Accepts a path-like object.
The mode argument no longer affects the file permission bits of newly created intermediate-
level directories.
The current umask value is first masked out
from the mode.
This function can also support paths relative to directory descriptors.
FIFOs exist until they are deleted (for example with os.
unlink()).
Generally, FIFOs are used as rendezvous between “client” and “server” type processes: the server
opens the FIFO for reading, and the client opens it for writing.
Note that mkfifo() doesn’t open the FIFO — it
just creates the rendezvous point.
Availability: Unix, not Emscripten, not WASI.
The dir_fd argument.
Accepts a path-like object.
S_IFREG,
stat.
S_IFCHR, stat.
S_IFBLK, and stat.
S_IFIFO (those constants are available in stat).
For stat.
S_IFCHR and stat.
S_IFBLK, device defines the newly created device special file (probably using os.
makedev()), otherwise it is ignored.
This function can also support paths relative to directory descriptors.
Availability: Unix, not Emscripten, not WASI.
The dir_fd argument.
Accepts a path-like object.
Compose a raw device number from the major and minor device numbers.
Some platforms define additional names as well.
The names known to
the host operating system are given in the pathconf_names dictionary.
For configuration variables not included
in that mapping, passing an integer for name is also accepted.
If name is a string and is not known, ValueError is raised.
If a specific value for name is not supported by the
host system, even if it is included in pathconf_names, an OSError is raised with errno.
EINVAL for the
error number.
This function can support specifying a file descriptor.
Availability: Unix.
Changed in version 3.6: Accepts a path-like object.
This can be used to determine the set of names known to the system.
Availability: Unix.
os.readlink(path, *, dir_fd=None)
Return a string representing the path to which the symbolic link points.
The result may be either an absolute or
relative pathname; if it is relative, it may be converted to an absolute pathname using os.path.join(os.
path.dirname(path), result).
If the path is a string object (directly or indirectly through a PathLike interface), the result will also be a string
object, and the call may raise a UnicodeDecodeError.
If the path is a bytes object (direct or indirectly), the result
will be a bytes object.
This function can also support paths relative to directory descriptors.
When trying to resolve a path that may contain links, use realpath() to properly handle recursion and platform
differences.
Availability: Unix, Windows.
Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.
The dir_fd argument.
Accepts a path-like object on Unix.
Changed in version 3.8: Accepts a path-like object and a bytes object on Windows.
Changed in version 3.8: Added support for directory junctions, and changed to return the substitution path (which
typically includes \\?\ prefix) rather than the optional “print name” field that was previously returned.
If path is a directory, an OSError is raised.
Use rmdir() to remove directories.
If the file does not exist, a FileNotFoundError is raised.
This function can support paths relative to directory descriptors.
On Windows, attempting to remove a file that is in use causes an exception to be raised; on Unix, the directory
entry is removed but the storage allocated to the file is not made available until the original file is no longer in use.
This function is semantically identical to unlink().
Raises an auditing event os.remove with arguments path, dir_fd.
The dir_fd argument.
Accepts a path-like object.
os.removedirs(name)
Remove directories recursively.
Works like rmdir() except that, if the leaf directory is successfully re-
moved, removedirs() tries to successively remove every parent directory mentioned in path until an error
is raised (which is ignored, because it generally means that a parent directory is not empty).
For example, os.
removedirs('foo/bar/baz') will first remove the directory 'foo/bar/baz', and then remove 'foo/
bar' and 'foo' if they are empty.
Raises OSError if the leaf directory could not be successfully removed.
Raises an auditing event os.remove with arguments path, dir_fd.
Accepts a path-like object.
If dst exists, the operation will fail with an OSError subclass in a number
of cases:
On Windows, if dst exists a FileExistsError is always raised.
The operation may fail if src and dst are on
different filesystems.
Use shutil.move() to support moves to a different filesystem.
On Unix,
if src is a file and dst is a directory or vice-versa,
an IsADirectoryError or a
NotADirectoryError will be raised respectively.
If both are directories and dst is empty, dst will be silently
replaced.
If dst is a non-empty directory, an OSError is raised.
If both are files, dst will be replaced silently if
the user has permission.
The operation may fail on some Unix flavors if src and dst are on different filesystems.
If
successful, the renaming will be an atomic operation (this is a POSIX requirement).
This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors.
If you want cross-platform overwriting of the destination, use replace().
Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.
The src_dir_fd and dst_dir_fd arguments.
Accepts a path-like object for src and dst.
Works like rename(), except creation of any intermediate direc-
tories needed to make the new pathname good is attempted first.
After the rename, directories corresponding to
rightmost path segments of the old name will be pruned away using removedirs().
Note: This function can fail with the new directory structure made if you lack permissions needed to remove the
leaf directory or file.
Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.
Accepts a path-like object for old and new.
If dst is a non-empty directory, OSError will be raised.
If dst exists and
is a file, it will be replaced silently if the user has permission.
The operation may fail if src and dst are on different
filesystems.
If successful, the renaming will be an atomic operation (this is a POSIX requirement).
This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors.
Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.
New in version 3.3.
Accepts a path-like object for src and dst.
If the directory does not exist or is not empty, a FileNotFoundError or
an OSError is raised respectively.
In order to remove whole directory trees, shutil.rmtree() can be used.
This function can support paths relative to directory descriptors.
Raises an auditing event os.rmdir with arguments path, dir_fd.
The dir_fd parameter.
Return an iterator of os.
DirEntry objects corresponding to the entries in the directory given by path.
If a file is removed
from or added to the directory after creating the iterator, whether an entry for that file be included is unspecified.
Using scandir() instead of listdir() can significantly increase the performance of code that also needs file
type or file attribute information, because os.
DirEntry objects expose this information if the operating system
provides it when scanning a directory.
All os.
DirEntry methods may perform a system call, but is_dir()
and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires
a system call on Unix but only requires one for symbolic links on Windows.
If path is of type bytes (directly or indirectly through the PathLike interface),
the type of the name and path attributes of each os.
DirEntry will be bytes; in all other circumstances,
they will be of type str.
This function can also support specifying a file descriptor; the file descriptor must refer to a directory.
The scandir() iterator supports the context manager protocol and has the following method:
scandir.close()
Close the iterator and free acquired resources.
This is called automatically when the iterator is exhausted or garbage collected, or when an error happens
during iterating.
However it is advisable to call it explicitly or use the with statement.
New in version 3.6.
The following example shows a simple use of scandir() to display all the files (excluding directories) in the
given path that don’t start with '.'.
The entry.is_file() call will generally not make an additional system
call:
with os.scandir(path) as it:
for entry in it:
if not entry.name.startswith('.') and entry.is_file():
print(entry.name)
Note: On Unix-based systems, scandir() uses the system’s opendir() and readdir() functions.
On Windows,
it uses the Win32 FindFirstFileW and FindNextFileW functions.
New in version 3.5.
New in version 3.6: Added support for the context manager protocol and the close() method.
If a scandir()
iterator is neither exhausted nor explicitly closed a ResourceWarning will be emitted in its destructor.
The function accepts a path-like object.
Changed in version 3.7: Added support for file descriptors on Unix.
DirEntry
Object yielded by scandir() to expose the file path and other file attributes of a directory entry.
scandir() will provide as much of this information as possible without making additional system calls.
When
a stat() or lstat() system call is made, the os.
DirEntry object will cache the result.
DirEntry instances are not intended to be stored in long-lived data structures; if you know the file metadata
has changed or if a long time has elapsed since calling scandir(), call os.stat(entry.path) to fetch
up-to-date information.
Because the os.
DirEntry methods can make operating system calls, they may also raise OSError.
If you need
very fine-grained control over errors, you can catch OSError when calling one of the os.
DirEntry methods
and handle as appropriate.
To be directly usable as a path-like object, os.
DirEntry implements the PathLike interface.
Attributes and methods on a os.
DirEntry instance are as follows:
name
The entry’s base filename, relative to the scandir() path argument.
The name attribute will be bytes if the scandir() path argument is of type bytes and str otherwise.
Use fsdecode() to decode byte filenames.
The path is only absolute if the scandir() path argument
16.1.
If the scandir() path argument was a file descriptor, the path attribute is the same as the
name attribute.
The path attribute will be bytes if the scandir() path argument is of type bytes and str otherwise.
Use fsdecode() to decode byte filenames.
The
result
is
cached
on
the
os.
DirEntry
object.
Use
os.stat(entry.path,
follow_symlinks=False).st_ino to fetch up-to-date information.
On the first, uncached call, a system call is required on Windows but not on Unix.
The result is cached on the os.
DirEntry object, with a separate cache for follow_symlinks True and
False.
Call os.stat() along with stat.
S_ISDIR() to fetch up-to-date information.
On the first, uncached call, no system call is required in most cases.
If the entry is a symlink, a system call will be required to
follow the symlink unless follow_symlinks is False.
This method can raise OSError, such as PermissionError, but FileNotFoundError is caught
and not raised.
If follow_symlinks is False, return True only if this entry is a file (without following symlinks); return
False if the entry is a directory or other non-file entry, or if it doesn’t exist anymore.
The result is cached on the os.
DirEntry object.
Caching, system calls made, and exceptions raised are
as per is_dir().
The result is cached on the os.
DirEntry object.
Call os.path.islink() to fetch up-to-date infor-
mation.
On the first, uncached call, no system call is required in most cases.
Specifically, neither Windows or Unix
require a system call, except on certain Unix file systems, such as network file systems, that return dirent.
This method can raise OSError, such as PermissionError, but FileNotFoundError is caught
and not raised.
The result is cached on the os.
DirEntry object.
Call os.path.isjunction() to fetch up-to-date
information.
This method follows symbolic links by default; to stat a
symbolic link add the follow_symlinks=False argument.
On Unix, this method always requires a system call.
On Windows, it only requires a system call if fol-
low_symlinks is True and the entry is a reparse point (for example, a symbolic link or directory junction).
On Windows, the st_ino, st_dev and st_nlink attributes of the stat_result are always set to
zero.
Call os.stat() to get these attributes.
The result is cached on the os.
DirEntry object, with a separate cache for follow_symlinks True and
False.
Call os.stat() to fetch up-to-date information.
Note that there is a nice correspondence between several attributes and methods of os.
DirEntry and of
pathlib.
Path.
In particular, the name attribute has the same meaning, as do the is_dir(), is_file(),
is_symlink(), is_junction(), and stat() methods.
New in version 3.5.
Changed in version 3.6: Added support for the PathLike interface.
Added support for bytes paths on Win-
dows.
Changed in version 3.12: The st_ctime attribute of a stat result is deprecated on Windows.
The file creation
time is properly available as st_birthtime, and in the future st_ctime may be changed to return zero or
the metadata change time, if available.
Perform the equivalent of a stat() system call on the given path.
Return a stat_result object.
This function normally follows symlinks; to stat a symlink add the argument follow_symlinks=False, or
use lstat().
This function can support specifying a file descriptor and not following symlinks.
On Windows, passing follow_symlinks=False will disable following all name-surrogate reparse points,
which includes symlinks and directory junctions.
Other types of reparse points that do not resemble links or that
the operating system is unable to follow will be opened directly.
When following a chain of multiple links, this
may result in the original link being returned instead of the non-link that prevented full traversal.
To obtain stat
results for the final path in this case, use the os.path.realpath() function to resolve the path name as far as
possible and call lstat() on the result.
This does not apply to dangling symlinks or junction points, which will
raise the usual exceptions.
Example:
os.stat_result(st_mode=33188, st_ino=7876932, st_dev=234881026,
st_nlink=1, st_uid=501, st_gid=501, st_size=264, st_atime=1297230295,
st_mtime=1297230027, st_ctime=1297230027)

See also:
fstat() and lstat() functions.
Added the dir_fd and follow_symlinks arguments, specifying a file descriptor instead of a path.
Changed in version 3.8: On Windows, all reparse points that can be resolved by the operating system are now
followed, and passing follow_symlinks=False disables following all name surrogate reparse points.
If the
operating system reaches a reparse point that it is not able to follow, stat now returns the information for the original
path as if follow_symlinks=False had been specified instead of raising an error.
It is used for the result of
os.stat(), os.fstat() and os.lstat().
Attributes:
st_mode
File mode: file type and file mode bits (permissions).
Typically:
• the inode number on Unix,
• the file index on Windows
st_dev
Identifier of the device on which this file resides.
The size of a symbolic link is the length of
the pathname it contains, without a terminating null byte.
Timestamps:
st_atime
Time of most recent access expressed in seconds.
Changed in version 3.12: st_ctime is deprecated on Windows.
Use st_birthtime for the file creation
time.
In the future, st_ctime will contain the time of the most recent metadata change, as for other
platforms.
Changed in version 3.12: st_ctime_ns is deprecated on Windows.
Use st_birthtime_ns for the
file creation time.
In the future, st_ctime will contain the time of the most recent metadata change, as for
other platforms.
This attribute is not always available, and may raise
AttributeError.
Changed in version 3.12: st_birthtime is now available on Windows.
This attribute is not always available, and may
raise AttributeError.
New in version 3.12.
Note:
The exact meaning and resolution of the st_atime, st_mtime, st_ctime and st_birthtime
attributes depend on the operating system and the file system.
For example, on Windows systems using the FAT32
file systems, st_mtime has 2-second resolution, and st_atime has only 1-day resolution.
See your operating
system documentation for details.
Similarly, although st_atime_ns, st_mtime_ns, st_ctime_ns and st_birthtime_ns are always
expressed in nanoseconds, many systems do not provide nanosecond precision.
On systems that do provide nanosec-
ond precision, the floating-point object used to store st_atime, st_mtime, st_ctime and st_birthtime
cannot preserve all of it, and as such will be slightly inexact.
If you need the exact timestamps you should always
use st_atime_ns, st_mtime_ns, st_ctime_ns and st_birthtime_ns.
On some Unix systems (such as Linux), the following attributes may also be available:
st_blocks
Number of 512-byte blocks allocated for file.
This may be smaller than st_size/512 when the file has
holes.
On other Unix systems (such as FreeBSD), the following attributes may be available (but may be only filled out if
root tries to use them):
st_gen
File generation number.
On Solaris and derivatives, the following attributes may also be available:
st_fstype
String that uniquely identifies the type of the filesystem that contains the file.
On macOS systems, the following attributes may also be available:
16.1.
On Windows systems, the following attributes are also available:
st_file_attributes
Windows file attributes: dwFileAttributes member of the BY_HANDLE_FILE_INFORMATION
structure returned by GetFileInformationByHandle().
See the FILE_ATTRIBUTE_*
<stat.
FILE_ATTRIBUTE_ARCHIVE> constants in the stat module.
See the IO_REPARSE_TAG_* constants in the stat module.
The standard module stat defines functions and constants that are useful for extracting information from a stat
structure.
For backward compatibility, a stat_result instance is also accessible as a tuple of at least 10 integers giving
the most important (and portable) members of the stat structure, in the order st_mode, st_ino, st_dev,
st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime.
More items may be added
at the end by some implementations.
For compatibility with older Python versions, accessing stat_result as
a tuple always returns integers.
Added the st_atime_ns, st_mtime_ns, and st_ctime_ns members.
New in version 3.5: Added the st_file_attributes member on Windows.
Windows now returns the file index as st_ino when available.
Added the st_fstype member to Solaris/derivatives.
New in version 3.8: Added the st_reparse_tag member on Windows.
Changed in version 3.8: On Windows, the st_mode member now identifies special files as S_IFCHR, S_IFIFO
or S_IFBLK as appropriate.
Changed in version 3.12: On Windows, st_ctime is deprecated.
Eventually, it will contain the last metadata
change time, for consistency with other platforms, but for now still contains creation time.
Use st_birthtime
for the creation time.
Changed in version 3.12: On Windows, st_ino may now be up to 128 bits, depending on the file system.
Previ-
ously it would not be above 64 bits, and larger file identifiers would be arbitrarily packed.
Changed in version 3.12: On Windows, st_rdev no longer returns a value.
Previously it would contain the same
as st_dev, which was incorrect.
New in version 3.12: Added the st_birthtime member on Windows.
os.statvfs(path)
Perform a statvfs() system call on the given path.
The return value is an object whose attributes de-
scribe the filesystem on the given path, and correspond to the members of the statvfs structure, namely:
f_bsize, f_frsize, f_blocks, f_bfree, f_bavail, f_files, f_ffree, f_favail, f_flag,
f_namemax, f_fsid.
Two module-level constants are defined for the f_flag attribute’s bit-flags: if ST_RDONLY is set, the filesystem
is mounted read-only, and if ST_NOSUID is set, the semantics of setuid/setgid bits are disabled or not supported.
These are ST_NODEV (disal-
low access to device special files), ST_NOEXEC (disallow program execution), ST_SYNCHRONOUS (writes are
synced at once), ST_MANDLOCK (allow mandatory locks on an FS), ST_WRITE (write on file/directory/symlink),
ST_APPEND (append-only file), ST_IMMUTABLE (immutable file), ST_NOATIME (do not update access
times), ST_NODIRATIME (do not update directory access times), ST_RELATIME (update atime relative to
mtime/ctime).
This function can support specifying a file descriptor.
Availability: Unix.
Changed in version 3.2: The ST_RDONLY and ST_NOSUID constants were added.
New in version 3.3: Added support for specifying path as an open file descriptor.
The ST_NODEV, ST_NOEXEC, ST_SYNCHRONOUS, ST_MANDLOCK, ST_WRITE,
ST_APPEND, ST_IMMUTABLE, ST_NOATIME, ST_NODIRATIME, and ST_RELATIME constants were
added.
Accepts a path-like object.
New in version 3.7: Added f_fsid.
Different platforms provide different features, and the underlying functionality Python uses to implement the dir_fd
parameter is not available on all platforms Python supports.
For consistency’s sake, functions that may support
dir_fd always allow specifying the parameter, but will throw an exception if the functionality is used when it’s not
locally available.
To check whether a particular function accepts an open file descriptor for its dir_fd parameter, use the in operator
on supports_dir_fd.
As an example, this expression evaluates to True if os.stat() accepts open file
descriptors for dir_fd on the local platform:
os.stat in os.supports_dir_fd
Currently dir_fd parameters only work on Unix platforms; none of them work on Windows.
New in version 3.3.
If the local platform
supports it, the collection will contain os.access(); otherwise it will be empty.
This expression evaluates to True if os.access() supports effective_ids=True on the local platform:
os.access in os.supports_effective_ids
Currently effective_ids is only supported on Unix platforms; it does not work on Windows.
New in version 3.3.
Different platforms provide different features, and the underlying functionality
Python uses to accept open file descriptors as path arguments is not available on all platforms Python supports.
To determine whether a particular function permits specifying an open file descriptor for its path parameter, use
the in operator on supports_fd.
As an example, this expression evaluates to True if os.chdir() accepts
open file descriptors for path on your local platform:
16.1.
Different platforms provide different features, and the underlying functionality Python uses to
implement follow_symlinks is not available on all platforms Python supports.
For consistency’s sake, functions that
may support follow_symlinks always allow specifying the parameter, but will throw an exception if the functionality
is used when it’s not locally available.
To check whether a particular function accepts False for its follow_symlinks parameter, use the in operator
on supports_follow_symlinks.
As an example, this expression evaluates to True if you may specify
follow_symlinks=False when calling os.stat() on the local platform:
os.stat in os.supports_follow_symlinks
New in version 3.3.
os.symlink(src, dst, target_is_directory=False, *, dir_fd=None)
Create a symbolic link pointing to src named dst.
On Windows, a symlink represents either a file or a directory, and does not morph to the target dynamically.
If
the target is present, the type of the symlink will be created to match.
Otherwise, the symlink will be created as
a directory if target_is_directory is True or a file symlink (the default) otherwise.
On non-Windows platforms,
target_is_directory is ignored.
This function can support paths relative to directory descriptors.
Note: On newer versions of Windows 10, unprivileged accounts can create symlinks if Developer Mode is enabled.
When Developer Mode is not available/enabled, the SeCreateSymbolicLinkPrivilege privilege is required, or the
process must be run as an administrator.
OSError is raised when the function is called by an unprivileged user.
Raises an auditing event os.symlink with arguments src, dst, dir_fd.
Availability: Unix, Windows.
The function is limited on Emscripten and WASI, see WebAssembly platforms for more information.
Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.
Added the dir_fd argument, and now allow target_is_directory on non-Windows platforms.
Accepts a path-like object for src and dst.
Changed in version 3.8: Added support for unelevated symlinks on Windows with Developer Mode.
Force write of everything to disk.
Availability: Unix.
New in version 3.3.
This function can support specifying a file descriptor.
Availability: Unix, Windows.
New in version 3.3.
Accepts a path-like object.
This function is semantically identical to remove(); the unlink name is its
traditional Unix name.
Please see the documentation for remove() for further information.
Raises an auditing event os.remove with arguments path, dir_fd.
The dir_fd parameter.
Accepts a path-like object.
If ns is specified, it must be a 2-tuple of the form (atime_ns, mtime_ns) where each member is an
int expressing nanoseconds.
If times is not None, it must be a 2-tuple of the form (atime, mtime) where each member is an int or
float expressing seconds.
If times is None and ns is unspecified, this is equivalent to specifying ns=(atime_ns, mtime_ns)
where both times are the current time.
It is an error to specify tuples for both times and ns.
Note that the exact times you set here may not be returned by a subsequent stat() call, depending on the res-
olution with which your operating system records access and modification times; see stat().
The best way to
preserve exact times is to use the st_atime_ns and st_mtime_ns fields from the os.stat() result object with the
ns parameter to utime().
This function can support specifying a file descriptor, paths relative to directory descriptors and not following sym-
links.
Raises an auditing event os.utime with arguments path, times, ns, dir_fd.
New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd, follow_symlinks,
and ns parameters.
Accepts a path-like object.
For each di-
rectory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames,
filenames).
Note that the names in the lists contain no path components.
To get a full path (which begins with top)
to a file or directory in dirpath, do os.path.join(dirpath, name).
Whether or not the lists are sorted
depends on the file system.
If a file is removed from or added to the dirpath directory during generating the lists,
whether a name for that file be included is unspecified.
If optional argument topdown is True or not specified, the triple for a directory is generated before the triples for
any of its subdirectories (directories are generated top-down).
If topdown is False, the triple for a directory is
generated after the triples for all of its subdirectories (directories are generated bottom-up).
No matter the value of
topdown, the list of subdirectories is retrieved before the tuples for the directory and its subdirectories are generated.
When topdown is True, the caller can modify the dirnames list in-place (perhaps using del or slice assignment),
and walk() will only recurse into the subdirectories whose names remain in dirnames; this can be used to prune
the search, impose a specific order of visiting, or even to inform walk() about directories the caller creates or
renames before it resumes walk() again.
Modifying dirnames when topdown is False has no effect on the
behavior of the walk, because in bottom-up mode the directories in dirnames are generated before dirpath itself is
generated.
By default, errors from the scandir() call are ignored.
If optional argument onerror is specified, it should be a
function; it will be called with one argument, an OSError instance.
It can report the error to continue with the
walk, or raise the exception to abort the walk.
Note that the filename is available as the filename attribute of
the exception object.
By default, walk() will not walk down into symbolic links that resolve to directories.
Set followlinks to True to
visit directories pointed to by symlinks, on systems that support them.
Note: Be aware that setting followlinks to True can lead to infinite recursion if a link points to a parent directory
of itself.
Note:
If you pass a relative pathname, don’t change the current working directory between resumptions of
walk().
This example displays the number of bytes taken by non-directory files in each directory under the starting directory,
except that it doesn’t look under any CVS subdirectory:
import os
from os.path import join, getsize
for root, dirs, files in os.walk('python/Lib/email'):
print(root, "consumes", end=" ")
print(sum(getsize(join(root, name)) for name in files), end=" ")
print("bytes in", len(files), "non-directory files")
if 'CVS' in dirs:
dirs.remove('CVS')
# don't visit CVS directories
In the next example (simple implementation of shutil.rmtree()), walking the tree bottom-up is essential,
rmdir() doesn’t allow deleting a directory before the directory is empty:
# Delete everything reachable from the directory named in "top",
# assuming there are no symbolic links.
This is dangerous!
For example, if top == '/', it
# could delete all your disk files.
This behaves exactly like walk(), except that it yields a 4-tuple (dirpath, dirnames, filenames,
dirfd), and it supports dir_fd.
This function always supports paths relative to directory descriptors and not following symlinks.
Note however that,
unlike other functions, the fwalk() default value for follow_symlinks is False.
Note:
Since fwalk() yields file descriptors, those are only valid until the next iteration step, so you should
duplicate them (e.g. with dup()) if you want to keep them longer.
This example displays the number of bytes taken by non-directory files in each directory under the starting directory,
except that it doesn’t look under any CVS subdirectory:
import os
for root, dirs, files, rootfd in os.fwalk('python/Lib/email'):
print(root, "consumes", end="")
print(sum([os.stat(name, dir_fd=rootfd).st_size for name in files]),
end="")
print("bytes in", len(files), "non-directory files")
if 'CVS' in dirs:
dirs.remove('CVS')
# don't visit CVS directories
In the next example, walking the tree bottom-up is essential: rmdir() doesn’t allow deleting a directory before
the directory is empty:
# Delete everything reachable from the directory named in "top",
# assuming there are no symbolic links.
This is dangerous!
For example, if top == '/', it
# could delete all your disk files.
Raises an auditing event os.fwalk with arguments top, topdown, onerror, follow_symlinks,
dir_fd.
Availability: Unix.
New in version 3.3.
Accepts a path-like object.
Changed in version 3.7: Added support for bytes paths.
MFD_CLOEXEC])
Create an anonymous file and return a file descriptor that refers to it.
MFD_* constants
available on the system (or a bitwise ORed combination of them).
By default, the new file descriptor is non-
inheritable.
The name supplied in name is used as a filename and will be displayed as the target of the corresponding symbolic
link in the directory /proc/self/fd/. The displayed name is always prefixed with memfd: and serves only
for debugging purposes.
Names do not affect the behavior of the file descriptor, and as such multiple files can have
the same name without any side effects.
Availability: Linux >= 3.17 with glibc >= 2.27.
New in version 3.8.
MFD_HUGETLB
os.MFD_HUGE_SHIFT
os.
MFD_HUGE_MASK
os.MFD_HUGE_64KB
os.
MFD_HUGE_512KB
os.
MFD_HUGE_1MB
os.
MFD_HUGE_2MB
os.
MFD_HUGE_8MB
os.MFD_HUGE_16MB
os.
MFD_HUGE_32MB
os.
MFD_HUGE_256MB
os.
MFD_HUGE_512MB
os.
MFD_HUGE_1GB
os.
MFD_HUGE_2GB
os.
MFD_HUGE_16GB
These flags can be passed to memfd_create().
Availability: Linux >= 3.17 with glibc >= 2.27
The MFD_HUGE* flags are only available since Linux 4.14.
New in version 3.8.
EFD_CLOEXEC])
Create and return an event file descriptor.
The file descriptors supports raw read() and write() with a buffer
size of 8, select(), poll() and similar.
See man page eventfd(2) for more information.
By default, the
new file descriptor is non-inheritable.
The initial value must be an 32 bit unsigned integer.
Please note
that the initial value is limited to a 32 bit unsigned int although the event counter is an unsigned 64 bit integer with
a maximum value of 264-2.
flags can be constructed from EFD_CLOEXEC, EFD_NONBLOCK, and EFD_SEMAPHORE.
If EFD_SEMAPHORE is specified and the event counter is non-zero, eventfd_read() returns 1 and decrements
the counter by one.
If EFD_SEMAPHORE is not specified and the event counter is non-zero, eventfd_read() returns the current
event counter value and resets the counter to zero.
If the event counter is zero and EFD_NONBLOCK is not specified, eventfd_read() blocks.
eventfd_write() increments the event counter.
Read value from an eventfd() file descriptor and return a 64 bit unsigned int.
The function does not verify that
fd is an eventfd().
Add value to an eventfd() file descriptor.
The function does not verify
that fd is an eventfd().
Availability: Linux >= 2.6.27
New in version 3.10.
EFD_CLOEXEC
Set close-on-exec flag for new eventfd() file descriptor.
Availability: Linux >= 2.6.27
New in version 3.10.
EFD_NONBLOCK
Set O_NONBLOCK status flag for new eventfd() file descriptor.
Availability: Linux >= 2.6.27
New in version 3.10.
EFD_SEMAPHORE
Provide semaphore-like semantics for reads from a eventfd() file descriptor.
On read the internal counter is
decremented by one.
Availability: Linux >= 2.6.30
New in version 3.10.
16.1.
These functions are all available on Linux only.
If it is str, it is encoded with the filesystem encoding.
This function can support specifying a file descriptor and not following symlinks.
Raises an auditing event os.getxattr with arguments path, attribute.
Accepts a path-like object for path and attribute.
os.listxattr(path=None, *, follow_symlinks=True)
Return a list of the extended filesystem attributes on path.
The attributes in the list are represented as strings
decoded with the filesystem encoding.
If path is None, listxattr() will examine the current directory.
This function can support specifying a file descriptor and not following symlinks.
Raises an auditing event os.listxattr with argument path.
Accepts a path-like object.
Removes the extended filesystem attribute attribute from path.
If it is a string, it is encoded with the filesystem encoding and error handler.
This function can support specifying a file descriptor and not following symlinks.
Raises an auditing event os.removexattr with arguments path, attribute.
Accepts a path-like object for path and attribute.
If it is a str, it is encoded with the filesystem
encoding and error handler.
If XATTR_REPLACE is
given and the attribute does not exist, ENODATA will be raised.
If XATTR_CREATE is given and the attribute
already exists, the attribute will not be created and EEXISTS will be raised.
This function can support specifying a file descriptor and not following symlinks.
Note: A bug in Linux kernel versions less than 2.6.39 caused the flags argument to be ignored on some filesystems.
Raises an auditing event os.setxattr with arguments path, attribute, value, flags.
Accepts a path-like object for path and attribute.
Currently, this is 64 KiB on Linux.
This is a possible value for the flags argument in setxattr().
It indicates the operation must create an attribute.
This is a possible value for the flags argument in setxattr().
The various exec* functions take a list of arguments for the new program loaded into the process.
In each case, the
first of these arguments is passed to the new program as its own name rather than as an argument a user may have typed
on a command line.
For the C programmer, this is the argv[0] passed to a program’s main().
For example, os.
execv('/bin/echo', ['foo', 'bar']) will only print bar on standard output; foo will seem to be ignored.
On Unix, the default behavior is to produce a core dump; on
Windows, the process immediately returns an exit code of 3.
Be aware that calling this function will not call the
Python signal handler registered for SIGABRT with signal.signal().
os.add_dll_directory(path)
Add a path to the DLL search path.
This search path is used when resolving dependencies for imported extension modules (the module itself is resolved
through sys.path), and also by ctypes.
Remove the directory by calling close() on the returned object or using it in a with statement.
See the Microsoft documentation for more information about how DLLs are loaded.
Raises an auditing event os.add_dll_directory with argument path.
Availability: Windows.
New in version 3.8: Previous versions of CPython would resolve DLLs using the default behavior for the current
process.
This led to inconsistencies, such as only sometimes searching PATH or the current working directory, and
OS functions such as AddDllDirectory having no effect.
In 3.8, the two primary ways DLLs are loaded now explicitly override the process-wide behavior to ensure consis-
tency.
See the porting notes for information on updating libraries.
These functions all execute a new program, replacing the current process; they do not return.
On Unix, the new
executable is loaded into the current process, and will have the same process id as the caller.
Errors will be reported
as OSError exceptions.
The current process is replaced immediately.
Open file objects and descriptors are not flushed, so if there may be
data buffered on these open files, you should flush them using sys.stdout.flush() or os.fsync() before
calling an exec* function.
The “l” and “v” variants of the exec* functions differ in how command-line arguments are passed.
The “l” variants
are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual
parameters simply become additional parameters to the execl*() functions.
The “v” variants are good when
the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter.
In
either case, the arguments to the child process should start with the name of the command being run, but this is not
enforced.
The variants which include a “p” near the end (execlp(), execlpe(), execvp(), and execvpe()) will
use the PATH environment variable to locate the program file.
When the environment is being replaced (using one
of the exec*e variants, discussed in the next paragraph), the new environment is used as the source of the PATH
variable.
The other variants, execl(), execle(), execv(), and execve(), will not use the PATH variable
to locate the executable; path must contain an appropriate absolute or relative path.
Relative paths must include at
least one slash, even on Windows, as plain names will not be resolved.
For execle(), execlpe(), execve(), and execvpe() (note that these all end in “e”), the env parameter
must be a mapping which is used to define the environment variables for the new process (these are used instead
of the current process’ environment); the functions execl(), execlp(), execv(), and execvp() all cause
the new process to inherit the environment of the current process.
For execve() on some platforms, path may also be specified as an open file descriptor.
This functionality may
not be supported on your platform; you can check whether or not it is available using os.supports_fd.
If it is
unavailable, using it will raise a NotImplementedError.
Raises an auditing event os.exec with arguments path, args, env.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.3: Added support for specifying path as an open file descriptor for execve().
Accepts a path-like object.
Exit the process with status n, without calling cleanup handlers, flushing stdio buffers, etc.
Note: The standard way to exit is sys.exit(n).
The following exit codes are defined and can be used with _exit(), although they are not required.
These are typically
used for system programs written in Python, such as a mail server’s external command delivery program.
Note:
Some of these may not be available on all Unix platforms, since there is some variation.
These constants are
defined where they are defined by the underlying platform.
May be taken from the defined value of EXIT_SUCCESS on some
platforms.
Generally has a value of zero.
Availability: Unix, Windows.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
EX_NOUSER
Exit code that means a specified user did not exist.
Availability: Unix, not Emscripten, not WASI.
EX_NOHOST
Exit code that means a specified host did not exist.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
EX_IOERR
Exit code that means that an error occurred while doing I/O on some file.
Availability: Unix, not Emscripten, not WASI.
EX_TEMPFAIL
Exit code that means a temporary failure occurred.
This indicates something that may not really be an error, such
as a network connection that couldn’t be made during a retryable operation.
Availability: Unix, not Emscripten, not WASI.
EX_PROTOCOL
Exit code that means that a protocol exchange was illegal, invalid, or not understood.
Availability: Unix, not Emscripten, not WASI.
EX_NOPERM
Exit code that means that there were insufficient permissions to perform the operation (but not intended for file
system problems).
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
EX_NOTFOUND
Exit code that means something like “an entry was not found”.
Availability: Unix, not Emscripten, not WASI.
Return 0 in the child and the child’s process id in the parent.
If an error occurs OSError is
raised.
Note that some platforms including FreeBSD <= 6.3 and Cygwin have known issues when using fork() from a
thread.
Raises an auditing event os.fork with no arguments.
Warning: If you use TLS sockets in an application calling fork(), see the warning in the ssl documen-
tation.
Changed in version 3.8: Calling fork() in a subinterpreter is no longer supported (RuntimeError is raised).
Changed in version 3.12: If Python is able to detect that your process has multiple threads, os.fork() now
raises a DeprecationWarning.
We chose to surface this as a warning, when detectable, to better inform developers of a design problem that the
POSIX platform specifically notes as not supported.
Even in code that appears to work, it has never been safe to
mix threading with os.fork() on POSIX platforms.
The CPython runtime itself has always made API calls
that are not safe for use in the child process when threads existed in the parent (such as malloc and free).
Users of macOS or users of libc or malloc implementations other than those typically found in glibc to date are
among those already more likely to experience deadlocks running such code.
See this discussion on fork being incompatible with threads for technical details of why we’re surfacing this long-
standing platform compatibility problem to developers.
Availability: POSIX, not Emscripten, not WASI.
Return a pair of (pid,
fd), where pid is 0 in the child, the new child’s process id in the parent, and fd is the file descriptor of the master
end of the pseudo-terminal.
For a more portable approach, use the pty module.
If an error occurs OSError is
raised.
Raises an auditing event os.forkpty with no arguments.
Changed in version 3.12: If Python is able to detect that your process has multiple threads, this now raises a
DeprecationWarning.
See the longer explanation on os.fork().
Calling forkpty() in a subinterpreter is no longer supported (RuntimeError is
raised).
Availability: Unix, not Emscripten, not WASI.
Constants for the specific signals available on the host platform are defined in the
signal module.
Windows: The signal.
CTRL_C_EVENT and signal.
CTRL_BREAK_EVENT signals are special signals
which can only be sent to console processes which share a common console window, e.g., some subprocesses.
Any
other value for sig will cause the process to be unconditionally killed by the TerminateProcess API, and the exit
code will be set to sig.
The Windows version of kill() additionally takes process handles to be killed.
Raises an auditing event os.kill with arguments pid, sig.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.2: Windows support.
Raises an auditing event os.killpg with arguments pgid, sig.
Availability: Unix, not Emscripten, not WASI.
Add increment to the process’s “niceness”.
Return the new niceness.
Availability: Unix, not Emscripten, not WASI.
This descriptor can be used to perform process
management without races and signals.
See the pidfd_open(2) man page for more details.
Availability: Linux >= 5.3
New in version 3.9.
This flag indicates that the file descriptor will be non-blocking.
Availability: Linux >= 5.10
New in version 3.12.
The value of op (defined in <sys/lock.h>) determines which segments
are locked.
Availability: Unix, not Emscripten, not WASI.
The return value is an open file object connected to the pipe, which can
be read or written depending on whether mode is 'r' (default) or 'w'.
The buffering argument have the same
meaning as the corresponding argument to the built-in open() function.
The returned file object reads or writes
text strings rather than bytes.
The close method returns None if the subprocess exited successfully, or the subprocess’s return code if there was
an error.
On POSIX systems, if the return code is positive it represents the return value of the process left-shifted
by one byte.
If the return code is negative, the process was terminated by the signal given by the negated value of
the return code.
On Windows systems, the return value contains the signed integer return code from the child process.
On Unix, waitstatus_to_exitcode() can be used to convert the close method result (exit status) into
an exit code if it is not None.
On Windows, the close method result is directly the exit code (or None).
This is implemented using subprocess.
Popen; see that class’s documentation for more powerful ways to
manage and communicate with subprocesses.
Availability: not Emscripten, not WASI.
Note: The Python UTF-8 Mode affects encodings used for cmd and pipe contents.
popen() is a simple wrapper around subprocess.
Popen.
Use subprocess.
Popen or subprocess.
Most users should use subprocess.run() instead of posix_spawn().
The positional-only arguments path, args, and env are similar to execve().
The path parameter is the path to the executable file.
The path should contain a directory.
Use posix_spawnp()
to pass an executable file without directory.
The file_actions argument may be a sequence of tuples describing actions to take on specific file descriptors in the
child process between the C library implementation’s fork() and exec() steps.
The first item in each tuple
must be one of the three type indicator listed below describing the remaining tuple elements:
os.
POSIX_SPAWN_OPEN
(os.POSIX_SPAWN_OPEN, fd, path, flags, mode)
Performs os.dup2(os.open(path, flags, mode), fd).
POSIX_SPAWN_DUP2
(os.POSIX_SPAWN_DUP2, fd, new_fd)
Performs os.dup2(fd, new_fd).
These
tuples
correspond
to
the
C
library
posix_spawn_file_actions_addopen(),
posix_spawn_file_actions_addclose(), and posix_spawn_file_actions_adddup2()
API calls used to prepare for the posix_spawn() call itself.
The setpgroup argument will set the process group of the child to the value specified.
If the value specified is 0, the
child’s process group ID will be made the same as its process ID.
If the value of setpgroup is not set, the child will
inherit the parent’s process group ID.
This argument corresponds to the C library POSIX_SPAWN_SETPGROUP
flag.
If the resetids argument is True it will reset the effective UID and GID of the child to the real UID and GID of the
parent process.
If the argument is False, then the child retains the effective UID and GID of the parent.
In either
case, if the set-user-ID and set-group-ID permission bits are enabled on the executable file, their effect will override
the setting of the effective UID and GID.
This argument corresponds to the C library POSIX_SPAWN_RESETIDS
flag.
If the setsid argument is True, it will create a new session ID for posix_spawn.
Otherwise, NotImplementedError is
raised.
The setsigmask argument will set the signal mask to the signal set specified.
If the parameter is not
used, then the child inherits the parent’s signal mask.
This argument corresponds to the C library
POSIX_SPAWN_SETSIGMASK flag.
The sigdef argument will reset the disposition of all signals in the set specified.
This argument corresponds to the
C library POSIX_SPAWN_SETSIGDEF flag.
The scheduler argument must be a tuple containing the (optional) scheduler policy and an instance of
sched_param with the scheduler parameters.
A value of None in the place of the scheduler policy indicates
that is not being provided.
This argument is a combination of the C library POSIX_SPAWN_SETSCHEDPARAM
and POSIX_SPAWN_SETSCHEDULER flags.
Raises an auditing event os.posix_spawn with arguments path, argv, env.
New in version 3.8.
Availability: Unix, not Emscripten, not WASI.
Similar to posix_spawn() except that the system searches for the executable file in the list of directories spec-
ified by the PATH environment variable (in the same way as for execvp(3)).
Raises an auditing event os.posix_spawn with arguments path, argv, env.
New in version 3.8.
Availability: POSIX, not Emscripten, not WASI.
See posix_spawn() documentation.
The parameters are optional and keyword-only.
Each specifies a different call point.
These calls are only made if control is expected to return to the Python interpreter.
A typical subprocess launch
will not trigger them as the child is not going to re-enter the interpreter.
Functions registered for execution before forking are called in reverse registration order.
Functions registered for
execution after forking (either in the parent or in the child) are called in registration order.
Note that fork() calls made by third-party C code may not call those functions, unless it explicitly calls
PyOS_BeforeFork(), PyOS_AfterFork_Parent() and PyOS_AfterFork_Child().
There is no way to unregister a function.
Availability: Unix, not Emscripten, not WASI.
New in version 3.7.
If mode is P_NOWAIT, this function returns the process id of the new process; if mode is P_WAIT, returns the
process’s exit code if it exits normally, or -signal, where signal is the signal that killed the process.
On Windows,
the process id will actually be the process handle, so can be used with the waitpid() function.
Note on VxWorks, this function doesn’t return -signal when the new process is killed.
Instead it raises OSError
exception.
The “l” and “v” variants of the spawn* functions differ in how command-line arguments are passed.
The “l”
variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the
individual parameters simply become additional parameters to the spawnl*() functions.
The “v” variants are
good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args
parameter.
In either case, the arguments to the child process must start with the name of the command being run.
The variants which include a second “p” near the end (spawnlp(), spawnlpe(), spawnvp(), and
spawnvpe()) will use the PATH environment variable to locate the program file.
When the environment is being
replaced (using one of the spawn*e variants, discussed in the next paragraph), the new environment is used as
the source of the PATH variable.
The other variants, spawnl(), spawnle(), spawnv(), and spawnve(),
will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path.
For spawnle(), spawnlpe(), spawnve(), and spawnvpe() (note that these all end in “e”), the env
parameter must be a mapping which is used to define the environment variables for the new process (they are
used instead of the current process’ environment); the functions spawnl(), spawnlp(), spawnv(), and
spawnvp() all cause the new process to inherit the environment of the current process.
Note that keys and
values in the env dictionary must be strings; invalid keys or values will cause the function to fail, with a return value
of 127.
As an example, the following calls to spawnlp() and spawnvpe() are equivalent:
import os
os.spawnlp(os.
L = ['cp', 'index.html', '/dev/null']
os.spawnvpe(os.
P_WAIT, 'cp', L, os.environ)
Raises an auditing event os.spawn with arguments mode, path, args, env.
Availability: Unix, Windows, not Emscripten, not WASI.
spawnlp(), spawnlpe(), spawnvp() and spawnvpe() are not available on Windows.
Accepts a path-like object.
P_NOWAITO
Possible values for the mode parameter to the spawn* family of functions.
If either of these values is given, the
spawn*() functions will return as soon as the new process has been created, with the process id as the return
value.
Availability: Unix, Windows.
P_WAIT
Possible value for the mode parameter to the spawn* family of functions.
If this is given as mode, the spawn*()
functions will not return until the new process has run to completion and will return the exit code of the process
the run is successful, or -signal if a signal kills the process.
Availability: Unix, Windows.
P_OVERLAY
Possible values for the mode parameter to the spawn* family of functions.
These are less portable than those
listed above.
P_DETACH is similar to P_NOWAIT, but the new process is detached from the console of the calling
process.
If P_OVERLAY is used, the current process will be replaced; the spawn* function will not return.
Availability: Windows.
When operation is not specified, this acts like double-clicking the file in Windows Explorer, or giving the file name
as an argument to the start command from the interactive command shell: the file is opened with whatever
application (if any) its extension is associated.
When another operation is given, it must be a “command verb” that specifies what should be done with the file.
Common verbs documented by Microsoft are 'open', 'print' and 'edit' (to be used on files) as well as
'explore' and 'find' (to be used on directories).
When launching an application, specify arguments to be passed as a single string.
This argument may have no effect
when using this function to launch a document.
The default working directory is inherited, but may be overridden by the cwd argument.
This should be an absolute
path.
A relative path will be resolved against this argument.
Use show_cmd to override the default window style.
Whether this has any effect will depend on the application
being launched.
Values are integers as supported by the Win32 ShellExecute() function.
There is no option to wait for the
application to close, and no way to retrieve the application’s exit status.
The path parameter is relative to the
current directory or cwd.
Use
pathlib or the os.path.normpath() function to ensure that paths are properly encoded for Win32.
To reduce interpreter startup overhead, the Win32 ShellExecute() function is not resolved until this function
is first called.
If the function cannot be resolved, NotImplementedError will be raised.
Raises an auditing event os.startfile with arguments path, operation.
Raises an auditing event os.startfile/2 with arguments path, operation, arguments, cwd,
show_cmd.
Availability: Windows.
Changed in version 3.10: Added the arguments, cwd and show_cmd arguments, and the os.startfile/2 audit
event.
os.system(command)
Execute the command (a string) in a subshell.
This is implemented by calling the Standard C function system(),
and has the same limitations.
Changes to sys.stdin, etc. are not reflected in the environment of the executed
command.
If command generates any output, it will be sent to the interpreter standard output stream.
The C
standard does not specify the meaning of the return value of the C function, so the return value of the Python
function is system-dependent.
On Unix, the return value is the exit status of the process encoded in the format specified for wait().
The shell is given by the
Windows environment variable COMSPEC: it is usually cmd.exe, which returns the exit status of the command
run; on systems using a non-native shell, consult your shell documentation.
The subprocess module provides more powerful facilities for spawning new processes and retrieving their
results; using that module is preferable to using this function.
See the Replacing Older Functions with the subprocess
Module section in the subprocess documentation for some helpful recipes.
On Unix, waitstatus_to_exitcode() can be used to convert the result (exit status) into an exit code.
On
Windows, the result is directly the exit code.
Raises an auditing event os.system with argument command.
Availability: Unix, Windows, not Emscripten, not WASI.
The return value is an object with five attributes:
• user - user time
• system - system time
• children_user - user time of all child processes
• children_system - system time of all child processes
• elapsed - elapsed real time since a fixed point in the past
For backwards compatibility,
this object also behaves like a five-tuple containing user,
system,
children_user, children_system, and elapsed in that order.
See the Unix manual page times(2) and times(3) manual page on Unix or the GetProcessTimes MSDN on
Windows.
On Windows, only user and system are known; the other attributes are zero.
Availability: Unix, Windows.
Changed in version 3.3: Return type changed from a tuple to a tuple-like object with named attributes.
If there are no children that could be waited for, ChildProcessError is raised.
Availability: Unix, not Emscripten, not WASI.
See also:
The other wait*() functions documented below can be used to wait for the completion of a specific child process
and have more options.
The interpretation of id depends on it; see their
individual descriptions.
At least one of WEXITED, WSTOPPED or WCONTINUED is required;
WNOHANG and WNOWAIT are additional optional flags.
If WNOHANG is specified and there are no matching children in the requested state, None is returned.
Otherwise,
if there are no matching children that could be waited for, ChildProcessError is raised.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
The details of this function differ on Unix and Windows.
On Unix: Wait for completion of a child process given by process id pid, and return a tuple containing its process
id and exit status indication (encoded as for wait()).
The semantics of the call are affected by the value of the
integer options, which should be 0 for normal operation.
If pid is greater than 0, waitpid() requests status information for that specific process.
If pid is 0, the request
is for the status of any child in the process group of the current process.
If pid is -1, the request pertains to any
child of the current process.
If pid is less than -1, status is requested for any process in the process group -pid
(the absolute value of pid).
If it contains WNOHANG and there are no matching children in the re-
quested state, (0, 0) is returned.
Otherwise, if there are no matching children that could be waited for,
ChildProcessError is raised.
Other options that can be used are WUNTRACED and WCONTINUED.
On Windows: Wait for completion of a process given by process handle pid, and return a tuple containing pid, and
its exit status shifted left by 8 bits (shifting makes cross-platform use of the function easier).
A pid less than or
equal to 0 has no special meaning on Windows, and raises an exception.
The value of integer options has no effect.
The spawn* functions called with
P_NOWAIT return suitable process handles.
Availability: Unix, Windows, not Emscripten, not WASI.
If the system call is interrupted and the signal handler does not raise an exception, the
function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Similar to waitpid(), except no process id argument is given and a 3-element tuple containing the child’s process
id, exit status indication, and resource usage information is returned.
Refer to resource.getrusage() for
details on resource usage information.
The options argument is the same as that provided to waitpid() and
wait4().
Availability: Unix, not Emscripten, not WASI.
os.wait4(pid, options)
Similar to waitpid(), except a 3-element tuple, containing the child’s process id, exit status indication, and
resource usage information is returned.
Refer to resource.getrusage() for details on resource usage in-
formation.
The arguments to wait4() are the same as those provided to waitpid().
P_ALL
os.
These are the possible values for idtype in waitid().
They affect how id is interpreted:
• P_PID - wait for the child whose PID is id.
• P_PGID - wait for any child whose progress group ID is id.
• P_ALL - wait for any child; id is ignored.
Availability: Unix, not Emscripten, not WASI.
Note: P_PIDFD is only available on Linux >= 5.4.
New in version 3.3.
The P_PIDFD constant.
This options flag for waitpid(), wait3(), wait4(), and waitid() causes child processes to be reported
if they have been continued from a job control stop since they were last reported.
Availability: Unix, not Emscripten, not WASI.
The other wait* functions always report children that have terminated, so this option is not available for them.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
This options flag for waitid() causes child processes that have been stopped by the delivery of a signal to be
reported.
This option is not available for the other wait* functions.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
This options flag for waitpid(), wait3(), and wait4() causes child processes to also be reported if they
have been stopped but their current state has not been reported since they were stopped.
This option is not available for waitid().
Availability: Unix, not Emscripten, not WASI.
WNOHANG
This options flag causes waitpid(), wait3(), wait4(), and waitid() to return right away if no child
process status is available immediately.
Availability: Unix, not Emscripten, not WASI.
This options flag causes waitid() to leave the child in a waitable state, so that a later wait*() call can be used
to retrieve the child status information again.
This option is not available for the other wait* functions.
Availability: Unix, not Emscripten, not WASI.
CLD_EXITED
os.
CLD_KILLED
os.
CLD_DUMPED
os.
CLD_TRAPPED
os.
CLD_STOPPED
os.
These are the possible values for si_code in the result returned by waitid().
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
Convert a wait status to an exit code.
If the process exited normally (if WIFEXITED(status) is true), return the process exit status (return
WEXITSTATUS(status)): result greater than or equal to 0.
If the process was terminated by a signal (if WIFSIGNALED(status) is true), return -signum where
signum is the number of the signal that caused the process to terminate (return -WTERMSIG(status)):
result less than 0.
On Windows, return status shifted right by 8 bits.
On Unix, if the process is being traced or if waitpid() was called with WUNTRACED option, the caller must
first check if WIFSTOPPED(status) is true.
This function must not be called if WIFSTOPPED(status) is
true.
See also:
WIFEXITED(), WEXITSTATUS(), WIFSIGNALED(), WTERMSIG(), WIFSTOPPED(), WSTOPSIG()
functions.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.9.
The following functions take a process status code as returned by system(), wait(), or waitpid() as a parameter.
They may be used to determine the disposition of a process.
WCOREDUMP(status, /)
Return True if a core dump was generated for the process, otherwise return False.
This function should be employed only if WIFSIGNALED() is true.
Availability: Unix, not Emscripten, not WASI.
See WCONTINUED option.
Availability: Unix, not Emscripten, not WASI.
Availability: Unix, not Emscripten, not WASI.
WIFSIGNALED(status)
Return True if the process was terminated by a signal, otherwise return False.
Availability: Unix, not Emscripten, not WASI.
WIFEXITED(status)
Return True if the process exited terminated normally, that is, by calling exit() or _exit(), or by returning
from main(); otherwise return False.
Availability: Unix, not Emscripten, not WASI.
This function should be employed only if WIFEXITED() is true.
Availability: Unix, not Emscripten, not WASI.
This function should be employed only if WIFSTOPPED() is true.
Availability: Unix, not Emscripten, not WASI.
This function should be employed only if WIFSIGNALED() is true.
Availability: Unix, not Emscripten, not WASI.
Interface to the scheduler
These functions control how a process is allocated CPU time by the operating system.
They are only available on some
Unix platforms.
For more detailed information, consult your Unix manpages.
New in version 3.3.
The following scheduling policies are exposed if they are supported by the operating system.
The default scheduling policy.
SCHED_IDLE
Scheduling policy for extremely low priority background tasks.
A First In First Out scheduling policy.
This flag can be OR’ed with any other scheduling policy.
When a process with this flag set forks, its child’s schedul-
ing policy and priority are reset to the default.
This
class
represents
tunable
scheduling
parameters
used
in
sched_setparam(),
sched_setscheduler(), and sched_getparam().
It is immutable.
At the moment, there is only one possible parameter:
sched_priority
The scheduling priority for a scheduling policy.
Get the minimum priority value for policy.
Get the maximum priority value for policy.
A pid of 0 means the calling process.
A pid of 0 means the calling process.
The result is one
of the scheduling policy constants above.
A pid of 0 means the calling process.
A pid of 0 means
the calling process.
A pid of 0 means the calling process.
If pid is zero, return the set of CPUs the calling thread of the current process is restricted to.
Return string-valued system configuration values.
Some platforms define additional names as well.
The names known to the host operating
system are given as the keys of the confstr_names dictionary.
For configuration variables not included in that
mapping, passing an integer for name is also accepted.
If the configuration value specified by name isn’t defined, None is returned.
If name is a string and is not known, ValueError is raised.
If a specific value for name is not supported by the
host system, even if it is included in confstr_names, an OSError is raised with errno.
EINVAL for the
error number.
Availability: Unix.
os.confstr_names
Dictionary mapping names accepted by confstr() to the integer values defined for those names by the host
operating system.
This can be used to determine the set of names known to the system.
Availability: Unix.
Returns None if undetermined.
This number is not equivalent to the number of logical CPUs the current process can use.
len(os.
sched_getaffinity(0)) gets the number of logical CPUs the calling thread of the current process is re-
stricted to
New in version 3.4.
Availability: Unix.
If the configuration value specified by name isn’t defined, -1
is returned.
The comments regarding the name parameter for confstr() apply here as well; the dictionary that
provides information on the known names is given by sysconf_names.
Availability: Unix.
This can be used to determine the set of names known to the system.
Availability: Unix.
Changed in version 3.11: Add 'SC_MINSIGSTKSZ' name.
The following data values are used to support path manipulation operations.
These are defined for all platforms.
Higher-level operations on pathnames are defined in the os.path module.
The constant string used by the operating system to refer to the current directory.
Also available via os.path.
The constant string used by the operating system to refer to the parent directory.
This is '..' for Windows and
POSIX.
Also available via os.path.
This is '/' for POSIX and '\
\' for Windows.
Note that knowing this is not sufficient to be able to parse or concatenate pathnames — use
os.path.split() and os.path.join() — but it is occasionally useful.
Also available via os.path.
os.altsep
An alternative character used by the operating system to separate pathname components, or None if only one
separator character exists.
This is set to '/' on Windows systems where sep is a backslash.
Also available
via os.path.
Also available via os.path.
os.defpath
The default search path used by exec*p* and spawn*p* if the environment doesn’t have a 'PATH' key.
Also
available via os.path.
This may be a single character,
such as '\n' for POSIX, or multiple characters, for example, '\r\n' for Windows.
Do not use os.linesep as a
line terminator when writing files opened in text mode (the default); use a single '\n' instead, on all platforms.
For example: '/dev/null' for POSIX, 'nul' for Windows.
Also available
via os.path.
RTLD_NOW
os.
Flags for use with the setdlopenflags() and getdlopenflags() functions.
See the Unix manual page
dlopen(3) for what the different flags mean.
New in version 3.3.
The function can return less bytes than requested.
These bytes can be used to seed user-space random number generators or for cryptographic purposes.
Un-
necessarily reading large quantities of data will have a negative impact on other users of the /dev/random and
/dev/urandom devices.
The flags argument is a bit mask that can contain zero or more of the following values ORed together: os.
GRND_RANDOM and GRND_NONBLOCK.
See also the Linux getrandom() manual page.
Availability: Linux >= 3.17.
New in version 3.6.
This function returns random bytes from an OS-specific randomness source.
The returned data should be unpre-
dictable enough for cryptographic applications, though its exact quality depends on the OS implementation.
On Linux, if the getrandom() syscall is available, it is used in blocking mode: block until the system uran-
dom entropy pool is initialized (128 bits of entropy are collected by the kernel).
See the PEP 524 for the ratio-
nale.
On a Unix-like system, random bytes are read from the /dev/urandom device.
If the /dev/urandom device
is not available or not readable, the NotImplementedError exception is raised.
On Windows, it will use BCryptGenRandom().
See also:
The secrets module provides higher level functions.
For an easy-to-use interface to the random number gener-
ator provided by your platform, please see random.
SystemRandom.
Changed in version 3.6.0: On Linux, getrandom() is now used in blocking mode to increase the security.
On Linux, if the getrandom() syscall blocks (the urandom entropy pool is not
initialized yet), fall back on reading /dev/urandom.
On Linux 3.17 and newer, the getrandom() syscall is now used when available.
On
OpenBSD 5.6 and newer, the C getentropy() function is now used.
These functions avoid the usage of an
internal file descriptor.
If the GRND_NONBLOCK flag is set, then getrandom() does not block in these cases, but instead immediately
raises BlockingIOError.
New in version 3.6.
If this bit is set, then random bytes are drawn from the /dev/random pool instead of the /dev/urandom
pool.
New in version 3.6.
A concrete object belonging to any of these categories is called a file object.
Other common terms are stream and file-like
object.
Independent of its category, each concrete stream object will also have various capabilities: it can be read-only, write-
only, or read-write.
It can also allow arbitrary random access (seeking forwards or backwards to any location), or only
sequential access (for example in the case of a socket or pipe).
All streams are careful about the type of data you give to them.
For example giving a str object to the write() method
of a binary stream will raise a TypeError.
So will giving a bytes object to the write() method of a text stream.
Operations that used to raise IOError now raise OSError, since IOError is now an alias
of OSError.
I/O expects and produces str objects.
This means that whenever the backing store is natively made of bytes (such
as in the case of a file), encoding and decoding of data is made transparently as well as optional translation of platform-
specific newline characters.
The text stream API is described in detail in the documentation of TextIOBase.
No encoding, decoding,
or newline translation is performed.
This category of streams can be used for all kinds of non-text data, and also when
manual control over the handling of text data is desired.
The easiest way to create a binary stream is with open() with 'b' in the mode string:
f = open("myfile.jpg", "rb")
In-memory binary streams are also available as BytesIO objects:
f = io.
The binary stream API is described in detail in the docs of BufferedIOBase.
Other library modules may provide additional ways to create text or binary streams.
See socket.socket.
Raw I/O
Raw I/O (also called unbuffered I/O) is generally used as a low-level building-block for binary and text streams; it is rarely
useful to directly manipulate a raw stream from user code.
The raw stream API is described in detail in the docs of RawIOBase.
Text Encoding
The default encoding of TextIOWrapper and open() is locale-specific (locale.getencoding()).
However, many developers forget to specify the encoding when opening text files encoded in UTF-8 (e.g. JSON, TOML,
Markdown, etc…) since most Unix platforms use UTF-8 locale by default.
This causes bugs because the locale encoding
is not UTF-8 for most Windows users.
For example:
# May not work on Windows when non-ASCII characters in the file.
Accordingly, it is highly recommended that you specify the encoding explicitly when opening text files.
If you want to use
UTF-8, pass encoding="utf-8".
To use the current locale encoding, encoding="locale" is supported since
Python 3.10.
See also:
Python UTF-8 Mode Python UTF-8 Mode can be used to change the default encoding to UTF-8 from locale-specific
encoding.
PEP 686 Python 3.15 will make Python UTF-8 Mode default.
To find where the default locale encoding is used, you can enable the -X warn_default_encoding command line
option or set the PYTHONWARNDEFAULTENCODING environment variable, which will emit an EncodingWarning
when the default encoding is used.
If you are providing an API that uses open() or TextIOWrapper and passes encoding=None as a parameter,
you can use text_encoding() so that callers of the API will emit an EncodingWarning if they don’t pass an
encoding.
However, please consider using UTF-8 by default (i.e. encoding="utf-8") for new APIs.
An int containing the default buffer size used by the module’s buffered I/O classes.
This is an alias for the builtin open() function.
This function raises an auditing event open with arguments path, mode and flags.
The mode and flags arguments
may have been modified or inferred from the original call.
This function should be used when the intent is to treat the contents as
executable code.
The behavior of this function may be overridden by an earlier call to the PyFile_SetOpenCodeHook().
However, assuming that path is a str and an absolute path, open_code(path) should always behave the same
as open(path, 'rb').
Overriding the behavior is intended for additional validation or preprocessing of the
file.
New in version 3.8.
This is a helper function for callables that use open() or TextIOWrapper and have an encoding=None
parameter.
This function returns encoding if it is not None.
Otherwise, it returns "locale" or "utf-8" depending on
UTF-8 Mode.
This function emits an EncodingWarning if sys.flags.warn_default_encoding is true and en-
coding is None. stacklevel specifies where the warning is emitted.
In this example, an EncodingWarning is emitted for the caller of read_text().
See Text Encoding for more information.
New in version 3.10.
This is a compatibility alias for the builtin BlockingIOError exception.
UnsupportedOperation
An exception inheriting OSError and ValueError that is raised when an unsupported operation is called on a
stream.
See also:
sys contains the standard IO streams: sys.stdin, sys.stdout, and sys.stderr.
First abstract base classes (ABCs), which are
used to specify the various categories of streams, then concrete classes providing the standard stream implementations.
Note: The abstract base classes also provide default implementations of some methods in order to help implementation
of concrete stream classes.
For example, BufferedIOBase provides unoptimized implementations of readinto()
and readline().
At the top of the I/O hierarchy is the abstract base class IOBase.
It defines the basic interface to a stream.
Note,
however, that there is no separation between reading and writing to streams; implementations are allowed to raise
UnsupportedOperation if they do not support a given operation.
The RawIOBase ABC extends IOBase.
It deals with the reading and writing of bytes to a stream.
FileIO subclasses
RawIOBase to provide an interface to files in the machine’s file system.
The BufferedIOBase ABC extends IOBase.
It deals with buffering on a raw binary stream (RawIOBase).
Its
subclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer raw binary streams that are
writable, readable, and both readable and writable, respectively.
BufferedRandom provides a buffered interface to
seekable streams.
Another BufferedIOBase subclass, BytesIO, is a stream of in-memory bytes.
The TextIOBase ABC extends IOBase.
It deals with streams whose bytes represent text, and handles encoding and
decoding to and from strings.
TextIOWrapper, which extends TextIOBase, is a buffered text interface to a buffered
raw stream (BufferedIOBase).
Finally, StringIO is an in-memory stream for text.
Argument names are not part of the specification, and only the arguments of open() are intended to be used as keyword
arguments.
IOBase
The abstract base class for all I/O classes.
This class provides empty abstract implementations for many methods that derived classes can override selectively;
the default implementations represent a file that cannot be read, written or seeked.
Even though IOBase does not declare read() or write() because their signatures will vary, implementations
and clients should consider those methods part of the interface.
Also, implementations may raise a ValueError
(or UnsupportedOperation) when operations they do not support are called.
The basic type used for binary data read from or written to a file is bytes.
Other bytes-like objects are accepted
as method arguments too.
Text I/O classes work with str data.
Note that calling any method (even inquiries) on a closed stream is undefined.
Implementations may raise
ValueError in this case.
IOBase (and its subclasses) supports the iterator protocol, meaning that an IOBase object can be iterated over
yielding the lines in a stream.
Lines are defined slightly differently depending on whether the stream is a binary
stream (yielding bytes), or a text stream (yielding character strings).
See readline() below.
IOBase is also a context manager and therefore supports the with statement.
IOBase provides these data attributes and methods:
close()
Flush and close this stream.
This method has no effect if the file is already closed.
Once the file is closed, any
operation on the file (e.g. reading or writing) will raise a ValueError.
As a convenience, it is allowed to call this method more than once; only the first call, however, will have an
effect.
An OSError is raised if the IO
object does not use a file descriptor.
Flush the write buffers of the stream if applicable.
This does nothing for read-only and non-blocking streams.
If False, read() will raise OSError.
If size is specified, at most size bytes will be read.
The line terminator is always b'\n' for binary files; for text files, the newline argument to open() can be
used to select the line terminator(s) recognized.
Note that it’s already possible to iterate on file objects using for line in file: ... without calling
file.readlines().
SEEK_SET, /)
Change the stream position to the given byte offset, interpreted relative to the position indicated by whence,
and return the new absolute position.
Values for whence are:
• os.SEEK_SET or 0 – start of the stream (the default); offset should be zero or positive
• os.
SEEK_CUR or 1 – current stream position; offset may be negative
• os.
SEEK_END or 2 – end of the stream; offset is usually negative
New in version 3.1: The SEEK_* constants.
SEEK_DATA.
The valid values for a file could depend on it being open in text or binary mode.
If False, seek(), tell() and truncate() will
raise OSError.
tell()
Return the current stream position.
Resize the stream to the given size in bytes (or the current position if size is not specified).
The current stream
position isn’t changed.
This resizing can extend or reduce the current file size.
In case of extension, the
contents of the new file area depend on the platform (on most systems, additional bytes are zero-filled).
The
new file size is returned.
Changed in version 3.5: Windows will now zero-fill files when extending.
Return True if the stream supports writing.
If False, write() and truncate() will raise OSError.
Line separators are not added, so it is usual for each of the lines provided
to have a line separator at the end.
IOBase provides a default implementation of this method that calls the
instance’s close() method.
It inherits IOBase.
Raw binary streams typically provide low-level access to an underlying OS device or API, and do not try to en-
capsulate it in high-level primitives (this functionality is done at a higher-level in buffered binary streams and text
streams, described later in this page).
As a convenience, if size is unspecified or -1, all bytes
until EOF are returned.
Otherwise, only one system call is ever made.
Fewer than size bytes may be returned
if the operating system call returns fewer than size bytes.
If 0 bytes are returned, and size was not 0, this indicates end of file.
If the object is in non-blocking mode and
no bytes are available, None is returned.
The default implementation defers to readall() and readinto().
Read bytes into a pre-allocated, writable bytes-like object b, and return the number of bytes read.
For example,
b might be a bytearray.
If the object is in non-blocking mode and no bytes are available, None is returned.
This
can be less than the length of b in bytes, depending on specifics of the underlying raw stream, and especially
if it is in non-blocking mode.
None is returned if the raw stream is set not to block and no single byte could
be readily written to it.
The caller may release or mutate b after this method returns, so the implementation
should only access b during the method call.
It inherits IOBase.
The main difference with RawIOBase is that methods read(), readinto() and write() will try (respec-
tively) to read as much input as requested or to consume all given output, at the expense of making perhaps more
than one system call.
In addition, those methods can raise BlockingIOError if the underlying raw stream is in non-blocking mode
and cannot take or give enough data; unlike their RawIOBase counterparts, they will never return None.
Besides, the read() method does not have a default implementation that defers to readinto().
A typical BufferedIOBase implementation should not inherit from a RawIOBase implementation, but wrap
one, like BufferedWriter and BufferedReader do.
BufferedIOBase provides or overrides these data attributes and methods in addition to those from IOBase:
16.2.
This is not part
of the BufferedIOBase API and may not exist on some implementations.
After the raw stream has been detached, the buffer is in an unusable state.
Some buffers, like BytesIO, do not have the concept of a single raw stream to return from this method.
They raise UnsupportedOperation.
New in version 3.1.
If the argument is omitted, None, or negative, data is read and returned
until EOF is reached.
An empty bytes object is returned if the stream is already at EOF.
If the argument is positive, and the underlying raw stream is not interactive, multiple raw reads may be issued
to satisfy the byte count (unless EOF is reached first).
But for interactive raw streams, at most one raw read
will be issued, and a short result does not imply that EOF is imminent.
A BlockingIOError is raised if the underlying raw stream is in non blocking-mode, and has no data
available at the moment.
This can be useful if you are implementing your own buffering on top of a
BufferedIOBase object.
If size is -1 (the default), an arbitrary number of bytes are returned (more than zero unless EOF is reached).
For example,
b might be a bytearray.
Like read(), multiple reads may be issued to the underlying raw stream, unless the latter is interactive.
A BlockingIOError is raised if the underlying raw stream is in non blocking-mode, and has no data
available at the moment.
Return the number of bytes read.
A BlockingIOError is raised if the underlying raw stream is in non blocking-mode, and has no data
available at the moment.
New in version 3.5.
Depending on the actual implementation, these
bytes may be readily written to the underlying stream, or held in a buffer for performance and latency reasons.
When in non-blocking mode, a BlockingIOError is raised if the data needed to be written to the raw
stream but it couldn’t accept all the data without blocking.
The caller may release or mutate b after this method returns, so the implementation should only access b
during the method call.
A raw binary stream representing an OS-level file containing bytes data.
It inherits RawIOBase.
The name can be one of two things:
• a character string or bytes object representing the path to the file which will be opened.
In this case closefd
must be True (the default) otherwise an error will be raised.
• an integer representing the number of an existing OS-level file descriptor to which the resulting FileIO
object will give access.
When the FileIO object is closed this fd will be closed as well, unless closefd is set to
False.
The mode can be 'r', 'w', 'x' or 'a' for reading (default), writing, exclusive creation or appending.
The
file will be created if it doesn’t exist when opened for writing or appending; it will be truncated when opened
for writing.
FileExistsError will be raised if it already exists when opened for creating.
Opening a file
for creating implies writing, so this mode behaves in a similar way to 'w'.
Add a '+' to the mode to allow
simultaneous reading and writing.
The read() (when called with a positive argument), readinto() and write() methods on this class will
only make one system call.
A custom opener can be used by passing a callable as opener.
The underlying file descriptor for the file object is
then obtained by calling opener with (name, flags).
The newly created file is non-inheritable.
See the open() built-in function for examples on using the opener parameter.
Changed in version 3.3: The opener parameter was added.
The 'x' mode was added.
The file is now non-inheritable.
FileIO provides these data attributes in addition to those from RawIOBase and IOBase:
mode
The mode as given in the constructor.
This is the file descriptor of the file when no name is given in the constructor.
Buffered Streams
Buffered I/O streams provide a higher-level interface to an I/O device than raw I/O does.
A binary stream using an in-memory bytes buffer.
It inherits BufferedIOBase.
The buffer is discarded when
the close() method is called.
The optional argument initial_bytes is a bytes-like object that contains initial data.
BytesIO provides or overrides these methods in addition to those from BufferedIOBase and IOBase:
getbuffer()
Return a readable and writable view over the contents of the buffer without copying them.
Also, mutating the
view will transparently update the contents of the buffer:
16.2.
New in version 3.2.
In BytesIO, this is the same as readinto().
A buffered binary stream providing higher-level access to a readable, non seekable RawIOBase raw binary stream.
It inherits BufferedIOBase.
When reading data from this object, a larger amount of data may be requested from the underlying raw stream, and
kept in an internal buffer.
The buffered data can then be returned directly on subsequent reads.
The constructor creates a BufferedReader for the given readable raw stream and buffer_size.
If buffer_size is
omitted, DEFAULT_BUFFER_SIZE is used.
Return bytes from the stream without advancing the position.
At most one single read on the raw stream is
done to satisfy the call.
The number of bytes returned may be less or more than requested.
If at least one byte is buffered, only
buffered bytes are returned.
Otherwise, one raw stream read call is made.
The size argument is now optional.
A buffered binary stream providing higher-level access to a writeable, non seekable RawIOBase raw binary
stream.
It inherits BufferedIOBase.
When writing to this object, data is normally placed into an internal buffer.
The constructor creates a BufferedWriter for the given writeable raw stream.
If the buffer_size is not given,
it defaults to DEFAULT_BUFFER_SIZE.
BufferedWriter provides or overrides these methods in addition to those from BufferedIOBase and
IOBase:
flush()
Force bytes held in the buffer into the raw stream.
A BlockingIOError should be raised if the raw
stream blocks.
When in non-blocking mode, a
BlockingIOError is raised if the buffer needs to be written out but the raw stream blocks.
A buffered binary stream providing higher-level access to a seekable RawIOBase raw binary stream.
It inherits
BufferedReader and BufferedWriter.
The constructor creates a reader and writer for a seekable raw stream, given in the first argument.
If the buffer_size
is omitted it defaults to DEFAULT_BUFFER_SIZE.
BufferedRandom is capable of anything BufferedReader or BufferedWriter can do.
In addition,
seek() and tell() are guaranteed to be implemented.
A buffered binary stream providing higher-level access to two non seekable RawIOBase raw binary streams—one
readable, the other writeable.
It inherits BufferedIOBase.
reader and writer are RawIOBase objects that are readable and writeable respectively.
If the buffer_size is omitted
it defaults to DEFAULT_BUFFER_SIZE.
BufferedRWPair implements all of BufferedIOBase’s methods except for detach(), which raises
UnsupportedOperation.
Warning: BufferedRWPair does not attempt to synchronize accesses to its underlying raw streams.
You
should not pass it the same object as reader and writer; use BufferedRandom instead.
Text I/O
class io.TextIOBase
Base class for text streams.
This class provides a character and line based interface to stream I/O.
It inherits
IOBase.
TextIOBase provides or overrides these data attributes and methods in addition to those from IOBase:
encoding
The name of the encoding used to decode the stream’s bytes into strings, and to encode strings into bytes.
Depending on the implemen-
tation and the initial constructor flags, this may not be available.
This is not
part of the TextIOBase API and may not exist in some implementations.
After the underlying buffer has been detached, the TextIOBase is in an unusable state.
Some TextIOBase implementations, like StringIO, may not have the concept of an underlying buffer
and calling this method will raise UnsupportedOperation.
New in version 3.1.
If size is negative or None, reads
until EOF.
If the stream is already at EOF, an empty string is
returned.
If size is specified, at most size characters will be read.
Behaviour depends on the whence parameter.
The default
value for whence is SEEK_SET.
Any other offset value produces undefined behaviour.
Return the new absolute position as an opaque number.
The SEEK_* constants.
The number does not usually represent a number
of bytes in the underlying binary storage.
It inherits
TextIOBase.
It defaults to locale.
See
Text Encoding for more information.
Pass 'strict' to
raise a ValueError exception if there is an encoding error (the default of None has the same effect), or pass
'ignore' to ignore errors.
When writing, 'xmlcharrefreplace'
(replace with the appropriate XML character reference) or 'namereplace' (replace with \N{...} es-
cape sequences) can be used.
Any other error handling name that has been registered with codecs.
It can be None, '', '\n', '\r', and '\r\n'.
When reading input from the stream, if newline is None, universal newlines mode is enabled.
Lines in the
input can end in '\n', '\r', or '\r\n', and these are translated into '\n' before being returned to
the caller.
If newline is '', universal newlines mode is enabled, but line endings are returned to the caller
untranslated.
If newline has any of the other legal values, input lines are only terminated by the given string,
and the line ending is returned to the caller untranslated.
When writing output to the stream, if newline is None, any '\n' characters written are translated to the
system default line separator, os.linesep.
If newline is '' or '\n', no translation takes place.
If newline
is any of the other legal values, any '\n' characters written are translated to the given string.
If line_buffering is True, flush() is implied when a call to write contains a newline character or a carriage
return.
If write_through is True, calls to write() are guaranteed not to be buffered: any data written on the
TextIOWrapper object is immediately handled to its underlying binary buffer.
Changed in version 3.3: The write_through argument has been added.
Changed in version 3.3: The default encoding is now locale.getpreferredencoding(False) instead
of locale.getpreferredencoding().
Don’t change temporary the locale encoding using locale.
The encoding argument now supports the "locale" dummy encoding name.
TextIOWrapper provides these data attributes and methods in addition to those from TextIOBase and
IOBase:
line_buffering
Whether line buffering is enabled.
New in version 3.7.
reconfigure(*, encoding=None, errors=None, newline=None, line_buffering=None, write_through=None)
Reconfigure this text stream using new settings for encoding, errors, newline, line_buffering and write_through.
Parameters not specified keep current settings, except errors='strict' is used when encoding is spec-
ified but errors is not specified.
It is not possible to change the encoding or newline if some data has already been read from the stream.
On
the other hand, changing encoding after write is possible.
This method does an implicit stream flush before setting the new parameters.
New in version 3.7.
The method supports encoding="locale" option.
16.2.
SEEK_SET, /)
Set the stream position.
Return the new stream position as an int.
Four operations are supported, given by the following argument combinations:
• seek(0, SEEK_SET): Rewind to the start of the stream.
Restore a previous position; cookie must be a number returned by
tell().
Any other argument combinations are invalid, and may raise exceptions.
See also:
os.
SEEK_SET, os.
SEEK_CUR, and os.
SEEK_END.
tell()
Return the stream position as an opaque number.
The return value of tell() can be given as input to
seek(), to restore a previous stream position.
A text stream using an in-memory text buffer.
It inherits TextIOBase.
The text buffer is discarded when the close() method is called.
The initial value of the buffer can be set by providing initial_value.
If newline translation is enabled, newlines will be
encoded as if by write().
The stream is positioned at the start of the buffer which emulates opening an existing
file in a w+ mode, making it ready for an immediate write from the beginning or for a write that would overwrite the
initial value.
To emulate opening a file in an a+ mode ready for appending, use f.seek(0, io.SEEK_END)
to reposition the stream at the end of the buffer.
The newline argument works like that of TextIOWrapper, except that when writing output to the stream, if
newline is None, newlines are written as \n on all platforms.
StringIO provides this method in addition to those from TextIOBase and IOBase:
getvalue()
Return a str containing the entire contents of the buffer.
Newlines are decoded as if by read(), although
the stream position is not changed.
Example usage:
import io
output = io.StringIO()
output.write('First line.\n')
print('Second line.', file=output)
# Retrieve file contents -- this will be
# 'First line.\nSecond line.\n'
contents = output.getvalue()
# Close object and discard memory buffer --
# .getvalue() will now raise an exception.
IncrementalNewlineDecoder
A
helper
codec
that
decodes
newlines
for
universal
newlines
mode.
It
inherits
codecs.
IncrementalDecoder.
This section discusses the performance of the provided concrete I/O implementations.
Binary I/O
By reading and writing only large chunks of data even when the user asks for a single byte, buffered I/O hides any
inefficiency in calling and executing the operating system’s unbuffered I/O routines.
The gain depends on the OS and the
kind of I/O which is performed.
For example, on some modern OSes such as Linux, unbuffered disk I/O can be as fast
as buffered I/O.
The bottom line, however, is that buffered I/O offers predictable performance regardless of the platform
and the backing device.
Therefore, it is almost always preferable to use buffered I/O rather than unbuffered I/O for binary
data.
I/O over a binary storage (such as a file) is significantly slower than binary I/O over the same storage, because it
requires conversions between unicode and binary data using a character codec.
This can become noticeable handling
huge amounts of text data like large log files.
Also, tell() and seek() are both quite slow due to the reconstruction
algorithm used.
StringIO, however, is a native in-memory unicode container and will exhibit similar speed to BytesIO.
Multi-threading
FileIO objects are thread-safe to the extent that the operating system calls (such as read(2) under Unix) they wrap
are thread-safe too.
Binary buffered objects (instances of BufferedReader,
BufferedWriter,
BufferedRandom and
BufferedRWPair) protect their internal structures using a lock; it is therefore safe to call them from multiple threads
at once.
TextIOWrapper objects are not thread-safe.
Reentrancy
Binary buffered objects (instances of BufferedReader,
BufferedWriter,
BufferedRandom and
BufferedRWPair) are not reentrant.
While reentrant calls will not happen in normal situations, they can arise
from doing I/O in a signal handler.
If a thread tries to re-enter a buffered object which it is already accessing, a
RuntimeError is raised.
Note this doesn’t prohibit a different thread from entering the buffered object.
The above implicitly extends to text files, since the open() function will wrap a buffered object inside a
TextIOWrapper.
This includes standard streams and therefore affects the built-in print() function as well.
16.2.
This module provides various time-related functions.
For related functionality, see also the datetime and calendar
modules.
Although this module is always available, not all functions are available on all platforms.
Most of the functions defined in
this module call platform C library functions with the same name.
It may sometimes be helpful to consult the platform
documentation, because the semantics of these functions varies among platforms.
An explanation of some terminology and conventions is in order.
The epoch is the point where the time starts, the return value of time.gmtime(0).
It is January 1, 1970,
00:00:00 (UTC) on all platforms.
The term seconds since the epoch refers to the total number of elapsed seconds since the epoch, typically excluding
leap seconds.
Leap seconds are excluded from this total on all POSIX-compliant platforms.
The functions in this module may not handle dates and times before the epoch or far in the future.
The cut-off point
in the future is determined by the C library; for 32-bit systems, it is typically in 2038.
When 2-digit years are parsed, they
are converted according to the POSIX and ISO C standards: values 69–99 are mapped to 1969–1999, and values
0–68 are mapped to 2000–2068.
The acronym UTC is
not a mistake but a compromise between English and French.
DST
rules are magic (determined by local law) and can change from year to year.
The C library has a table containing
the local rules (often it is read from a system file for flexibility) and is the only source of True Wisdom in this
respect.
The precision of the various real-time functions may be less than suggested by the units in which their value or
argument is expressed.
E.g. on most Unix systems, the clock “ticks” only 50 or 100 times a second.
On the other hand, the precision of time() and sleep() is better than their Unix equivalents: times
are expressed as floating point numbers, time() returns the most accurate time available (using Unix
gettimeofday() where available), and sleep() will accept a time with a nonzero fraction (Unix select()
is used to implement this, where available).
The time value as returned by gmtime(), localtime(), and strptime(), and accepted by asctime(),
mktime() and strftime(), is a sequence of 9 integers.
The return values of gmtime(), localtime(),
and strptime() also offer attribute names for individual fields.
See struct_time for a description of these objects.
The struct_time type was extended to provide the tm_gmtoff and tm_zone
attributes when platform supports corresponding struct tm members.
The struct_time attributes tm_gmtoff and tm_zone are now available on all
platforms.
The day field is two characters long and is space
padded if the day is a single digit, e.g.: 'Wed Jun 9 04:26:40 1993'.
If t is not provided, the current time as returned by localtime() is used.
Locale information is not used by
asctime().
Note: Unlike the C function of the same name, asctime() does not add a trailing newline.
time.pthread_getcpuclockid(thread_id)
Return the clk_id of the thread-specific CPU-time clock for the specified thread_id.
Use threading.get_ident() or the ident attribute of threading.
Thread objects to get a suitable
value for thread_id.
Warning:
Passing an invalid or expired thread_id may result in undefined behavior, such as segmentation
fault.
Availability: Unix
See the man page for pthread_getcpuclockid(3) for further information.
New in version 3.7.
time.clock_getres(clk_id)
Return the resolution (precision) of the specified clock clk_id.
Refer to Clock ID Constants for a list of accepted
values for clk_id.
Availability: Unix.
New in version 3.3.
Refer to Clock ID Constants for a list of accepted values for clk_id.
Use clock_gettime_ns() to avoid the precision loss caused by the float type.
Availability: Unix.
New in version 3.3.
Availability: Unix.
New in version 3.7.
Currently, CLOCK_REALTIME is the only accepted value for clk_id.
Use clock_settime_ns() to avoid the precision loss caused by the float type.
Availability: Unix.
New in version 3.3.
Similar to clock_settime() but set time with nanoseconds.
Availability: Unix.
New in version 3.7.
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993'
representing local time.
The day field is two characters long and is space padded if the day is a single digit, e.g.:
'Wed Jun 9 04:26:40 1993'.
If secs is not provided or None, the current time as returned by time() is used.
Locale information is not used by ctime().
Get information on the specified clock as a namespace object.
The result has the following attributes:
• adjustable: True if the clock can be changed automatically (e.g. by a NTP daemon) or manually by the
system administrator, False otherwise
• implementation: The name of the underlying C function used to get the clock value.
Refer to Clock ID
Constants for possible values.
If secs is not provided or None, the current time as returned by time() is used.
Fractions of a second are
ignored.
See above for a description of the struct_time object.
See calendar.timegm() for the inverse
of this function.
If secs is not provided or None, the current time as returned by
time() is used.
The dst flag is set to 1 when DST applies to the given time.
It’s common for this to be restricted to years between 1970 and 2038.
This is the inverse function of localtime().
Its argument is the struct_time or full 9-tuple (since the dst
flag is needed; use -1 as the dst flag if it is unknown) which expresses the time in local time, not UTC.
It returns
a floating point number, for compatibility with time().
If the input value cannot be represented as a valid time,
either OverflowError or ValueError will be raised (which depends on whether the invalid value is caught
by Python or the underlying C libraries).
The clock
is not affected by system clock updates.
The reference point of the returned value is undefined, so that only the
difference between the results of two calls is valid.
Use monotonic_ns() to avoid the precision loss caused by the float type.
New in version 3.3.
The function is now always available and always system-wide.
Changed in version 3.10: On macOS, the function is now system-wide.
New in version 3.7.
It does include time elapsed during sleep and is system-wide.
The reference point of
the returned value is undefined, so that only the difference between the results of two calls is valid.
Use perf_counter_ns() to avoid the precision loss caused by the float type.
New in version 3.3.
Changed in version 3.10: On Windows, the function is now system-wide.
New in version 3.7.
It does
not include time elapsed during sleep.
It is process-wide by definition.
The reference point of the returned value is
undefined, so that only the difference between the results of two calls is valid.
Use process_time_ns() to avoid the precision loss caused by the float type.
New in version 3.3.
Suspend execution of the calling thread for the given number of seconds.
The argument may be a floating point
number to indicate a more precise sleep time.
If the sleep is interrupted by a signal and no exception is raised by the signal handler, the sleep is restarted with a
recomputed timeout.
The suspension time may be longer than requested by an arbitrary amount, because of the scheduling of other
activity in the system.
On Windows, if secs is zero, the thread relinquishes the remainder of its time slice to any other thread that is
ready to run.
If there are no other threads ready to run, the function returns immediately, and the thread continues
execution.
On Windows 8.1 and newer the implementation uses a high-resolution timer which provides resolution
of 100 nanoseconds.
If secs is zero, Sleep(0) is used.
Unix implementation:
16.3.
Changed in version 3.11: On Unix, the clock_nanosleep() and nanosleep() functions are now used if
available.
On Windows, a waitable timer is now used.
The function now sleeps at least secs even if the sleep is interrupted by a signal, except if
the signal handler raises an exception (see PEP 475 for the rationale).
If t is not provided, the current time as returned by localtime() is used.
ValueError is raised if any field in t is outside of the allowed range.
The following directives can be embedded in the format string.
They are shown without the optional field width
and precision specification, and are replaced by the indicated characters in the strftime() result:
Di-
rec-
tive
Meaning
Notes
%a
Locale’s abbreviated weekday name.
Day of the month as a decimal number [01,31].
All
days in a new year preceding the first Sunday are considered to be in week 0.
All days in a new year preceding the first Monday are considered to be in week 0.
A literal '%' character.
The use of %Z is now deprecated, but the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries.
Also, a strict reading of the original 1982 RFC 822 standard calls for a two-digit year (%y rather than %Y), but practice moved to 4-digit years long
before the year 2000.
After that, RFC 822 became obsolete and the 4-digit year has been first recommended by RFC 1123 and then mandated by
RFC 2822.
W are only used in calculations when the day of the
week and the year are specified.
Here is an example, a format for dates compatible with that specified in the RFC 2822 Internet email stan-
dard.
Page 740, 1
'Thu, 28 Jun 2001 14:17:15 +0000'
Additional directives may be supported on certain platforms, but only the ones listed here have a meaning stan-
dardized by ANSI C.
To see the full set of format codes supported on your platform, consult the strftime(3)
documentation.
On some platforms, an optional field width and precision specification can immediately follow the initial '%' of a
directive in the following order; this is also not portable.
The return value is a struct_time as returned by
gmtime() or localtime().
Y" which matches the formatting returned by ctime().
If string cannot be parsed according
to format, or if it has excess data after parsing, ValueError is raised.
The default values used to fill in any
missing data when more accurate values cannot be inferred are (1900, 1, 1, 0, 0, 0, 0, 1, -1).
Both string and format must be strings.
For example:
time.struct_time(tm_year=2000, tm_mon=11, tm_mday=30, tm_hour=0, tm_min=0,
tm_sec=0, tm_wday=3, tm_yday=335, tm_isdst=-1)
Support for the %Z directive is based on the values contained in tzname and whether daylight is true.
Because
of this, it is platform-specific except for recognizing UTC and GMT which are always known (and are considered
to be non-daylight savings timezones).
Only the directives specified in the documentation are supported.
Because strftime() is implemented per
platform it can sometimes offer more directives than those listed.
But strptime() is independent of any platform
and thus does not necessarily support all directives available that are not documented as supported.
It is an
object with a named tuple interface: values can be accessed by index and by attribute name.
The following values
are present:
16.3.
Note that unlike the C structure, the month value is a range of [1, 12], not [0, 11].
In calls to mktime(), tm_isdst may be set to 1 when daylight savings time is in effect, and 0 when it is not.
A value of -1 indicates that this is not known, and will usually result in the correct state being filled in.
When a tuple with an incorrect length is passed to a function expecting a struct_time, or having elements of
the wrong type, a TypeError is raised.
The handling of leap seconds is platform
dependent.
On Windows and most Unix systems, the leap seconds are not counted towards the time in seconds
since the epoch.
This is commonly referred to as Unix time.
Note that even though the time is always returned as a floating point number, not all systems provide time with a
better precision than 1 second.
While this function normally returns non-decreasing values, it can return a lower
value than a previous call if the system clock has been set back between the two calls.
The number returned by time() may be converted into a more common time format (i.e. year, month, day,
hour, etc…) in UTC by passing it to gmtime() function or in local time by passing it to the localtime()
function.
In both cases a struct_time object is returned, from which the components of the calendar date may
be accessed as attributes.
New in version 3.7.
It does
not include time elapsed during sleep.
It is thread-specific by definition.
The reference point of the returned value
is undefined, so that only the difference between the results of two calls in the same thread is valid.
Use thread_time_ns() to avoid the precision loss caused by the float type.
Availability: Linux, Unix, Windows.
Unix systems supporting CLOCK_THREAD_CPUTIME_ID.
New in version 3.7.
New in version 3.7.
time.tzset()
Reset the time conversion rules used by the library routines.
The environment variable TZ specifies how this is
done.
It will also set the variables tzname (from the TZ environment variable), timezone (non-DST seconds
West of UTC), altzone (DST seconds west of UTC) and daylight (to 0 if this timezone does not have any
daylight saving time rules, or to nonzero if there is a time, past, present or future when daylight saving time applies).
Availability: Unix.
Although in many cases, changing the TZ environment variable may affect the output of functions like
localtime() without calling tzset(), this behavior should not be relied on.
The TZ environment variable should contain no whitespace.
These will be propagated into
time.tzname
offset The offset has the form: ± hh[:mm[:ss]].
This indicates the value added the local time to arrive
at UTC.
If preceded by a ‘-’, the timezone is east of the Prime Meridian; otherwise, it is west.
If no offset
follows dst, summer time is assumed to be one hour ahead of standard time.
Jn The Julian day n (1 <= n <= 365).
Leap days are not counted, so in all years February 28 is day 59 and
March 1 is day 60.
Leap days are counted, and it is possible to refer to February
29.
Week
1 is the first week in which the d’th day occurs.
Day zero is a Sunday.
The default, if time
is not given, is 02:00:00.
To do this, set the TZ environment variable
to the path of the required timezone datafile, relative to the root of the systems ‘zoneinfo’ timezone database,
usually located at /usr/share/zoneinfo.
For example, 'US/Eastern', 'Australia/Melbourne',
'Egypt' or 'Europe/Amsterdam'.
CLOCK_BOOTTIME
Identical to CLOCK_MONOTONIC, except it also includes any time that the system is suspended.
This allows applications to get a suspend-aware monotonic clock without having to deal with the complications of
CLOCK_REALTIME, which may have discontinuities if the time is changed using settimeofday() or similar.
Availability: Linux >= 2.6.39.
New in version 3.7.
time.
The Solaris OS has a CLOCK_HIGHRES timer that attempts to use an optimal hardware source, and may give
close to nanosecond resolution.
CLOCK_HIGHRES is the nonadjustable, high-resolution clock.
Availability: Solaris.
New in version 3.3.
Clock that cannot be set and represents monotonic time since some unspecified starting point.
Availability: Unix.
New in version 3.3.
CLOCK_MONOTONIC_RAW
Similar to CLOCK_MONOTONIC, but provides access to a raw hardware-based time that is not subject to NTP
adjustments.
Availability: Linux >= 2.6.28, macOS >= 10.12.
New in version 3.3.
CLOCK_PROCESS_CPUTIME_ID
High-resolution per-process timer from the CPU.
Availability: Unix.
New in version 3.3.
CLOCK_PROF
High-resolution per-process timer from the CPU.
Availability: FreeBSD, NetBSD >= 7, OpenBSD.
New in version 3.7.
time.
The system must have a current leap second table in order for this to give the correct answer.
PTP or NTP software
can maintain a leap second table.
Availability: Linux.
New in version 3.9.
CLOCK_THREAD_CPUTIME_ID
Thread-specific CPU-time clock.
Availability: Unix.
New in version 3.3.
CLOCK_UPTIME
Time whose absolute value is the time the system has been running and not suspended, providing accurate uptime
measurement, both absolute and interval.
Availability: FreeBSD, OpenBSD >= 5.5.
New in version 3.7.
time.
CLOCK_UPTIME_RAW
Clock that increments monotonically, tracking the time since an arbitrary point, unaffected by frequency or time
adjustments and not incremented while the system is asleep.
Availability: macOS >= 10.12.
New in version 3.8.
The following constant is the only parameter that can be sent to clock_settime().
time.
CLOCK_REALTIME
System-wide real-time clock.
Setting this clock requires appropriate privileges.
Availability: Unix.
New in version 3.3.
This is negative if the local DST
timezone is east of UTC (as in Western Europe, including the UK).
Only use this if daylight is nonzero.
See
note below.
See note below.
See note below.
A tuple of two strings: the first is the name of the local non-DST timezone, the second is the name of the local
DST timezone.
If no DST timezone is defined, the second string should not be used.
See note below.
Note: For the above Timezone constants (altzone, daylight, timezone, and tzname), the value is determined
by the timezone rules in effect at module load time or the last time tzset() is called and may be incorrect for times
in the past.
It is recommended to use the tm_gmtoff and tm_zone results from localtime() to obtain timezone
information.
See also:
Module datetime More object-oriented interface to dates and times.
Module locale Internationalization services.
The locale setting affects the interpretation of many format specifiers in
strftime() and strptime().
Module calendar General calendar-related functions.
Source code: Lib/argparse.py
Tutorial
This page contains the API reference information.
For a more gentle introduction to Python command-line parsing,
have a look at the argparse tutorial.
The argparse module makes it easy to write user-friendly command-line interfaces.
The program defines what argu-
ments it requires, and argparse will figure out how to parse those out of sys.argv.
The argparse module also
automatically generates help and usage messages.
The module will also issue errors when users give the program invalid
arguments.
ArgumentParser.
It is a container for argument specifications and has options that apply to the parser as whole:
parser = argparse.
The ArgumentParser.add_argument() method attaches individual argument specifications to the parser.
It
supports positional arguments, options that accept values, and on/off flags:
parser.add_argument('filename')
# positional argument
parser.add_argument('-c', '--count')
# option that takes a value
parser.add_argument('-v', '--verbose',
action='store_true')
# on/off flag
The ArgumentParser.parse_args() method runs the parser and places the extracted data in a argparse.
Name Description
Values
ac-
tion
Specify how an argument should
be handled
'store',
'store_const',
'store_true',
'append',
'append_const', 'count', 'help', 'version'
choices Limit values to a specific set of
choices
['foo', 'bar'], range(1, 10), or Container instance
const
Store a constant value
de-
fault
Default value used when an argu-
ment is not provided
Defaults to None
dest
Specify the attribute name used
in the result namespace
help
Help message for an argument
metavarAlternate display name for the ar-
gument as shown in help
nargs
Number of times the argument
can be used
int, '?', '*', or '+'
re-
quired
Indicate whether an argument is
required or optional
True or False
type
Automatically convert an argu-
ment to the given type
int, float, argparse.
FileType('w'), or callable function
16.4.
Example
The following code is a Python program that takes a list of integers and produces either the sum or the max:
import argparse
parser = argparse.
The first step in using the argparse is creating an ArgumentParser object:
The ArgumentParser object will hold all the information necessary to parse the command line into Python data types.
Adding arguments
Filling an ArgumentParser with information about program arguments is done by making calls to the
add_argument() method.
Generally, these calls tell the ArgumentParser how to take the strings on the command
line and turn them into objects.
This information is stored and used when parse_args() is called.
For example:

help='an integer for the accumulator')

const=sum, default=max,

help='sum the integers (default: find the max)')
Later, calling parse_args() will return an object with two attributes, integers and accumulate.
The
integers attribute will be a list of one or more integers, and the accumulate attribute will be either the sum()
function, if --sum was specified at the command line, or the max() function if it was not.
Parsing arguments
ArgumentParser parses arguments through the parse_args() method.
This will inspect the command line,
convert each argument to the appropriate type and then invoke the appropriate action.
In a script, parse_args() will typically be called with no arguments, and the ArgumentParser will automatically
determine the command-line arguments from sys.argv.
ArgumentParser objects
class argparse.
HelpFormatter, prefix_chars=’-’,
fromfile_prefix_chars=None, argument_default=None,
conflict_handler=’error’, add_help=True, allow_abbrev=True,
exit_on_error=True)
Create a new ArgumentParser object.
All parameters should be passed as keyword arguments.
The strategy for resolving conflicting optionals (usually unnecessary)
• add_help - Add a -h/--help option to the parser (default: True)
• allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous.
Changed in version 3.8: In previous versions, allow_abbrev also disabled grouping of short flags such as -vv to
mean -v -v.
The following sections describe how each of these are used.
This default is almost always desirable because it will make the help messages match how the program
was invoked on the command line.
For example, consider a file named myprogram.py with the following code:
import argparse
parser = argparse.
The %(prog)s format specifier is available to fill in the program name in your usage messages.
This argument
gives a brief description of what the program does and how it works.
To change this behavior, see the
formatter_class argument.
Rather than repeating the definitions of these arguments,
a single parser with all the shared arguments and passed to parents= argument to ArgumentParser can be used.
Otherwise, the ArgumentParser will see two -h/
--help options (one in the parent and one in the child) and raise an error.
Note: You must fully initialize the parsers before passing them via parents=.
If you change the parent parsers after
the child parser, those changes will not be reflected in the child.
Currently, there are four such classes:
class argparse.RawDescriptionHelpFormatter
class argparse.
RawTextHelpFormatter
class argparse.
ArgumentDefaultsHelpFormatter
class argparse.
MetavarTypeHelpFormatter
RawDescriptionHelpFormatter and RawTextHelpFormatter give more control over how textual descrip-
tions are displayed.
However,
multiple new lines are replaced with one.
If you wish to preserve multiple blank lines, add spaces between the newlines.
The prefix_chars= argument defaults to '-'.
Supplying a set of characters that does not include - will cause
-f/--foo options to be disallowed.
If the fromfile_prefix_chars= argument is given to the
ArgumentParser constructor, then arguments that start with any of the specified characters will be treated as files,
and will be replaced by the arguments they contain.
For example:

fp.write('-f\nbar')

Namespace(f='bar')
Arguments read from a file must by default be one per line (but see also convert_arg_line_to_args()) and
are treated as if they were in the same place as the original file referencing argument on the command line.
So in
the example above, the expression ['-f', 'foo', '@args.txt'] is considered equivalent to the expression
['-f', 'foo', '-f', 'bar'].
ArgumentParser uses filesystem encoding and error handler to read the file containing arguments.
The fromfile_prefix_chars= argument defaults to None, meaning that arguments will never be treated as file
references.
Changed in version 3.12: ArgumentParser changed encoding and errors to read arguments files from default (e.g.
locale.getpreferredencoding(False) and "strict") to filesystem encoding and error handler.
Argu-
ments file should be encoded in UTF-8 instead of ANSI Codepage on Windows.
Sometimes however, it may be useful to specify
a single parser-wide default for arguments.
This can be accomplished by passing the argument_default= keyword
argument to ArgumentParser.
For example, to globally suppress attribute creation on parse_args() calls, we
supply argument_default=SUPPRESS:
(continues on next page)
16.4.
By default, ArgumentParser
objects raise an exception if an attempt is made to create an argument with an option string that is already in use:
Traceback (most recent call last):
..
FOO
old foo help
--foo FOO
new foo help
Note that ArgumentParser objects only remove an action if all of its option strings are overridden.
So, in the example
above, the old -f/--foo action is retained as the -f action, because only the --foo option string was overridden.
For example, consider
a file named myprogram.py containing the following code:
import argparse
parser = argparse.
FOO
foo help
The help option is typically -h/--help.
The exception to this is if the prefix_chars= is specified and does not
include -, in which case -h and --help are not valid options.
If the user would like to catch errors manually, the feature can be enabled by setting exit_on_error to False:
_StoreAction(option_strings=['--integers'], dest='integers', nargs=None, const=None,␣
�→default=None, type=<class 'int'>, choices=None, help=None, metavar=None)

parser.parse_args('--integers a'.split())
... except argparse.
ArgumentError:

print('Catching an argumentError')
(continues on next page)
16.4.
Catching an argumentError
New in version 3.9.
The add_argument() method
ArgumentParser.add_argument(name or flags...
Each parameter has its own more detailed descrip-
tion below, but in short they are:
• name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.
A name for the argument in usage messages.
The following sections describe how each of these are used.
name or flags
The add_argument() method must know whether an optional argument, like -f or --foo, or a positional argument,
like a list of filenames, is expected.
The first arguments passed to add_argument() must therefore be either a series
of flags, or a simple argument name.
These actions can do just about anything
with the command-line arguments associated with them, though most actions simply add an attribute to the object returned
by parse_args().
The action keyword argument specifies how the command-line arguments should be handled.
This just stores the argument’s value.
This is the default action.
For example:
Namespace(foo='1')
• 'store_const' - This stores the value specified by the const keyword argument; note that the const keyword
argument defaults to None.
The 'store_const' action is most commonly used with optional arguments that
specify some sort of flag.
For example:
Namespace(foo=42)
• 'store_true' and 'store_false' - These are special cases of 'store_const' used for storing the
values True and False respectively.
In addition, they create default values of False and True respectively.
For example:
Namespace(foo=True, bar=False, baz=True)
• 'append' - This stores a list, and appends each argument value to the list.
It is useful to allow an option to be
specified multiple times.
If the default value is non-empty, the default elements will be present in the parsed value
for the option, with any values from the command line appended after those default values.
Example usage:
Namespace(foo=['1', '2'])
• 'append_const' - This stores a list, and appends the value specified by the const keyword argument to the list;
note that the const keyword argument defaults to None.
The 'append_const' action is typically useful when
multiple arguments need to store constants to the same list.
For example:
16.4.
Note, the default will be None unless explicitly set to 0.
• 'help' - This prints a complete help message for all the options in the current parser and then exits.
By default a
help action is automatically added to the parser.
See ArgumentParser for details of how the output is created.
Example usage:
Namespace(foo=['f1', 'f2', 'f3', 'f4'])
New in version 3.8.
You may also specify an arbitrary action by passing an Action subclass or other object that implements the same interface.
Namespace(foo=False)
New in version 3.9.
The recommended way to create a custom action is to extend Action, overriding the __call__ method and optionally
the __init__ and format_usage methods.
For more details, see Action.
nargs
ArgumentParser objects usually associate a single command-line argument with a single action to be taken.
The nargs
keyword argument associates a different number of command-line arguments with a single action.
See also specifying-
ambiguous-arguments.
The supported values are:
• N (an integer).
N arguments from the command line will be gathered together into a list.
For example:
Namespace(bar=['c'], foo=['a', 'b'])
Note that nargs=1 produces a list of one item.
This is different from the default, in which the item is produced
by itself.
One argument will be consumed from the command line if possible, and produced as a single item.
If no
command-line argument is present, the value from default will be produced.
Note that for optional arguments,
there is an additional case - the option string is present but not followed by a command-line argument.
In this case
the value from const will be produced.
One of the more common uses of nargs='?' is to allow optional input and output files:

default=sys.stdin)

default=sys.stdout)
(continues on next page)
16.4.
TextIOWrapper name='input.txt' encoding='UTF-8'>,
outfile=<_io.
TextIOWrapper name='output.txt' encoding='UTF-8'>)
Namespace(infile=<_io.
TextIOWrapper name='<stdin>' encoding='UTF-8'>,
outfile=<_io.
TextIOWrapper name='<stdout>' encoding='UTF-8'>)
• '*'.
All command-line arguments present are gathered into a list.
Note that it generally doesn’t make much sense
to have more than one positional argument with nargs='*', but multiple optional arguments with nargs='*'
is possible.
For example:
Namespace(bar=['1', '2'], baz=['a', 'b'], foo=['x', 'y'])
• '+'.
Just like '*', all command-line args present are gathered into a list.
Additionally, an error message will be
generated if there wasn’t at least one command-line argument present.
Gen-
erally this means a single command-line argument will be consumed and a single item (not a list) will be produced.
The two most common uses of it are:
• When add_argument() is called with action='store_const' or action='append_const'.
These actions add the const value to one of the attributes of the object returned by parse_args().
See
the action description for examples.
If const is not provided to add_argument(), it will receive a default
value of None.
This creates an
optional argument that can be followed by zero or one command-line arguments.
When parsing the command
line, if the option string is encountered with no command-line argument following it, the value of const will be
assumed to be None instead.
See the nargs description for examples.
The default keyword
argument of add_argument(), whose value defaults to None, specifies what value should be used if the command-
line argument is not present.
If the default value is a string, the parser parses the value as if it were a command-line argument.
In particular, the
parser applies any type conversion argument, if provided, before setting the attribute on the Namespace return value.
For positional arguments with nargs equal to ?
Providing default=argparse.
SUPPRESS causes no attribute to be added if the command-line argument was not
present:
Namespace()
Namespace(foo='1')
16.4.
However, quite often the command-line string
should instead be interpreted as another type, such as a float or int.
The type keyword for add_argument()
allows any necessary type-checking and type conversions to be performed.
If the type keyword is used with the default keyword, the type converter is only applied if the default is a string.
The argument to type can be any callable that accepts a single string.
If the function raises ArgumentTypeError,
TypeError, or ValueError, the exception is caught and a nicely formatted error message is displayed.
No other
exception types are handled.
Common built-in types and functions can be used as type converters:
import argparse
import pathlib
parser = argparse.
ArgumentParser()
parser.add_argument('count', type=int)
parser.add_argument('distance', type=float)
parser.add_argument('street', type=ascii)
parser.add_argument('code_point', type=ord)
parser.add_argument('source_file', type=open)
parser.add_argument('dest_file', type=argparse.
The bool() function is not recommended as a type converter.
All it does is convert empty strings to False and
non-empty strings to True.
This is usually not what is desired.
In general, the type keyword is a convenience that should only be used for simple conversions that can only raise one of
the three supported exceptions.
Anything with more interesting error-handling or resource management should be done
downstream after the arguments are parsed.
For example, JSON or YAML conversions have complex error cases that require better reporting than can be given by
the type keyword.
A JSONDecodeError would not be well formatted and a FileNotFoundError exception
would not be handled at all.
Even FileType has its limitations for use with the type keyword.
If one argument uses FileType and then a subsequent
argument fails, an error is reported but the file is not automatically closed.
In this case, it would be better to wait until
after the parser has run and then use the with-statement to manage the files.
For type checkers that simply check against a fixed set of values, consider using the choices keyword instead.
These can be handled by passing a
sequence object as the choices keyword argument to add_argument().
Any sequence can be passed as the choices value, so list objects, tuple objects, and custom sequences are all sup-
ported.
Use of enum.
Enum is not recommended because it is difficult to control its appearance in usage, help, and error messages.
Formatted choices override the default metavar which is normally derived from dest.
This is usually what you want because
the user never sees the dest parameter.
If this display isn’t desirable (perhaps because there are many choices), just specify
an explicit metavar.
Note:
Required options are generally considered bad form because users expect options to be optional, and thus they
should be avoided when possible.
By default,
ArgumentParser objects use the dest value as the “name” of each object.
By default, for positional argument actions,
the dest value is used directly, and for optional argument actions, the dest value is uppercased.
So, a single positional
argument with dest='bar' will be referred to as bar.
A single optional argument --foo that should be followed by
a single command-line argument will be referred to as FOO.
Different values of nargs may cause the metavar to be used multiple times.
The name
of this attribute is determined by the dest keyword argument of add_argument().
For optional argument actions, the value of dest is normally inferred from the option strings.
ArgumentParser
generates the value of dest by taking the first long option string and stripping away the initial -- string.
If no long
option strings were supplied, dest will be derived from the first short option string by stripping the initial - character.
Any internal - characters will be converted to _ characters to make sure the string is a valid attribute name.
The examples
below illustrate this behavior:
Namespace(foo_bar='1', x='2')
Namespace(foo_bar='1', x='2')
dest allows a custom attribute name to be provided:
Namespace(bar='XXX')
Action classes
Action classes implement the Action API, a callable which returns a callable which processes arguments from the
command-line.
Any object which follows this API may be passed as the action parameter to add_argument().
Action(option_strings, dest, nargs=None, const=None, default=None, type=None,
choices=None, required=False, help=None, metavar=None)
Action objects are used by an ArgumentParser to represent the information needed to parse a single argument from one
or more strings from the command line.
The Action class must accept the two positional arguments plus any keyword
arguments passed to ArgumentParser.add_argument() except for the action itself.
Instances of Action (or return value of any callable to the action parameter) should have attributes “dest”, “op-
tion_strings”, “default”, “type”, “required”, “help”, etc. defined.
The easiest way to ensure these attributes are defined is
to call Action.__init__.
Action instances should be callable, so subclasses must override the __call__ method, which should accept four pa-
rameters:
• parser - The ArgumentParser object which contains this action.
Most actions add an attribute
to this object using setattr().
Type conversions are
specified with the type keyword argument to add_argument().
The option_string argument is
optional, and will be absent if the action is associated with a positional argument.
The __call__ method may perform arbitrary actions, but will typically set attributes on the namespace based on
dest and values.
Action subclasses can define a format_usage method that takes no argument and return a string which will be used
when printing the usage of the program.
If such method is not provided, a sensible default will be used.
The parse_args() method
ArgumentParser.parse_args(args=None, namespace=None)
Convert argument strings to objects and assign them as attributes of the namespace.
Return the populated names-
pace.
Previous calls to add_argument() determine exactly what objects are created and how they are assigned.
See
the documentation for add_argument() for details.
The default is taken from sys.argv.
The default is a new empty Namespace object.
Option value syntax
The parse_args() method supports several ways of specifying the value of an option (if it takes one).
Namespace(foo=None, x='X')
Several short options can be joined together, using only a single - prefix, as long as only the last option (or none of them)
requires a value:
Namespace(x=True, y=True, z='Z')
16.4.
For example, the command-line argument -1 could either be an attempt to specify an option
or an attempt to provide a positional argument.
An error is produced for arguments that could produce more than one options.
This feature can be disabled by setting
allow_abbrev to False.
Beyond sys.argv
Sometimes it may be useful to have an ArgumentParser parse arguments other than those of sys.argv.
This can be
accomplished by passing a list of strings to parse_args().
This is useful for testing at the interactive prompt:

'integers', metavar='int', type=int, choices=range(10),

nargs='+', help='an integer in the range 0..9')

'--sum', dest='accumulate', action='store_const', const=sum,

default=max, help='sum the integers (default: find the max)')
Namespace(accumulate=<built-in function max>, integers=[1, 2, 3, 4])
Namespace(accumulate=<built-in function sum>, integers=[1, 2, 3, 4])
16.4.
The Namespace object
class argparse.
Namespace
Simple class used by default by parse_args() to create an object holding attributes and return it.
This class is deliberately simple, just an object subclass with a readable string representation.
If you prefer to have
dict-like view of the attributes, you can use the standard Python idiom, vars():
{'foo': 'BAR'}
It may also be useful to have an ArgumentParser assign attributes to an already existing object, rather than a new
Namespace object.
Splitting up functionality this
way can be a particularly good idea when a program performs several different functions which require differ-
ent kinds of command-line arguments.
ArgumentParser supports the creation of such sub-commands with
the add_subparsers() method.
The add_subparsers() method is normally called with no arguments
and returns a special action object.
Some example usage:
Namespace(bar=12, foo=False)
Namespace(baz='Z', foo=True)
Note that the object returned by parse_args() will only contain attributes for the main parser and the subparser
that was selected by the command line (and not any other subparsers).
So in the example above, when the a
command is specified, only the foo and bar attributes are present, and when the b command is specified, only
the foo and baz attributes are present.
Similarly, when a help message is requested from a subparser, only the help for that particular parser will be printed.
The help message will not include parent parser or sibling parser messages.
When either
is present, the subparser’s commands will appear in their own group in the help output.
One particularly effective way of handling sub-commands is to combine the use of the add_subparsers()
method with calls to set_defaults() so that each subparser knows which Python function it should execute.
This way, you can let parse_args() do the job of calling the appropriate function after argument parsing is
complete.
Associating functions with actions like this is typically the easiest way to handle the different actions for
each of your subparsers.
However, if it is necessary to check the name of the subparser that was invoked, the dest
keyword argument to the add_subparsers() call will work:
Namespace(subparser_name='2', y='frobble')
Changed in version 3.7: New required keyword argument.
FileType objects
class argparse.
The FileType factory creates objects that can be passed to the type argument of ArgumentParser.
Arguments that have FileType objects as their type will open command-line arguments
as files with the requested modes, buffer sizes, encodings and error handling (see the open() function for more
details):
Namespace(out=<_io.
TextIOWrapper name='file.txt' mode='w' encoding='UTF-8'>, raw=
�→<_io.
FileType objects understand the pseudo-argument '-' and automatically convert this into sys.stdin for read-
able FileType objects and sys.stdout for writable FileType objects:
16.4.
The encodings and errors keyword arguments.
By default, ArgumentParser groups command-line arguments into “positional arguments” and “options” when
displaying help messages.
When an argument is added to the group, the parser treats
it just like a normal argument, but displays the argument in a separate group for help messages.
Calling add_argument_group() on an argument group is deprecated.
This feature
was never supported and does not always work correctly.
The function exists on the API by accident through
inheritance and will be removed in the future.
However, a mutually exclusive group can be added to an argument group that
has a title and description.
Calling add_argument_group() or add_mutually_exclusive_group()
on a mutually exclusive group is deprecated.
These features were never supported and do not always work correctly.
The functions exist on the API by accident through inheritance and will be removed in the future.
Most of the time, the attributes of the object returned by parse_args() will be fully determined by inspecting
the command-line arguments and the argument actions.
See
the
add_subparsers() method for an example of this type.
ArgumentParser.get_default(dest)
Get the default value for a namespace attribute, as set by either add_argument() or by set_defaults():
'badger'
Printing help
In most typical applications, parse_args() will take care of formatting and printing any usage or error messages.
However, several formatting methods are available:
ArgumentParser.print_usage(file=None)
Print a brief description of how the ArgumentParser should be invoked on the command line.
If file is None,
sys.stdout is assumed.
ArgumentParser.print_help(file=None)
Print a help message, including the program usage and information about the arguments registered with the
ArgumentParser.
If file is None, sys.stdout is assumed.
There are also variants of these methods that simply return a string instead of printing it:
ArgumentParser.format_usage()
Return a string containing a brief description of how the ArgumentParser should be invoked on the command
line.
ArgumentParser.format_help()
Return a string containing a help message, including the program usage and information about the arguments
registered with the ArgumentParser.
Sometimes a script may only parse a few of the command-line arguments, passing the remaining arguments on to an-
other script or program.
In these cases, the parse_known_args() method can be useful.
It works much like
parse_args() except that it does not produce an error when extra arguments are present.
Instead, it returns a two
item tuple containing the populated namespace and the list of remaining argument strings.
(Namespace(bar='BAR', foo=True), ['--badger', 'spam'])
Warning: Prefix matching rules apply to parse_known_args().
The parser may consume an option even if it’s
just a prefix of one of its known options, instead of leaving it in the remaining arguments list.
Arguments that are read from a file (see the fromfile_prefix_chars keyword argument to the ArgumentParser
constructor) are read one argument per line.
This method takes a single argument arg_line which is a string read from the argument file.
It returns a list of
arguments parsed from this string.
The method is called once per line read from the argument file, in order.
A useful override of this method is one that treats each space-separated word as an argument.
The following
example demonstrates how to do this:
class MyArgumentParser(argparse.
This method terminates the program, exiting with the specified status and, if given, it prints a message before that.
The user can override this method to handle these steps differently:
class ErrorCatchingArgumentParser(argparse.
This method prints a usage message including the message to the standard error and terminates the program with a
status code of 2.
16.4.
A number of Unix commands allow the user to intermix optional arguments with positional arguments.
The
parse_intermixed_args() and parse_known_intermixed_args() methods support this parsing style.
These parsers do not support all the argparse features, and will raise exceptions if unsupported features are used.
In
particular, subparsers, and mutually exclusive groups that include both optionals and positionals are not supported.
The following example shows the difference between parse_known_args() and parse_intermixed_args():
the former returns ['2', '3'] as unparsed arguments, while the latter collects all the positionals into rest.
Namespace(cmd='doit', foo='bar', rest=[1, 2, 3])
parse_known_intermixed_args() returns a two item tuple containing the populated namespace and the list
of remaining argument strings.
New in version 3.7.
However, optparse
was difficult to extend transparently, particularly with the changes required to support the new nargs= specifiers and
better usage messages.
When most everything in optparse had either been copy-pasted over or monkey-patched, it no
longer seemed practical to try to maintain the backwards compatibility.
The argparse module improves on the standard library optparse module in a number of ways including:
• Handling positional arguments.
Producing more informative usage messages.
Providing a much simpler interface for custom type and action.
A partial upgrade path from optparse to argparse:
• Replace
all
optparse.
OptionParser.add_option()
calls
with
ArgumentParser.
Keep in mind
that what was previously called options, now in the argparse context is called args.
OptionParser.disable_interspersed_args()
by
using
parse_intermixed_args() instead of parse_args().
Values with Namespace and optparse.
OptionError and optparse.
OptionValueError with ArgumentError.
ArgumentError
An error from creating or using an argument (optional or positional).
The string value of this exception is the message, augmented with information about the argument that caused it.
ArgumentTypeError
Raised when something goes wrong converting a command line string to a type.
16.5 getopt — C-style parser for command line options
Source code: Lib/getopt.py
Note: The getopt module is a parser for command line options whose API is designed to be familiar to users of the
C getopt() function.
Users who are unfamiliar with the C getopt() function or who would like to write less code
and get better help and error messages should consider using the argparse module instead.
This module helps scripts to parse the command line arguments in sys.argv.
It supports the same conventions as the
Unix getopt() function (including the special meanings of arguments of the form ‘-’ and ‘--‘).
Long options similar
to those supported by GNU software may be used as well via an optional third argument.
This module provides two functions and an exception:
getopt.getopt(args, shortopts, longopts=[])
Parses command line options and parameter list.
Typically, this means sys.argv[1:].
Note:
Unlike GNU getopt(), after a non-option argument, all further arguments are considered also non-
options.
This is similar to the way non-GNU Unix systems work.
The
leading '--' characters should not be included in the option name.
Long options which require an argument
16.5.
Optional arguments are not supported.
To accept only long options,
shortopts should be an empty string.
Long options on the command line can be recognized so long as they provide
a prefix of the option name that matches exactly one of the accepted options.
For example, if longopts is ['foo',
'frob'], the option --fo will match as --foo, but --f will not match uniquely, so GetoptError will be
raised.
The return value consists of two elements: the first is a list of (option, value) pairs; the second is the list of
program arguments left after the option list was stripped (this is a trailing slice of args).
Each option-and-value pair
returned has the option as its first element, prefixed with a hyphen for short options (e.g., '-x') or two hyphens
for long options (e.g., '--long-option'), and the option argument as its second element, or an empty string if
the option has no argument.
The options occur in the list in the same order in which they were found, thus allowing
multiple occurrences.
Long and short options may be mixed.
This function works like getopt(), except that GNU style scanning mode is used by default.
This means that
option and non-option arguments may be intermixed.
The getopt() function stops processing options as soon
as a non-option argument is encountered.
If the first character of the option string is '+', or if the environment variable POSIXLY_CORRECT is set, then
option processing stops as soon as a non-option argument is encountered.
exception getopt.
GetoptError
This is raised when an unrecognized option is found in the argument list or when an option requiring an argument
is given none.
The argument to the exception is a string indicating the cause of the error.
For long options, an
argument given to an option which does not require one will also cause this exception to be raised.
The attributes
msg and opt give the error message and related option; if there is no specific option to which the exception relates,
opt is an empty string.
See also:
Module argparse Alternative command line option and argument parsing library.
For tutorial information and discussion of more advanced topics,
see
• Basic Tutorial
16.6.
The key benefit of having the logging API provided by a standard library module is that all Python modules can participate
in logging, so your application log can include your own messages integrated with messages from third-party modules.
The simplest example:
WARNING:root:Watch out!
The module provides a lot of functionality and flexibility.
If you are unfamiliar with logging, the best way to get to grips
with it is to view the tutorials (see the links above and on the right).
The basic classes defined by the module, together with their functions, are listed below.
Handlers send the log records (created by loggers) to the appropriate destination.
Logger Objects
Loggers have the following attributes and methods.
Note that Loggers should NEVER be instantiated directly, but always
through the module-level function logging.getLogger(name).
Multiple calls to getLogger() with the same
name will always return a reference to the same Logger object.
The name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain
foo, for example).
Loggers that are further down in the hierarchical list are children of loggers higher up in the list.
For
example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all
descendants of foo.
The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you or-
ganise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__).
That’s because in a module, __name__ is the module’s name in the Python package namespace.
Logger
propagate
If this attribute evaluates to true, events logged to this logger will be passed to the handlers of higher level
(ancestor) loggers, in addition to any handlers attached to this logger.
Messages are passed directly to the
ancestor loggers’ handlers - neither the level nor filters of the ancestor loggers in question are considered.
If this evaluates to false, logging messages are not passed to the handlers of ancestor loggers.
Spelling it out with an example: If the propagate attribute of the logger named A.B.C evaluates to true, any
event logged to A.B.C via a method call such as logging.getLogger('A.B.C').error(...)
will [subject to passing that logger’s level and filter settings] be passed in turn to any handlers attached to
loggers named A.B, A and the root logger, after first being passed to any handlers attached to A.B.C.
If
any logger in the chain A.B.C, A.B, A has its propagate attribute set to false, then that is the last logger
whose handlers are offered the event to handle, and propagation stops at that point.
In general, you should not need to attach a handler to more than one logger - if you just attach
it to the appropriate logger which is highest in the logger hierarchy, then it will see all events logged by all
descendant loggers, provided that their propagate setting is left set to True.
A common scenario is to attach
handlers only to the root logger, and to let propagation take care of the rest.
Sets the threshold for this logger to level.
Logging messages which are less severe than level will be ignored;
logging messages which have severity level or higher will be emitted by whichever handler or handlers service
this logger, unless a handler’s level has been set to a higher severity level than level.
When a logger is created, the level is set to NOTSET (which causes all messages to be processed when the
logger is the root logger, or delegation to the parent when the logger is a non-root logger).
Note that the root
logger is created with level WARNING.
The term ‘delegation to the parent’ means that if a logger has a level of NOTSET, its chain of ancestor loggers
is traversed until either an ancestor with a level other than NOTSET is found, or the root is reached.
If an ancestor is found with a level other than NOTSET, then that ancestor’s level is treated as the effective
level of the logger where the ancestor search began, and is used to determine how a logging event is handled.
If the root is reached, and it has a level of NOTSET, then all messages will be processed.
Otherwise, the
root’s level will be used as the effective level.
See Logging Levels for a list of levels.
The level parameter now accepts a string representation of the level such as ‘INFO’
as an alternative to the integer constants such as INFO.
Note, however, that levels are internally stored as
integers, and methods such as e.g. getEffectiveLevel() and isEnabledFor() will return/expect
to be passed integers.
isEnabledFor(level)
Indicates if a message of severity level would be processed by this logger.
This method checks first the
module-level level set by logging.disable(level) and then the logger’s effective level as determined
by getEffectiveLevel().
If a value other than NOTSET has been set using setLevel(),
it is returned.
Otherwise, the hierarchy is traversed towards the root until a value other than NOTSET is
found, and that value is returned.
The value returned is an integer, typically one of logging.
DEBUG,
logging.
INFO etc.
getChild(suffix)
Returns a logger which is a descendant to this logger, as determined by the suffix.
Thus, logging.
This is a convenience method, useful when the parent
logger is named using e.g. __name__ rather than a literal string.
New in version 3.2.
So for example logging.
Likewise, logging.getLogger('foo').
Logs a message with level DEBUG on this logger.
The msg is the message format string, and the args are the
arguments which are merged into msg using the string formatting operator.
No % formatting operation is
performed on msg when no args are supplied.
There are four keyword arguments in kwargs which are inspected: exc_info, stack_info, stacklevel and extra.
If exc_info does not evaluate as false, it causes exception information to be added to the logging message.
If
an exception tuple (in the format returned by sys.exc_info()) or an exception instance is provided, it
is used; otherwise, sys.exc_info() is called to get the exception information.
The second optional keyword argument is stack_info, which defaults to False.
If true, stack information is
added to the logging message, including the actual logging call.
Note that this is not the same stack information
as that displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to
the logging call in the current thread, whereas the latter is information about stack frames which have been
unwound, following an exception, while searching for exception handlers.
You can specify stack_info independently of exc_info, e.g. to just show how you got to a certain point in your
code, even when no exceptions were raised.
The stack frames are printed following a header line which says:
Stack (most recent call last):
This mimics the Traceback (most recent call last): which is used when displaying excep-
tion frames.
The third optional keyword argument is stacklevel, which defaults to 1.
If greater than 1, the correspond-
ing number of stack frames are skipped when computing the line number and function name set in the
LogRecord created for the logging event.
This can be used in logging helpers so that the function name,
filename and line number recorded are not the information for the helper function/method, but rather its
caller.
The name of this parameter mirrors the equivalent one in the warnings module.
The fourth keyword argument is extra which can be used to pass a dictionary which is used to populate
the __dict__ of the LogRecord created for the logging event with user-defined attributes.
These custom
attributes can then be used as you like.
For example, they could be incorporated into logged messages.
For
example:
FORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'
logging.basicConfig(format=FORMAT)
d = {'clientip': '192.168.0.1', 'user': 'fbloggs'}
logger = logging.getLogger('tcpserver')
logger.warning('Protocol problem: %s', 'connection reset', extra=d)
would print something like
2006-02-08 22:20:02,165 192.168.0.1 fbloggs
Protocol problem: connection␣
�→reset
The keys in the dictionary passed in extra should not clash with the keys used by the logging system.
If you choose to use these attributes in logged messages, you need to exercise some care.
In the above example,
for instance, the Formatter has been set up with a format string which expects ‘clientip’ and ‘user’ in the
attribute dictionary of the LogRecord.
If these are missing, the message will not be logged because a string
formatting exception will occur.
So in this case, you always need to pass the extra dictionary with these keys.
In such circumstances, it is likely that specialized Formatters would be used with particular
Handlers.
If no handler is attached to this logger (or any of its ancestors, taking into account the relevant Logger.
The stack_info parameter was added.
The exc_info parameter can now accept exception instances.
The stacklevel parameter was added.
Logs a message with level INFO on this logger.
The arguments are interpreted as for debug().
Logs a message with level WARNING on this logger.
The arguments are interpreted as for debug().
Note: There is an obsolete method warn which is functionally identical to warning.
As warn is depre-
cated, please do not use it - use warning instead.
The arguments are interpreted as for debug().
Logs a message with level CRITICAL on this logger.
The arguments are interpreted as for debug().
Logs a message with integer level level on this logger.
The other arguments are interpreted as for debug().
The arguments are interpreted as for debug().
Exception
info is added to the logging message.
This method should only be called from an exception handler.
Removes the specified filter filter from this logger.
filter(record)
Apply this logger’s filters to the record and return True if the record is to be processed.
The filters are
consulted in turn, until one of them returns a false value.
If none of them return a false value, the record will
be processed (passed to handlers).
If one returns a false value, no further processing of the record occurs.
Removes the specified handler hdlr from this logger.
Returns the filename, line number, function name and
stack information as a 4-element tuple.
The stack information is returned as None unless stack_info is True.
The stacklevel parameter is passed from code calling the debug() and other APIs.
If greater than 1, the
excess is used to skip stack frames before determining the values to be returned.
This will generally be useful
16.6.
Handles a record by passing it to all handlers associated with this logger and its ancestors (until a false value
of propagate is found).
This method is used for unpickled records received from a socket, as well as those
created locally.
Logger-level filtering is applied using filter().
This is a factory method which can be overridden in subclasses to create specialized LogRecord instances.
This is done by looking for handlers in this logger
and its parents in the logger hierarchy.
Returns True if a handler was found, else False.
The method stops
searching up the hierarchy whenever a logger with the ‘propagate’ attribute set to false is found - that will be
the last logger which is checked for the existence of handlers.
New in version 3.2.
Changed in version 3.7: Loggers can now be pickled and unpickled.
These are primarily of interest if you want to define
your own levels, and need them to have specific values relative to the predefined levels.
If you define a level with the same
numeric value, it overwrites the predefined value; the predefined name is lost.
Level
Numeric value
What it means / When to use it
logging.
NOTSET

When set on a logger, indicates that
ancestor loggers are to be consulted
to determine the effective level.
If
that still resolves to NOTSET, then all
events are logged.
When set on a han-
dler, all events are handled.
DEBUG

Detailed information, typically only
of interest to a developer trying to di-
agnose a problem.
logging.
INFO

Confirmation that things are working
as expected.
logging.
WARNING

An indication that something unex-
pected happened, or that a problem
might occur in the near future (e.g.
‘disk space low’).
The software is still
working as expected.
ERROR

Due to a more serious problem, the
software has not been able to perform
some function.
CRITICAL

A serious error, indicating that the
program itself may be unable to con-
tinue running.
Note that Handler is never instantiated directly; this class acts
as a base for more useful subclasses.
However, the __init__() method in subclasses needs to call Handler.
Handler
__init__(level=NOTSET)
Initializes the Handler instance by setting its level, setting the list of filters to the empty list and creating a
lock (using createLock()) for serializing access to an I/O mechanism.
Logging messages which are less severe than level will be ignored.
When a handler is created, the level is set to NOTSET (which causes all messages to be processed).
See Logging Levels for a list of levels.
Removes the specified filter filter from this handler.
Apply this handler’s filters to the record and return True if the record is to be processed.
The filters are
consulted in turn, until one of them returns a false value.
If none of them return a false value, the record will
be emitted.
If one returns a false value, the handler will not emit the record.
Ensure all logging output has been flushed.
This version does nothing and is intended to be implemented by
subclasses.
Tidy up any resources used by the handler.
This version does no output but removes the handler from an
internal list of handlers which is closed when shutdown() is called.
Conditionally emits the specified logging record, depending on filters which may have been added to the
handler.
Wraps the actual emission of the record with acquisition/release of the I/O thread lock.
16.6.
This method should be called from handlers when an exception is encountered during an emit() call.
If
the module-level attribute raiseExceptions is False, exceptions get silently ignored.
This is what is
mostly wanted for a logging system - most users will not care about errors in the logging system, they are
more interested in application errors.
You could, however, replace this with a custom handler if you wish.
The specified record is the one which was being processed when the exception occurred.
Do formatting for a record - if a formatter is set, use it.
Otherwise, use the default formatter for the module.
emit(record)
Do whatever it takes to actually log the specified logging record.
This version is intended to be implemented
by subclasses and so raises a NotImplementedError.
Warning:
This method is called after a handler-level lock is acquired, which is released after this
method returns.
When you override this method, note that you should be careful when calling anything
that invokes other parts of the logging API which might do locking, because that might result in a deadlock.
Specifically:
• Logging configuration APIs acquire the module-level lock, and then individual handler-level locks
as those handlers are configured.
Many logging APIs lock the module-level lock.
If such an API is called from this method, it could
cause a deadlock if a configuration call is made on another thread, because that thread will try to
acquire the module-level lock before the handler-level lock, whereas this thread tries to acquire the
module-level lock after the handler-level lock (because in this method, the handler-level lock has
already been acquired).
For a list of handlers included as standard, see logging.handlers.
Formatter(fmt=None, datefmt=None, style=’%’, validate=True, *, defaults=None)
Responsible for converting a LogRecord to an output string to be interpreted by a human or external system.
Parameters
• fmt (str) – A format string in the given style for the logged output as a whole.
The possible
mapping keys are drawn from the LogRecord object’s LogRecord attributes.
If not specified,
'%(message)s' is used, which is just the logged message.
If not specified, the default described in formatTime() is used.
Template ($).
This only applies to fmt and datefmt (e.g. '%(message)s'
versus '{message}'), not to the actual log messages passed to the logging methods.
How-
ever, there are other ways to use {- and $-formatting for log messages.
Formatter('%(asctime)s -
%(message)s', style='{').
For example,
logging.
Formatter('%(ip)s %(message)s',
defaults={"ip": None})
New in version 3.2: The style parameter.
The validate parameter.
The record’s attribute dictionary is used as the operand to a string formatting operation.
Returns the re-
sulting string.
Before formatting the dictionary, a couple of preparatory steps are carried out.
The message
attribute of the record is computed using msg % args.
If the formatting string contains '(asctime)',
formatTime() is called to format the event time.
If there is exception information, it is formatted using
formatException() and appended to the message.
Note that the formatted exception information is
cached in attribute exc_text.
This is useful because the exception information can be pickled and sent across
the wire, but you should be careful if you have more than one Formatter subclass which customizes the
formatting of exception information.
In this case, you will have to clear the cached value (by setting the
exc_text attribute to None) after a formatter has done its formatting, so that the next formatter to handle the
event doesn’t use the cached value, but recalculates it afresh.
If stack information is available, it’s appended after the exception information, using formatStack() to
transform it if necessary.
This method should be called from format() by a formatter which wants to make use of a formatted time.
This method can be overridden in formatters to provide for any specific requirement, but the basic behavior
is as follows: if datefmt (a string) is specified, it is used with time.strftime() to format the creation
time of the record.
Otherwise, the format ‘%Y-%m-%d %H:%M:%S,uuu’ is used, where the uuu part is
a millisecond value and the other letters are as per the time.strftime() documentation.
An example
time in this format is 2003-01-23 00:29:50,411.
The resulting string is returned.
This function uses a user-configurable function to convert the creation time to a tuple.
By default, time.
To change it for all
formatters, for example if you want all logging times to be shown in GMT, set the converter attribute in
the Formatter class.
Changed in version 3.3: Previously, the default format was hard-coded as in this example: 2010-09-06
22:38:15,292 where the part before the comma is handled by a strptime format string ('%Y-%m-%d
%H:%M:%S'), and the part after the comma is a millisecond value.
Because strptime does not have a format
placeholder for milliseconds, the millisecond value is appended using another format string, '%s,%03d' —
and both of these format strings have been hardcoded into this method.
With the change, these strings are
defined as class-level attributes which can be overridden at the instance level when desired.
The names of the
attributes are default_time_format (for the strptime format string) and default_msec_format
(for appending the millisecond value).
Formats the specified exception information (a standard exception tuple as returned by sys.exc_info())
as a string.
This default implementation just uses traceback.print_exception().
The resulting
string is returned.
This default implementation just returns the input value.
16.6.
BufferingFormatter(linefmt=None)
A base formatter class suitable for subclassing when you want to format a number of records.
You can pass a
Formatter instance which you want to use to format each line (that corresponds to a single record).
If not
specified, the default formatter (which just outputs the event message) is used as the line formatter.
formatHeader(records)
Return a header for a list of records.
The base implementation just returns the empty string.
You will need
to override this method if you want specific behaviour, e.g. to show the count of records, a title or a separator
line.
formatFooter(records)
Return a footer for a list of records.
The base implementation just returns the empty string.
You will need to
override this method if you want specific behaviour, e.g. to show the count of records or a separator line.
format(records)
Return formatted text for a list of records.
The base implementation just returns the empty string if there
are no records; otherwise, it returns the concatenation of the header, each record formatted with the line
formatter, and the footer.
Filter Objects
Filters can be used by Handlers and Loggers for more sophisticated filtering than is provided by levels.
The base
filter class only allows events which are below a certain point in the logger hierarchy.
For example, a filter initialized with
‘A.B’ will allow events logged by loggers ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ etc.
If initialized
with the empty string, all events are passed.
Filter(name=”)
Returns an instance of the Filter class.
If name is specified, it names a logger which, together with its children,
will have its events allowed through the filter.
Is the specified record to be logged?
Returns false for no, true for yes.
Filters can either modify log records
in-place or return a completely different record instance which will replace the original log record in any
future processing of the event.
Note that filters attached to handlers are consulted before an event is emitted by the handler, whereas filters attached to
loggers are consulted whenever an event is logged (using debug(), info(), etc.), before sending an event to handlers.
This means that events which have been generated by descendant loggers will not be filtered by a logger’s filter setting,
unless the filter has also been applied to those descendant loggers.
You don’t actually need to subclass Filter: you can pass any instance which has a filter method with the same
semantics.
Changed in version 3.2: You don’t need to create specialized Filter classes, or use other classes with a filter
method: you can use a function (or other callable) as a filter.
The filtering logic will check to see if the filter object has a
filter attribute: if it does, it’s assumed to be a Filter and its filter() method is called.
Otherwise, it’s assumed
to be a callable and called with the record as the single parameter.
The returned value should conform to that returned by
filter().
Changed in version 3.12: You can now return a LogRecord instance from filters to replace the log record rather than
modifying it in place.
This allows filters attached to a Handler to modify the log record before it is emitted, without
having side effects on other handlers.
Obviously changing the LogRecord needs to be done with some care, but it does
allow the injection of contextual information into logs (see filters-contextual).
LogRecord(name, level, pathname, lineno, msg, args, exc_info, func=None, sinfo=None)
Contains all the information pertinent to the event being logged.
The primary information is passed in msg and args, which are combined using msg % args to create the
message attribute of the record.
Parameters
• name (str) – The name of the logger used to log the event represented by this LogRecord.
Note that the logger name in the LogRecord will always have this value, even though it may
be emitted by a handler attached to a different (ancestor) logger.
Note that this is converted to two attributes of the LogRecord: levelno for the numeric
value and levelname for the corresponding level name.
TracebackType] | None) – An exception tuple with the current exception information,
as returned by sys.exc_info(), or None if no exception information is available.
If the user-supplied message argument to the logging call is not a string, str() is called on it to convert
it to a string.
This allows use of user-defined classes as messages, whose __str__ method can return the
actual format string to be used.
The creation of a LogRecord has been made more configurable by providing a
factory which is used to create the record.
The factory can be set using getLogRecordFactory() and
setLogRecordFactory() (see this for the factory’s signature).
This functionality can be used to inject your own values into a LogRecord at creation time.
You can use the
following pattern:
old_factory = logging.getLogRecordFactory()

16.6.
With this pattern, multiple factories could be chained, and as long as they don’t overwrite each other’s attributes or
unintentionally overwrite the standard attributes listed above, there should be no surprises.
LogRecord attributes
The LogRecord has a number of attributes, most of which are derived from the parameters to the constructor.
These attributes can be used to merge data from the record into the format string.
The following table lists
(in alphabetical order) the attribute names, their meanings and the corresponding placeholder in a %-style format string.
If you are using {}-formatting (str.format()), you can use {attrname} as the placeholder in the format string.
If you are using $-formatting (string.
Template), use the form ${attrname}.
In both cases, of course, replace
attrname with the actual attribute name you want to use.
In the case of {}-formatting, you can specify formatting flags by placing them after the attribute name, separated from it
with a colon.
For example: a placeholder of {msecs:03.0f} would format a millisecond value of 4 as 004.
Refer
to the str.format() documentation for full details on the options available to you.
The tuple of arguments merged into msg to produce message, or a dict whose values
are used for the merge (when there is only one argument, and it is a dictionary).
By default this is of the
form ‘2003-07-08 16:49:45,896’ (the numbers after the comma are millisecond por-
tion of the time).
Exception tuple (à la sys.exc_info) or, if no exception has occurred, None.
file-
name
%(filename)s
Filename portion of pathname.
This is set when Formatter.
The format string passed in the original logging call.
Merged with args to produce
message, or an arbitrary object (see arbitrary-object-messages).
Stack frame information (where available) from the bottom of the stack in the current
thread, up to and including the stack frame of the logging call which resulted in the
creation of this record.
Task name (if available).
Changed in version 3.1: processName was added.
16.6.
For a usage example,
see the section on adding contextual information to your logging output.
LoggerAdapter(logger, extra)
Returns an instance of LoggerAdapter initialized with an underlying Logger instance and a dict-like object.
This implementation takes the object passed as extra to the constructor and adds it to kwargs using key
‘extra’.
The return value is a (msg, kwargs) tuple which has the (possibly modified) versions of the arguments
passed in.
In addition to the above, LoggerAdapter supports the following methods of Logger:
debug(),
info(),
warning(),
error(),
exception(),
critical(),
log(),
isEnabledFor(),
getEffectiveLevel(), setLevel() and hasHandlers().
These methods have the same signatures
as their counterparts in Logger, so you can use the two types of instances interchangeably.
Changed in version 3.2:
The isEnabledFor(),
getEffectiveLevel(),
setLevel() and
hasHandlers() methods were added to LoggerAdapter.
These methods delegate to the underlying logger.
Attribute manager and method _log() were added, which delegate to the underlying
logger and allow adapters to be nested.
It achieves
this though using threading locks; there is one lock to serialize access to the module’s shared data, and each handler also
creates a lock to serialize access to its underlying I/O.
If you are implementing asynchronous signal handlers using the signal module, you may not be able to use logging
from within such handlers.
This is because lock implementations in the threading module are not always re-entrant,
and so cannot be invoked from such signal handlers.
In addition to the classes described above, there are a number of module-level functions.
If specified, the name is typically a dot-separated hierarchical name like ‘a’, ‘a.b’ or ‘a.b.c.d’.
Choice of
these names is entirely up to the developer who is using logging.
All calls to this function with a given name return the same logger instance.
This function may
be called from within a new class definition, to ensure that installing a customized Logger class will not undo
customizations already applied by other code.
For example:
class MyLogger(logging.getLoggerClass()):
# ... override behaviour here
logging.getLogRecordFactory()
Return a callable which is used to create a LogRecord.
New in version 3.2: This function has been provided, along with setLogRecordFactory(), to allow devel-
opers more control over how the LogRecord representing a logging event is constructed.
Logs a message with level DEBUG on the root logger.
The msg is the message format string, and the args are the
arguments which are merged into msg using the string formatting operator.
There are three keyword arguments in kwargs which are inspected: exc_info which, if it does not evaluate as false,
causes exception information to be added to the logging message.
If an exception tuple (in the format returned by
sys.exc_info()) or an exception instance is provided, it is used; otherwise, sys.exc_info() is called to
get the exception information.
The second optional keyword argument is stack_info, which defaults to False.
If true, stack information is added
to the logging message, including the actual logging call.
Note that this is not the same stack information as that
displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to the logging
call in the current thread, whereas the latter is information about stack frames which have been unwound, following
an exception, while searching for exception handlers.
You can specify stack_info independently of exc_info, e.g. to just show how you got to a certain point in your code,
even when no exceptions were raised.
The stack frames are printed following a header line which says:
Stack (most recent call last):
This mimics the Traceback (most recent call last): which is used when displaying exception
frames.
The third optional keyword argument is extra which can be used to pass a dictionary which is used to populate the
__dict__ of the LogRecord created for the logging event with user-defined attributes.
These custom attributes can
then be used as you like.
For example, they could be incorporated into logged messages.
For example:
FORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'
logging.basicConfig(format=FORMAT)
d = {'clientip': '192.168.0.1', 'user': 'fbloggs'}
logging.warning('Protocol problem: %s', 'connection reset', extra=d)
would print something like:
2006-02-08 22:20:02,165 192.168.0.1 fbloggs
Protocol problem: connection reset
The keys in the dictionary passed in extra should not clash with the keys used by the logging system.
If you choose to use these attributes in logged messages, you need to exercise some care.
In the above example, for
instance, the Formatter has been set up with a format string which expects ‘clientip’ and ‘user’ in the attribute
16.6.
If these are missing, the message will not be logged because a string formatting
exception will occur.
So in this case, you always need to pass the extra dictionary with these keys.
While this might be annoying, this feature is intended for use in specialized circumstances, such as multi-threaded
servers where the same code executes in many contexts, and interesting conditions which arise are dependent
on this context (such as remote client IP address and authenticated user name, in the above example).
In such
circumstances, it is likely that specialized Formatters would be used with particular Handlers.
This function (as well as info(), warning(), error() and critical()) will call basicConfig() if
the root logger doesn’t have any handler attached.
The stack_info parameter was added.
Logs a message with level INFO on the root logger.
The arguments are interpreted as for debug().
Logs a message with level WARNING on the root logger.
The arguments are interpreted as for debug().
Note: There is an obsolete function warn which is functionally identical to warning.
As warn is deprecated,
please do not use it - use warning instead.
Logs a message with level ERROR on the root logger.
The arguments are interpreted as for debug().
Logs a message with level CRITICAL on the root logger.
The arguments are interpreted as for debug().
Logs a message with level ERROR on the root logger.
The arguments are interpreted as for debug().
Exception
info is added to the logging message.
This function should only be called from an exception handler.
Logs a message with level level on the root logger.
The other arguments are interpreted as for debug().
When the
need arises to temporarily throttle logging output down across the whole application, this function can be useful.
Its effect is to disable all logging calls of severity level and below, so that if you call it with a value of INFO,
then all INFO and DEBUG events would be discarded, whereas those of severity WARNING and above would be
processed according to the logger’s effective level.
If logging.disable(logging.
NOTSET) is called, it
effectively removes this overriding level, so that logging output again depends on the effective levels of individual
loggers.
Note that if you have defined any custom logging level higher than CRITICAL (this is not recommended), you
won’t be able to rely on the default value for the level parameter, but will have to explicitly supply a suitable value.
The level parameter was defaulted to level CRITICAL.
See bpo-28524 for more infor-
mation about this change.
logging.addLevelName(level, levelName)
Associates level level with text levelName in an internal dictionary, which is used to map numeric levels to a textual
representation, for example when a Formatter formats a message.
This function can also be used to define your
own levels.
The only constraints are that all levels used must be registered using this function, levels should be
positive integers and they should increase in increasing order of severity.
For example, the string “CRITICAL”
maps to CRITICAL.
The returned mapping is copied from an internal mapping on each call to this function.
Returns the textual or numeric representation of logging level level.
If level is one of the predefined levels CRITICAL, ERROR, WARNING, INFO or DEBUG then you get the cor-
responding string.
If you have associated levels with names using addLevelName() then the name you have
associated with level is returned.
If a numeric value corresponding to one of the defined levels is passed in, the
corresponding string representation is returned.
The level parameter also accepts a string representation of the level such as ‘INFO’.
In such cases, this functions
returns the corresponding numeric value of the level.
If no matching numeric or string value is passed in, the string ‘Level %s’ % level is returned.
Note:
Levels are internally integers (as they need to be compared in the logging logic).
This function is used
to convert between an integer level and the level name displayed in the formatted log output by means of the
%(levelname)s format specifier (see LogRecord attributes), and vice versa.
This undocumented behaviour was considered a mistake, and
was removed in Python 3.4, but reinstated in 3.4.2 due to retain backward compatibility.
Returns a handler with the specified name, or None if there is no handler with that name.
New in version 3.12.
New in version 3.12.
logging.makeLogRecord(attrdict)
Creates and returns a new LogRecord instance whose attributes are defined by attrdict.
This function is useful
for taking a pickled LogRecord attribute dictionary, sent over a socket, and reconstituting it as a LogRecord
instance at the receiving end.
The functions debug(), info(), warning(), error() and critical()
will call basicConfig() automatically if no handlers are defined for the root logger.
This function does nothing if the root logger already has handlers configured, unless the keyword argument force
is set to True.
Note: This function should be called from the main thread before other threads are started.
In versions of Python
prior to 2.7.1 and 3.2, if this function is called from multiple threads, it is possible (in rare circumstances) that
16.6.
The following keyword arguments are supported.
Format
Description
filename
Specifies that a FileHandler be created, using the specified filename, rather than a
StreamHandler.
Defaults to 'a'.
format
Use the specified format string for the handler.
Defaults to attributes levelname, name and
message separated by colons.
One of '%', '{' or '$' for
printf-style, str.format() or string.
Template respectively.
Defaults to '%'.
Note that this argument is
incompatible with filename - if both are present, a ValueError is raised.
Any
handlers which don’t already have a formatter set will be assigned the default formatter created in
this function.
Note that this argument is incompatible with filename or stream - if both are
present, a ValueError is raised.
If not specified, the
value ‘backslashreplace’ is used.
Note that if None is specified, it will be passed as such to
open(), which means that it will be treated the same as passing ‘errors’.
Changed in version 3.2: The style argument was added.
Changed in version 3.3: The handlers argument was added.
Additional checks were added to catch situations
where incompatible arguments are specified (e.g. handlers together with stream or filename, or stream together
with filename).
Changed in version 3.8: The force argument was added.
The encoding and errors arguments were added.
Informs the logging system to perform an orderly shutdown by flushing and closing all handlers.
This should be
called at application exit and no further use of the logging system should be made after this call.
When the logging module is imported, it registers this function as an exit handler (see atexit), so normally there’s
no need to do that manually.
The class should define __init__()
such that only a name argument is required, and the __init__() should call Logger.__init__().
This
function is typically called before any loggers are instantiated by applications which need to use custom logger
behavior.
After this call, as at any other time, do not instantiate loggers directly using the subclass: continue to use
the logging.getLogger() API to get your loggers.
Parameters factory – The factory callable to be used to instantiate a log record.
New in version 3.2: This function has been provided, along with getLogRecordFactory(), to allow devel-
opers more control over how the LogRecord representing a logging event is constructed.
The factory has the following signature:
factory(name, level, fn, lno, msg, args, exc_info, func=None, sinfo=None,
**kwargs)
name The logger name.
The line number in the file where the logging call was made.
msg The logging message.
The arguments for the logging message.
None.
func The name of the function or method which invoked the logging call.
This is a StreamHandler writing to sys.stderr
with a level of WARNING, and is used to handle logging events in the absence of any logging configuration.
The
end result is to just print the message to sys.stderr.
This replaces the earlier error message saying that “no
handlers could be found for logger XYZ”.
If you need the earlier behaviour for some reason, lastResort can
be set to None.
New in version 3.2.
This function is used to turn the capture of warnings by logging on and off.
If capture is True, warnings issued by the warnings module will be redirected to the logging system.
Specif-
ically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a
logger named 'py.warnings' with a severity of WARNING.
If capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected
to their original destinations (i.e. those in effect before captureWarnings(True) was called).
See also:
16.6.
Module logging.handlers Useful handlers included with the logging module.
The proposal which described this feature for inclusion in the Python standard library.
Original Python logging package This is the original source for the logging package.
The version of the package
available from this site is suitable for use with Python 1.5.2, 2.1.x and 2.2.x, which do not include the logging
package in the standard library.
16.7 logging.config — Logging configuration
Source code: Lib/logging/config.py
Important
This page contains only reference information.
For tutorials, please see
• Basic Tutorial
• Advanced Tutorial
• Logging Cookbook
This section describes the API for configuring the logging module.
They are located in the logging.config module.
Their use
is optional — you can configure the logging module using these functions or by making calls to the main API (defined in
logging itself) and defining handlers which are declared either in logging or logging.handlers.
The contents of this dictionary are described in Configuration
dictionary schema below.
If an error is encountered during configuration, this function will raise a ValueError, TypeError,
AttributeError or ImportError with a suitably descriptive message.
A level which is not a string or which is a string not corresponding to an actual logging level.
A propagate value which is not a boolean.
An id which does not have a corresponding destination.
Inability to resolve to an internal or external object.
Parsing is performed by the DictConfigurator class, whose constructor is passed the dictionary used
for configuration, and has a configure() method.
The logging.config module has a callable at-
tribute dictConfigClass which is initially set to DictConfigurator.
You can replace the value of
dictConfigClass with a suitable implementation of your own.
For example, a subclass of DictConfigurator could call DictConfigurator.__init__() in its
own __init__(), then set up custom prefixes which would be usable in the subsequent configure() call.
New in version 3.2.
The format of the file should be as described
in Configuration file format.
This function can be called several times from an application, allowing an end user to
select from various pre-canned configurations (if the developer provides a mechanism to present the choices and
load the chosen configuration).
It will raise FileNotFoundError if the file doesn’t exist and RuntimeError if the file is invalid or empty.
Parameters
• fname – A filename, or a file-like object, or an instance derived from RawConfigParser.
If a RawConfigParser-derived instance is passed, it is used as is.
Otherwise, a
Configparser is instantiated, and the configuration read by it from the object passed in
fname.
If that has a readline() method, it is assumed to be a file-like object and read
using read_file(); otherwise, it is assumed to be a filename and passed to read().
The default is True because this enables old behaviour in a
backward-compatible way.
This behaviour is to disable any existing non-root loggers unless
they or their ancestors are explicitly named in the logging configuration.
An instance of a subclass of RawConfigParser is now accepted as a value for fname.
This facilitates:
• Use of a configuration file where logging configuration is just part of the overall application configuration.
The encoding parameter is added.
Changed in version 3.12: An exception will be thrown if the provided file doesn’t exist or is invalid or empty.
If no port is specified, the
module’s default DEFAULT_LOGGING_CONFIG_PORT is used.
Logging configurations will be sent as a file
suitable for processing by dictConfig() or fileConfig().
Returns a Thread instance on which you
can call start() to start the server, and which you can join() when appropriate.
To stop the server, call
stopListening().
The verify argument, if specified, should be a callable which should verify whether bytes received across the
socket are valid and should be processed.
This could be done by encrypting and/or signing what is sent across the
socket, such that the verify callable can perform signature verification and/or decryption.
The verify callable
is called with a single argument - the bytes received across the socket - and should return the bytes to be processed,
16.7.
The returned bytes could be the same as the passed in bytes
(e.g. when only verification is done), or they could be completely different (perhaps if decryption were performed).
To send a configuration to the socket, read in the configuration file and send it to the socket as a sequence of bytes
preceded by a four-byte length string packed in binary using struct.pack('>L', n).
Note: Because portions of the configuration are passed through eval(), use of this function may open its users
to a security risk.
While the function only binds to a socket on localhost, and so does not accept connections
from remote machines, there are scenarios where untrusted code could be run under the account of the process
which calls listen().
Specifically, if the process calling listen() runs on a multi-user machine where users
cannot trust each other, then a malicious user could arrange to run essentially arbitrary code in a victim user’s
process, simply by connecting to the victim’s listen() socket and sending a configuration which runs whatever
code the attacker wants to have executed in the victim’s process.
This is especially easy to do if the default port is
used, but not hard even if a different port is used.
To avoid the risk of this happening, use the verify argument
to listen() to prevent unrecognised configurations from being applied.
Changed in version 3.4: The verify argument was added.
Note: If you want to send configurations to the listener which don’t disable existing loggers, you will need to use
a JSON format for the configuration, which will use dictConfig() for configuration.
This method allows you
to specify disable_existing_loggers as False in the configuration you send.
This is typically called before calling
join() on the return value from listen().
Security considerations
The logging configuration functionality tries to offer convenience, and in part this is done by offering the ability to convert
text in configuration files into Python objects used in logging configuration - for example, as described in User-defined
objects.
However, these same mechanisms (importing callables from user-defined modules and calling them with param-
eters from the configuration) could be used to invoke any code you like, and for this reason you should treat configuration
files from untrusted sources with extreme caution and satisfy yourself that nothing bad can happen if you load them, before
actually loading them.
These objects aren’t limited to those provided by the logging module because you might write
your own formatter or handler class.
The parameters to these classes may also need to include external objects such as
sys.stderr.
The syntax for describing these objects and connections is defined in Object connections below.
The only valid value at present is 1, but
having this key allows the schema to evolve while still preserving backwards compatibility.
All other keys are optional, but if present they will be interpreted as described below.
In all cases below where a ‘con-
figuring dict’ is mentioned, it will be checked for the special '()' key to see if a custom instantiation is required.
If so,
the mechanism described in User-defined objects below is used to create an instance; otherwise, the context is used to
determine what to instantiate.
An optional class key indicates the name of the formatter’s class (as a dotted module and class name).
The in-
stantiation arguments are as for Formatter, thus this key is most useful for instantiating a customised subclass of
Formatter.
For example, the alternative class might present exception tracebacks in an expanded or condensed
format.
If your formatter requires different or extra configuration keys, you should use User-defined objects.
The configuring dict is searched for the key name (defaulting to the empty string) and this is used to construct a
logging.
Filter instance.
• handlers - the corresponding value will be a dict in which each key is a handler id and each value is a dict describing
how to configure the corresponding Handler instance.
The configuring dict is searched for the following keys:
– class (mandatory).
This is the fully qualified name of the handler class.
– level (optional).
The level of the handler.
– formatter (optional).
The id of the formatter for this handler.
– filters (optional).
A list of ids of the filters for this handler.
Changed in version 3.11: filters can take filter instances in addition to ids.
All other keys are passed through as keyword arguments to the handler’s constructor.
For example, given the
snippet:
handlers:
console:
class : logging.
RotatingFileHandler
formatter: precise
filename: logconfig.log
maxBytes: 1024
backupCount: 3
the
handler
with
id
console
is
instantiated
as
a
logging.
StreamHandler,
using
sys.
stdout as the underlying stream.
The handler with id file is instantiated as a logging.
handlers.
RotatingFileHandler with the keyword arguments filename='logconfig.log',
maxBytes=1024, backupCount=3.
• loggers - the corresponding value will be a dict in which each key is a logger name and each value is a dict describing
how to configure the corresponding Logger instance.
The configuring dict is searched for the following keys:
– level (optional).
The level of the logger.
– propagate (optional).
The propagation setting of the logger.
– filters (optional).
A list of ids of the filters for this logger.
Changed in version 3.11: filters can take filter instances in addition to ids.
– handlers (optional).
A list of ids of the handlers for this logger.
The specified loggers will be configured according to the level, propagation, filters and handlers specified.
Processing of the configuration will be as for any logger,
except that the propagate setting will not be applicable.
This value
defaults to False, which means that the specified configuration replaces the existing configuration with the same
semantics as used by the existing fileConfig() API.
If the specified value is True, the configuration is processed as described in the section on Incremental Configu-
ration.
This setting mirrors the param-
eter of the same name in fileConfig().
If absent, this parameter defaults to True.
This value is ignored if
incremental is True.
Incremental Configuration
It is difficult to provide complete flexibility for incremental configuration.
For example, because objects such as filters
and formatters are anonymous, once a configuration is set up, it is not possible to refer to such anonymous objects when
augmenting a configuration.
Furthermore, there is not a compelling case for arbitrarily altering the object graph of loggers, handlers, filters, formatters
at run-time, once a configuration is set up; the verbosity of loggers and handlers can be controlled just by setting levels
(and, in the case of loggers, propagation flags).
Changing the object graph arbitrarily in a safe way is problematic in a
multi-threaded environment; while not impossible, the benefits are not worth the complexity it adds to the implementation.
Thus, when the incremental key of a configuration dict is present and is True, the system will completely ignore any
formatters and filters entries, and process only the level settings in the handlers entries, and the level
and propagate settings in the loggers and root entries.
Thus, the logging verbosity of a long-running application can be altered over time with no need to stop and restart the
application.
Object connections
The schema describes a set of logging objects - loggers, handlers, formatters, filters - which are connected to each other
in an object graph.
Thus, the schema needs to represent connections between the objects.
For example, say that, once
configured, a particular logger has attached to it a particular handler.
For the purposes of this discussion, we can say
that the logger represents the source, and the handler the destination, of a connection between the two.
Of course in the
configured objects this is represented by the logger holding a reference to the handler.
In the configuration dict, this is
done by giving each destination object an id which identifies it unambiguously, and then using the id in the source object’s
configuration to indicate that a connection exists between the source and the destination object with that id.
The ids for loggers are the logger names which would be used programmatically to obtain a reference to those loggers,
e.g. foo.bar.baz.
The ids for Formatters and Filters can be any string value (such as brief, precise above)
and they are transient, in that they are only meaningful for processing the configuration dictionary and used to determine
connections between objects, and are not persisted anywhere when the configuration call is complete.
The above snippet indicates that logger named foo.bar.baz should have two handlers attached to it, which are de-
scribed by the handler ids h1 and h2.
The formatter for h1 is that described by id brief, and the formatter for h2 is
that described by id precise.
User-defined objects
The schema supports user-defined objects for handlers, filters and formatters.
Objects to be configured are described by dictionaries which detail their configuration.
In some places, the logging
system will be able to infer from the context how an object is to be instantiated, but when a user-defined object is to
be instantiated, the system will not know how to do this.
In order to provide complete flexibility for user-defined object
instantiation, the user needs to provide a ‘factory’ - a callable which is called with a configuration dictionary and which
returns the instantiated object.
This is signalled by an absolute import path to the factory being made available under the
special key '()'.
The first, with id brief, is a standard logging.
Formatter
instance with the specified format string.
The second, with id default, has a longer format and also defines the time
format explicitly, and will result in a logging.
Formatter initialized with those two format strings.
Formatter instances are created.
The configuration sub-dictionary for the third formatter,
with id custom, is:
{
'()' : 'my.package.customFormatterFactory',
'bar' : 'baz',
'spam' : 99.9,
'answer' : 42
}
and this contains the special key '()', which means that user-defined instantiation is wanted.
In this case, the specified
factory callable will be used.
If it is an actual callable it will be used directly - otherwise, if you specify a string (as in
the example) the actual callable will be located using normal import mechanisms.
The callable will be called with the
remaining items in the configuration sub-dictionary as keyword arguments.
Warning:
The values for keys such as bar, spam and answer in the above example should not be configu-
ration dictionaries or references such as cfg://foo or ext://bar, because they will not be processed by the
configuration machinery, but passed to the callable as-is.
The key '()' has been used as the special key because it is not a valid keyword parameter name, and so will not clash
with the names of the keyword arguments used in the call.
The filters member of handlers and loggers can take filter instances in addition to
ids.
If found,
the specified attributes will be set on the user-defined object before it is returned.
Warning: The values for attributes such as foo and baz in the above example should not be configuration dictio-
naries or references such as cfg://foo or ext://bar, because they will not be processed by the configuration
machinery, but set as attribute values as-is.
Handler configuration order
Handlers are configured in alphabetical order of their keys, and a configured handler replaces the configuration dictionary
in (a working copy of) the handlers dictionary in the schema.
If you use a construct such as cfg://handlers.
foo, then initially handlers['foo'] points to the configuration dictionary for the handler named foo, and later
(once that handler has been configured) it points to the configured handler instance.
Thus, cfg://handlers.foo
could resolve to either a dictionary or a handler instance.
In general, it is wise to name handlers in a way such that dependent
handlers are configured _after_ any handlers they depend on; that allows something like cfg://handlers.foo to
be used in configuring a handler that depends on handler foo.
If that dependent handler were named bar, problems
would result, because the configuration of bar would be attempted before that of foo, and foo would not yet have been
configured.
However, if the dependent handler were named foobar, it would be configured after foo, with the result
that cfg://handlers.foo would resolve to configured handler foo, and not its configuration dictionary.
Access to external objects
There are times where a configuration needs to refer to objects external to the configuration, for example sys.stderr.
If the configuration dict is constructed using Python code, this is straightforward, but a problem arises when the configu-
ration is provided via a text file (e.g. JSON, YAML).
In a text file, there is no standard way to distinguish sys.stderr
from the literal string 'sys.stderr'.
To facilitate this distinction, the configuration system looks for certain special
prefixes in string values and treat them specially.
For example, if the literal string 'ext://sys.stderr' is provided
as a value in the configuration, then the ext:// will be stripped off and the remainder of the value processed using
normal import mechanisms.
The handling of such prefixes is done in a way analogous to protocol handling: there is a generic mechanism to look
for prefixes which match the regular expression ^(?P<prefix>[a-z]+)://(?P<suffix>.*)$ whereby, if the
prefix is recognised, the suffix is processed in a prefix-dependent manner and the result of the processing replaces
the string value.
If the prefix is not recognised, then the string value will be left as-is.
This will be done
implicitly by the configuration system for things that it knows about.
For example, the string value 'DEBUG' for a
level in a logger or handler will automatically be converted to the value logging.
DEBUG, and the handlers,
filters and formatter entries will take an object id and resolve to the appropriate destination object.
However, a more generic mechanism is needed for user-defined objects which are not known to the logging module.
For example, consider logging.handlers.
MemoryHandler, which takes a target argument which is another
handler to delegate to.
Since the system already knows about this class, then in the configuration, the given target just
needs to be the object id of the relevant target handler, and the system will resolve to the handler from the id.
If, however,
a user defines a my.package.
MyHandler which has an alternate handler, the configuration system would not
know that the alternate referred to a handler.
To cater for this, a generic resolution system allows the user to specify:
handlers:
file:
# configuration of file handler goes here
custom:
(): my.package.
MyHandler
alternate: cfg://handlers.file
The literal string 'cfg://handlers.file' will be resolved in an analogous way to strings with the ext:// prefix,
but looking in the configuration itself rather than the import namespace.
The mechanism allows access by dot or by index,
in a similar way to that provided by str.format.
Thus, given the following snippet:
handlers:
email:
class: logging.handlers.
SMTPHandler
mailhost: localhost
fromaddr: my_app@domain.tld
toaddrs:
- support_team@domain.tld
- dev_team@domain.tld
subject: Houston, we have a problem.
The
string 'cfg://handlers.email.toaddrs[1] would resolve to 'dev_team@domain.tld' and the string
'cfg://handlers.email.toaddrs[0]' would resolve to the value 'support_team@domain.tld'.
The subject value could be accessed using either 'cfg://handlers.email.subject' or, equivalently,
'cfg://handlers.email[subject]'.
The latter form only needs to be used if the key contains spaces or
non-alphanumeric characters.
If an index value consists only of decimal digits, access will be attempted using the corre-
sponding integer value, falling back to the string value if needed.
Given
a
string
cfg://handlers.myhandler.mykey.123,
this
will
resolve
to
config_dict['handlers']['myhandler']['mykey']['123'].
If
the
string
is
speci-
fied
as
cfg://handlers.myhandler.mykey[123],
the
system
will
attempt
to
retrieve
the
value
from
config_dict['handlers']['myhandler']['mykey'][123],
and
fall
back
to
config_dict['handlers']['myhandler']['mykey']['123'] if that fails.
You may want to replace
this with your own importing mechanism: if so, you can replace the importer attribute of the DictConfigurator
or its superclass, the BaseConfigurator class.
However, you need to be careful because of the way functions are
accessed from classes via descriptors.
If you are using a Python callable to do your imports, and you want to define it at
class level rather than instance level, you need to wrap it with staticmethod().
For example:
from importlib import import_module
from logging.config import BaseConfigurator
BaseConfigurator.importer = staticmethod(import_module)
You don’t need to wrap with staticmethod() if you’re setting the import callable on a configurator instance.
If you want to configure a QueueHandler, noting that this is normally used in conjunction with a QueueListener,
you can configure both together.
After the configuration, the QueueListener instance will be available as the
listener attribute of the created handler, and that in turn will be available to you using getHandlerByName() and
passing the name you have used for the QueueHandler in your configuration.
The dictionary schema for configuring
the pair is shown in the example YAML snippet below.
QueueHandler
queue: my.module.queue_factory
listener: my.package.
CustomListener
handlers:
- hand_name_1
- hand_name_2

The queue and listener keys are optional.
If the queue key is present, the corresponding value can be one of the following:
• An actual instance of queue.
Queue or a subclass thereof.
This is of course only possible if you are constructing
or modifying the configuration dictionary in code.
A string that resolves to a callable which, when called with no arguments, returns the queue.
Queue instance to
use.
That callable could be a queue.
Queue subclass or a function which returns a suitable queue instance, such
as my.module.queue_factory().
A dict with a '()' key which is constructed in the usual way as discussed in User-defined objects.
The result of
this construction should be a queue.
Queue instance.
If the queue key is absent, a standard unbounded queue.
Queue instance is created and used.
A subclass of logging.handlers.
QueueListener.
This is of course only possible if you are constructing
or modifying the configuration dictionary in code.
A string which resolves to a class which is a subclass of QueueListener, such as 'my.package.
CustomListener'.
A dict with a '()' key which is constructed in the usual way as discussed in User-defined objects.
The result of
this construction should be a callable with the same signature as the QueueListener initializer.
If the listener key is absent, logging.handlers.
QueueListener is used.
The values under the handlers key are the names of other handlers in the configuration (not shown in the above snippet)
which will be passed to the queue listener.
Any custom queue handler and listener classes will need to be defined with the same initialization signatures as
QueueHandler and QueueListener.
New in version 3.12.
The file
must contain sections called [loggers], [handlers] and [formatters] which identify by name the entities
of each type which are defined in the file.
For each such entity, there is a separate section which identifies how that
entity is configured.
Thus, for a logger named log01 in the [loggers] section, the relevant configuration details are
held in a section [logger_log01].
Similarly, a handler called hand01 in the [handlers] section will have its
configuration held in a section called [handler_hand01], while a formatter called form01 in the [formatters]
section will have its configuration specified in a section called [formatter_form01].
The root logger configuration
must be specified in a section called [logger_root].
Note: The fileConfig() API is older than the dictConfig() API and does not provide functionality to cover
certain aspects of logging.
For example, you cannot configure Filter objects, which provide for filtering of messages
beyond simple integer levels, using fileConfig().
If you need to have instances of Filter in your logging con-
figuration, you will need to use dictConfig().
Note that future enhancements to configuration functionality will be
added to dictConfig(), so it’s worth considering transitioning to this newer API when it’s convenient to do so.
Examples of these sections in the file are given below.
An example of a root logger section is given below.
For the root logger
only, NOTSET means that all messages will be logged.
Level values are evaluated in the context of the logging
package’s namespace.
The handlers entry is a comma-separated list of handler names, which must appear in the [handlers] section.
These names must appear in the [handlers] section and have corresponding sections in the configuration file.
For loggers other than the root logger, some additional information is required.
This is illustrated by the following example.
The propagate entry is set to 1 to indicate that messages must propagate to handlers higher up the logger hierarchy
from this logger, or 0 to indicate that messages are not propagated to handlers up the hierarchy.
The qualname entry
is the hierarchical channel name of the logger, that is to say the name used by the application to get the logger.
Sections which specify handler configuration are exemplified by the following.
The class entry indicates the handler’s class (as determined by eval() in the logging package’s namespace).
The
level is interpreted as for loggers, and NOTSET is taken to mean ‘log everything’.
The formatter entry indicates the key name of the formatter for this handler.
If blank, a default formatter (logging.
_defaultFormatter) is used.
If a name is specified, it must appear in the [formatters] section and have a
corresponding section in the configuration file.
The args entry, when evaluated in the context of the logging package’s namespace, is the list of arguments to the
constructor for the handler class.
Refer to the constructors for the relevant handlers, or to the examples below, to see how
typical entries are constructed.
If not provided, it defaults to ().
The optional kwargs entry, when evaluated in the context of the logging package’s namespace, is the keyword
argument dict to the constructor for the handler class.
If not provided, it defaults to {}.
SocketHandler
level=INFO
formatter=form03
args=('localhost', handlers.
DatagramHandler
level=WARN
formatter=form04
args=('localhost', handlers.
SysLogHandler
level=ERROR
formatter=form05
args=(('localhost', handlers.
SYSLOG_UDP_PORT), handlers.
SysLogHandler.
HTTPHandler
level=NOTSET
formatter=form09
args=('localhost:9022', '/log', 'GET')
kwargs={'secure': True}
Sections which specify formatter configuration are typified by the following.
The arguments for the formatter configuration are the same as the keys in the dictionary schema formatters section.
The defaults entry, when evaluated in the context of the logging package’s namespace, is a dictionary of default
values for custom formatting fields.
If not provided, it defaults to None.
Note:
Due to the use of eval() as described above, there are potential security risks which result from using the
listen() to send and receive configurations via sockets.
The risks are limited to where multiple users with no mutual
trust run code on the same machine; see the listen() documentation for more information.
See also:
Module logging API reference for the logging module.
Lib/logging/handlers.py
Important
This page contains only reference information.
For tutorials, please see
• Basic Tutorial
• Advanced Tutorial
• Logging Cookbook
The following useful handlers are provided in the package.
Note that three of the handlers (StreamHandler,
FileHandler and NullHandler) are actually defined in the logging module itself, but have been documented
here along with the other handlers.
StreamHandler(stream=None)
Returns a new instance of the StreamHandler class.
If a formatter is specified, it is used to format the record.
The record is then written to the stream
followed by terminator.
If exception information is present, it is formatted using traceback.
Flushes the stream by calling its flush() method.
Note that the close() method is inherited from
Handler and so does no output, so an explicit flush() call may be needed at times.
setStream(stream)
Sets the instance’s stream to the specified value, if it is different.
The old stream is flushed before the new
stream is set.
Parameters stream – The stream that the handler should use.
Returns the old stream, if the stream was changed, or None if it wasn’t.
New in version 3.7.
terminator
String used as the terminator when writing a formatted record to a stream.
Default value is '\n'.
If you don’t want a newline termination, you can set the handler instance’s terminator attribute to the
empty string.
In earlier versions, the terminator was hardcoded as '\n'.
New in version 3.2.
It inherits the
output functionality from StreamHandler.
FileHandler(filename, mode=’a’, encoding=None, delay=False, errors=None)
Returns a new instance of the FileHandler class.
The specified file is opened and used as the stream for logging.
If mode is not specified, 'a' is used.
If encoding is not None, it is used to open the file with that encoding.
If
delay is true, then file opening is deferred until the first call to emit().
By default, the file grows indefinitely.
If
errors is specified, it’s used to determine how encoding errors are handled.
Changed in version 3.6: As well as string values, Path objects are also accepted for the filename argument.
The errors parameter was added.
Closes the file.
Outputs the record to the file.
Note that if the file was closed due to logging shutdown at exit and the file mode is ‘w’, the record will not be
emitted (see bpo-42378).
The NullHandler class, located in the core logging package, does not do any formatting or output.
It is essentially
a ‘no-op’ handler for use by library developers.
NullHandler
Returns a new instance of the NullHandler class.
This method does nothing.
This method returns None for the lock, since there is no underlying I/O to which access needs to be serialized.
See library-config for more information on how to use NullHandler.
If the file changes, it is closed and reopened using the file name.
A file change can happen because of usage of programs such as newsyslog and logrotate which perform log file rotation.
This handler, intended for use under Unix/Linux, watches the file to see if it has changed since the last emit.
If the file has changed, the old file stream is closed, and
the file opened to get a new stream.
This handler is not appropriate for use under Windows, because under Windows open log files cannot be moved or
renamed - logging opens the files with exclusive locks - and so there is no need for such a handler.
WatchedFileHandler(filename, mode=’a’, encoding=None, delay=False,
errors=None)
Returns a new instance of the WatchedFileHandler class.
The specified file is opened and used as the stream
for logging.
If mode is not specified, 'a' is used.
If encoding is not None, it is used to open the file with that
encoding.
If delay is true, then file opening is deferred until the first call to emit().
By default, the file grows
indefinitely.
If errors is provided, it determines how encoding errors are handled.
Changed in version 3.6: As well as string values, Path objects are also accepted for the filename argument.
The errors parameter was added.
If it has, the existing stream is flushed and closed and the file opened
again, typically as a precursor to outputting the record to the file.
Outputs the record to the file, but first calls reopenIfNeeded() to reopen the file if it has changed.
You should not need to instantiate
this class, but it has attributes and methods you may need to override.
The parameters are as for FileHandler.
The attributes are:
namer
If this attribute is set to a callable, the rotation_filename() method delegates to this callable.
The
parameters passed to the callable are those passed to rotation_filename().
Note: The namer function is called quite a few times during rollover, so it should be as simple and as fast as
possible.
It should also return the same output every time for a given input, otherwise the rollover behaviour
may not work as expected.
It’s also worth noting that care should be taken when using a namer to preserve certain attributes in
the filename which are used during rotation.
For example, RotatingFileHandler expects to have
a set of log files whose names contain successive integers, so that rotation works as expected, and
TimedRotatingFileHandler deletes old log files (based on the backupCount parameter passed
to the handler’s initializer) by determining the oldest files to delete.
For this to happen, the filenames should
be sortable using the date/time portion of the filename, and a namer needs to respect this.
New in version 3.3.
The parameters passed
to the callable are those passed to rotate().
New in version 3.3.
This is provided so that a custom filename can be provided.
The default implementation calls the ‘namer’ attribute of the handler, if it’s callable, passing the default name
to it.
If the attribute isn’t callable (the default is None), the name is returned unchanged.
Parameters default_name – The default name for the log file.
New in version 3.3.
When rotating, rotate the current log.
The default implementation calls the ‘rotator’ attribute of the handler, if it’s callable, passing the source and
dest arguments to it.
If the attribute isn’t callable (the default is None), the source is simply renamed to the
destination.
Parameters
• source – The source filename.
This is normally the base filename, e.g. ‘test.log’.
The destination filename.
This is normally what the source is rotated to, e.g.
‘test.log.1’.
New in version 3.3.
The reason the attributes exist is to save you having to subclass - you can use the same callables for instances of
RotatingFileHandler and TimedRotatingFileHandler.
If either the namer or rotator callable raises
an exception, this will be handled in the same way as any other exception during an emit() call, i.e.
via the
handleError() method of the handler.
If you need to make more significant changes to rotation processing, you can override the methods.
For an example, see cookbook-rotator-namer.
RotatingFileHandler(filename, mode=’a’, maxBytes=0, backupCount=0,
encoding=None, delay=False, errors=None)
Returns a new instance of the RotatingFileHandler class.
The specified file is opened and used as the
stream for logging.
If mode is not specified, 'a' is used.
If encoding is not None, it is used to open the file with
that encoding.
If delay is true, then file opening is deferred until the first call to emit().
By default, the file grows
indefinitely.
If errors is provided, it determines how encoding errors are handled.
You can use the maxBytes and backupCount values to allow the file to rollover at a predetermined size.
When the size
is about to be exceeded, the file is closed and a new file is silently opened for output.
Rollover occurs whenever the
current log file is nearly maxBytes in length; but if either of maxBytes or backupCount is zero, rollover never occurs,
so you generally want to set backupCount to at least 1, and have a non-zero maxBytes.
When backupCount is non-
zero, the system will save old log files by appending the extensions ‘.1’, ‘.2’ etc., to the filename.
For example, with
a backupCount of 5 and a base file name of app.log, you would get app.log, app.log.1, app.log.2,
up to app.log.5.
The file being written to is always app.log.
When this file is filled, it is closed and renamed
to app.log.1, and if files app.log.1, app.log.2, etc. exist, then they are renamed to app.log.2,
app.log.3 etc. respectively.
Changed in version 3.6: As well as string values, Path objects are also accepted for the filename argument.
The errors parameter was added.
Outputs the record to the file, catering for rollover as described previously.
TimedRotatingFileHandler
The TimedRotatingFileHandler class, located in the logging.handlers module, supports rotation of disk
log files at certain timed intervals.
TimedRotatingFileHandler(filename, when=’h’, interval=1,
backupCount=0, encoding=None,
delay=False, utc=False, atTime=None,
errors=None)
Returns a new instance of the TimedRotatingFileHandler class.
The specified file is opened and used as
the stream for logging.
On rotating it also sets the filename suffix.
Rotating happens based on the product of when
and interval.
You can use the when to specify the type of interval.
The list of possible values is below.
Note that they are not
case sensitive.
Value
Type of interval
If/how atTime is used
'S'
Seconds
Ignored
'M'
Minutes
Ignored
'H'
Hours
Ignored
'D'
Days
Ignored
'W0'-'W6'
Weekday (0=Monday)
Used to compute initial rollover
time
'midnight' Roll over at midnight, if atTime not specified, else at time
atTime
Used to compute initial rollover
time
When using weekday-based rotation, specify ‘W0’ for Monday, ‘W1’ for Tuesday, and so on up to ‘W6’ for Sunday.
In this case, the value passed for interval isn’t used.
The system will save old log files by appending extensions to the filename.
The extensions are date-and-time based,
using the strftime format %Y-%m-%d_%H-%M-%S or a leading portion thereof, depending on the rollover interval.
When computing the next rollover time for the first time (when the handler is created), the last modification time
of an existing log file, or else the current time, is used to compute when the next rotation will occur.
If the utc argument is true, times in UTC will be used; otherwise local time is used.
If backupCount is nonzero, at most backupCount files will be kept, and if more would be created when rollover
occurs, the oldest one is deleted.
The deletion logic uses the interval to determine which files to delete, so changing
the interval may leave old files lying around.
If delay is true, then file opening is deferred until the first call to emit().
If atTime is not None, it must be a datetime.time instance which specifies the time of day when rollover
occurs, for the cases where rollover is set to happen “at midnight” or “on a particular weekday”.
Note that in
these cases, the atTime value is effectively used to compute the initial rollover, and subsequent rollovers would be
calculated via the normal interval calculation.
If errors is specified, it’s used to determine how encoding errors are handled.
Calculation of subsequent
rollover times is done only when rollover occurs, and rollover occurs only when emitting output.
If this is not kept
in mind, it might lead to some confusion.
For example, if an interval of “every minute” is set, that does not mean
you will always see log files with times (in the filename) separated by a minute; if, during application execution,
logging output is generated more frequently than once a minute, then you can expect to see log files with times
separated by a minute.
If, on the other hand, logging messages are only output once every five minutes (say), then
there will be gaps in the file times corresponding to the minutes where no output (and hence no rollover) occurred.
Changed in version 3.4: atTime parameter was added.
Changed in version 3.6: As well as string values, Path objects are also accepted for the filename argument.
The errors parameter was added.
Outputs the record to the file, catering for rollover as described above.
These are the absolute paths of the
oldest backup log files written by the handler.
SocketHandler
The SocketHandler class, located in the logging.handlers module, sends logging output to a network socket.
The base class uses a TCP socket.
SocketHandler(host, port)
Returns a new instance of the SocketHandler class intended to communicate with a remote machine whose
address is given by host and port.
If port is specified as None, a Unix domain socket is created using the value in host -
otherwise, a TCP socket is created.
Closes the socket.
Pickles the record’s attribute dictionary and writes it to the socket in binary format.
If there is an error with
the socket, silently drops the packet.
If the connection was previously lost, re-establishes the connection.
To
unpickle the record at the receiving end into a LogRecord, use the makeLogRecord() function.
The most likely cause is a lost connection.
Closes the
socket so that we can retry on the next event.
This is a factory method which allows subclasses to define the precise type of socket they want.
The default
implementation creates a TCP socket (socket.SOCK_STREAM).
Pickles the record’s attribute dictionary in binary format with a length prefix, and returns it ready for trans-
mission across the socket.
If you are concerned about security, you may want to override this
method to implement a more secure mechanism.
Send a pickled byte-string packet to the socket.
The format of the sent byte-string is as described in the
documentation for makePickle().
This function allows for partial sends, which can happen when the network is busy.
On initial failure, the handler will
drop the message it was trying to send.
When subsequent messages are handled by the same instance, it will
not try connecting until some time has passed.
The default parameters are such that the initial delay is one
second, and if after that delay the connection still can’t be made, the handler will double the delay each time
up to a maximum of 30 seconds.
This behaviour is controlled by the following handler attributes:
• retryStart (initial delay, defaulting to 1.0 seconds).
This means that if the remote listener starts up after the handler has been used, you could lose messages (since
the handler won’t even attempt a connection until the delay has elapsed, but just silently drop messages during
the delay period).
DatagramHandler
The DatagramHandler class, located in the logging.handlers module, inherits from SocketHandler to
support sending logging messages over UDP sockets.
Note: As UDP is not a streaming protocol, there is no persistent connection between an instance of this handler
and host.
For this reason, when using a network socket, a DNS lookup might have to be made each time an event
is logged, which can introduce some latency into the system.
If this affects you, you can do a lookup yourself and
initialize this handler using the looked-up IP address rather than the hostname.
If port is specified as None, a Unix domain socket is created using the value in host -
otherwise, a UDP socket is created.
Pickles the record’s attribute dictionary and writes it to the socket in binary format.
If there is an error with
the socket, silently drops the packet.
To unpickle the record at the receiving end into a LogRecord, use the
makeLogRecord() function.
16.8.
The factory method of SocketHandler is here overridden to create a UDP socket (socket.
SOCK_DGRAM).
The format of the sent byte-string is as described in the documentation
for SocketHandler.makePickle().
SysLogHandler(address=(’localhost’, SYSLOG_UDP_PORT),
facility=LOG_USER, socktype=socket.SOCK_DGRAM)
Returns a new instance of the SysLogHandler class intended to communicate with a remote Unix ma-
chine whose address is given by address in the form of a (host, port) tuple.
If address is not specified,
('localhost', 514) is used.
The address is used to open a socket.
An alternative to providing a (host,
port) tuple is providing an address as a string, for example ‘/dev/log’.
In this case, a Unix domain socket is used to
send the message to the syslog.
If facility is not specified, LOG_USER is used.
To open a
TCP socket (for use with the newer syslog daemons such as rsyslog), specify a value of socket.SOCK_STREAM.
Note that if your server is not listening on UDP port 514, SysLogHandler may appear not to work.
In that
case, check what address you should be using for a domain socket - it’s system dependent.
For example, on Linux
it’s usually ‘/dev/log’ but on OS/X it’s ‘/var/run/syslog’.
You’ll need to check your platform and use the appropriate
address (you may need to do this check at runtime if your application needs to run on several platforms).
On
Windows, you pretty much have to use the UDP option.
Note: On macOS 12.x (Monterey), Apple has changed the behaviour of their syslog daemon - it no longer listens
on a domain socket.
Therefore, you cannot expect SysLogHandler to work on this system.
See gh-91070 for more information.
Changed in version 3.2: socktype was added.
Closes the socket to the remote host.
This method is called
during handler initialization, but it’s not regarded as an error if the other end isn’t listening at this point - the
method will be called again when emitting an event, if there is no socket at that point.
The record is formatted, and then sent to the syslog server.
If exception information is present, it is not sent
to the server.
In earlier versions, the message sent to the syslog daemons was
always terminated with a NUL byte, because early versions of these daemons expected a NUL terminated
message - even though it’s not in the relevant specification (RFC 5424).
More recent versions of these dae-
mons don’t expect the NUL byte but strip it off if it’s there, and even more recent daemons (which adhere
more closely to RFC 5424) pass the NUL byte on as part of the message.
To enable easier handling of syslog messages in the face of all these differing daemon behaviours, the append-
ing of the NUL byte has been made configurable, through the use of a class-level attribute, append_nul.
This defaults to True (preserving the existing behaviour) but can be set to False on a SysLogHandler
instance in order for that instance to not append the NUL terminator.
In earlier versions, there was no facility for an “ident” or “tag”
prefix to identify the source of the message.
This can now be specified using a class-level attribute, defaulting
to "" to preserve existing behaviour, but which can be overridden on a SysLogHandler instance in order
for that instance to prepend the ident to every message handled.
Encodes the facility and priority into an integer.
You can pass in strings or integers - if strings are passed,
internal mapping dictionaries are used to convert them to integers.
The symbolic LOG_ values are defined in SysLogHandler and mirror the values defined in the sys/
syslog.h header file.
Symbolic value
auth
LOG_AUTH
authpriv
LOG_AUTHPRIV
cron
LOG_CRON
daemon
LOG_DAEMON
ftp
LOG_FTP
kern
LOG_KERN
lpr
LOG_LPR
mail
LOG_MAIL
news
LOG_NEWS
syslog
LOG_SYSLOG
user
LOG_USER
uucp
LOG_UUCP
local0
LOG_LOCAL0
local1
LOG_LOCAL1
local2
LOG_LOCAL2
local3
LOG_LOCAL3
local4
LOG_LOCAL4
local5
LOG_LOCAL5
local6
LOG_LOCAL6
local7
LOG_LOCAL7
16.8.
Maps a logging level name to a syslog priority name.
You may need to override this if you are using custom
levels, or if the default algorithm is not suitable for your needs.
The default algorithm maps DEBUG, INFO,
WARNING, ERROR and CRITICAL to the equivalent syslog names, and all other level names to ‘warning’.
The NTEventLogHandler class, located in the logging.handlers module, supports sending logging messages
to a local Windows NT, Windows 2000 or Windows XP event log.
Before you can use it, you need Mark Hammond’s
Win32 extensions for Python installed.
Returns a new instance of the NTEventLogHandler class.
The appname is used to define the application name
as it appears in the event log.
An appropriate registry entry is created using this name.
The dllname should give
the fully qualified pathname of a .dll or .exe which contains message definitions to hold in the log (if not specified,
'win32service.pyd' is used - this is installed with the Win32 extensions and contains some basic placeholder
message definitions.
Note that use of these placeholders will make your event logs big, as the entire message source
is held in the log.
The logtype is one of 'Application', 'System'
or 'Security', and defaults to 'Application'.
At this point, you can remove the application name from the registry as a source of event log entries.
However,
if you do this, you will not be able to see the events as you intended in the Event Log Viewer - it needs to be
able to access the registry to get the .dll name.
Determines the message ID, event category and event type, and then logs the message in the NT event log.
getEventCategory(record)
Returns the event category for the record.
Override this if you want to specify your own categories.
This
version returns 0.
getEventType(record)
Returns the event type for the record.
Override this if you want to specify your own types.
This version
does a mapping using the handler’s typemap attribute, which is set up in __init__() to a dictionary which
contains mappings for DEBUG, INFO, WARNING, ERROR and CRITICAL.
Returns the message ID for the record.
If you are using your own messages, you could do this by having the
msg passed to the logger being an ID rather than a format string.
Then, in here, you could use a dictionary
lookup to get the message ID.
This version returns 1, which is the base message ID in win32service.
SMTPHandler(mailhost, fromaddr, toaddrs, subject, credentials=None,
secure=None, timeout=1.0)
Returns a new instance of the SMTPHandler class.
The instance is initialized with the from and to addresses and
subject line of the email.
The toaddrs should be a list of strings.
To specify a non-standard SMTP port, use the
(host, port) tuple format for the mailhost argument.
If you use a string, the standard SMTP port is used.
If your
SMTP server requires authentication, you can specify a (username, password) tuple for the credentials argument.
To specify the use of a secure protocol (TLS), pass in a tuple to the secure argument.
This will only be used when
authentication credentials are supplied.
The tuple should be either an empty tuple, or a single-value tuple with the
name of a keyfile, or a 2-value tuple with the names of the keyfile and certificate file.
A timeout can be specified for communication with the SMTP server using the timeout argument.
Formats the record and sends it to the specified addressees.
If you want to specify a subject line which is record-dependent, override this method.
Flushing occurs whenever the buffer is full, or when an event of
a certain severity or greater is seen.
MemoryHandler is a subclass of the more general BufferingHandler, which is an abstract class.
This buffers
logging records in memory.
Whenever each record is added to the buffer, a check is made by calling shouldFlush()
to see if the buffer should be flushed.
If it should, then flush() is expected to do the flushing.
Initializes the handler with a buffer of the specified capacity.
Here, capacity means the number of logging records
buffered.
emit(record)
Append the record to the buffer.
If shouldFlush() returns true, call flush() to process the buffer.
For a BufferingHandler instance, flushing means that it sets the buffer to an empty list.
This method
can be overwritten to implement more useful flushing behavior.
This method can be overridden to implement custom flushing
strategies.
MemoryHandler(capacity, flushLevel=ERROR, target=None,
flushOnClose=True)
Returns a new instance of the MemoryHandler class.
The instance is initialized with a buffer size of capacity
(number of records buffered).
If flushLevel is not specified, ERROR is used.
If no target is specified, the target
will need to be set using setTarget() before this handler does anything useful.
If flushOnClose is specified
16.8.
If not specified or specified as True, the
previous behaviour of flushing the buffer will occur when the handler is closed.
The flushOnClose parameter was added.
For a MemoryHandler instance, flushing means just sending the buffered records to the target, if there is
one.
The buffer is also cleared when buffered records are sent to the target.
Override if you want different
behavior.
setTarget(target)
Sets the target handler for this handler.
shouldFlush(record)
Checks for buffer full or a record at the flushLevel or higher.
HTTPHandler(host, url, method=’GET’, secure=False, credentials=None,
context=None)
Returns a new instance of the HTTPHandler class.
The host can be of the form host:port, should you need
to use a specific port number.
If no method is specified, GET is used.
If secure is true, a HTTPS connection will be
used.
The context parameter may be set to a ssl.
SSLContext instance to configure the SSL settings used for
the HTTPS connection.
If credentials is specified, it should be a 2-tuple consisting of userid and password, which
will be placed in a HTTP ‘Authorization’ header using Basic authentication.
If you specify credentials, you should
also specify secure=True so that your userid and password are not passed in cleartext across the wire.
Provides a dictionary, based on record, which is to be URL-encoded and sent to the web server.
The default
implementation just returns record.__dict__.
Sends the record to the web server as a URL-encoded dictionary.
The mapLogRecord() method is used
to convert the record to the dictionary to be sent.
Note: Since preparing a record for sending it to a web server is not the same as a generic formatting operation,
using setFormatter() to specify a Formatter for a HTTPHandler has no effect.
Instead of calling
format(), this handler calls mapLogRecord() and then urllib.parse.urlencode() to encode the
dictionary in a form suitable for sending to a web server.
The QueueHandler class, located in the logging.handlers module, supports sending logging messages to a
queue, such as those implemented in the queue or multiprocessing modules.
Along with the QueueListener class, QueueHandler can be used to let handlers do their work on a separate
thread from the one which does the logging.
This is important in web applications and also other service applications
where threads servicing clients need to respond as quickly as possible, while any potentially slow operations (such as
sending an email via SMTPHandler) are done on a separate thread.
QueueHandler(queue)
Returns a new instance of the QueueHandler class.
The instance is initialized with the queue to send messages
to.
The queue can be any queue-like object; it’s used as-is by the enqueue() method, which needs to know how
to send messages to it.
The queue is not required to have the task tracking API, which means that you can use
SimpleQueue instances for queue.
Note:
If you are using multiprocessing, you should avoid using SimpleQueue and instead use
multiprocessing.
Queue.
Enqueues the result of preparing the LogRecord.
Should an exception occur (e.g. because a bounded queue
has filled up), the handleError() method is called to handle the error.
This can result in the record silently
being dropped (if logging.raiseExceptions is False) or a message printed to sys.stderr (if
logging.raiseExceptions is True).
The object returned by this method is enqueued.
The base implementation formats the record to merge the message, arguments, exception and stack in-
formation, if present.
It also removes unpickleable items from the record in-place.
Specifically, it over-
writes the record’s msg and message attributes with the merged message (obtained by calling the handler’s
format() method), and sets the args, exc_info and exc_text attributes to None.
You might want to override this method if you want to convert the record to a dict or JSON string, or send a
modified copy of the record while leaving the original intact.
Note: The base implementation formats the message with arguments, sets the message and msg attributes
to the formatted message and sets the args and exc_text attributes to None to allow pickling and to
prevent further attempts at formatting.
This means that a handler on the QueueListener side won’t have
the information to do custom formatting, e.g. of exceptions.
You may wish to subclass QueueHandler
and override this method to e.g. avoid setting exc_text to None.
Note that the message / msg / args
changes are related to ensuring the record is pickleable, and you might or might not be able to avoid doing
that depending on whether your args are pickleable.
Otherwise, it will be None.
16.8.
The QueueListener class, located in the logging.handlers module, supports receiving logging messages from
a queue, such as those implemented in the queue or multiprocessing modules.
The messages are received
from a queue in an internal thread and passed, on the same thread, to one or more handlers for processing.
While
QueueListener is not itself a handler, it is documented here because it works hand-in-hand with QueueHandler.
Along with the QueueHandler class, QueueListener can be used to let handlers do their work on a separate
thread from the one which does the logging.
This is important in web applications and also other service applications
where threads servicing clients need to respond as quickly as possible, while any potentially slow operations (such as
sending an email via SMTPHandler) are done on a separate thread.
QueueListener(queue, *handlers, respect_handler_level=False)
Returns a new instance of the QueueListener class.
The instance is initialized with the queue to send messages
to and a list of handlers which will handle entries placed on the queue.
The queue can be any queue-like object;
it’s passed as-is to the dequeue() method, which needs to know how to get messages from it.
The queue is not
required to have the task tracking API (though it’s used if available), which means that you can use SimpleQueue
instances for queue.
Note:
If you are using multiprocessing, you should avoid using SimpleQueue and instead use
multiprocessing.
Queue.
If respect_handler_level is True, a handler’s level is respected (compared with the level for the message)
when deciding whether to pass messages to that handler; otherwise, the behaviour is as in previous Python versions
- to always pass each message to each handler.
Changed in version 3.5: The respect_handler_level argument was added.
dequeue(block)
Dequeues a record and return it, optionally blocking.
The base implementation uses get().
Prepare a record for handling.
This implementation just returns the passed-in record.
You may want to override this method if you need to
do any custom marshalling or manipulation of the record before passing it to the handlers.
This just loops through the handlers offering them the record to handle.
The actual object passed to the
handlers is that which is returned from prepare().
This asks the thread to terminate, and then waits for it to do so.
Note that if you don’t call this before your
application exits, there may be some records still left on the queue, which won’t be processed.
This implementation uses put_nowait().
You
may want to override this method if you want to use timeouts or work with custom queue implementations.
New in version 3.3.
See also:
Module logging API reference for the logging module.
Module logging.config Configuration API for the logging module.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The getpass module provides two functions:
getpass.getpass(prompt=’Password: ’, stream=None)
Prompt the user for a password without echoing.
The user is prompted using the string prompt, which defaults to
'Password: '.
On Unix, the prompt is written to the file-like object stream using the replace error handler if
needed.
If echo free input is unavailable getpass() falls back to printing a warning message to stream and reading from
sys.stdin and issuing a GetPassWarning.
Note: If you call getpass from within IDLE, the input may be done in the terminal you launched IDLE from rather
than the idle window itself.
GetPassWarning
A UserWarning subclass issued when password input may be echoed.
This function checks the environment variables LOGNAME, USER, LNAME and USERNAME, in order, and returns
the value of the first one which is set to a non-empty string.
If none are set, the login name from the password
database is returned on systems which support the pwd module, otherwise, an exception is raised.
In general, this function should be preferred over os.getlogin().
Lib/curses
The curses module provides an interface to the curses library, the de-facto standard for portable advanced terminal
handling.
While curses is most widely used in the Unix environment, versions are available for Windows, DOS, and possibly other
systems as well.
This extension module is designed to match the API of ncurses, an open-source curses library hosted on
Linux and the BSD variants of Unix.
Whenever the documentation mentions a character it can be specified as an integer, a one-character Unicode
string or a one-byte byte string.
Whenever the documentation mentions a character string it can be specified as a Unicode string or a byte string.
See also:
Module curses.ascii Utilities for working with ASCII characters, regardless of your locale settings.
A panel stack extension that adds depth to curses windows.
Module curses.textpad Editable text widget for curses supporting Emacs-like bindings.
curses-howto Tutorial material on using curses with Python, by Andrew Kuchling and Eric Raymond.
The module curses defines the following exception:
exception curses.error
Exception raised when a curses library function returns an error.
Whenever x or y arguments to a function or a method are optional, they default to the current cursor location.
Whenever attr is optional, it defaults to A_NORMAL.
The module curses defines the following functions:
curses.baudrate()
Return the output speed of the terminal in bits per second.
On software terminal emulators it will have a fixed
high value.
Included for historical reasons; in former times, it was used to write output loops for time delays and
occasionally to change interfaces depending on the line speed.
In cbreak mode (sometimes called “rare” mode) normal tty line buffering is turned off and
characters are available to be read one by one.
However, unlike raw mode, special characters (interrupt, quit,
suspend, and flow control) retain their effects on the tty driver and calling program.
Calling first raw() then
cbreak() leaves the terminal in cbreak mode.
Return a 3-tuple, containing the R,G,B values for the given color, which will be
between 0 (no component) and 1000 (maximum amount of component).
Only the first 256 color pairs are sup-
ported.
This attribute value can be combined with A_STANDOUT, A_REVERSE, and the other A_* attributes.
If the terminal supports
the visibility requested, return the previous cursor state; otherwise raise an exception.
On many terminals, the
“visible” mode is an underline cursor and the “very visible” mode is a block cursor.
Subsequent calls to
reset_prog_mode() will restore this mode.
Subsequent calls to
reset_shell_mode() will restore this mode.
curses.delay_output(ms)
Insert an ms millisecond pause in output.
The curses library keeps two data structures, one representing the current physical
screen contents and a virtual screen representing the desired next state.
The doupdate() ground updates the
physical screen to match the virtual screen.
The virtual screen may be updated by a noutrefresh() call after write operations such as addstr()
have been performed on a window.
The normal refresh() call is simply noutrefresh() followed by
doupdate(); if you have to update multiple windows, you can speed performance and perhaps reduce screen
flicker by issuing noutrefresh() calls on all windows, followed by a single doupdate().
In echo mode, each character input is echoed to the screen as it is entered.
De-initialize the library, and return terminal to normal status.
Under Unix operating systems this is a property
of the controlling tty of the curses program, and is not set by the curses library itself.
The effect is that, during those
calls, LINES is set to 1; the capabilities clear, cup, cud, cud1, cuu1, cuu, vpa are disabled; and the home
string is set to the value of cr.
The effect is that the cursor is confined to the current line, and so are screen updates.
Flash the screen.
That is, change it to reverse-video and then change it back in a short interval.
Some people prefer
such as ‘visible bell’ to the audible attention signal produced by beep().
This throws away any typeahead that has been typed by the user and has not yet been
processed by the program.
The BUTTON5_* constants are now exposed if they are provided by the underlying
curses library.
If leaveok is currently True,
then return (-1, -1).
The routine then creates and
initializes a new window using that data, returning the new window object.
Extended color support allows
more than 256 color pairs for terminals that support more than 16 colors (e.g. xterm-256color).
Extended color support requires ncurses version 6.1 or later.
New in version 3.10.
This function is included for historical
reasons only, as all modern software terminal emulators have such capabilities.
This function is included for historical reasons only, as all modern software terminal emulators have such capabil-
ities.
However, after blocking for tenths tenths of seconds, raise an exception if nothing has
been typed.
The value of tenths must be a number between 1 and 255.
Use nocbreak() to leave half-delay
mode.
curses.init_color(color_number, r, g, b)
Change the definition of a color, taking the number of the color to be changed followed by three RGB values (for
the amounts of red, green, and blue components).
The value of color_number must be between 0 and COLORS
- 1.
Each of r, g, b, must be a value between 0 and 1000.
This function is a no-op on most terminals; it is
active only if can_change_color() returns True.
It takes three arguments: the number of the color-pair to be changed, the
foreground color number, and the background color number.
The value of pair_number must be between 1 and
COLOR_PAIRS - 1 (the 0 color pair is wired to white on black and cannot be changed).
The value of fg and
bg arguments must be between 0 and COLORS - 1, or, after calling use_default_colors(), -1.
If the
color-pair was previously initialized, the screen is refreshed and all occurrences of that color-pair are changed to
the new definition.
Return a window object which represents the whole screen.
Note: If there is an error opening the terminal, the underlying curses library may cause the interpreter to exit.
The name of a key generating printable ASCII character
is the key’s character.
The name of a control-key combination is a two-byte bytes object consisting of a caret
(b'^') followed by the corresponding printable ASCII character.
The name of an alt-key combination (128–255)
is a bytes object consisting of the prefix b'M-' followed by the name of the corresponding ASCII character.
Under Unix operating systems this is a
property of the controlling tty of the curses program, and is not set by the curses library itself.
The maximum length
of a verbose description is 128 characters.
If flag is True, allow 8-bit characters to be input.
If flag is False, allow only 7-bit chars.
curses.mouseinterval(interval)
Set the maximum time in milliseconds that can elapse between press and release events in order for them to be
recognized as a click, and return the previous interval value.
The default value is 200 milliseconds, or one fifth of
a second.
curses.mousemask(mousemask)
Set the mouse events to be reported, and return a tuple (availmask, oldmask).
If this function is never called, no mouse events are ever reported.
Return a pad
as a window object.
A pad is like a window, except that it is not restricted by the screen size, and is not necessarily associated with
a particular part of the screen.
Pads can be used when a large window is needed, and only a part of the window
will be on the screen at one time.
Automatic refreshes of pads (such as from scrolling or echoing of input) do
not occur.
The refresh() and noutrefresh() methods of a pad require 6 arguments to specify the part
of the pad to be displayed and the location on the screen to be used for the display.
The arguments are pminrow,
pmincol, sminrow, smincol, smaxrow, smaxcol; the p arguments refer to the upper left corner of the pad region to
be displayed and the s arguments define a clipping box on the screen within which the pad region is to be displayed.
By default, the window will extend from the specified position to the lower right corner of the screen.
This mode translates the return key into newline on input, and translates newline into return
and line-feed on output.
Newline mode is initially on.
Return to normal “cooked” mode with line buffering.
Echoing of input characters is turned off.
Disable translation of return into newline on input, and disable low-level translation of newline
into newline/return on output (but this does not change the behavior of addch('\n'), which always does the
equivalent of return and line feed on the virtual screen).
With translation off, curses can sometimes speed up
vertical motion a little; also, it will be able to detect the return key on input.
When the noqiflush() routine is used, normal flush of input and output queues associated with the INTR,
QUIT and SUSP characters will not be done.
You may want to call noqiflush() in a signal handler if you want
output to continue as though the interrupt had not occurred, after the handler exits.
Return to normal “cooked” mode with line buffering.
Return the number of the color-pair set by the attribute value attr.
Equivalent to tputs(str, 1, putchar); emit the value of a specified terminfo capability for the current
terminal.
Note that the output of putp() always goes to standard output.
If flag is False, the effect is the same as calling noqiflush().
If flag is True, or no argument is provided,
the queues will be flushed when these control characters are read.
Backend function used by resizeterm(), performing most of the work; when resizing the windows,
resize_term() blank-fills the areas that are extended.
The calling application should fill in these areas with
appropriate data.
The resize_term() function attempts to resize all windows.
However, due to the calling
convention of pads, it is not possible to resize these without additional interaction with the application.
Resize the standard and current windows to the specified dimensions, and adjusts other bookkeeping data used by
the curses library that record the window dimensions (in particular the SIGWINCH handler).
New in version 3.9.
New in version 3.9.
New in version 3.9.
New in version 3.9.
If y and x are both -1, then leaveok is set True.
It
is good practice to call this routine right after initscr().
It also restores the colors on the terminal to the values they had
when the terminal was just turned on.
This information is useful when a curses
program needs complete control over the appearance of the screen.
Return the value -1 if capname is not a Boolean capability, or 0 if it is canceled or absent from the terminal
description.
curses.tigetnum(capname)
Return the value of the numeric capability corresponding to the terminfo capability name capname as an integer.
Return the value of the string capability corresponding to the terminfo capability name capname as a bytes object.
Return None if capname is not a terminfo “string capability”, or is canceled or absent from the terminal description.
E.g. tparm(tigetstr("cup"), 5, 3) could result in b'\033[6;4H', the
exact result depending on terminal type.
curses.typeahead(fd)
Specify that the file descriptor fd be used for typeahead checking.
If fd is -1, then no typeahead checking is done.
The curses library does “line-breakout optimization” by looking for typeahead periodically while updating the
screen.
If input is found, and it is coming from a tty, the current update is postponed until refresh or doupdate is
called again, allowing faster response to commands typed in advance.
This function allows specifying a different
file descriptor for typeahead checking.
curses.unctrl(ch)
Return a bytes object which is a printable representation of the character ch.
Control characters are represented as
a caret followed by the character, for example as b'^C'.
Printing characters are left as they are.
Note: Only one ch can be pushed before getch() is called.
Useful for detecting manual screen resize.
Note: Only one ch can be pushed before get_wch() is called.
If used, this function should be called before initscr() or newterm are called.
When flag is False, the values
of lines and columns specified in the terminfo database will be used, even if environment variables LINES and
COLUMNS (used by default) are set, or if curses is running in a window (in which case default behavior would be
to use the window size if LINES and COLUMNS are not set).
Use this to support transparency in your
application.
The default color is assigned to the color number -1.
After calling this function, init_pair(x,
curses.
COLOR_RED, -1) initializes, for instance, color pair x to a red foreground color on the default back-
ground.
Initialize curses and call another callable object, func, which should be the rest of your curses-using application.
If the application raises an exception, this function will restore the terminal to a sane state before re-raising the
exception and generating a traceback.
The callable object func is then passed the main window ‘stdscr’ as its first
argument, followed by any other arguments passed to wrapper().
Before calling func, wrapper() turns on
cbreak mode, turns off echo, enables the terminal keypad, and initializes colors if the terminal has color support.
On
exit (whether normally or by exception) it restores cooked mode, turns on echo, and disables the terminal keypad.
By
default, the character position and attributes are the current settings for the window object.
Writing outside the window, subwindow, or pad raises a curses.error.
Attempting to write to the lower
right corner of a window, subwindow, or pad will cause an exception to be raised after the character is printed.
Writing outside the window, subwindow, or pad raises curses.error.
Attempting to write to the lower
right corner of a window, subwindow, or pad will cause an exception to be raised after the string is printed.
A bug in ncurses, the backend for this Python module, can cause SegFaults when resizing windows.
This is
fixed in ncurses-6.1-20190511.
If you are stuck with an earlier ncurses, you can avoid triggering this if you
do not call addstr() with a str that has embedded newlines.
Instead, call addstr() separately for each
line.
16.10.
Remove attribute attr from the “background” set applied to all writes to the current window.
Add attribute attr from the “background” set applied to all writes to the current window.
window.attrset(attr)
Set the “background” set of attributes to attr.
This set is initially 0 (no attributes).
The attribute of every character in the window is changed to the new background attribute.
Wherever the former background character appears, it is changed to the new background character.
A window’s background consists of a character and any combination of attributes.
The attribute part of the background is combined (OR’ed) with all non-blank characters that are written into the win-
dow.
Both the character and attribute parts of the background are combined with the blank characters.
The back-
ground becomes a property of the character and moves with the character through any scrolling and insert/delete
line/character operations.
Each parameter specifies the character to use for a specific part of
the border; see the table below for more details.
Note:
A 0 value for any parameter will cause the default character to be used for that parameter.
Keyword
parameters can not be used.
Similar to border(), but both ls and rs are vertch and both ts and bs are horch.
The default corner characters are
always used by this function.
If num is
not given or is -1, the attribute will be set on all the characters to the end of the line.
The changed line will be touched using the touchline() method so that the
contents will be redisplayed by the next window refresh.
If flag is True, the next call to refresh() will clear the window completely.
Erase from cursor to the end of the line.
window.cursyncup()
Update the current cursor position of all the ancestors of the window to reflect the current cursor position of the
window.
All following lines are moved up by one line.
An abbreviation for “derive window”, derwin() is the same as calling subwin(), except that begin_y and
begin_x are relative to the origin of the window, rather than relative to the entire screen.
Return a window object
for the derived window.
Add character ch with attribute attr, and immediately call refresh() on the window.
It is useful for determining what subset of the screen windows enclose the location of a mouse
event.
Previously it returned 1 or 0 instead of True or False.
The encoding attribute is inherited
from the parent window when a subwindow is created, for example with window.subwin().
By default, current
locale encoding is used (see locale.getencoding()).
New in version 3.3.
Clear the window.
Note that the integer returned does not have to be in ASCII range: function keys, keypad keys and
so on are represented by numbers higher than 255.
Return a character for most keys, or an integer for function keys, keypad keys, and other
special keys.
In no-delay mode, raise an exception if there is no input.
New in version 3.3.
Function keys, keypad keys and other
special keys return a multibyte string containing the key name.
In no-delay mode, raise an exception if there is no
input.
If flag is False, curses no longer considers using the hardware insert/delete character feature of the terminal; if
flag is True, use of character insertion and deletion is enabled.
If flag is True, curses will try and use hardware line editing facilities.
If flag is True, any change in the window image automatically causes the window to be refreshed; you no longer
have to call refresh() yourself.
However, it may degrade performance considerably, due to repeated calls to
wrefresh.
This option is disabled by default.
The bottom 8 bits are the character proper, and upper
bits are the attributes.
The nlines bottom lines are lost.
For negative
nlines, delete nlines lines starting with the one under the cursor, and move the remaining lines up.
The bottom
nlines lines are cleared.
The current cursor position remains the same.
All following lines are moved down by one line.
If n is zero or negative, the entire string is inserted.
All characters to the right of the cursor are shifted
right, with the rightmost characters on the line being lost.
The cursor position does not change (after moving to y,
x, if specified).
All
characters to the right of the cursor are shifted right, with the rightmost characters on the line being lost.
The
cursor position does not change (after moving to y, x, if specified).
Attributes are stripped from the characters.
If n is specified, instr() returns a string at most n
characters long (exclusive of the trailing NUL).
Raise
a curses.error exception if line is not valid for the given window.
If flag is True, escape sequences generated by some keys (keypad, function keys) will be interpreted by curses.
This reduces cursor
movement where possible.
If possible the cursor will be made invisible.
If flag is False, cursor will always be at “cursor position” after an update.
The screen-relative parameters of the window are not changed.
This
routine is used to display different parts of the parent window at the same physical position on the screen.
window.mvwin(new_y, new_x)
Move the window so its upper-left corner is at (new_y, new_x).
If flag is True, escape sequences will not be timed out.
If flag is False, after a few milliseconds, an escape sequence will not be interpreted, and will be left in the input
stream as is.
This function updates the data structure representing the desired state of the window,
but does not force an update of the physical screen.
To accomplish that, call doupdate().
The windows need not be the same size, only the overlapping region is
copied.
This copy is non-destructive, which means that the current background character does not overwrite the
old contents of destwin.
To get fine-grained control over the copied region, the second form of overlay() can be used.
The windows need not be the same size, in which case only the overlapping
region is copied.
This copy is destructive, which means that the current background character overwrites the old
contents of destwin.
To get fine-grained control over the copied region, the second form of overwrite() can be used.
This information can be later retrieved
using the getwin() function.
The 6 optional arguments can only be specified when the window is a pad created with newpad().
The additional
parameters are needed to indicate what part of the pad and screen are involved.
The lower right-hand corner of the rectangle to be displayed
in the pad is calculated from the screen coordinates, since the rectangles must be the same size.
Both rectangles
must be entirely contained within their respective structures.
Negative values of pminrow, pmincol, sminrow, or
smincol are treated as if they were zero.
If either dimension is larger
than the current values, the window’s data is filled with blanks that have the current background rendition (as set
by bkgdset()) merged into them.
If flag is False, the
cursor is left on the bottom line.
If flag is True, the window is scrolled up one line.
Note that in order to get the
physical scrolling effect on the terminal, it is also necessary to call idlok().
All scrolling actions will take place in this region.
On some terminals this has the side effect of turning off all attributes.
By default, the sub-window will extend from the specified position to the lower right corner of the window.
If flag is True, then syncup() is called automatically whenever there is a change in the window.
If delay is negative, blocking read is used (which will
wait indefinitely for input).
If delay is zero, then non-blocking read is used, and getch() will return -1 if no
input is waiting.
If delay is positive, then getch() will block for delay milliseconds, and return -1 if there is still
no input at the end of that time.
window.touchline(start, count[, changed])
Pretend count lines have been changed, starting with line start.
If changed is supplied, it specifies whether the
affected lines are marked as having been changed (changed=True) or unchanged (changed=False).
ERR
Some curses routines that return an integer, such as getch(), return ERR upon failure.
curses.
OK
Some curses routines that return an integer, such as napms(), return OK upon success.
A bytes object representing the current version of the module.
All values
are integers.
The components can also be accessed by name, so curses.ncurses_version[0] is equivalent
to curses.ncurses_version.major and so on.
Availability: if the ncurses library is used.
New in version 3.8.
curses.
COLORS
The maximum number of colors the terminal can support.
It is defined only after the call to start_color().
curses.
The maximum number of color pairs the terminal can support.
It is defined only after the call to
start_color().
curses.
COLS
The width of the screen, i.e., the number of columns.
It is defined only after the call to initscr().
Updated by
update_lines_cols(), resizeterm() and resize_term().
curses.
The height of the screen, i.e., the number of lines.
It is defined only after the call to initscr().
Updated by
update_lines_cols(), resizeterm() and resize_term().
Some constants are available to specify character cell attributes.
A_ALTCHARSET
Alternate character set mode
curses.
A_BLINK
Blink mode
curses.
A_BOLD
Bold mode
curses.
A_DIM
Dim mode
curses.
A_INVIS
Invisible or blank mode
curses.
A_ITALIC
Italic mode
curses.
A_NORMAL
Normal attribute
curses.
A_PROTECT
Protected mode
curses.
A_REVERSE
Reverse background and foreground colors
curses.
A_STANDOUT
Standout mode
curses.
A_UNDERLINE
Underline mode
curses.
A_HORIZONTAL
Horizontal highlight
curses.
A_LEFT
Left highlight
curses.
A_LOW
Low highlight
curses.
A_RIGHT
Right highlight
curses.
A_TOP
Top highlight
curses.
A_VERTICAL
Vertical highlight
16.10.
Several constants are available to extract corresponding attributes returned by some methods.
Bit-mask
Meaning
curses.
A_ATTRIBUTES
Bit-mask to extract attributes
curses.
A_CHARTEXT
Bit-mask to extract a character
curses.
A_COLOR
Bit-mask to extract color-pair field information
Keys are referred to by integer constants with names starting with KEY_.
The exact keycaps available are system depen-
dent.
Key constant
Key
curses.
Minimum key value
curses.
KEY_BREAK
Break key (unreliable)
curses.
KEY_DOWN
Down-arrow
curses.
KEY_UP
Up-arrow
curses.
KEY_LEFT
Left-arrow
curses.
KEY_RIGHT
Right-arrow
curses.
KEY_HOME
Home key (upward+left arrow)
curses.
KEY_BACKSPACE
Backspace (unreliable)
curses.
KEY_F0
Function keys.
Up to 64 function keys are supported.
KEY_DL
Delete line
curses.
KEY_IL
Insert line
curses.
KEY_DC
Delete character
curses.
Exit insert char mode
curses.
KEY_CLEAR
Clear screen
curses.
Clear to end of screen
curses.
KEY_EOL
Clear to end of line
curses.
KEY_SF
Scroll 1 line forward
curses.
KEY_SR
Scroll 1 line backward (reverse)
curses.
KEY_NPAGE
Next page
curses.
KEY_PPAGE
Previous page
curses.
KEY_STAB
Set tab
curses.
KEY_CTAB
Clear tab
curses.
Clear all tabs
continues on next page
16.10.
KEY_ENTER
Enter or send (unreliable)
curses.
KEY_SRESET
Soft (partial) reset (unreliable)
curses.
KEY_RESET
Reset or hard reset (unreliable)
curses.
KEY_PRINT
Print
curses.
KEY_LL
Home down or bottom (lower left)
curses.
Upper left of keypad
curses.
KEY_A3
Upper right of keypad
curses.
KEY_B2
Center of keypad
curses.
KEY_C1
Lower left of keypad
curses.
KEY_C3
Lower right of keypad
curses.
KEY_BTAB
Back tab
curses.
KEY_BEG
Beg (beginning)
curses.
KEY_CANCEL
Cancel
curses.
KEY_CLOSE
Close
curses.
KEY_COPY
Copy
curses.
KEY_CREATE
Create
curses.
End
curses.
KEY_EXIT
Exit
curses.
KEY_FIND
Find
curses.
KEY_HELP
Help
curses.
KEY_MARK
Mark
curses.
KEY_MESSAGE
Message
curses.
KEY_MOVE
Move
curses.
KEY_NEXT
Next
curses.
KEY_OPEN
Open
curses.
KEY_OPTIONS
Options
curses.
KEY_PREVIOUS
Prev (previous)
curses.
KEY_REDO
Redo
curses.
KEY_REFERENCE
Ref (reference)
continues on next page
16.10.
KEY_REFRESH
Refresh
curses.
KEY_REPLACE
Replace
curses.
KEY_RESTART
Restart
curses.
KEY_RESUME
Resume
curses.
KEY_SAVE
Save
curses.
KEY_SBEG
Shifted Beg (beginning)
curses.
KEY_SCANCEL
Shifted Cancel
curses.
KEY_SCOMMAND
Shifted Command
curses.
KEY_SCOPY
Shifted Copy
curses.
KEY_SCREATE
Shifted Create
curses.KEY_SDC
Shifted Delete char
curses.KEY_SDL
Shifted Delete line
curses.
KEY_SELECT
Select
curses.
KEY_SEND
Shifted End
curses.
KEY_SEXIT
Shifted Exit
curses.
KEY_SFIND
Shifted Find
curses.
KEY_SHELP
Shifted Help
curses.
Shifted Input
curses.
KEY_SLEFT
Shifted Left arrow
curses.
KEY_SMESSAGE
Shifted Message
curses.
KEY_SMOVE
Shifted Move
curses.
Shifted Next
curses.
KEY_SOPTIONS
Shifted Options
curses.
KEY_SPREVIOUS
Shifted Prev
curses.
KEY_SPRINT
Shifted Print
curses.
KEY_SREDO
Shifted Redo
curses.
KEY_SREPLACE
Shifted Replace
curses.
KEY_SRIGHT
Shifted Right arrow
continues on next page
16.10.
KEY_SRSUME
Shifted Resume
curses.
KEY_SSAVE
Shifted Save
curses.
KEY_SSUSPEND
Shifted Suspend
curses.
KEY_SUNDO
Shifted Undo
curses.
KEY_SUSPEND
Suspend
curses.
KEY_UNDO
Undo
curses.
KEY_MOUSE
Mouse event has occurred
curses.
KEY_RESIZE
Terminal resize event
curses.
Maximum key value
On VT100s and their software emulations, such as X terminal emulators, there are normally at least four function keys
(KEY_F1, KEY_F2, KEY_F3, KEY_F4) available, and the arrow keys mapped to KEY_UP, KEY_DOWN, KEY_LEFT
and KEY_RIGHT in the obvious way.
The following table lists characters from the alternate character set.
These are inherited from the VT100 terminal, and
will generally be available on software emulations such as X terminals.
When there is no graphic available, curses falls
back on a crude printable ASCII approximation.
ACS_BBSS
alternate name for upper right corner
curses.
ACS_BLOCK
solid square block
curses.
ACS_BOARD
board of squares
curses.
ACS_BSBS
alternate name for horizontal line
curses.
ACS_BSSB
alternate name for upper left corner
curses.
ACS_BSSS
alternate name for top tee
curses.
ACS_BTEE
bottom tee
curses.
ACS_BULLET
bullet
curses.
ACS_CKBOARD
checker board (stipple)
curses.
ACS_DARROW
arrow pointing down
curses.
ACS_DEGREE
degree symbol
curses.
ACS_DIAMOND
diamond
curses.
ACS_GEQUAL
greater-than-or-equal-to
curses.
ACS_HLINE
horizontal line
curses.
ACS_LANTERN
lantern symbol
continues on next page
16.10.
ACS_LARROW
left arrow
curses.
ACS_LEQUAL
less-than-or-equal-to
curses.
ACS_LLCORNER
lower left-hand corner
curses.
ACS_LRCORNER
lower right-hand corner
curses.
ACS_LTEE
left tee
curses.
ACS_NEQUAL
not-equal sign
curses.
ACS_PI
letter pi
curses.
ACS_PLMINUS
plus-or-minus sign
curses.
ACS_PLUS
big plus sign
curses.
ACS_RARROW
right arrow
curses.
ACS_RTEE
right tee
curses.
ACS_S1
scan line 1
curses.
ACS_S3
scan line 3
curses.
ACS_S7
scan line 7
curses.
ACS_SBBS
alternate name for lower right corner
curses.
ACS_SBSB
alternate name for vertical line
curses.
ACS_SBSS
alternate name for right tee
curses.
ACS_SSBB
alternate name for lower left corner
curses.
ACS_SSBS
alternate name for bottom tee
curses.
ACS_SSSB
alternate name for left tee
curses.
ACS_SSSS
alternate name for crossover or big plus
curses.
ACS_STERLING
pound sterling
curses.
ACS_TTEE
top tee
curses.
ACS_UARROW
up arrow
curses.
ACS_ULCORNER
upper left corner
curses.
ACS_URCORNER
upper right corner
curses.
ACS_VLINE
vertical line
The following table lists mouse button constants used by getmouse():
16.10.
Mouse button n pressed
curses.
Mouse button n released
curses.
BUTTONn_CLICKED
Mouse button n clicked
curses.
BUTTONn_DOUBLE_CLICKED
Mouse button n double clicked
curses.
BUTTONn_TRIPLE_CLICKED
Mouse button n triple clicked
curses.
BUTTON_SHIFT
Shift was down during button state change
curses.
BUTTON_CTRL
Control was down during button state change
curses.
BUTTON_ALT
Control was down during button state change
Changed in version 3.10: The BUTTON5_* constants are now exposed if they are provided by the underlying curses
library.
COLOR_BLACK
Black
curses.COLOR_BLUE
Blue
curses.
COLOR_CYAN
Cyan (light greenish blue)
curses.
Magenta (purplish red)
curses.
COLOR_RED
Red
curses.
COLOR_WHITE
White
curses.
COLOR_YELLOW
Yellow
16.11 curses.textpad — Text input widget for curses programs
The curses.textpad module provides a Textbox class that handles elementary text editing in a curses window,
supporting a set of keybindings resembling those of Emacs (thus, also of Netscape Navigator, BBedit 6.x, FrameMaker,
and many other programs).
The module also provides a rectangle-drawing function useful for framing text boxes or for
other purposes.
The module curses.textpad defines the following function:
curses.textpad.rectangle(win, uly, ulx, lry, lrx)
Draw a rectangle.
The first argument must be a window object; the remaining arguments are coordinates relative
to that window.
The second and third arguments are the y and x coordinates of the upper left hand corner of the
rectangle to be drawn; the fourth and fifth arguments are the y and x coordinates of the lower right hand corner.
The rectangle will be drawn using VT100/IBM PC forms characters on terminals that make this possible (including
xterm and most other software terminal emulators).
Otherwise it will be drawn with ASCII dashes, vertical bars,
and plus signs.
Textbox(win)
Return a textbox widget object.
The win argument should be a curses window object in which the textbox is to be
contained.
The edit cursor of the textbox is initially located at the upper left hand corner of the containing window,
with coordinates (0, 0).
The instance’s stripspaces flag is initially on.
This is the entry point you will normally use.
It accepts editing keystrokes until one of the termination
keystrokes is entered.
If validator is supplied, it must be a function.
It will be called for each keystroke
entered with the keystroke as a parameter; command dispatch is done on the result.
Process a single command keystroke.
Here are the supported special keystrokes:
Keystroke
Action
Control-A
Go to left edge of window.
Control-B
Cursor left, wrapping to previous line if appropriate.
Control-D
Delete character under cursor.
Control-E
Go to right edge (stripspaces off) or end of line (stripspaces on).
Control-F
Cursor right, wrapping to next line when appropriate.
Control-G
Terminate, returning the window contents.
Control-H
Delete character backward.
Control-J
Terminate if the window is 1 line, otherwise insert newline.
Control-K
If line is blank, delete it, otherwise clear to end of line.
Control-L
Refresh screen.
Control-N
Cursor down; move down one line.
Control-O
Insert a blank line at cursor location.
Control-P
Cursor up; move up one line.
Move operations do nothing if the cursor is at an edge where the movement is not possible.
The following
synonyms are supported where possible:
Constant
Keystroke
KEY_LEFT
Control-B
KEY_RIGHT
Control-F
KEY_UP
Control-P
KEY_DOWN
Control-N
KEY_BACKSPACE
Control-h
All other keystrokes are treated as a command to insert the given character and move right (with line wrap-
ping).
Lib/curses/ascii.py
The curses.ascii module supplies name constants for ASCII characters and functions to test membership in various
ASCII character classes.
The constants supplied are names for control characters as follows:
Name
Meaning
curses.ascii.
NUL
curses.ascii.
SOH
Start of heading, console interrupt
curses.ascii.
STX
Start of text
curses.ascii.
ETX
End of text
curses.ascii.
EOT
End of transmission
curses.ascii.
ENQ
Enquiry, goes with ACK flow control
curses.ascii.
ACK
Acknowledgement
curses.ascii.BEL
Bell
curses.ascii.
BS
Backspace
curses.ascii.
TAB
Tab
curses.ascii.
HT
Alias for TAB: “Horizontal tab”
continues on next page
16.12.
LF
Line feed
curses.ascii.
NL
Alias for LF: “New line”
curses.ascii.
VT
Vertical tab
curses.ascii.
FF
Form feed
curses.ascii.
CR
Carriage return
curses.ascii.
SO
Shift-out, begin alternate character set
curses.ascii.
SI
Shift-in, resume default character set
curses.ascii.
DLE
Data-link escape
curses.ascii.
DC1
XON, for flow control
curses.ascii.
DC2
Device control 2, block-mode flow control
curses.ascii.
DC3
XOFF, for flow control
curses.ascii.
DC4
Device control 4
curses.ascii.
NAK
Negative acknowledgement
curses.ascii.
SYN
Synchronous idle
curses.ascii.
CAN
Cancel
curses.ascii.
EM
End of medium
curses.ascii.
SUB
Substitute
curses.ascii.
ESC
Escape
curses.ascii.
FS
File separator
curses.ascii.
GS
Group separator
curses.ascii.
RS
Record separator, block-mode terminator
curses.ascii.
US
Unit separator
curses.ascii.
SP
Space
curses.ascii.
DEL
Delete
Note that many of these have little practical significance in modern usage.
The mnemonics derive from teleprinter con-
ventions that predate digital computers.
The module supplies the following functions, patterned on those in the standard C library:
curses.ascii.isalnum(c)
Checks for an ASCII alphanumeric character; it is equivalent to isalpha(c) or isdigit(c).
Checks for an ASCII alphabetic character; it is equivalent to isupper(c) or islower(c).
Checks for a character value that fits in the 7-bit ASCII set.
This is equivalent to c in string.digits.
curses.ascii.isgraph(c)
Checks for ASCII any printable character except space.
This is equivalent to c in string.hexdigits.
Checks for a non-ASCII character (ordinal values 0x80 and above).
These functions accept either integers or single-character strings; when the argument is a string, it is first converted using
the built-in function ord().
Note that all these functions check ordinal bit values derived from the character of the string you pass in; they do not
actually know anything about the host machine’s character encoding.
The following two functions take either a single-character string or integer byte value; they return a value of the same
type.
The following function takes either a single-character string or integer value; it returns a string.
If c is printable, this string is the character itself.
If the
character is a control character (0x00–0x1f) the string consists of a caret ('^') followed by the corresponding
uppercase letter.
If the character is an ASCII delete (0x7f) the string is '^?'.
A 33-element string array that contains the ASCII mnemonics for the thirty-two ASCII control characters from 0
(NUL) to 0x1f (US), in order, plus the mnemonic SP for the space character.
A panel stack extension for curses
Panels are windows with the added feature of depth, so they can be stacked on top of each other, and only the visible
portions of each window will be displayed.
Panels can be added, moved up or down in the stack, and removed.
Be aware that you need to keep the returned panel
object referenced explicitly.
If you don’t, the panel object is garbage collected and removed from the panel stack.
This does not call curses.doupdate(), so you’ll
have to do this yourself.
There’s always a window
associated with a panel which determines the content, while the panel methods are responsible for the window’s depth in
the panel stack.
Panel objects have the following methods:
Panel.above()
Returns the panel above the current panel.
Panel.below()
Returns the panel below the current panel.
Panel.bottom()
Push the panel to the bottom of the stack.
Panel.hidden()
Returns True if the panel is hidden (not visible), False otherwise.
Hide the panel.
This does not delete the object, it just makes the window on screen invisible.
Panel.replace(win)
Change the window associated with the panel to the window win.
Panel.set_userptr(obj)
Set the panel’s user pointer to obj.
This is used to associate an arbitrary piece of data with the panel, and can be
any Python object.
Panel.show()
Display the panel (which might have been hidden).
Panel.top()
Push panel to the top of the stack.
Panel.userptr()
Returns the user pointer for the panel.
This might be any Python object.
Panel.window()
Returns the window object associated with the panel.
Returns a tuple (bits, linkage) which contain information about the bit architecture and the linkage format
used for the executable.
Both values are returned as strings.
Values that cannot be determined are returned as given by the parameter presets.
If bits is given as '', the
sizeof(pointer) (or sizeof(long) on Python version < 1.5.2) is used as indicator for the supported
pointer size.
The function relies on the system’s file command to do the actual work.
This is available on most if not all Unix
platforms and some non-Unix platforms and then only if the executable points to the Python interpreter.
Reasonable
defaults are used when the above needs are not met.
Note: On macOS (and perhaps other platforms), executable files may be universal files containing multiple archi-
tectures.
An empty string is returned if the value cannot be determined.
An empty string is returned if the value cannot
be determined.
The output is intended to be human readable rather than machine parseable.
It may look different on different
platforms and this is intended.
If aliased is true, the function will use aliases for various platforms that report system names which differ from
their common names, for example SunOS will be reported as Solaris.
The system_alias() function is used
to implement this.
Setting terse to true causes the function to return only the absolute minimum information needed to identify the
platform.
Changed in version 3.8: On macOS, the function now uses mac_ver(), if it returns a non-empty release string,
to get the macOS version rather than the darwin version.
An empty string is returned if the value cannot be determined.
Note that many platforms do not provide this
information or simply return the same value as for machine().
NetBSD does this.
Possible return values are: ‘CPython’, ‘IronPython’,
‘Jython’, ‘PyPy’.
Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to
0).
Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to
'0').
An empty string is returned if the value cannot be deter-
mined.
An empty string is re-
turned if the value cannot be determined.
It
also does some reordering of the information in some cases where it would otherwise cause confusion.
An empty string is returned if the value cannot be
determined.
Returns a namedtuple() containing six attributes:
system, node,
release, version, machine, and processor.
Note: the first two attribute names differ from the names presented by os.uname(), where they are named
sysname and nodename.
Entries which cannot be determined are set to ''.
Result changed from a tuple to a namedtuple().
Changed in version 3.9: processor is resolved late instead of immediately.
Returns a tuple (release, vendor, vminfo, osinfo) with vminfo being a tuple (vm_name,
vm_release, vm_vendor) and osinfo being a tuple (os_name, os_version, os_arch).
Values
which cannot be determined are set to the defaults given as parameters (which all default to '').
Values which cannot be determined are set to the defaults given as parameters (which all default to an
empty string).
As a hint: ptype is 'Uniprocessor Free' on single processor NT machines and 'Multiprocessor
Free' on multi processor machines.
The ‘Free’ refers to the OS version being free of debugging code.
It could
also state ‘Checked’ which means the OS version uses debugging code, i.e. code that checks arguments, ranges, etc.
platform.win32_edition()
Returns a string representing the current Windows edition, or None if the value cannot be determined.
Possible values include but are not limited to 'Enterprise', 'IoTUAP', 'ServerStandard', and
'nanoserver'.
New in version 3.8.
Get macOS version information and return it as tuple (release, versioninfo, machine) with version-
info being a tuple (version, dev_stage, non_release_version).
Entries which cannot be determined are set to ''.
All tuple entries are strings.
Returns a tuple of strings (lib, version) which default to the given parameters in case the lookup fails.
Note that this function has intimate knowledge of how different libc versions add symbols to the executable is
probably only usable for executables compiled using gcc.
The file is read and scanned in chunks of chunksize bytes.
The os-release file is
a freedesktop.org standard and is available in most Linux distributions.
A noticeable exception is Android and
Android-based distributions.
Raises OSError or subclass when neither /etc/os-release nor /usr/lib/os-release can be read.
On success, the function returns a dictionary where keys and values are strings.
Values have their special characters
like " and $ unquoted.
The fields NAME, ID, and PRETTY_NAME are always defined according to the standard.
All other fields are optional.
Vendors may include additional fields.
Note that fields like NAME, VERSION, and VARIANT are strings suitable for presentation to users.
Programs
should use fields like ID, ID_LIKE, VERSION_ID, or VARIANT_ID to identify Linux distributions.
The value of each symbol is the corresponding integer
value.
The names and descriptions are borrowed from linux/include/errno.h, which should be all-inclusive.
errno.errorcode
Dictionary providing a mapping from the errno value to the string name in the underlying system.
For instance,
errno.errorcode[errno.
EPERM] maps to 'EPERM'.
To translate a numeric error code to an error message, use os.strerror().
Of the following list, symbols that are not used on the current platform are not defined by the module.
The specific list of
defined symbols is available as errno.errorcode.keys().
Symbols available can include:
errno.
EPERM
Operation not permitted.
This error is mapped to the exception PermissionError.
ENOENT
No such file or directory.
This error is mapped to the exception FileNotFoundError.
No such process.
This error is mapped to the exception ProcessLookupError.
EINTR
Interrupted system call.
This error is mapped to the exception InterruptedError.
EIO
I/O error
errno.
No such device or address
errno.
E2BIG
Arg list too long
errno.
ENOEXEC
Exec format error
errno.
EBADF
Bad file number
errno.
ECHILD
No child processes.
This error is mapped to the exception ChildProcessError.
EAGAIN
Try again.
This error is mapped to the exception BlockingIOError.
ENOMEM
Out of memory
errno.
EACCES
Permission denied.
This error is mapped to the exception PermissionError.
ENOTBLK
Block device required
errno.
EBUSY
Device or resource busy
errno.
EEXIST
File exists.
This error is mapped to the exception FileExistsError.
EXDEV
Cross-device link
errno.
No such device
errno.
ENOTDIR
Not a directory.
This error is mapped to the exception NotADirectoryError.
EISDIR
Is a directory.
This error is mapped to the exception IsADirectoryError.
EINVAL
Invalid argument
errno.
ENFILE
File table overflow
errno.
EMFILE
Too many open files
errno.
ENOTTY
Not a typewriter
errno.
ETXTBSY
Text file busy
errno.
EFBIG
File too large
errno.
No space left on device
errno.
ESPIPE
Illegal seek
errno.
EROFS
Read-only file system
errno.
Too many links
errno.
EPIPE
Broken pipe.
This error is mapped to the exception BrokenPipeError.
EDOM
Math argument out of domain of func
16.15.
ERANGE
Math result not representable
errno.
EDEADLK
Resource deadlock would occur
errno.
ENAMETOOLONG
File name too long
errno.
No record locks available
errno.
ENOSYS
Function not implemented
errno.
ENOTEMPTY
Directory not empty
errno.
ELOOP
Too many symbolic links encountered
errno.
EWOULDBLOCK
Operation would block.
This error is mapped to the exception BlockingIOError.
No message of desired type
errno.
EIDRM
Identifier removed
errno.
ECHRNG
Channel number out of range
errno.
EL2NSYNC
Level 2 not synchronized
errno.
EL3HLT
Level 3 halted
errno.
EL3RST
Level 3 reset
errno.
ELNRNG
Link number out of range
errno.
EUNATCH
Protocol driver not attached
errno.
ENOCSI
No CSI structure available
errno.
EL2HLT
Level 2 halted
errno.
EBADR
Invalid request descriptor
errno.
EXFULL
Exchange full
errno.
ENOANO
No anode
errno.
EBADRQC
Invalid request code
errno.
EBADSLT
Invalid slot
errno.
EDEADLOCK
File locking deadlock error
errno.
EBFONT
Bad font file format
errno.
ENOSTR
Device not a stream
errno.
No data available
errno.
ETIME
Timer expired
errno.
ENOSR
Out of streams resources
errno.
ENONET
Machine is not on the network
errno.
ENOPKG
Package not installed
errno.
EREMOTE
Object is remote
errno.
ENOLINK
Link has been severed
errno.
EADV
Advertise error
errno.
ESRMNT
Srmount error
errno.
ECOMM
Communication error on send
errno.
EPROTO
Protocol error
16.15.
EMULTIHOP
Multihop attempted
errno.
EDOTDOT
RFS specific error
errno.
EBADMSG
Not a data message
errno.
EOVERFLOW
Value too large for defined data type
errno.
ENOTUNIQ
Name not unique on network
errno.
EBADFD
File descriptor in bad state
errno.
EREMCHG
Remote address changed
errno.
ELIBACC
Can not access a needed shared library
errno.
ELIBBAD
Accessing a corrupted shared library
errno.
ELIBSCN
.lib section in a.out corrupted
errno.
ELIBMAX
Attempting to link in too many shared libraries
errno.
ELIBEXEC
Cannot exec a shared library directly
errno.
EILSEQ
Illegal byte sequence
errno.
ERESTART
Interrupted system call should be restarted
errno.
ESTRPIPE
Streams pipe error
errno.
Too many users
errno.
ENOTSOCK
Socket operation on non-socket
errno.
EDESTADDRREQ
Destination address required
errno.
EPROTOTYPE
Protocol wrong type for socket
errno.
ENOPROTOOPT
Protocol not available
errno.
EPROTONOSUPPORT
Protocol not supported
errno.
ESOCKTNOSUPPORT
Socket type not supported
errno.
EOPNOTSUPP
Operation not supported on transport endpoint
errno.
ENOTSUP
Operation not supported
New in version 3.2.
errno.
EPFNOSUPPORT
Protocol family not supported
errno.
EAFNOSUPPORT
Address family not supported by protocol
errno.
EADDRINUSE
Address already in use
errno.
EADDRNOTAVAIL
Cannot assign requested address
errno.
ENETDOWN
Network is down
errno.
ENETUNREACH
Network is unreachable
errno.
ENETRESET
Network dropped connection because of reset
errno.
ECONNABORTED
Software caused connection abort.
This error is mapped to the exception ConnectionAbortedError.
ECONNRESET
Connection reset by peer.
This error is mapped to the exception ConnectionResetError.
ENOBUFS
No buffer space available
errno.
EISCONN
Transport endpoint is already connected
errno.
ENOTCONN
Transport endpoint is not connected
errno.
ESHUTDOWN
Cannot send after transport endpoint shutdown.
This error is mapped to the exception BrokenPipeError.
Too many references: cannot splice
errno.
ETIMEDOUT
Connection timed out.
This error is mapped to the exception TimeoutError.
ECONNREFUSED
Connection refused.
This error is mapped to the exception ConnectionRefusedError.
EHOSTDOWN
Host is down
errno.
No route to host
errno.
EALREADY
Operation already in progress.
This error is mapped to the exception BlockingIOError.
EINPROGRESS
Operation now in progress.
This error is mapped to the exception BlockingIOError.
ESTALE
Stale NFS file handle
errno.
EUCLEAN
Structure needs cleaning
errno.
ENOTNAM
Not a XENIX named type file
errno.
No XENIX semaphores available
errno.
EISNAM
Is a named type file
errno.
EREMOTEIO
Remote I/O error
errno.
EDQUOT
Quota exceeded
errno.
EQFULL
Interface output queue is full
New in version 3.11.
errno.
ENOTCAPABLE
Capabilities insufficient.
This error is mapped to the exception PermissionError.
Availability: WASI, FreeBSD
New in version 3.11.1.
errno.
EOWNERDEAD
Owner died
New in version 3.2.
errno.
ENOTRECOVERABLE
State not recoverable
New in version 3.2.
It provides C compatible data types, and allows calling functions in
DLLs or shared libraries.
It can be used to wrap these libraries in pure Python.
Since some code samples
behave differently under Linux, Windows, or macOS, they contain doctest directives in comments.
Note:
Some code samples reference the ctypes c_int type.
On platforms where sizeof(long) ==
sizeof(int) it is an alias to c_long.
So, you should not be confused if c_long is printed if you would expect
c_int — they are actually the same type.
Loading dynamic link libraries
ctypes exports the cdll, and on Windows windll and oledll objects, for loading dynamic link libraries.
You load libraries by accessing them as attributes of these objects.
The error
code is used to automatically raise an OSError exception when the function call fails.
Windows errors used to raise WindowsError, which is now an alias of OSError.
Here are some examples for Windows.
Where possible, use native Python functionality, or else import and use
the msvcrt module.
On Linux, it is required to specify the filename including the extension to load a library, so attribute access can not be
used to load libraries.
AttributeError: function 'MyOwnFunction' not found
Note that win32 system dlls like kernel32 and user32 often export ANSI as well as UNICODE versions of a function.
The UNICODE version is exported with an W appended to the name, while the ANSI version is exported with an A
appended to the name.
Sometimes, dlls export functions with names which aren’t valid Python identifiers, like "??2@YAPAXI@Z".
On Windows, some dlls export functions not by name but by ordinal.
AttributeError: function ordinal 0 not found
Calling functions
You can call these functions like any other Python callable.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: Procedure probably called with too many arguments (4 bytes in excess)
To find out the correct calling convention you have to look into the C header file or the documentation for the function
you want to call.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
OSError: exception: access violation reading 0x00000020
There are, however, enough ways to crash Python with ctypes, so you should be careful anyway.
The faulthandler
module can be helpful in debugging crashes (e.g. from segmentation faults produced by erroneous C library calls).
None, integers, bytes objects and (unicode) strings are the only native Python objects that can directly be used as pa-
rameters in these function calls.
None is passed as a C NULL pointer, bytes objects and strings are passed as pointer to
16.16.
Python integers are passed as the platforms default
C int type, their value is masked to fit into the C type.
Before we move on calling functions with other parameter types, we have to learn more about ctypes data types.
Fundamental data types
ctypes defines a number of primitive C compatible data types:
ctypes type
C type
Python type
c_bool
_Bool
bool (1)
c_char
char
1-character bytes object
c_wchar
wchar_t
1-character string
c_byte
char
int
c_ubyte
unsigned char
int
c_short
short
int
c_ushort
unsigned short
int
c_int
int
int
c_uint
unsigned int
int
c_long
long
int
c_ulong
unsigned long
int
c_longlong
__int64 or long long
int
c_ulonglong
unsigned __int64 or unsigned long long
int
c_size_t
size_t
int
c_ssize_t
ssize_t or Py_ssize_t
int
c_time_t
time_t
int
c_float
float
float
c_double
double
float
c_longdouble
long double
float
c_char_p
char* (NUL terminated)
bytes object or None
c_wchar_p
wchar_t* (NUL terminated)
string or None
c_void_p
void*
int or None
(1) The constructor accepts any object with a truth value.
Hi, there
# first object is unchanged
Hello, World
You should be careful, however, not to pass them to functions expecting pointers to mutable memory.
If you need mutable
memory blocks, ctypes has a create_string_buffer() function which creates these in various ways.
The create_string_buffer() function replaces the old c_buffer() function (which is still available as
an alias).
To create a mutable memory block containing unicode characters of the C type wchar_t, use the
create_unicode_buffer() function.
16.16.
Hello, World!

Hello, World!
On some platforms, and in particular ARM64 for Apple Platforms, the calling convention for variadic
functions is different than that for regular functions.
Calling functions with your own custom data types
You can also customize ctypes argument conversion to allow instances of your own classes be used as function argu-
ments.
It is possible to specify the required argument types of functions exported from DLLs by setting the argtypes attribute.
File "<stdin>", line 1, in <module>
ArgumentError: argument 2: TypeError: wrong type
X 2 3.000000

If you have defined your own classes which you pass to function calls, you have to implement a from_param() class
method for them to be able to use them in the argtypes sequence.
The from_param() class method receives the
Python object passed to the function call, it should do a typecheck or whatever is needed to make sure this object is
acceptable, and then return the object itself, its _as_parameter_ attribute, or whatever you want to pass as the C
function argument in this case.
Again, the result should be an integer, string, bytes, a ctypes instance, or an object with
an _as_parameter_ attribute.
Return types
By default functions are assumed to return the C int type.
Other return types can be specified by setting the restype
attribute of the function object.
The C prototype of time() is time_t time(time_t *).
Because time_t might be of a different type than the
default return type int, you should specify the restype attribute:
The argument types can be specified using argtypes:
To call the function with a NULL pointer as first argument, use None:
16.16.
ArgumentError: argument 2: TypeError: one character bytes, bytearray or␣
�→integer expected
None
b'def'
You can also use a callable Python object (a function or a class for example) as the restype attribute, if the foreign
function returns an integer.
The callable will be called with the integer the C function returns, and the result of this call
will be used as the result of your function call.
WinError is a function which will call Windows FormatMessage() api to get the string representation of an
error code, and returns an exception.
Please note that a much more powerful error checking mechanism is available through the errcheck attribute; see the
reference manual for details.
Sometimes a C api function expects a pointer to a data type as parameter, probably to write into the corresponding location,
or if the data is too large to be passed by value.
This is also known as passing parameters by reference.
The same effect can be achieved
with the pointer() function, although pointer() does a lot more work since it constructs a real pointer object, so
it is faster to use byref() if you don’t need the pointer object in Python itself:
0 0.0 b''

byref(i), byref(f), s)

1 3.1400001049 b'Hello'
Structures and unions
Structures and unions must derive from the Structure and Union base classes which are defined in the ctypes
module.
Each subclass must define a _fields_ attribute.
The field type must be a ctypes type like c_int, or any other derived ctypes type: structure, union, array, pointer.
File "<stdin>", line 1, in <module>
TypeError: too many initializers
You can, however, build much more complicated structures.
A structure can itself contain other structures by using a
structure as a field type.
16.16.
While this
may work on 32-bit x86, it’s not guaranteed by the library to work in the general case.
Unions and structures with
bit-fields should always be passed to functions by pointer.
Structure/union alignment and byte order
By default, Structure and Union fields are aligned in the same way the C compiler does it.
It is possible to override this
behavior by specifying a _pack_ class attribute in the subclass definition.
This must be set to a positive integer and
specifies the maximum alignment for the fields.
This is what #pragma pack(n) also does in MSVC.
To build structures with non-native byte or-
der, you can use one of the BigEndianStructure, LittleEndianStructure, BigEndianUnion, and
LittleEndianUnion base classes.
These classes cannot contain pointer fields.
Bit fields in structures and unions
It is possible to create structures and unions containing bit fields.
Arrays are sequences, containing a fixed number of instances of the same type.
The above code print a series of 0 0 lines, because the array contents is initialized to zeros.
Pointer instances have a contents attribute which returns the object to which the pointer points, the i object above:
c_long(42)
16.16.
It is also possible to use indexes different from 0, but you must know what you’re doing, just as in C: You can access or
change arbitrary memory locations.
Generally you only use this feature if you receive a pointer from a C function, and
you know that the pointer actually points to an array instead of a single item.
Behind the scenes, the pointer() function does more than simply create pointer instances, it has to create pointer types
first.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: expected c_long instead of int
<ctypes.
LP_c_long object at 0x...>
Calling the pointer type without an argument creates a NULL pointer.
NULL pointer access
Type conversions
Usually, ctypes does strict type checking.
This means, if you have POINTER(c_int) in the argtypes list of a
function or as the type of a member field in a structure definition, only instances of exactly the same type are accepted.
There are some exceptions to this rule, where ctypes accepts other objects.
For example, you can pass compatible array
instances instead of pointer types.
To set a POINTER type field to NULL, you can assign None:
Sometimes you have instances of incompatible types.
In C, you can cast one type into another type.
File "<stdin>", line 1, in <module>
TypeError: incompatible types, c_byte_Array_4 instance instead of LP_c_long instance
For these cases, the cast() function is handy.
It returns
an instance of the second argument, which references the same memory block as the first argument:
<ctypes.
So, cast() can be used to assign to the values field of Bar the structure:

Incomplete Types
Incomplete Types are structures, unions or arrays whose members are not yet specified.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 2, in cell
NameError: name 'cell' is not defined
because the new class cell is not available in the class statement itself.
In ctypes, we can define the cell class
and set the _fields_ attribute later, after the class statement:



("next", POINTER(cell))]
Let’s try it.
These are sometimes called callback func-
tions.
First, you must create a class for the callback function.
The class knows the calling convention, the return type, and the
number and types of arguments this function will receive.
The CFUNCTYPE() factory function creates types for callback functions using the cdecl calling convention.
On
Windows, the WINFUNCTYPE() factory function creates types for callback functions using the stdcall calling con-
vention.
Both of these factory functions are called with the result type as first argument, and the callback functions expected
argument types as the remaining arguments.
I will present an example here which uses the standard C library’s qsort() function, that is used to sort items with the
help of a callback function.
The callback will then be called with two pointers to items, and
it must return a negative integer if the first item is smaller than the second, a zero if they are equal, and a positive integer
otherwise.
So our callback function receives pointers to integers, and must return an integer.
First we create the type for the
callback function:
To get started, here is a simple callback that shows the values it gets passed:

print("py_cmp_func", a[0], b[0])



16.16.
Also, note that if the callback function is called in a thread created outside of Python’s control (e.g. by the foreign code
that calls the callback), ctypes creates a new dummy Python thread on every invocation.
This behavior is correct for most
purposes, but it means that values stored with threading.local will not survive across different callbacks, even
when those calls are made from the same C thread.
An example in the Python library itself is the
Py_Version, Python runtime version number encoded in a single constant integer.
An extended example which also demonstrates the use of pointers accesses the PyImport_FrozenModules pointer
exported by Python.
This pointer is initialized to point to an array of _frozen records, terminated by one whose members are
all NULL or zero.
When a frozen module is imported, it is searched in this table.
Third-party code could
play tricks with this to provide a dynamically created collection of frozen modules.
So manipulating this pointer could even prove useful.
Since table is a pointer to the array of struct_frozen records, we can iterate over it, but we just have to
make sure that our loop terminates, because pointers have no size.
Sooner or later it would probably crash with an access
violation or whatever, so it’s better to break out of the loop when we hit the NULL entry:

if item.name is None:

break

print(item.name.decode("ascii"), item.size)

_frozen_importlib 31764
_frozen_importlib_external 41499
zipimport 12345
16.16.
The fact that standard Python has a frozen module and a frozen package (indicated by the negative size member) is not
well known, it is only used for testing.
Try it out with import __hello__ for example.
Surprises
There are some edges in ctypes where you might expect something other than what actually happens.
We certainly expected the last statement to print 3 4 1 2.
What happened?
Here are the steps of the rc.a,
rc.b = rc.b, rc.a line above:
Note that temp0 and temp1 are objects still using the internal buffer of the rc object above.
So executing rc.a =
temp0 copies the buffer contents of temp0 into rc ‘s buffer.
This, in turn, changes the contents of temp1.
So, the last
assignment rc.b = temp1, doesn’t have the expected effect.
Keep in mind that retrieving sub-objects from Structure, Unions, and Arrays doesn’t copy the sub-object, instead it retrieves
a wrapper object accessing the root-object’s underlying buffer.
Another example that may behave differently from what one would expect is this:
b'abc def ghi'
False
Note: Objects instantiated from c_char_p can only have their value set to bytes or integers.
Why is it printing False?
Accessing the contents again constructs a new Python object each time!
Variable-sized data types
ctypes provides some support for variable-sized arrays and structures.
The resize() function can be used to resize the memory buffer of an existing ctypes object.
The function takes the
object as first argument, and the requested size in bytes as the second argument.
ValueError: minimum size is 8


This is nice and fine, but how would one access the additional elements contained in this array?
IndexError: invalid index
Another way to use variable-sized data types with ctypes is to use the dynamic nature of Python, and (re-)define the
data type after the required size is already known, on a case by case basis.
The purpose of the find_library() function is to locate a library in a way similar to what the compiler or runtime
loader does (on platforms with several versions of a shared library the most recent should be loaded), while the ctypes
library loaders act like when a program is run, and call the runtime loader directly.
The ctypes.util module provides a function which can help to determine the library to load.
ctypes.util.find_library(name)
Try to find a library and return a pathname.
If no library can be found,
returns None.
16.16.
On Linux, find_library() tries to run external programs (/sbin/ldconfig, gcc, objdump and ld) to find
the library file.
It returns the filename of the library file.
On Linux, the value of the environment variable LD_LIBRARY_PATH is used when searching
for libraries, if a library cannot be found by any other means.
Here are some examples:
'libm.so.6'
'libc.so.6'
'libbz2.so.1.0'
On macOS, find_library() tries several predefined naming schemes and paths to locate the library, and returns a
full pathname if successful:
'/usr/lib/libc.dylib'
'/usr/lib/libm.dylib'
'/usr/lib/libbz2.dylib'
'/System/Library/Frameworks/AGL.framework/AGL'
On Windows, find_library() searches along the system search path, and returns the full pathname, but since there
is no predefined naming scheme a call like find_library("c") will fail and return None.
If wrapping a shared library with ctypes, it may be better to determine the shared library name at development time,
and hardcode that into the wrapper module instead of using find_library() to locate the library at runtime.
Loading shared libraries
There are several ways to load shared libraries into the Python process.
One way is to instantiate one of the following
classes:
class ctypes.
CDLL(name, mode=DEFAULT_MODE, handle=None, use_errno=False, use_last_error=False,
winmode=None)
Instances of this class represent loaded shared libraries.
Functions in these libraries use the standard C calling
convention, and are assumed to return int.
On Windows creating a CDLL instance may fail even if the DLL name exists.
The specified module could not
be found”.
To resolve this error and determine which DLL is not
found, you need to find the list of dependent DLLs and determine which one is not found using Windows debugging
and tracing tools.
Changed in version 3.12: The name parameter can now be a path-like object.
None)
Windows only: Instances of this class represent loaded shared libraries, functions in these libraries use the
stdcall calling convention, and are assumed to return the windows specific HRESULT code.
HRESULT values
contain information specifying whether the function call failed or succeeded, together with additional error code.
If the return value signals a failure, an OSError is automatically raised.
WindowsError used to be raised, which is now an alias of OSError.
Changed in version 3.12: The name parameter can now be a path-like object.
WinDLL(name, mode=DEFAULT_MODE, handle=None, use_errno=False, use_last_error=False,
winmode=None)
Windows only: Instances of this class represent loaded shared libraries, functions in these libraries use the
stdcall calling convention, and are assumed to return int by default.
Changed in version 3.12: The name parameter can now be a path-like object.
The Python global interpreter lock is released before calling any function exported by these libraries, and reacquired
afterwards.
PyDLL(name, mode=DEFAULT_MODE, handle=None)
Instances of this class behave like CDLL instances, except that the Python GIL is not released during the function
call, and after the function execution the Python error flag is checked.
If the error flag is set, a Python exception is
raised.
Thus, this is only useful to call Python C api functions directly.
Changed in version 3.12: The name parameter can now be a path-like object.
All these classes can be instantiated by calling them with at least one argument, the pathname of the shared library.
If you
have an existing handle to an already loaded shared library, it can be passed as the handle named parameter, otherwise
the underlying platforms dlopen() or LoadLibrary() function is used to load the library into the process, and to
get a handle to it.
The mode parameter can be used to specify how the library is loaded.
For details, consult the dlopen(3) manpage.
On Windows, mode is ignored.
On posix systems, RTLD_NOW is always added, and is not configurable.
The use_errno parameter, when set to true, enables a ctypes mechanism that allows accessing the system errno error
number in a safe way.
The function ctypes.get_errno() returns the value of the ctypes private copy, and the function ctypes.
The use_last_error parameter, when set to true, enables the same mechanism for the Windows error code which is man-
aged by the GetLastError() and SetLastError() Windows API functions; ctypes.get_last_error()
and ctypes.set_last_error() are used to request and change the ctypes private copy of the windows error code.
The winmode parameter is used on Windows to specify how the library is loaded (since mode is ignored).
It takes any
value that is valid for the Win32 API LoadLibraryEx flags parameter.
When omitted, the default is to use the flags
that result in the most secure DLL load, which avoids issues such as DLL hijacking.
Passing the full path to the DLL is
the safest way to ensure the correct library and dependencies are loaded.
Changed in version 3.8: Added winmode parameter.
Flag to use as mode parameter.
On platforms where this flag is not available, it is defined as the integer zero.
ctypes.
Flag to use as mode parameter.
On platforms where this is not available, it is the same as RTLD_GLOBAL.
ctypes.
The default mode which is used to load shared libraries.
On OSX 10.3, this is RTLD_GLOBAL, otherwise it is the
same as RTLD_LOCAL.
Instances of these classes have no public methods.
Functions exported by the shared library can be accessed as attributes
or by index.
Please note that accessing the function through an attribute caches the result and therefore accessing it
repeatedly returns the same object each time.
On the other hand, accessing it through an index returns a new object each
time:
# On Linux
True
False
The following public attributes are available, their name starts with an underscore to not clash with exported function
names:
PyDLL._handle
The system handle used to access the library.
PyDLL._name
The name of the library passed in the constructor.
Shared libraries can also be loaded by using one of the prefabricated objects, which are instances of the
LibraryLoader class, either by calling the LoadLibrary() method, or by retrieving the library as attribute of the
loader instance.
Class which loads shared libraries.
The result is cached, so repeated attribute accesses return the same library each time.
LoadLibrary(name)
Load a shared library into the process and return it.
This method always returns a new instance of the library.
These prefabricated library loaders are available:
ctypes.cdll
Creates CDLL instances.
Note that all these functions are assumed
to return C int, which is of course not always the truth, so you have to assign the correct restype attribute to
use these functions.
Loading a library through any of these objects raises an auditing event ctypes.dlopen with string argument name,
the name used to load the library.
Accessing a function on a loaded library raises an auditing event ctypes.dlsym with arguments library (the library
object) and name (the symbol’s name as a string or integer).
In cases when only the library handle is available rather than the object, accessing a function raises an auditing event
ctypes.dlsym/handle with arguments handle (the raw library handle) and name.
Foreign functions
As explained in the previous section, foreign functions can be accessed as attributes of loaded shared libraries.
The
function objects created in this way by default accept any number of arguments, accept any ctypes data instances as
arguments, and return the default result type specified by the library loader.
They are instances of a private class:
class ctypes._FuncPtr
Base class for C callable foreign functions.
Instances of foreign functions are also C compatible data types; they represent C function pointers.
This behavior can be customized by assigning to special attributes of the foreign function object.
Use None for void, a function not
returning anything.
It is possible to assign a callable Python object that is not a ctypes type, in this case the function is assumed
to return a C int, and the callable will be called with this integer, allowing further processing or error
checking.
Using this is deprecated, for more flexible post processing or error checking use a ctypes data type
as restype and assign a callable to the errcheck attribute.
Functions using the
stdcall calling convention can only be called with the same number of arguments as the length of this
tuple; functions using the C calling convention accept additional, unspecified arguments as well.
When a foreign function is called, each actual argument is passed to the from_param() class method of the
items in the argtypes tuple, this method allows adapting the actual argument to an object that the foreign
function accepts.
For example, a c_char_p item in the argtypes tuple will convert a string passed as
argument into a bytes object using ctypes conversion rules.
New: It is now possible to put items in argtypes which are not ctypes types, but each item must have a
from_param() method which returns a value usable as argument (integer, string, ctypes instance).
This
allows defining adapters that can adapt custom objects as function parameters.
The callable will be called with three or more
arguments:
16.16.
The object that this function returns will be returned from the foreign function call, but it can also check the
result value and raise an exception if the foreign function call failed.
This exception is raised when a foreign function call cannot convert one of the passed arguments.
On Windows, when a foreign function call raises a system exception (for example, due to an access violation), it will be
captured and replaced with a suitable Python exception.
Further, an auditing event ctypes.set_exception with
argument code will be raised, allowing an audit hook to replace the exception with its own.
Some ways to invoke foreign function calls may raise an auditing event ctypes.call_function with arguments
function pointer and arguments.
Function prototypes
Foreign functions can also be created by instantiating function prototypes.
Function prototypes are similar to function
prototypes in C; they describe a function (return type, argument types, calling convention) without defining an imple-
mentation.
The factory functions must be called with the desired result type and the argument types of the function, and
can be used as decorator factories, and as such, be applied to functions through the @wrapper syntax.
See Callback
functions for examples.
ctypes.
The returned function prototype creates functions that use the standard C calling convention.
The function will
release the GIL during the call.
If use_errno is set to true, the ctypes private copy of the system errno variable
is exchanged with the real errno value before and after the call; use_last_error does the same for the Windows
error code.
ctypes.
WINFUNCTYPE(restype, *argtypes, use_errno=False, use_last_error=False)
Windows only: The returned function prototype creates functions that use the stdcall calling convention.
The
function will release the GIL during the call.
The returned function prototype creates functions that use the Python calling convention.
The function will not
release the GIL during the call.
Function prototypes created by these factory functions can be instantiated in different ways, depending on the type and
number of the parameters in the call:
prototype(address)
Returns a foreign function at the specified address which must be an integer.
The first item is the name of the exported function as string, or the ordinal of the exported function
as small integer.
The second item is the shared library instance.
COM methods use a special calling convention: They require a pointer to the COM interface as first argument, in
addition to those parameters that are specified in the argtypes tuple.
The optional paramflags parameter creates foreign function wrappers with much more functionality than the features
described above.
Each item in this tuple contains further information about a parameter, it must be a tuple containing one, two, or three
items.
The first item is an integer containing a combination of direction flags for the parameter:
1 Specifies an input parameter to the function.
The foreign function fills in a value.
The optional second item is the parameter name as string.
If this is specified, the foreign function can be called with
named parameters.
The optional third item is the default value for this parameter.
The following example demonstrates how to wrap the Windows MessageBoxW function so that it supports default
parameters and named arguments.
The C declaration from the windows header file is this:
WINUSERAPI int WINAPI
MessageBoxW(
HWND hWnd,
LPCWSTR lpText,
LPCWSTR lpCaption,
UINT uType);
Here is the wrapping with ctypes:
�→"), (1, "flags", 0)
The MessageBox foreign function can now be called in these ways:
A second example demonstrates output parameters.
The win32 GetWindowRect function retrieves the dimensions of
a specified window by copying them into RECT structure that the caller has to supply.
Here is the C declaration:
16.16.
Output parameters can be combined with the errcheck protocol to do further output processing and error checking.
If the errcheck function returns the argument tuple it receives unchanged, ctypes continues the normal processing
it does on the output parameters.
Raises an auditing event ctypes.addressof with argument obj.
Returns a light-weight pointer to obj, which must be an instance of a ctypes type.
The returned object can only be used as a foreign function call parameter.
It behaves similar to pointer(obj),
but the construction is a lot faster.
This function is similar to the cast operator in C. It returns a new instance of type which points to the same memory
block as obj. type must be a pointer type, and obj must be an object that can be interpreted as a pointer.
This function creates a mutable character buffer.
The returned object is a ctypes array of c_char.
If a bytes object is specified as first argument, the buffer is made one item larger than its length so that the last
element in the array is a NUL termination character.
An integer can be passed as second argument which allows
specifying the size of the array if the length of the bytes should not be used.
Raises an auditing event ctypes.create_string_buffer with arguments init, size.
This function creates a mutable unicode character buffer.
The returned object is a ctypes array of c_wchar.
If a string is specified as first argument, the buffer is made one item larger than the length of the string so that
the last element in the array is a NUL termination character.
An integer can be passed as second argument which
allows specifying the size of the array if the length of the string should not be used.
Raises an auditing event ctypes.create_unicode_buffer with arguments init, size.
ctypes.
Windows only: This function is a hook which allows implementing in-process COM servers with ctypes.
It is called
from the DllCanUnloadNow function that the _ctypes extension dll exports.
ctypes.
DllGetClassObject()
Windows only: This function is a hook which allows implementing in-process COM servers with ctypes.
It is called
from the DllGetClassObject function that the _ctypes extension dll exports.
If no library can be found,
returns None.
The exact functionality is system dependent.
Windows only: return the filename of the VC runtime library used by Python, and by the extension modules.
If
the name of the library cannot be determined, None is returned.
If you need to free memory, for example, allocated by an extension module with a call to the free(void *), it
is important that you use the function in the same library that allocated the memory.
ctypes.
FormatError([code])
Windows only: Returns a textual description of the error code code.
If no error code is specified, the last error
code is used by calling the Windows api function GetLastError.
GetLastError()
Windows only: Returns the last error code set by Windows in the calling thread.
This function calls the Windows
GetLastError() function directly, it does not return the ctypes-private copy of the error code.
Raises an auditing event ctypes.get_errno with no arguments.
Raises an auditing event ctypes.get_last_error with no arguments.
POINTER(type, /)
Create and return a new ctypes pointer type.
Pointer types are cached and reused internally, so calling this function
repeatedly is cheap.
Create a new pointer instance, pointing to obj.
The returned object is of the type POINTER(type(obj)).
Note: If you just want to pass a pointer to an object to a foreign function call, you should use byref(obj) which
is much faster.
This function resizes the internal memory buffer of obj, which must be an instance of a ctypes type.
It is not possible
to make the buffer smaller than the native size of the objects type, as given by sizeof(type(obj)), but it is
possible to enlarge the buffer.
Raises an auditing event ctypes.set_errno with argument errno.
Windows only: set the current value of the ctypes-private copy of the system LastError variable in the calling
thread to value and return the previous value.
Raises an auditing event ctypes.set_last_error with argument error.
Does the same as the C sizeof operator.
This function returns the C string starting at memory address address as a bytes object.
If size is specified, it is
used as size, otherwise the string is assumed to be zero-terminated.
Raises an auditing event ctypes.string_at with arguments address, size.
WinError(code=None, descr=None)
Windows only: this function is probably the worst-named thing in ctypes.
It creates an instance of OSError.
If code is not specified, GetLastError is called to determine the error code.
If descr is not specified,
FormatError() is called to get a textual description of the error.
This function returns the wide character string starting at memory address address as a string.
If size is specified, it
is used as the number of characters of the string, otherwise the string is assumed to be zero-terminated.
Raises an auditing event ctypes.wstring_at with arguments address, size.
Data types
class ctypes._CData
This non-public class is the common base class of all ctypes data types.
Among other things, all ctypes type
instances contain a memory block that hold C compatible data; the address of the memory block is returned by the
addressof() helper function.
Another instance variable is exposed as _objects; this contains other Python
objects that need to be kept alive in case the memory block contains pointers.
This method returns a ctypes instance that shares the buffer of the source object.
The source object must
support the writeable buffer interface.
The optional offset parameter specifies an offset into the source buffer
in bytes; the default is zero.
If the source buffer is not large enough a ValueError is raised.
Raises an auditing event ctypes.cdata/buffer with arguments pointer, size, offset.
This method creates a ctypes instance, copying the buffer from the source object buffer which must be read-
able.
The optional offset parameter specifies an offset into the source buffer in bytes; the default is zero.
If
the source buffer is not large enough a ValueError is raised.
This method returns a ctypes type instance using the memory specified by address which must be an integer.
This method adapts obj to a ctypes type.
It is called with the actual object used in a foreign function call when
the type is present in the foreign function’s argtypes tuple; it must return an object that can be used as a
function call parameter.
All ctypes data types have a default implementation of this classmethod that normally returns obj if that is an
instance of the type.
This method returns a ctypes type instance exported by a shared library.
Common instance variables of ctypes data types:
16.16.
This read-only variable is true when the ctypes data instance has allocated the memory block itself, false
otherwise.
This object is only exposed for debugging; never modify the contents
of this dictionary.
Fundamental data types
class ctypes._SimpleCData
This non-public class is the base class of all fundamental ctypes data types.
It is mentioned here because it contains
the common attributes of the fundamental ctypes data types.
Instances have a single attribute:
value
This attribute contains the actual value of the instance.
For integer and pointer types, it is an integer, for
character types, it is a single character bytes object or string, for character pointer types it is a Python bytes
object or string.
When the value attribute is retrieved from a ctypes instance, usually a new object is returned each time.
The same is true for
all other ctypes object instances.
Fundamental data types, when returned as foreign function call results, or, for example, by retrieving structure field
members or array items, are transparently converted to native Python types.
In other words, if a foreign function has a
restype of c_char_p, you will always receive a Python bytes object, not a c_char_p instance.
Subclasses of fundamental data types do not inherit this behavior.
So, if a foreign functions restype is a subclass of
c_void_p, you will receive an instance of this subclass from the function call.
Of course, you can get the value of the
pointer by accessing the value attribute.
These are the fundamental ctypes data types:
class ctypes.c_byte
Represents the C signed char datatype, and interprets the value as small integer.
The constructor accepts an
optional integer initializer; no overflow checking is done.
The constructor accepts an optional
string initializer, the length of the string must be exactly one character.
For a general character pointer that
may also point to binary data, POINTER(c_char) must be used.
The constructor accepts an integer address, or
a bytes object.
The constructor accepts an optional float initializer.
On platforms
where sizeof(long double) == sizeof(double) it is an alias to c_double.
The constructor accepts an optional float initializer.
The constructor accepts an optional integer initializer; no overflow
checking is done.
On platforms where sizeof(int) == sizeof(long) it is an alias to c_long.
Usually an alias for c_byte.
Usually an alias for c_short.
Usually an alias for c_int.
Usually an alias for c_longlong.
The constructor accepts an optional integer initializer; no overflow
checking is done.
The constructor accepts an optional integer initializer; no
overflow checking is done.
The constructor accepts an optional integer initializer; no overflow
checking is done.
New in version 3.2.
New in version 3.12.
The constructor accepts an
optional integer initializer; no overflow checking is done.
The constructor accepts an optional integer initializer; no overflow
checking is done.
On platforms where sizeof(int) == sizeof(long) it is an alias for c_ulong.
Usually an alias for c_ubyte.
Usually an alias for c_ushort.
Usually an alias for c_uint.
Usually an alias for c_ulonglong.
The constructor accepts an optional integer initializer; no overflow
checking is done.
The constructor accepts an optional integer initializer; no
overflow checking is done.
The constructor accepts an optional integer initializer; no overflow
checking is done.
The value is represented as integer.
The constructor accepts an optional integer
initializer.
The constructor
accepts an optional string initializer, the length of the string must be exactly one character.
The
constructor accepts an integer address, or a string.
Its value can be True or False, and the
constructor accepts any object that has a truth value.
HRESULT
Windows only: Represents a HRESULT value, which contains success or error information for a function or method
call.
Calling this without an argument creates a NULL PyObject* pointer.
The ctypes.wintypes module provides quite some other Windows specific data types, for example HWND, WPARAM,
or DWORD.
Some useful structures like MSG or RECT are also defined.
Structured data types
class ctypes.
Union(*args, **kw)
Abstract base class for unions in native byte order.
BigEndianUnion(*args, **kw)
Abstract base class for unions in big endian byte order.
New in version 3.11.
LittleEndianUnion(*args, **kw)
Abstract base class for unions in little endian byte order.
New in version 3.11.
class ctypes.
BigEndianStructure(*args, **kw)
Abstract base class for structures in big endian byte order.
LittleEndianStructure(*args, **kw)
Abstract base class for structures in little endian byte order.
Structures and unions with non-native byte order cannot contain pointer type fields, or any other data types containing
pointer type fields.
Structure(*args, **kw)
Abstract base class for structures in native byte order.
A sequence defining the structure fields.
The items must be 2-tuples or 3-tuples.
The first item is the name
of the field, the second item specifies the type of the field; it can be any ctypes data type.
For integer type fields like c_int, a third optional item can be given.
It must be a small positive integer
defining the bit width of the field.
Field names must be unique within one structure or union.
This is not checked, only one field can be accessed
when names are repeated.
An optional small integer that allows overriding the alignment of structure fields in the instance.
Setting this attribute
to 0 is the same as not setting it at all.
An optional sequence that lists the names of unnamed (anonymous) fields.
The fields listed in this variable must be structure or union type fields.
Here is an example type (Windows):
16.16.
The TYPEDESC structure describes a COM data type, the vt field specifies which one of the union fields is
valid.
Since the u field is defined as anonymous field, it is now possible to access the members directly off the
TYPEDESC instance.
If the subclass definition
has a separate _fields_ variable, the fields specified in this are appended to the fields of the base class.
Structure and union constructors accept both positional and keyword arguments.
Positional arguments are used to
initialize member fields in the same order as they are appear in _fields_.
Arrays and pointers
class ctypes.
Abstract base class for arrays.
The recommended way to create concrete array types is by multiplying any ctypes data type with a non-negative
integer.
Alternatively, you can subclass this type and define _length_ and _type_ class variables.
Array
elements can be read and written using standard subscript and slice accesses; for slice reads, the resulting object is
not itself an Array.
A positive integer specifying the number of elements in the array.
Out-of-range subscripts result in an
IndexError.
Will be returned by len().
Array subclass constructors accept positional arguments, used to initialize the elements in order.
Concrete pointer types are created by calling POINTER() with the type that will be pointed to; this is done
automatically by pointer().
If a pointer points to an array, its elements can be read and written using standard subscript and slice accesses.
Pointer objects have no size, so len() will raise TypeError.
Negative subscripts will read from the memory
before the pointer (as in C), and out-of-range subscripts will probably crash with an access violation (if you’re
lucky).
Assigning to this attribute changes the pointer to point to the
assigned object.
The modules described in this chapter provide support for concurrent execution of code.
The appropriate choice of
tool will depend on the task to be executed (CPU bound vs IO bound) and preferred style of development (event driven
cooperative multitasking vs preemptive multitasking).
Here’s an overview:
17.1 threading — Thread-based parallelism
Source code: Lib/threading.py
This module constructs higher-level threading interfaces on top of the lower level _thread module.
This module used to be optional, it is now always available.
See also:
concurrent.futures.
ThreadPoolExecutor offers a higher level interface to push tasks to a background
thread without blocking execution of the calling thread, while still being able to retrieve their results when needed.
Note: In the Python 2.x series, this module contained camelCase names for some methods and functions.
These are
deprecated as of Python 3.10, but they are still supported for compatibility with Python 2.5 and lower.
In CPython, due to the Global Interpreter Lock, only one thread can execute
Python code at once (even though certain performance-oriented libraries might overcome this limitation).
If you want
your application to make better use of the computational resources of multi-core machines, you are advised to use
multiprocessing or concurrent.futures.
ProcessPoolExecutor.
However, threading is still an ap-
propriate model if you want to run multiple I/O-bound tasks simultaneously.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The returned count is equal to the length of the list returned
by enumerate().
The function activeCount is a deprecated alias for this function.
If the caller’s thread of control
was not created through the threading module, a dummy thread object with limited functionality is returned.
The function currentThread is a deprecated alias for this function.
Handle uncaught exception raised by Thread.run().
The args argument has the following attributes:
• exc_type: Exception type.
If exc_type is SystemExit, the exception is silently ignored.
Otherwise, the exception is printed out on sys.
stderr.
If this function raises an exception, sys.excepthook() is called to handle it.
Storing exc_value using a custom hook can create a reference cycle.
It should be cleared explicitly to break the
reference cycle when the exception is no longer needed.
Storing thread using a custom hook can resurrect it if it is set to an object which is being finalized.
Avoid storing
thread after the custom hook completes to avoid resurrecting objects.
See also:
sys.excepthook() handles uncaught exceptions.
New in version 3.8.
It is saved so that the original value can be restored
in case they happen to get replaced with broken or alternative objects.
New in version 3.10.
threading.get_ident()
Return the ‘thread identifier’ of the current thread.
This is a nonzero integer.
Its value has no direct meaning; it is
intended as a magic cookie to be used e.g. to index a dictionary of thread-specific data.
Thread identifiers may be
recycled when a thread exits and another thread is created.
New in version 3.3.
This is a non-negative integer.
Its
value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which
the value may be recycled by the OS).
Availability: Windows, FreeBSD, Linux, macOS, OpenBSD, NetBSD, AIX, DragonFlyBSD.
The list includes daemonic threads and dummy thread objects
created by current_thread().
It excludes terminated threads and threads that have not yet been started.
However, the main thread is always part of the result, even when terminated.
In normal conditions, the main thread is the thread from which the Python
interpreter was started.
New in version 3.4.
threading.settrace(func)
Set a trace function for all threads started from the threading module.
The func will be passed to sys.
The func will be passed to sys.settrace() for each thread, before its run() method is called.
New in version 3.12.
New in version 3.10.
threading.setprofile(func)
Set a profile function for all threads started from the threading module.
The func will be passed to sys.
The func will be passed to sys.setprofile() for each thread, before its run() method is called.
New in version 3.12.
New in version 3.10.
The optional size argument specifies the stack size to
be used for subsequently created threads, and must be 0 (use platform or configured default) or a positive integer
value of at least 32,768 (32 KiB).
If size is not specified, 0 is used.
If changing the thread stack size is unsupported,
a RuntimeError is raised.
If the specified stack size is invalid, a ValueError is raised and the stack size
is unmodified.
Note that some platforms may have particular restrictions on values for the stack size, such
as requiring a minimum stack size > 32 KiB or requiring allocation in multiples of the system memory page size -
platform documentation should be referred to for more information (4 KiB pages are common; using multiples of
4096 for the stack size is the suggested approach in the absence of more specific information).
Availability: Windows, pthreads.
Unix platforms with POSIX threads support.
This module also defines the following constant:
threading.
TIMEOUT_MAX
The maximum value allowed for the timeout parameter of blocking functions (Lock.acquire(), RLock.
acquire(), Condition.wait(), etc.).
Specifying a timeout greater than this value will raise an
OverflowError.
New in version 3.2.
This module defines a number of classes, which are detailed in the sections below.
The design of this module is loosely based on Java’s threading model.
However, where Java makes locks and condition
variables basic behavior of every object, they are separate objects in Python.
Python’s Thread class supports a subset of
the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed,
stopped, suspended, resumed, or interrupted.
The static methods of Java’s Thread class, when implemented, are mapped
to module-level functions.
All of the methods described below are executed atomically.
For more details and extensive examples, see the documentation string of the _threading_local module:
Lib/_threading_local.py.
There are two ways to specify the
activity: by passing a callable object to the constructor, or by overriding the run() method in a subclass.
No other
methods (except for the constructor) should be overridden in a subclass.
In other words, only override the __init__()
and run() methods of this class.
Once a thread object is created, its activity must be started by calling the thread’s start() method.
This invokes the
run() method in a separate thread of control.
Once the thread’s activity is started, the thread is considered ‘alive’.
It stops being alive when its run() method terminates
– either normally, or by raising an unhandled exception.
The is_alive() method tests whether the thread is alive.
Other threads can call a thread’s join() method.
This blocks the calling thread until the thread whose join() method
is called is terminated.
A thread has a name.
The name can be passed to the constructor, and read or changed through the name attribute.
If the run() method raises an exception, threading.excepthook() is called to handle it.
By default,
threading.excepthook() ignores silently SystemExit.
A thread can be flagged as a “daemon thread”.
The significance of this flag is that the entire Python program exits when
only daemon threads are left.
The initial value is inherited from the creating thread.
The flag can be set through the
daemon property or the daemon constructor argument.
Their resources (such as open files, database transactions, etc.)
may not be released properly.
If you want your threads to stop gracefully, make them non-daemonic and use a suitable
signalling mechanism such as an Event.
There is a “main thread” object; this corresponds to the initial thread of control in the Python program.
It is not a daemon
thread.
There is the possibility that “dummy thread objects” are created.
These are thread objects corresponding to “alien threads”,
which are threads of control started outside the threading module, such as directly from C code.
Dummy thread objects
have limited functionality; they are always considered alive and daemonic, and cannot be joined.
They are never deleted,
since it is impossible to detect the termination of alien threads.
This constructor should always be called with keyword arguments.
Arguments are:
group should be None; reserved for future extension when a ThreadGroup class is implemented.
Defaults to None, meaning nothing is called.
name is the thread name.
By default, a unique name is constructed of the form “Thread-N” where N is a small
decimal number, or “Thread-N (target)” where “target” is target.__name__ if the target argument is specified.
Defaults to ().
Defaults to {}.
If not None, daemon explicitly sets whether the thread is daemonic.
If None (the default), the daemonic property
is inherited from the current thread.
If the subclass overrides the constructor, it must make sure to invoke the base class constructor (Thread.
__init__()) before doing anything else to the thread.
Changed in version 3.10: Use the target name if name argument is omitted.
Added the daemon argument.
It must be called at most once per thread object.
It arranges for the object’s run() method to be invoked in
a separate thread of control.
This method will raise a RuntimeError if called more than once on the same thread object.
You may override this method in a subclass.
The standard run() method invokes the callable object passed
to the object’s constructor as the target argument, if any, with positional and keyword arguments taken from
the args and kwargs arguments, respectively.
Using list or tuple as the args argument which passed to the Thread could achieve the same effect.
Example:

(continues on next page)
17.1.
This blocks the calling thread until the thread whose join() method is
called terminates – either normally or through an unhandled exception – or until the optional timeout occurs.
When the timeout argument is present and not None, it should be a floating point number specifying a time-
out for the operation in seconds (or fractions thereof).
As join() always returns None, you must call
is_alive() after join() to decide whether a timeout happened – if the thread is still alive, the join()
call timed out.
When the timeout argument is not present or None, the operation will block until the thread terminates.
A thread can be joined many times.
It is also an error to join() a thread before it has been started and attempts to do so raise the
same exception.
It has no semantics.
Multiple threads may be given the same
name.
The initial name is set by the constructor.
This is a nonzero integer.
See the get_ident() function.
Thread identifiers may be recycled when a thread exits and another thread
is created.
The identifier is available even after the thread has exited.
This is a non-negative integer, or None if
the thread has not been started.
See the get_native_id() function.
This value may be used to uniquely
identify this particular thread system-wide (until the thread terminates, after which the value may be recycled
by the OS).
Note: Similar to Process IDs, Thread IDs are only valid (guaranteed unique system-wide) from the time the
thread is created until the thread has been terminated.
Availability: Windows, FreeBSD, Linux, macOS, OpenBSD, NetBSD, AIX, DragonFlyBSD.
New in version 3.8.
This method returns True just before the run() method starts until just after the run() method termi-
nates.
This must be
set before start() is called, otherwise RuntimeError is raised.
Its initial value is inherited from the
creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread
default to daemon = False.
The entire Python program exits when no alive non-daemon threads are left.
Deprecated since version 3.10.
In Python, it is
currently the lowest level synchronization primitive available, implemented directly by the _thread extension module.
A primitive lock is in one of two states, “locked” or “unlocked”.
It is created in the unlocked state.
It has two basic
methods, acquire() and release().
When the state is unlocked, acquire() changes the state to locked and
returns immediately.
When the state is locked, acquire() blocks until a call to release() in another thread changes
it to unlocked, then the acquire() call resets it to locked and returns.
The release() method should only be called
in the locked state; it changes the state to unlocked and returns immediately.
If an attempt is made to release an unlocked
lock, a RuntimeError will be raised.
Locks also support the context management protocol.
When more than one thread is blocked in acquire() waiting for the state to turn to unlocked, only one thread proceeds
when a release() call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may
vary across implementations.
All methods are executed atomically.
Lock
The class implementing primitive lock objects.
Once a thread has acquired a lock, subsequent attempts to acquire
it block, until it is released; any thread may release it.
Note that Lock is actually a factory function which returns an instance of the most efficient version of the concrete
Lock class that is supported by the platform.
When invoked with the blocking argument set to True (the default), block until the lock is unlocked, then
set it to locked and return True.
When invoked with the blocking argument set to False, do not block.
If a call with blocking set to True
would block, return False immediately; otherwise, set the lock to locked and return True.
When invoked with the floating-point timeout argument set to a positive value, block for at most the number of
seconds specified by timeout and as long as the lock cannot be acquired.
A timeout argument of -1 specifies
an unbounded wait.
It is forbidden to specify a timeout when blocking is False.
The return value is True if the lock is acquired successfully, False if not (for example if the timeout
expired).
The timeout parameter is new.
This can be called from any thread, not only the thread which has acquired the lock.
When the lock is locked, reset it to unlocked, and return.
If any other threads are blocked waiting for the lock
to become unlocked, allow exactly one of them to proceed.
When invoked on an unlocked lock, a RuntimeError is raised.
There is no return value.
Internally, it uses
the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks.
In
the locked state, some thread owns the lock; in the unlocked state, no thread owns it.
To lock the lock, a thread calls its acquire() method; this returns once the thread owns the lock.
To unlock the lock, a
thread calls its release() method.
Reentrant locks also support the context management protocol.
RLock
This class implements reentrant lock objects.
A reentrant lock must be released by the thread that acquired it.
Once a thread has acquired a reentrant lock, the same thread may acquire it again without blocking; the thread
must release it once for each time it has acquired it.
Note that RLock is actually a factory function which returns an instance of the most efficient version of the concrete
RLock class that is supported by the platform.
When invoked without arguments: if this thread already owns the lock, increment the recursion level by one,
and return immediately.
Otherwise, if another thread owns the lock, block until the lock is unlocked.
Once
the lock is unlocked (not owned by any thread), then grab ownership, set the recursion level to one, and return.
If more than one thread is blocked waiting until the lock is unlocked, only one at a time will be able to grab
ownership of the lock.
There is no return value in this case.
When invoked with the blocking argument set to True, do the same thing as when called without arguments,
and return True.
When invoked with the blocking argument set to False, do not block.
If a call without an argument would
block, return False immediately; otherwise, do the same thing as when called without arguments, and return
True.
When invoked with the floating-point timeout argument set to a positive value, block for at most the number
of seconds specified by timeout and as long as the lock cannot be acquired.
Return True if the lock has been
acquired, False if the timeout has elapsed.
The timeout parameter is new.
If after the decrement it is zero, reset the lock to unlocked
(not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked,
allow exactly one of them to proceed.
If after the decrement the recursion level is still nonzero, the lock
remains locked and owned by the calling thread.
Only call this method when the calling thread owns the lock.
A RuntimeError is raised if this method is
called when the lock is unlocked.
There is no return value.
Passing one in is useful when several condition variables must share the same lock.
The lock is part of the condition
object: you don’t have to track it separately.
A condition variable obeys the context management protocol: using the with statement acquires the associated lock for
the duration of the enclosed block.
The acquire() and release() methods also call the corresponding methods of
the associated lock.
Other methods must be called with the associated lock held.
The wait() method releases the lock, and then blocks
until another thread awakens it by calling notify() or notify_all().
Once awakened, wait() re-acquires the
lock and returns.
It is also possible to specify a timeout.
The notify() method wakes up one of the threads waiting for the condition variable, if any are waiting.
The
notify_all() method wakes up all threads waiting for the condition variable.
Note: the notify() and notify_all() methods don’t release the lock; this means that the thread or threads
awakened will not return from their wait() call immediately, but only when the thread that called notify() or
notify_all() finally relinquishes ownership of the lock.
The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads
that are interested in a particular change of state call wait() repeatedly until they see the desired state, while threads
that modify the state call notify() or notify_all() when they change the state in such a way that it could possibly
be a desired state for one of the waiters.
The while loop checking for the application’s condition is necessary because wait() can return after an arbitrary long
time, and the condition which prompted the notify() call may no longer hold true.
This is inherent to multi-threaded
programming.
E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to
wake up one consumer thread.
This class implements condition variable objects.
A condition variable allows one or more threads to wait until they
are notified by another thread.
If the lock argument is given and not None, it must be a Lock or RLock object, and it is used as the underlying
lock.
Otherwise, a new RLock object is created and used as the underlying lock.
Changed in version 3.3: changed from a factory function to a class.
This method calls the corresponding method on the underlying lock; the return
value is whatever that method returns.
If the calling thread has not acquired the lock when this method
is called, a RuntimeError is raised.
This method releases the underlying lock, and then blocks until it is awakened by a notify() or
notify_all() call for the same condition variable in another thread, or until the optional timeout oc-
curs.
Once awakened or timed out, it re-acquires the lock and returns.
When the timeout argument is present and not None, it should be a floating point number specifying a timeout
for the operation in seconds (or fractions thereof).
When the underlying lock is an RLock, it is not released using its release() method, since this may not
actually unlock the lock when it was acquired multiple times recursively.
Instead, an internal interface of
the RLock class is used, which really unlocks it even when it has been recursively acquired several times.
Another internal interface is then used to restore the recursion level when the lock is reacquired.
The return value is True unless a given timeout expired, in which case it is False.
Changed in version 3.2: Previously, the method always returned None.
wait_for(predicate, timeout=None)
Wait until a condition evaluates to true.
A timeout may be provided giving the maximum time to wait.
This utility method may call wait() repeatedly until the predicate is satisfied, or until a timeout occurs.
The return value is the last return value of the predicate and will evaluate to False if the method timed out.
Therefore, the same rules apply as with wait(): The lock must be held when called and is re-acquired on
return.
The predicate is evaluated with the lock held.
By default, wake up one thread waiting on this condition, if any.
If the calling thread has not acquired the
lock when this method is called, a RuntimeError is raised.
This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads
are waiting.
The current implementation wakes up exactly n threads, if at least n threads are waiting.
However, it’s not safe
to rely on this behavior.
A future, optimized implementation may occasionally wake up more than n threads.
Note: an awakened thread does not actually return from its wait() call until it can reacquire the lock.
Since
notify() does not release the lock, its caller should.
This method acts like notify(), but wakes up all wait-
ing threads instead of one.
If the calling thread has not acquired the lock when this method is called, a
RuntimeError is raised.
The method notifyAll is a deprecated alias for this method.
This is one of the oldest synchronization primitives in the history of computer science, invented by the early Dutch
computer scientist Edsger W. Dijkstra (he used the names P() and V() instead of acquire() and release()).
A semaphore manages an internal counter which is decremented by each acquire() call and incremented by each
release() call.
The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until
some other thread calls release().
Semaphores also support the context management protocol.
This class implements semaphore objects.
A semaphore manages an atomic counter representing the number of
release() calls minus the number of acquire() calls, plus an initial value.
The acquire() method blocks
if necessary until it can return without making the counter negative.
If not given, value defaults to 1.
The optional argument gives the initial value for the internal counter; it defaults to 1.
If the value given is less than
0, ValueError is raised.
Changed in version 3.3: changed from a factory function to a class.
When invoked without arguments:
• If the internal counter is larger than zero on entry, decrement it by one and return True immediately.
Once awoken (and
the counter is greater than 0), decrement the counter by 1 and return True.
Exactly one thread will be
awoken by each call to release().
The order in which threads are awoken should not be relied on.
When invoked with blocking set to False, do not block.
If a call without an argument would block, return
False immediately; otherwise, do the same thing as when called without arguments, and return True.
When invoked with a timeout other than None, it will block for at most timeout seconds.
If acquire does not
complete successfully in that interval, return False.
Return True otherwise.
The timeout parameter is new.
Added the n parameter to release multiple waiting threads at once.
Class implementing bounded semaphore objects.
A bounded semaphore checks to make sure its current value
doesn’t exceed its initial value.
If it does, ValueError is raised.
In most situations semaphores are used to guard
resources with limited capacity.
If the semaphore is released too many times it’s a sign of a bug.
If not given, value
defaults to 1.
Changed in version 3.3: changed from a factory function to a class.
Semaphore Example
Semaphores are often used to guard resources with limited capacity, for example, a database server.
In any situation
where the size of the resource is fixed, you should use a bounded semaphore.
The use of a bounded semaphore reduces the chance that a programming error which causes the semaphore to be released
more than it’s acquired will go undetected.
This is one of the simplest mechanisms for communication between threads: one thread signals an event and other threads
wait for it.
An event object manages an internal flag that can be set to true with the set() method and reset to false with the
clear() method.
The wait() method blocks until the flag is true.
Event
Class implementing event objects.
An event manages a flag that can be set to true with the set() method and
reset to false with the clear() method.
The wait() method blocks until the flag is true.
The flag is initially
false.
Changed in version 3.3: changed from a factory function to a class.
The method isSet is a deprecated alias for this method.
All threads waiting for it to become true are awakened.
Threads that call wait()
once the flag is true will not block at all.
Subsequently, threads calling wait() will block until set() is called to set
the internal flag to true again.
If the internal flag is true on entry, return immediately.
Otherwise, block
until another thread calls set() to set the flag to true, or until the optional timeout occurs.
When the timeout argument is present and not None, it should be a floating point number specifying a timeout
for the operation in seconds (or fractions thereof).
This method returns True if and only if the internal flag has been set to true, either before the wait call or
after the wait starts, so it will always return True except if a timeout is given and the operation times out.
Changed in version 3.1: Previously, the method always returned None.
Timer is a
subclass of Thread and as such also functions as an example of creating custom threads.
Timers are started, as with threads, by calling their Timer.start method.
The timer can be stopped (before its action
has begun) by calling the cancel() method.
The interval the timer will wait before executing its action may not be
exactly the same as the interval specified by the user.
For example:

t = Timer(30.0, hello)
t.start()
# after 30 seconds, "hello, world" will be printed
class threading.
Timer(interval, function, args=None, kwargs=None)
Create a timer that will run function with arguments args and keyword arguments kwargs, after interval seconds
have passed.
If args is None (the default) then an empty list will be used.
If kwargs is None (the default) then an
empty dict will be used.
Changed in version 3.3: changed from a factory function to a class.
This will only work if the timer is still in its
waiting stage.
This class provides a simple synchronization primitive for use by a fixed number of threads that need to wait for each
other.
Each of the threads tries to pass the barrier by calling the wait() method and will block until all of the threads
have made their wait() calls.
At this point, the threads are released simultaneously.
The barrier can be reused any number of times for the same number of threads.
As an example, here is a simple way to synchronize a client and server thread:
b = Barrier(2, timeout=5)

b.wait()
while True:
connection = accept_connection()
process_server_connection(connection)

while True:
connection = make_connection()
process_client_connection(connection)
class threading.
Barrier(parties, action=None, timeout=None)
Create a barrier object for parties number of threads.
An action, when provided, is a callable to be called by one
of the threads when they are released.
When all the threads party to the barrier have called this function, they are all released simul-
taneously.
If a timeout is provided, it is used in preference to any that was supplied to the class constructor.
The return value is an integer in the range 0 to parties – 1, different for each thread.
If an action was provided to the constructor, one of the threads will have called it prior to being released.
Should this call raise an error, the barrier is put into the broken state.
If the call times out, the barrier is put into the broken state.
This method may raise a BrokenBarrierError exception if the barrier is broken or reset while a thread
is waiting.
Any threads waiting on it will receive the
BrokenBarrierError exception.
Note that using this function may require some external synchronization if there are other threads whose state
is unknown.
If a barrier is broken it may be better to just leave it and create a new one.
This causes any active or future calls to wait() to fail with the
BrokenBarrierError.
Use this for example if one of the threads needs to abort, to avoid deadlocking
the application.
It may be preferable to simply create the barrier with a sensible timeout value to automatically guard against
one of the threads going awry.
BrokenBarrierError
This exception, a subclass of RuntimeError, is raised when the Barrier object is reset or broken.
The acquire method will be called when the block is entered, and release will be called
when the block is exited.
Currently, Lock, RLock, Condition, Semaphore, and BoundedSemaphore objects may be used as with
statement context managers.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
17.2.
The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global
Interpreter Lock by using subprocesses instead of threads.
Due to this, the multiprocessing module allows the
programmer to fully leverage multiple processors on a given machine.
It runs on both POSIX and Windows.
The multiprocessing module also introduces APIs which do not have analogs in the threading module.
A prime
example of this is the Pool object which offers a convenient means of parallelizing the execution of a function across
multiple input values, distributing the input data across processes (data parallelism).
The following example demonstrates
the common practice of defining such functions in a module so that child processes can successfully import that module.
ProcessPoolExecutor offers a higher level interface to push tasks to a background
process without blocking execution of the calling process.
Compared to using the Pool interface directly, the
concurrent.futures API more readily allows the submission of work to the underlying process pool to be separated
from waiting for the results.
The Process class
In multiprocessing, processes are spawned by creating a Process object and then calling its start() method.
Process follows the API of threading.
Thread.
Contexts and start methods
Depending on the platform, multiprocessing supports three ways to start a process.
These start methods are
spawn The parent process starts a fresh Python interpreter process.
The child process will only inherit
those resources necessary to run the process object’s run() method.
In particular, unnecessary file
descriptors and handles from the parent process will not be inherited.
Starting a process using this
method is rather slow compared to using fork or forkserver.
Available on POSIX and Windows platforms.
The default on Windows and macOS.
fork The parent process uses os.fork() to fork the Python interpreter.
The child process, when it begins,
is effectively identical to the parent process.
All resources of the parent are inherited by the child
process.
Note that safely forking a multithreaded process is problematic.
Available on POSIX systems.
Currently the default on POSIX except macOS.
Note: The default start method will change away from fork in Python 3.14.
Code that requires fork
should explicitly specify that via get_context() or set_start_method().
Changed in version 3.12: If Python is able to detect that your process has multiple threads, the os.
Use a
different start method.
See the os.fork() documentation for further explanation.
When the program starts and selects the forkserver start method, a server process is spawned.
From then on, whenever a new process is needed, the parent process connects to the server and requests
that it fork a new process.
No unnecessary
resources are inherited.
Available on POSIX platforms which support passing file descriptors over Unix pipes such as Linux.
Changed in version 3.8: On macOS, the spawn start method is now the default.
The fork start method should be considered
unsafe as it can lead to crashes of the subprocess as macOS system libraries may start threads.
See bpo-33725.
Changed in version 3.4: spawn added on all POSIX platforms, and forkserver added for some POSIX platforms.
Child
processes no longer inherit all of the parents inheritable handles on Windows.
On POSIX using the spawn or forkserver start methods will also start a resource tracker process which tracks the unlinked
named system resources (such as named semaphores or SharedMemory objects) created by processes of the program.
When all processes have exited the resource tracker unlinks any remaining tracked object.
Usually there should be none,
17.2.
Queue()
p = mp.Process(target=foo, args=(q,))
p.start()
print(q.get())
p.join()
set_start_method() should not be used more than once in the program.
Alternatively, you can use get_context() to obtain a context object.
Context objects have the same API as the
multiprocessing module, and allow one to use multiple start methods in the same program.
Note that objects related to one context may not be compatible with processes for a different context.
In particular, locks
created using the fork context cannot be passed to processes started using the spawn or forkserver start methods.
A library which wants to use a particular start method should probably use get_context() to avoid interfering with
the choice of the library user.
Warning: The 'spawn' and 'forkserver' start methods generally cannot be used with “frozen” executables
(i.e., binaries produced by packages like PyInstaller and cx_Freeze) on POSIX systems.
Queue.
Pipes
The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex
(two-way).
The two connection objects returned by Pipe() represent the two ends of the pipe.
Each connection object
has send() and recv() methods (among others).
Note that data in a pipe may become corrupted if two
processes (or threads) try to read from or write to the same end of the pipe at the same time.
Of course there
is no risk of corruption from processes using different ends of the pipe at the same time.
Synchronization between processes
multiprocessing contains equivalents of all the synchronization primitives from threading.
Sharing state between processes
As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible.
This is particularly true when using multiple processes.
However, if you really do need to use some shared data then multiprocessing provides a couple of ways of doing
so.
Shared memory
Data can be stored in a shared memory map using Value or Array.
These shared
objects will be process and thread-safe.
For more flexibility in using shared memory one can use the multiprocessing.sharedctypes
module which supports the creation of arbitrary ctypes objects allocated from shared memory.
Server process
A manager object returned by Manager() controls a server process which holds Python objects and allows
other processes to manipulate them using proxies.
A manager returned by Manager() will support types list, dict, Namespace, Lock, RLock,
Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value and Array.
Also, a single manager can be shared by processes on different computers
over a network.
They are, however, slower than using shared memory.
Using a pool of workers
The Pool class represents a pool of worker processes.
It has methods which allows tasks to be offloaded to the worker
processes in a few different ways.
Note that the methods of a pool should only ever be used by the process which created it.
This is
covered in Programming guidelines however it is worth pointing out here.
This means that some examples, such as the
multiprocessing.pool.
Pool examples will not work in the interactive interpreter.
The Process class has equivalents of all the
methods of threading.
Thread.
The constructor should always be called with keyword arguments.
Thread.
It defaults to None, meaning nothing is called.
If
provided, the keyword-only daemon argument sets the process daemon flag to True or False.
If None (the
default), this flag will be inherited from the creating process.
By default, no arguments are passed to target.
The args argument, which defaults to (), can be used to specify a
list or tuple of the arguments to pass to target.
If a subclass overrides the constructor, it must make sure it invokes the base class constructor (Process.
__init__()) before doing anything else to the process.
Added the daemon argument.
You may override this method in a subclass.
The standard run() method invokes the callable object passed
to the object’s constructor as the target argument, if any, with sequential and keyword arguments taken from
the args and kwargs arguments, respectively.
Using a list or tuple as the args argument passed to Process achieves the same effect.
Example:


start()
Start the process’s activity.
This must be called at most once per process object.
It arranges for the object’s run() method to be invoked
in a separate process.
If the optional argument timeout is None (the default), the method blocks until the process whose join()
method is called terminates.
If timeout is a positive number, it blocks at most timeout seconds.
Note that the
method returns None if its process terminates or if the method times out.
Check the process’s exitcode
to determine if it terminated.
A process can be joined many times.
It is an error to attempt to join a process
before it has been started.
The name is a string used for identification purposes only.
It has no semantics.
Multiple
processes may be given the same name.
The initial name is set by the constructor.
If no explicit name is provided to the constructor, a name of the
form ‘Process-N1:N2:…:Nk’ is constructed, where each Nk is the N-th child of its parent.
Roughly, a process object is alive from the moment the start() method returns until the child process
terminates.
This must be set before start() is called.
The initial value is inherited from the creating process.
When a process exits, it attempts to terminate all of its daemonic child processes.
Note that a daemonic process is not allowed to create child processes.
Otherwise a daemonic process would
leave its children orphaned if it gets terminated when its parent process exits.
Additionally, these are not
Unix daemons or services, they are normal processes that will be terminated (and not joined) if non-daemonic
processes have exited.
In addition to the threading.
Thread API, Process objects also support the following attributes and meth-
ods:
pid
Return the process ID.
Before the process is spawned, this will be None.
exitcode
The child’s exit code.
This will be None if the process has not yet terminated.
If the child’s run() method returned normally, the exit code will be 0.
If it terminated via sys.exit()
with an integer argument N, the exit code will be N.
If the child terminated due to an exception not caught within run(), the exit code will be 1.
The process’s authentication key (a byte string).
When multiprocessing is initialized the main process is assigned a random string using os.
When a Process object is created, it will inherit the authentication key of its parent process, although this
may be changed by setting authkey to another byte string.
See Authentication keys.
You can use this value if you want to wait on several events at once using multiprocessing.
Otherwise calling join() is simpler.
On
Windows,
this
is
an
OS
handle
usable
with
the
WaitForSingleObject
and
WaitForMultipleObjects family of API calls.
On POSIX, this is a file descriptor usable with
primitives from the select module.
On POSIX this is done using the SIGTERM signal;
on Windows
TerminateProcess() is used.
Note that exit handlers and finally clauses, etc., will not be executed.
Note that descendant processes of the process will not be terminated – they will simply become orphaned.
Warning: If this method is used when the associated process is using a pipe or queue then the pipe or
queue is liable to become corrupted and may become unusable by other process.
Similarly, if the process
has acquired a lock or semaphore etc.
Same as terminate() but using the SIGKILL signal on POSIX.
New in version 3.7.
Close the Process object, releasing all resources associated with it.
ValueError is raised if the under-
lying process is still running.
Once close() returns successfully, most other methods and attributes of the
Process object will raise ValueError.
New in version 3.7.
Note that the start(), join(), is_alive(), terminate() and exitcode methods should only be
called by the process that created the process object.
False
True
exception multiprocessing.
ProcessError
The base class of all multiprocessing exceptions.
BufferTooShort
Exception raised by Connection.recv_bytes_into() when the supplied buffer object is too small for the
message read.
If e is an instance of BufferTooShort then e.args[0] will give the message as a byte string.
AuthenticationError
Raised when there is an authentication error.
TimeoutError
Raised by methods with a timeout when the timeout expires.
17.2.
For passing messages one can use Pipe() (for a connection between two processes) or a queue (which allows multiple
producers and consumers).
The Queue, SimpleQueue and JoinableQueue types are multi-producer, multi-consumer FIFO queues modelled
on the queue.
Queue class in the standard library.
They differ in that Queue lacks the task_done() and join()
methods introduced into Python 2.5’s queue.
Queue class.
If you use JoinableQueue then you must call JoinableQueue.task_done() for each task removed from the
queue or else the semaphore used to count the number of unfinished tasks may eventually overflow, raising an exception.
Note that one can also create a shared queue by using a manager object – see Managers.
Note: multiprocessing uses the usual queue.
Empty and queue.
Full exceptions to signal a timeout.
They
are not available in the multiprocessing namespace so you need to import them from queue.
Note: When an object is put on a queue, the object is pickled and a background thread later flushes the pickled data to an
underlying pipe.
This has some consequences which are a little surprising, but should not cause any practical difficulties
– if they really bother you then you can instead use a queue created with a manager.
Empty.
However, objects enqueued by the same process will always be in the expected order with respect to each other.
Warning: If a process is killed using Process.terminate() or os.kill() while it is trying to use a Queue,
then the data in the queue is likely to become corrupted.
This may cause any other process to get an exception when
it tries to use the queue later on.
Warning: As mentioned above, if a child process has put items on a queue (and it has not used JoinableQueue.
cancel_join_thread), then that process will not terminate until all buffered items have been flushed to the pipe.
This means that if you try joining that process you may get a deadlock unless you are sure that all items which have
been put on the queue have been consumed.
Similarly, if the child process is non-daemonic then the parent process
may hang on exit when it tries to join all its non-daemonic children.
Note that a queue created using a manager does not have this issue.
See Programming guidelines.
For an example of the usage of queues for interprocess communication see Examples.
multiprocessing.Pipe([duplex])
Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.
If duplex is True (the default) then the pipe is bidirectional.
If duplex is False then the pipe is unidirectional:
conn1 can only be used for receiving messages and conn2 can only be used for sending messages.
Returns a process shared queue implemented using a pipe and a few locks/semaphores.
When a process first puts
an item on the queue a feeder thread is started which transfers objects from a buffer into the pipe.
The usual queue.
Empty and queue.
Full exceptions from the standard library’s queue module are raised
to signal timeouts.
Queue implements all the methods of queue.
Queue except for task_done() and join().
Because of multithreading/multiprocessing semantics, this number
is not reliable.
Note that this may raise NotImplementedError on platforms like macOS where sem_getvalue()
is not implemented.
Because of multithreading/multiprocessing semantics,
this is not reliable.
full()
Return True if the queue is full, False otherwise.
If the optional argument block is True (the default) and timeout is None (the default),
block if necessary until a free slot is available.
If timeout is a positive number, it blocks at most timeout
seconds and raises the queue.
Full exception if no free slot was available within that time.
Otherwise
(block is False), put an item on the queue if a free slot is immediately available, else raise the queue.
Full exception (timeout is ignored in that case).
If the queue is closed, ValueError is raised instead of AssertionError.
Equivalent to put(obj, False).
If optional args block is True (the default) and timeout is None
(the default), block if necessary until an item is available.
If timeout is a positive number, it blocks at most
timeout seconds and raises the queue.
Empty exception if no item was available within that time.
Otherwise
(block is False), return an item if one is immediately available, else raise the queue.
Empty exception
(timeout is ignored in that case).
If the queue is closed, ValueError is raised instead of OSError.
Equivalent to get(False).
Queue has a few additional methods not found in queue.
Queue.
Indicate that no more data will be put on this queue by the current process.
The background thread will
quit once it has flushed all buffered data to the pipe.
This is called automatically when the queue is garbage
collected.
This can only be used after close() has been called.
It blocks until the
background thread exits, ensuring that all data in the buffer has been flushed to the pipe.
The process can call cancel_join_thread() to make join_thread() do nothing.
In particular, this prevents the background thread from being
joined automatically when the process exits – see join_thread().
A better name for this method might be allow_exit_without_flush().
It is likely to cause enqueued
data to be lost, and you almost certainly will not need to use it.
It is really only there if you need the current
process to exit immediately without waiting to flush enqueued data to the underlying pipe, and you don’t care
about lost data.
Note:
This class’s functionality requires a functioning shared semaphore implementation on the host operating
system.
Without one, the functionality in this class will be disabled, and attempts to instantiate a Queue will result
in an ImportError.
See bpo-3770 for additional information.
The same holds true for any of the specialized
queue types listed below.
Close the queue: release internal resources.
A queue must not be used anymore after it is closed.
For example, get(), put() and empty() methods
must no longer be called.
New in version 3.9.
JoinableQueue, a Queue subclass, is a queue which additionally has task_done() and join() methods.
Used by queue consumers.
For each get() used to
fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete.
If a join() is currently blocking, it will resume when all items have been processed (meaning that a
task_done() call was received for every item that had been put() into the queue).
Raises a ValueError if called more times than there were items placed in the queue.
join()
Block until all items in the queue have been gotten and processed.
The count of unfinished tasks goes up whenever an item is added to the queue.
The count goes down whenever
a consumer calls task_done() to indicate that the item was retrieved and all work on it is complete.
Calling this has the side effect of “joining” any processes which have already finished.
This number is not equivalent to the number of CPUs the current process can use.
When the number of CPUs cannot be determined a NotImplementedError is raised.
Return the Process object corresponding to the current process.
An analogue of threading.current_thread().
Add support for when a program which uses multiprocessing has been frozen to produce a Windows exe-
cutable.
If the freeze_support() line is omitted then trying to run the frozen executable will raise RuntimeError.
Calling freeze_support() has no effect when invoked on any operating system other than Windows.
In
addition, if the module is being run normally by the Python interpreter on Windows (the program has not been
frozen), then freeze_support() has no effect.
multiprocessing.get_all_start_methods()
Returns a list of the supported start methods, the first of which is the default.
The possible start methods are
'fork', 'spawn' and 'forkserver'.
Not all platforms support all methods.
See Contexts and start methods.
New in version 3.4.
17.2.
If method is None then the default context is returned.
Otherwise method should be 'fork', 'spawn',
'forkserver'.
ValueError is raised if the specified start method is not available.
See Contexts and start
methods.
New in version 3.4.
If the start method has not been fixed and allow_none is false, then the start method is fixed to the default and the
name is returned.
If the start method has not been fixed and allow_none is true then None is returned.
The return value can be 'fork', 'spawn', 'forkserver' or None.
See Contexts and start methods.
Changed in version 3.8: On macOS, the spawn start method is now the default.
The fork start method should be considered
unsafe as it can lead to crashes of the subprocess.
See bpo-33725.
New in version 3.4.
multiprocessing.set_executable(executable)
Set the path of the Python interpreter to use when starting a child process.
Embedders will probably need to do some thing like
set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))
before they can create child processes.
Changed in version 3.4: Now supported on POSIX when the 'spawn' start method is used.
Set a list of module names for the forkserver main process to attempt to import so that their already imported state
is inherited by forked processes.
Any ImportError when doing so is silently ignored.
This can be used as a
performance enhancement to avoid repeated work in every process.
For this to work, it must be called before the forkserver process has been launched (before creating a Pool or
starting a Process).
Only meaningful when using the 'forkserver' start method.
See Contexts and start methods.
New in version 3.4.
The method argument can be 'fork', 'spawn'
or 'forkserver'.
Raises RuntimeError if the start method has already been set and force is not True.
If
method is None and force is True then the start method is set to None.
If method is None and force is False
then the context is set to the default context.
See Contexts and start methods.
Timer,
or
threading.local.
Connection Objects
Connection objects allow the sending and receiving of picklable objects or strings.
They can be thought of as message
oriented connected sockets.
Connection objects are usually created using Pipe – see also Listeners and Clients.
Connection
send(obj)
Send an object to the other end of the connection which should be read using recv().
The object must be picklable.
Very large pickles (approximately 32 MiB+, though it depends on the OS) may
raise a ValueError exception.
Blocks until there is something
to receive.
Raises EOFError if there is nothing left to receive and the other end was closed.
This is called automatically when the connection is garbage collected.
If timeout is not specified then it will return immediately.
If timeout is a number then this specifies the
maximum time in seconds to block.
If timeout is None then an infinite timeout is used.
Note that multiple connection objects may be polled at once by using multiprocessing.
If offset is given then data is read from that position in buffer.
If size is given then that many bytes will be
read from buffer.
Very large buffers (approximately 32 MiB+, though it depends on the OS) may raise a
ValueError exception
recv_bytes([maxlength])
Return a complete message of byte data sent from the other end of the connection as a string.
Blocks until
there is something to receive.
Raises EOFError if there is nothing left to receive and the other end has
closed.
If maxlength is specified and the message is longer than maxlength then OSError is raised and the connection
will no longer be readable.
Changed in version 3.3: This function used to raise IOError, which is now an alias of OSError.
Read into buffer a complete message of byte data sent from the other end of the connection and return the
number of bytes in the message.
Blocks until there is something to receive.
Raises EOFError if there is
nothing left to receive and the other end was closed.
If offset is given then the message will be written into the buffer
from that position.
Offset must be a non-negative integer less than the length of buffer (in bytes).
If the buffer is too short then a BufferTooShort exception is raised and the complete message is available
as e.args[0] where e is the exception instance.
Changed in version 3.3:
Connection objects themselves can now be transferred between processes using
Connection.send() and Connection.recv().
New in version 3.3: Connection objects now support the context management protocol – see Context Manager
Types.
Warning: The Connection.recv() method automatically unpickles the data it receives, which can be a security
risk unless you can trust the process which sent the message.
Therefore, unless the connection object was produced using Pipe() you should only use the recv() and send()
methods after performing some sort of authentication.
See Authentication keys.
Warning: If a process is killed while it is trying to read or write to a pipe then the data in the pipe is likely to become
corrupted, because it may become impossible to be sure where the message boundaries lie.
See the documentation for threading module.
Note that one can also create synchronization primitives by using a manager object – see Managers.
A barrier object: a clone of threading.
Barrier.
New in version 3.3.
A bounded semaphore object: a close analog of threading.
BoundedSemaphore.
A solitary difference from its close analog exists: its acquire method’s first argument is named block, as is
consistent with Lock.acquire().
Note: On macOS, this is indistinguishable from Semaphore because sem_getvalue() is not implemented
on that platform.
A condition variable: an alias for threading.
Condition.
If lock is specified then it should be a Lock or RLock object from multiprocessing.
Changed in version 3.3: The wait_for() method was added.
Event
A clone of threading.
Event.
Lock
A non-recursive lock object: a close analog of threading.
Lock.
Once a process or thread has acquired a lock,
subsequent attempts to acquire it from any process or thread will block until it is released; any process or thread
may release it.
The concepts and behaviors of threading.
Lock as it applies to threads are replicated here in
multiprocessing.
Lock as it applies to either processes or threads, except as noted.
Note that Lock is actually a factory function which returns an instance of multiprocessing.
synchronize.
Lock initialized with a default context.
Lock supports the context manager protocol and thus may be used in with statements.
With the block argument set to True (the default), the method call will block until the lock is in an unlocked
state, then set it to locked and return True.
Note that the name of this first argument differs from that in
threading.Lock.acquire().
With the block argument set to False, the method call does not block.
If the lock is currently in a locked
state, return False; otherwise set the lock to a locked state and return True.
When invoked with a positive, floating-point value for timeout, block for at most the number of seconds
specified by timeout as long as the lock can not be acquired.
Invocations with a negative value for timeout are
equivalent to a timeout of zero.
Invocations with a timeout value of None (the default) set the timeout period
to infinite.
Note that the treatment of negative or None values for timeout differs from the implemented
behavior in threading.Lock.acquire().
The timeout argument has no practical implications if the
block argument is set to False and is thus ignored.
Returns True if the lock has been acquired or False
if the timeout period has elapsed.
17.2.
This can be called from any process or thread, not only the process or thread which originally
acquired the lock.
Behavior is the same as in threading.Lock.release() except that when invoked on an unlocked
lock, a ValueError is raised.
RLock
A recursive lock object: a close analog of threading.
RLock.
A recursive lock must be released by the process
or thread that acquired it.
Once a process or thread has acquired a recursive lock, the same process or thread may
acquire it again without blocking; that process or thread must release it once for each time it has been acquired.
Note that RLock is actually a factory function which returns an instance of multiprocessing.
synchronize.
RLock initialized with a default context.
RLock supports the context manager protocol and thus may be used in with statements.
When invoked with the block argument set to True, block until the lock is in an unlocked state (not owned by
any process or thread) unless the lock is already owned by the current process or thread.
The current process
or thread then takes ownership of the lock (if it does not already have ownership) and the recursion level
inside the lock increments by one, resulting in a return value of True.
Note that there are several differences
in this first argument’s behavior compared to the implementation of threading.RLock.acquire(),
starting with the name of the argument itself.
When invoked with the block argument set to False, do not block.
If the lock has already been acquired
(and thus is owned) by another process or thread, the current process or thread does not take ownership and
the recursion level within the lock is not changed, resulting in a return value of False.
If the lock is in an
unlocked state, the current process or thread takes ownership and the recursion level is incremented, resulting
in a return value of True.
Use and behaviors of the timeout argument are the same as in Lock.acquire().
Note that some of these
behaviors of timeout differ from the implemented behaviors in threading.RLock.acquire().
If after the decrement the recursion level is zero, reset the
lock to unlocked (not owned by any process or thread) and if any other processes or threads are blocked
waiting for the lock to become unlocked, allow exactly one of them to proceed.
If after the decrement the
recursion level is still nonzero, the lock remains locked and owned by the calling process or thread.
Only call this method when the calling process or thread owns the lock.
An AssertionError is raised
if this method is called by a process or thread other than the owner or if the lock is in an unlocked (un-
owned) state.
Note that the type of exception raised in this situation differs from the implemented behavior
in threading.RLock.release().
A semaphore object: a close analog of threading.
Semaphore.
A solitary difference from its close analog exists: its acquire method’s first argument is named block, as is
consistent with Lock.acquire().
This differs from the behaviour of threading where SIGINT will be ignored while the equivalent blocking calls are
in progress.
Note: Some of this package’s functionality requires a functioning shared semaphore implementation on the host operating
system.
Without one, the multiprocessing.synchronize module will be disabled, and attempts to import it
will result in an ImportError.
See bpo-3770 for additional information.
Shared ctypes Objects
It is possible to create shared objects using shared memory which can be inherited by child processes.
Value(typecode_or_type, *args, lock=True)
Return a ctypes object allocated from shared memory.
By default the return value is actually a synchronized
wrapper for the object.
The object itself can be accessed via the value attribute of a Value.
If lock is True (the default) then a new recursive lock object is created to synchronize access to the value.
If lock
is a Lock or RLock object then that will be used to synchronize access to the value.
If lock is False then access
to the returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.
Operations like += which involve a read and write are not atomic.
Array(typecode_or_type, size_or_initializer, *, lock=True)
Return a ctypes array allocated from shared memory.
By default the return value is actually a synchronized wrapper
for the array.
If size_or_initializer is an integer, then it determines
the length of the array, and the array will be initially zeroed.
Otherwise, size_or_initializer is a sequence which is
used to initialize the array and whose length determines the length of the array.
If lock is True (the default) then a new lock object is created to synchronize access to the value.
If lock is a Lock
or RLock object then that will be used to synchronize access to the value.
If lock is False then access to the
returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.
Note that lock is a keyword only argument.
The multiprocessing.sharedctypes module
The multiprocessing.sharedctypes module provides functions for allocating ctypes objects from shared
memory which can be inherited by child processes.
Note: Although it is possible to store a pointer in shared memory remember that this will refer to a location in the address
space of a specific process.
However, the pointer is quite likely to be invalid in the context of a second process and trying
to dereference the pointer from the second process may cause a crash.
multiprocessing.sharedctypes.
RawArray(typecode_or_type, size_or_initializer)
Return a ctypes array allocated from shared memory.
If size_or_initializer is an integer then it determines
the length of the array, and the array will be initially zeroed.
Otherwise size_or_initializer is a sequence which is
used to initialize the array and whose length determines the length of the array.
Note that setting and getting an element is potentially non-atomic – use Array() instead to make sure that access
is automatically synchronized using a lock.
multiprocessing.sharedctypes.
RawValue(typecode_or_type, *args)
Return a ctypes object allocated from shared memory.
Note that setting and getting the value is potentially non-atomic – use Value() instead to make sure that access
is automatically synchronized using a lock.
Note that an array of ctypes.c_char has value and raw attributes which allow one to use it to store and
retrieve strings – see documentation for ctypes.
multiprocessing.sharedctypes.
Array(typecode_or_type, size_or_initializer, *, lock=True)
The same as RawArray() except that depending on the value of lock a process-safe synchronization wrapper
may be returned instead of a raw ctypes array.
If lock is True (the default) then a new lock object is created to synchronize access to the value.
If lock is a Lock
or RLock object then that will be used to synchronize access to the value.
If lock is False then access to the
returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.
Note that lock is a keyword-only argument.
multiprocessing.sharedctypes.
The same as RawValue() except that depending on the value of lock a process-safe synchronization wrapper
may be returned instead of a raw ctypes object.
If lock is True (the default) then a new lock object is created to synchronize access to the value.
If lock is a Lock
or RLock object then that will be used to synchronize access to the value.
If lock is False then access to the
returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.
Note that lock is a keyword-only argument.
Return a process-safe wrapper object for a ctypes object which uses lock to synchronize access.
If lock is None
(the default) then a multiprocessing.
RLock object is created automatically.
A synchronized wrapper will have two methods in addition to those of the object it wraps: get_obj() returns
the wrapped object and get_lock() returns the lock object used for synchronization.
Note that accessing the ctypes object through the wrapper can be a lot slower than accessing the raw ctypes object.
Changed in version 3.5: Synchronized objects support the context manager protocol.
The table below compares the syntax for creating shared ctypes objects from shared memory with the normal ctypes
syntax.
The results printed are
17.2.
A manager object controls a server process which manages shared
objects.
Other processes can access the shared objects by using proxies.
Manager()
Returns a started SyncManager object which can be used for sharing objects between processes.
The returned
manager object corresponds to a spawned child process and has methods which will create shared objects and return
corresponding proxies.
Manager processes will be shutdown as soon as they are garbage collected or their parent process exits.
The manager
classes are defined in the multiprocessing.managers module:
class multiprocessing.managers.
BaseManager(address=None, authkey=None, serializer=’pickle’,
ctx=None, *, shutdown_timeout=1.0)
Create a BaseManager object.
Once created one should call start() or get_server().serve_forever() to ensure that the manager
object refers to a started manager process.
If address is None then an
arbitrary one is chosen.
If authkey is None then current_process().authkey is used.
Otherwise authkey is used and it
must be a byte string.
See the get_context() function.
If the shutdown times out, the process is terminated.
If terminating the process also times
out, the process is killed.
Changed in version 3.11: Added the shutdown_timeout parameter.
If initializer is not None then the subprocess will call
initializer(*initargs) when it starts.
The Server
object supports the serve_forever() method:
Server additionally has an address attribute.
This is only available if start() has been used to start the server
process.
This can be called multiple times.
A classmethod which can be used for registering a type or callable with the manager class.
This must be a string.
If a manager instance will be connected
to the server using the connect() method, or if the create_method argument is False then this can be
left as None.
proxytype is a subclass of BaseProxy which is used to create proxies for shared objects with this typeid.
If
None then a proxy class is created automatically.
In the case where no exposed list is specified, all “public methods” of the shared
object will be accessible.
It maps method names to typeid strings.
If a method’s name is not a key of this mapping or if
the mapping is None then the object returned by the method will be copied by value.
By default it is True.
BaseManager instances also have one read-only property:
address
The address used by the manager.
Changed in version 3.3: Manager objects support the context management protocol – see Context Manager Types.
In previous versions __enter__() did not start the manager’s server process if it was not already started.
SyncManager
A subclass of BaseManager which can be used for the synchronization of processes.
Objects of this type are
returned by multiprocessing.
Manager().
Its methods create and return Proxy Objects for a number of commonly used data types to be synchronized across
processes.
This notably includes shared lists and dictionaries.
Barrier(parties[, action[, timeout]])
Create a shared threading.
Barrier object and return a proxy for it.
New in version 3.3.
BoundedSemaphore object and return a proxy for it.
Condition([lock])
Create a shared threading.
Condition object and return a proxy for it.
If lock is supplied then it should be a proxy for a threading.
Lock or threading.
RLock object.
Changed in version 3.3: The wait_for() method was added.
Event()
Create a shared threading.
Event object and return a proxy for it.
Lock()
Create a shared threading.
Lock object and return a proxy for it.
Namespace()
Create a shared Namespace object and return a proxy for it.
Queue([maxsize])
Create a shared queue.
Queue object and return a proxy for it.
RLock()
Create a shared threading.
RLock object and return a proxy for it.
Semaphore([value])
Create a shared threading.
Shared objects are capable of being nested.
For example, a shared container object such as
a shared list can contain other shared objects which will all be managed and synchronized by the SyncManager.
Namespace
A type that can register with SyncManager.
A namespace object has no public methods, but does have writable attributes.
Its representation shows the values
of its attributes.
Maths()
print(maths.add(4, 3))
# prints 7
print(maths.mul(7, 8))
# prints 56
Using a remote manager
It is possible to run a manager server on one machine and have clients use it from other machines (assuming that the
firewalls involved allow it).
Running the following commands creates a server for a single shared queue which remote clients can access:
One client can access the server as follows:
(continues on next page)
17.2.
Proxy Objects
A proxy is an object which refers to a shared object which lives (presumably) in a different process.
The shared object is
said to be the referent of the proxy.
Multiple proxy objects may have the same referent.
A proxy object has methods which invoke corresponding methods of its referent (although not every method of the referent
will necessarily be available through the proxy).
An important feature of proxy objects is that they are picklable so they can be passed between processes.
As such, a
referent can contain Proxy Objects.
Note: The proxy types in multiprocessing do nothing to support comparisons by value.
So, for instance, we have:
False
One should just use a copy of the referent instead when making comparisons.
BaseProxy
Proxy objects are instances of subclasses of BaseProxy.
17.2.
If proxy is a proxy whose referent is obj then the expression
proxy._callmethod(methodname, args, kwds)
will evaluate the expression
getattr(obj, methodname)(*args, **kwds)
in the manager’s process.
The returned value will be a copy of the result of the call or a proxy to a new shared object – see documentation
for the method_to_typeid argument of BaseManager.register().
If an exception is raised by the call, then is re-raised by _callmethod().
If some other exception is
raised in the manager’s process then this is converted into a RemoteError exception and is raised by
_callmethod().
Note in particular that an exception will be raised if methodname has not been exposed.
IndexError: list index out of range
_getvalue()
Return a copy of the referent.
If the referent is unpicklable then this will raise an exception.
Cleanup
A proxy object uses a weakref callback so that when it gets garbage collected it deregisters itself from the manager which
owns its referent.
A shared object gets deleted from the manager process when there are no longer any proxies referring to it.
A process pool object which controls a pool of worker processes to which jobs can be submitted.
It supports
asynchronous results with timeouts and callbacks and has a parallel map implementation.
If processes is None then the number returned by os.
If initializer is not None then each worker process will call initializer(*initargs) when it starts.
The default maxtasksperchild is None, which means
worker processes will live as long as the pool.
Usually a pool is created using
the function multiprocessing.
Pool() or the Pool() method of a context object.
In both cases context
is set appropriately.
Note that the methods of the pool object should only be called by the process which created the pool.
Warning:
multiprocessing.pool objects have internal resources that need to be properly managed
(like any other resource) by using the pool as a context manager or by calling close() and terminate()
manually.
Failure to do this can lead to the process hanging on finalization.
Note that it is not correct to rely on the garbage collector to destroy the pool as CPython does not assure that
the finalizer of the pool will be called (see object.__del__() for more information).
New in version 3.2: maxtasksperchild
New in version 3.4: context
Note:
Worker processes within a Pool typically live for the complete duration of the Pool’s work queue.
A
frequent pattern found in other systems (such as Apache, mod_wsgi, etc) to free resources held by workers is to
allow a worker within a pool to complete only a set amount of work before being exiting, being cleaned up and a
new process spawned to replace the old one.
The maxtasksperchild argument to the Pool exposes this ability to
the end user.
It blocks until the result is ready.
Given this
blocks, apply_async() is better suited for performing work in parallel.
Additionally, func is only exe-
cuted in one of the workers of the pool.
A variant of the apply() method which returns a AsyncResult object.
If callback is specified then it should be a callable which accepts a single argument.
When the result becomes
ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead.
If error_callback is specified then it should be a callable which accepts a single argument.
If the target function
fails, then the error_callback is called with the exception instance.
Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.
A parallel equivalent of the map() built-in function (it supports only one iterable argument though, for
multiple iterables see starmap()).
It blocks until the result is ready.
This method chops the iterable into a number of chunks which it submits to the process pool as separate tasks.
The (approximate) size of these chunks can be specified by setting chunksize to a positive integer.
Note that it may cause high memory usage for very long iterables.
Consider using imap() or
imap_unordered() with explicit chunksize option for better efficiency.
A variant of the map() method which returns a AsyncResult object.
If callback is specified then it should be a callable which accepts a single argument.
When the result becomes
ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead.
If error_callback is specified then it should be a callable which accepts a single argument.
If the target function
fails, then the error_callback is called with the exception instance.
Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.
A lazier version of map().
The chunksize argument is the same as the one used by the map() method.
For very long iterables using a
large value for chunksize can make the job complete much faster than using the default value of 1.
Also if chunksize is 1 then the next() method of the iterator returned by the imap() method has an
optional timeout parameter: next(timeout) will raise multiprocessing.
TimeoutError if the
result cannot be returned within timeout seconds.
The same as imap() except that the ordering of the results from the returned iterator should be considered
arbitrary.
Like map() except that the elements of the iterable are expected to be iterables that are unpacked as argu-
ments.
Hence an iterable of [(1,2), (3, 4)] results in [func(1,2), func(3,4)].
New in version 3.3.
A combination of starmap() and map_async() that iterates over iterable of iterables and calls func
with the iterables unpacked.
Returns a result object.
New in version 3.3.
Once all the tasks have been completed the worker
processes will exit.
terminate()
Stops the worker processes immediately without completing outstanding work.
When the pool object is
garbage collected terminate() will be called immediately.
One must call close() or terminate() before using join().
New in version 3.3: Pool objects now support the context management protocol – see Context Manager Types.
AsyncResult
The class of the result returned by Pool.apply_async() and Pool.map_async().
get([timeout])
Return the result when it arrives.
If timeout is not None and the result does not arrive within timeout seconds
then multiprocessing.
TimeoutError is raised.
If the remote call raised an exception then that
exception will be reraised by get().
Will raise ValueError if the result is not
ready.
If the result is not ready, ValueError is raised instead of AssertionError.
TimeoutError
Listeners and Clients
Usually message passing between processes is done using queues or by using Connection objects returned by Pipe().
However, the multiprocessing.connection module allows some extra flexibility.
It basically gives a high level
message oriented API for dealing with sockets or Windows named pipes.
It also has support for digest authentication
using the hmac module, and for polling multiple connections at the same time.
If the reply matches the digest of the message using authkey as the key then a welcome message is sent to the other
end of the connection.
Otherwise AuthenticationError is raised.
If a welcome message is not received, then AuthenticationError is raised.
Client(address[, family[, authkey]])
Attempt to set up a connection to the listener which is using address address, returning a Connection.
The type of the connection is determined by family argument, but this can generally be omitted since it can usually
be inferred from the format of address.
If authkey is given and not None, it should be a byte string and will be used as the secret key for an HMAC-based
authentication challenge.
No authentication is done if authkey is None.
AuthenticationError is raised if
authentication fails.
See Authentication keys.
A wrapper for a bound socket or Windows named pipe which is ‘listening’ for connections.
Note: If an address of ‘0.0.0.0’ is used, the address will not be a connectable end point on Windows.
If you require
a connectable end-point, you should use ‘127.0.0.1’.
This can be one of the strings 'AF_INET' (for a TCP socket),
'AF_UNIX' (for a Unix domain socket) or 'AF_PIPE' (for a Windows named pipe).
Of these only the first is
guaranteed to be available.
If family is None then the family is inferred from the format of address.
If address
is also None then a default is chosen.
This default is the family which is assumed to be the fastest available.
See
Address Formats.
Note that if family is 'AF_UNIX' and address is None then the socket will be created in a
private temporary directory created using tempfile.mkstemp().
If the listener object uses a socket then backlog (1 by default) is passed to the listen() method of the socket
once it has been bound.
If authkey is given and not None, it should be a byte string and will be used as the secret key for an HMAC-based
authentication challenge.
No authentication is done if authkey is None.
AuthenticationError is raised if
authentication fails.
See Authentication keys.
accept()
Accept a connection on the bound socket or named pipe of the listener object and return a Connection
object.
If authentication is attempted and fails, then AuthenticationError is raised.
This is called automatically when the listener is
garbage collected.
However it is advisable to call it explicitly.
Listener objects have the following read-only properties:
address
The address which is being used by the Listener object.
If this is unavailable then it is None.
New in version 3.3: Listener objects now support the context management protocol – see Context Manager Types.
Returns the list of those objects in object_list which are ready.
If timeout
is a float then the call blocks for at most that many seconds.
If timeout is None then it will block for an unlimited
period.
A negative timeout is equivalent to a zero timeout.
For both POSIX and Windows, an object can appear in object_list if it is
• a readable Connection object;
• a connected and readable socket.socket object; or
• the sentinel attribute of a Process object.
A connection or socket object is ready when there is data available to be read from it, or the other end has been
closed.
POSIX: wait(object_list, timeout) almost equivalent select.select(object_list, [],
[], timeout).
The difference is that, if select.select() is interrupted by a signal, it can raise OSError
with an error number of EINTR, whereas wait() will not.
Windows: An item in object_list must either be an integer handle which is waitable (according to the definition
used by the documentation of the Win32 function WaitForMultipleObjects()) or it can be an object with
a fileno() method which returns a socket handle or pipe handle.
New in version 3.3.
Examples
The following server code creates a listener which uses 'secret password' as an authentication key.
We close the writable end of the pipe now to be sure that
# p is the only process which owns a handle for it.
This
# ensures that when p closes its handle for the writable end,
# wait() will promptly report the readable end as being ready.
Address Formats
• An 'AF_INET' address is a tuple of the form (hostname, port) where hostname is a string and port is an
integer.
To use Client() to connect to a
named pipe on a remote computer called ServerName one should use an address of the form r'\\ServerName\
pipe\PipeName' instead.
Unfortunately unpickling data from
an untrusted source is a security risk.
Therefore Listener and Client() use the hmac module to provide digest
authentication.
An authentication key is a byte string which can be thought of as a password: once a connection is established both ends
will demand proof that the other knows the authentication key.
If authentication is requested but no authentication key is specified then the return value of current_process().
This value will be automatically inherited by any Process object that the current
process creates.
This means that (by default) all processes of a multi-process program will share a single authentication
key which can be used when setting up connections between themselves.
Suitable authentication keys can also be generated by using os.urandom().
Logging
Some support for logging is available.
Note, however, that the logging package does not use process shared locks so
it is possible (depending on the handler type) for messages from different processes to get mixed up.
multiprocessing.get_logger()
Returns the logger used by multiprocessing.
If necessary, a new one will be created.
When first created the logger has level logging.
NOTSET and no default handler.
Messages sent to this logger
will not by default propagate to the root logger.
Note that on Windows child processes will only inherit the level of the parent process’s logger – any other cus-
tomization of the logger will not be inherited.
This function performs a call to get_logger() but in addition to returning the logger created by
get_logger, it adds a handler which sends output to sys.stderr using format '[%(levelname)s/
%(processName)s] %(message)s'.
You can modify levelname of the logger by passing a level
argument.
The multiprocessing.dummy module
multiprocessing.dummy replicates the API of multiprocessing but is no more than a wrapper around the
threading module.
In particular, the Pool function provided by multiprocessing.dummy returns an instance of ThreadPool,
which is a subclass of Pool that supports all the same method calls but uses a pool of worker threads rather than worker
processes.
A thread pool object which controls a pool of worker threads to which jobs can be submitted.
ThreadPool
instances are fully interface compatible with Pool instances, and their resources must also be properly managed,
either by using the pool as a context manager or by calling close() and terminate() manually.
If processes is None then the number returned by os.
If initializer is not None then each worker process will call initializer(*initargs) when it starts.
Unlike Pool, maxtasksperchild and context cannot be provided.
Note:
A ThreadPool shares the same interface as Pool, which is designed around a pool of processes and
predates the introduction of the concurrent.futures module.
As such, it inherits some operations that don’t
make sense for a pool backed by threads, and it has its own type for representing the status of asynchronous jobs,
AsyncResult, that is not understood by any other libraries.
Users should generally prefer to use concurrent.futures.
ThreadPoolExecutor, which has a simpler
interface that was designed around threads from the start, and which returns concurrent.futures.
Future
instances that are compatible with many other libraries, including asyncio.
All start methods
The following applies to all start methods.
Avoid shared state
As far as possible one should try to avoid shifting large amounts of data between processes.
It is probably best to stick to using queues or pipes for communication between processes rather than using
the lower level synchronization primitives.
Picklability
Ensure that the arguments to the methods of proxies are picklable.
Thread safety of proxies
Do not use a proxy object from more than one thread unless you protect it with a lock.
There should never be very
many because each time a new process starts (or active_children() is called) all completed processes
which have not yet been joined will be joined.
Also calling a finished process’s Process.is_alive will
join the process.
Even so it is probably good practice to explicitly join all the processes that you start.
Better to inherit than pickle/unpickle
When using the spawn or forkserver start methods many types from multiprocessing need to be pick-
lable so that child processes can use them.
However, one should generally avoid sending shared objects to
other processes using pipes or queues.
Instead you should arrange the program so that a process which needs
access to a shared resource created elsewhere can inherit it from an ancestor process.
Avoid terminating processes
Using the Process.terminate method to stop a process is liable to cause any shared resources (such as
locks, semaphores, pipes and queues) currently being used by the process to become broken or unavailable
to other processes.
Therefore it is probably best to only consider using Process.terminate on processes which never use
any shared resources.
Joining processes that use queues
Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered
items are fed by the “feeder” thread to the underlying pipe.
This means that whenever you use a queue you need to make sure that all items which have been put on the
queue will eventually be removed before the process is joined.
Otherwise you cannot be sure that processes
which have put items on the queue will terminate.
Remember also that non-daemonic processes will be
joined automatically.
A fix here would be to swap the last two lines (or simply remove the p.join() line).
Explicitly pass resources to child processes
On POSIX using the fork start method, a child process can make use of a shared resource created in a parent
process using a global resource.
However, it is better to pass the object as an argument to the constructor for
the child process.
Apart from making the code (potentially) compatible with Windows and the other start methods this also
ensures that as long as the child process is still alive the object will not be garbage collected in the parent
process.
This might be important if some resource is freed when the object is garbage collected in the parent
process.
So for instance
17.2.
Beware of replacing sys.stdin with a “file like object”
multiprocessing originally unconditionally called:
os.close(sys.stdin.fileno())
in the multiprocessing.
Process._bootstrap() method — this resulted in issues with
processes-in-processes.
Which solves the fundamental issue of processes colliding with each other resulting in a bad file descriptor
error, but introduces a potential danger to applications which replace sys.stdin() with a “file-like object”
with output buffering.
This danger is that if multiple processes call close() on this file-like object, it could
result in the same data being flushed to the object multiple times, resulting in corruption.
If you write a file-like object and implement your own caching, you can make it fork-safe by storing the pid
whenever you append to the cache, and discarding the cache when the pid changes.
For example:


if pid !
The spawn and forkserver start methods
There are a few extra restriction which don’t apply to the fork start method.
More picklability
Ensure that all arguments to Process.__init__() are picklable.
Also, if you subclass Process then
make sure that instances will be picklable when the Process.start method is called.
However, global variables which are just module level constants cause no problems.
Safe importing of main module
Make sure that the main module can be safely imported by a new Python interpreter without causing unin-
tended side effects (such as starting a new process).
This allows the newly spawned Python interpreter to safely import the module and then run the module’s
foo() function.
Similar restrictions apply if a pool or manager is created in the main module.
TimeoutError:
sys.stdout.write('.')
(continues on next page)
17.2.
This module provides a class, SharedMemory, for the allocation and management of shared memory to be accessed by
one or more processes on a multicore or symmetric multiprocessor (SMP) machine.
To assist with the life-cycle manage-
17.3.
In this module, shared memory refers to “System V style” shared memory blocks (though is not necessarily implemented
explicitly as such) and does not refer to “distributed shared memory”.
This style of shared memory permits distinct
processes to potentially read and write to a common (or shared) region of volatile memory.
Processes are conventionally
limited to only have access to their own process memory space but shared memory permits the sharing of data between
processes, avoiding the need to instead send messages between processes containing that data.
Sharing data directly via
memory can provide significant performance benefits compared to sharing data via disk or socket or other communications
requiring the serialization/deserialization and copying of data.
SharedMemory(name=None, create=False, size=0)
Creates a new shared memory block or attaches to an existing shared memory block.
Each shared memory block
is assigned a unique name.
In this way, one process can create a shared memory block with a particular name and
a different process can attach to that same shared memory block using that same name.
As a resource for sharing data across processes, shared memory blocks may outlive the original process that created
them.
When one process no longer needs access to a shared memory block that might still be needed by other
processes, the close() method should be called.
When a shared memory block is no longer needed by any
process, the unlink() method should be called to ensure proper cleanup.
When creating a new shared
memory block, if None (the default) is supplied for the name, a novel name will be generated.
Because some platforms
choose to allocate chunks of memory based upon that platform’s memory page size, the exact size of the shared
memory block may be larger or equal to the size requested.
When attaching to an existing shared memory block,
the size parameter is ignored.
Closes access to the shared memory from this instance.
In order to ensure proper cleanup of resources, all
instances should call close() once the instance is no longer needed.
Note that calling close() does not
cause the shared memory block itself to be destroyed.
In order to ensure proper cleanup of re-
sources, unlink() should be called once (and only once) across all processes which have need for the
shared memory block.
After requesting its destruction, a shared memory block may or may not be imme-
diately destroyed and this behavior may differ across platforms.
Attempts to access data inside the shared
memory block after unlink() has been called may result in memory access errors.
Note: the last process
relinquishing its hold on a shared memory block may call unlink() and close() in either order.
A subclass of BaseManager which can be used for the management of shared memory blocks across processes.
A call to start() on a SharedMemoryManager instance causes a new process to be started.
This new
process’s sole purpose is to manage the life cycle of all shared memory blocks created through it.
To trigger the
release of all shared memory blocks managed by that process, call shutdown() on the instance.
This triggers
a SharedMemory.unlink() call on all of the SharedMemory objects managed by that process and then
stops the process itself.
By creating SharedMemory instances through a SharedMemoryManager, we avoid
the need to manually track and trigger the freeing of shared memory resources.
This class provides methods for creating and returning SharedMemory instances and for creating a list-like object
(ShareableList) backed by shared memory.
Refer to multiprocessing.managers.
BaseManager for a description of the inherited address and au-
thkey optional input arguments and how they may be used to connect to an existing SharedMemoryManager
service from other processes.
SharedMemory(size)
Create and return a new SharedMemory object with the specified size in bytes.
ShareableList(sequence)
Create and return a new ShareableList object, initialized by the values from the input sequence.
ShareableList(sequence=None, \*, name=None)
Provides a mutable list-like object where all values stored within are stored in a shared memory block.
This con-
strains storable values to only the int (signed 64-bit), float, bool, str (less than 10M bytes each when
encoded as utf-8), bytes (less than 10M bytes each), and None built-in data types.
It also notably differs from
the built-in list type in that these lists can not change their overall length (i.e. no append, insert, etc.) and do
not support the dynamic creation of new ShareableList instances via slicing.
Set to None to instead attach to an already
existing ShareableList by its unique shared memory name.
When attaching to an existing ShareableList, specify its shared memory block’s unique name while leaving
sequence set to None.
Note:
A known issue exists for bytes and str values.
If they end with \x00 nul bytes or characters, those
may be silently stripped when fetching them by index from the ShareableList.
This .rstrip(b'\x00')
behavior is considered a bug and may go away in the future.
See gh-106939.
For applications where rstripping of trailing nulls is a problem, work around it by always unconditionally appending
an extra non-0 byte to the end of such values when storing and unconditionally removing it when fetching:
�→x00'])
'?'
b'\x03\x02\x01'
�→x07'])
'?\x00'
b'\x03\x02\x01\x00\x00\x00'
count(value)
Returns the number of occurrences of value.
index(value)
Returns first index position of value.
Raises ValueError if value is not present.
format
Read-only attribute containing the struct packing format used by all currently stored values.
The following example demonstrates basic use of a ShareableList instance:
17.3.
The following examples demonstrates that ShareableList (and underlying SharedMemory) objects can be pickled
and unpickled if needed.
Note, that it will still be the same shared object.
The concurrent package
Currently, there is only one module in this package:
• concurrent.futures – Launching parallel tasks
17.5 concurrent.futures — Launching parallel tasks
New in version 3.2.
Source code: Lib/concurrent/futures/thread.py and Lib/concurrent/futures/process.py
The concurrent.futures module provides a high-level interface for asynchronously executing callables.
The asynchronous execution can be performed with threads, using ThreadPoolExecutor, or separate processes,
using ProcessPoolExecutor.
Both implement the same interface, which is defined by the abstract Executor
class.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Executor
An abstract class that provides methods to execute calls asynchronously.
It should not be used directly, but through
its concrete subclasses.
Schedules the callable, fn, to be executed as fn(*args, **kwargs) and returns a Future object
representing the execution of the callable.
If timeout is
not specified or None, there is no limit to the wait time.
If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator.
When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it
submits to the pool as separate tasks.
The (approximate) size of these chunks can be specified by setting
chunksize to a positive integer.
For very long iterables, using a large value for chunksize can significantly
improve performance compared to the default size of 1.
With ThreadPoolExecutor, chunksize has no
effect.
Added the chunksize argument.
Calls to Executor.submit() and Executor.map() made after shutdown will raise
RuntimeError.
If wait is True then this method will not return until all the pending futures are done executing and the
resources associated with the executor have been freed.
If wait is False then this method will return im-
mediately and the resources associated with the executor will be freed when all pending futures are done
executing.
Regardless of the value of wait, the entire Python program will not exit until all pending futures
are done executing.
If cancel_futures is True, this method will cancel all pending futures that the executor has not started running.
Any futures that are completed or running won’t be cancelled, regardless of the value of cancel_futures.
If both cancel_futures and wait are True, all futures that the executor has started running will be completed
prior to this method returning.
The remaining futures are cancelled.
You can avoid having to call this method explicitly if you use the with statement, which will shutdown the
Executor (waiting as if Executor.shutdown() were called with wait set to True):
import shutil
with ThreadPoolExecutor(max_workers=4) as e:
e.submit(shutil.copy, 'src1.txt', 'dest1.txt')
e.submit(shutil.copy, 'src2.txt', 'dest2.txt')
e.submit(shutil.copy, 'src3.txt', 'dest3.txt')
e.submit(shutil.copy, 'src4.txt', 'dest4.txt')
Changed in version 3.9: Added cancel_futures.
Deadlocks can occur when the callable associated with a Future waits on the results of another Future.
An Executor subclass that uses a pool of at most max_workers threads to execute calls asynchronously.
All threads enqueued to ThreadPoolExecutor will be joined before the interpreter can exit.
Note that the
exit handler which does this is executed before any exit handlers added using atexit.
This means exceptions
in the main thread must be caught and handled in order to signal threads to exit gracefully.
For this reason, it is
recommended that ThreadPoolExecutor not be used for long-running tasks.
Should initializer raise an exception, all currently pending jobs will raise a
BrokenThreadPool, as well as any attempt to submit more jobs to the pool.
If max_workers is None or not given, it will default to the number of processors on the
machine, multiplied by 5, assuming that ThreadPoolExecutor is often used to overlap I/O instead of CPU
work and the number of workers should be higher than the number of workers for ProcessPoolExecutor.
The thread_name_prefix argument was added to allow users to control the threading.
Thread names for worker threads created by the pool for easier debugging.
Added the initializer and initargs arguments.
Default value of max_workers is changed to min(32, os.cpu_count() + 4).
This default value preserves at least 5 workers for I/O bound tasks.
It utilizes at most 32 CPU cores for CPU bound
tasks which release the GIL.
And it avoids using very large resources implicitly on many-core machines.
ThreadPoolExecutor now reuses idle worker threads before starting max_workers worker threads too.
ThreadPoolExecutor Example
import concurrent.futures
import urllib.request
URLS = ['http://www.foxnews.com/',
'http://www.cnn.com/',
'http://europe.wsj.com/',
'http://www.bbc.co.uk/',
'http://nonexistant-subdomain.python.org/']
# Retrieve a single page and report the URL and contents


# We can use a with statement to ensure threads are cleaned up promptly
with concurrent.futures.
ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global
Interpreter Lock but also means that only picklable objects can be executed and returned.
The __main__ module must be importable by worker subprocesses.
This means that ProcessPoolExecutor will
not work in the interactive interpreter.
Calling Executor or Future methods from a callable submitted to a ProcessPoolExecutor will result in dead-
lock.
ProcessPoolExecutor(max_workers=None, mp_context=None,
initializer=None, initargs=(),
max_tasks_per_child=None)
An Executor subclass that executes calls asynchronously using a pool of at most max_workers processes.
If
max_workers is None or not given, it will default to the number of processors on the machine.
If max_workers is
less than or equal to 0, then a ValueError will be raised.
On Windows, max_workers must be less than or equal
to 61.
If it is not then ValueError will be raised.
If max_workers is None, then the default chosen will be at
most 61, even if more processors are available.
It
will be used to launch the workers.
If mp_context is None or not given, the default multiprocessing context
is used.
See Contexts and start methods.
Should initializer raise an exception, all currently pending jobs will raise a
BrokenProcessPool, as well as any attempt to submit more jobs to the pool.
By default max_tasks_per_child is None
which means worker processes will live as long as the pool.
When a max is specified, the “spawn” multiprocessing
start method will be used by default in absence of a mp_context parameter.
This feature is incompatible with the
“fork” start method.
Changed in version 3.3: When one of the worker processes terminates abruptly, a BrokenProcessPool error
is now raised.
Previously, behaviour was undefined but operations on the executor or its futures would often freeze
or deadlock.
The mp_context argument was added to allow users to control the start_method for worker
processes created by the pool.
Added the initializer and initargs arguments.
Note:
The default multiprocessing start method (see Contexts and start methods) will change away from
fork in Python 3.14.
Code that requires fork be used for their ProcessPoolExecutor should explicitly specify
that by passing a mp_context=multiprocessing.get_context("fork") parameter.
The max_tasks_per_child argument was added to allow users to control the lifetime of
workers in the pool.
Changed in version 3.12:
On POSIX systems,
if your application has multiple threads and the
multiprocessing context uses the "fork" start method: The os.fork() function called internally to
spawn workers may raise a DeprecationWarning.
Pass a mp_context configured to use a different start
method.
See the os.fork() documentation for further explanation.
Future instances are created by Executor.
Future
Encapsulates the asynchronous execution of a callable.
Future instances are created by Executor.submit()
and should not be created directly except for testing.
Attempt to cancel the call.
If the call is currently being executed or finished running and cannot be cancelled
then the method will return False, otherwise the call will be cancelled and the method will return True.
If the call hasn’t yet completed then this method will wait up to timeout
seconds.
If the call hasn’t completed in timeout seconds, then a TimeoutError will be raised.
If timeout is not specified or None, there is no limit to the wait time.
If the future is cancelled before completing then CancelledError will be raised.
If the call raised an exception, this method will raise the same exception.
If the call hasn’t yet completed then this method will wait up to timeout
seconds.
If the call hasn’t completed in timeout seconds, then a TimeoutError will be raised.
If timeout is not specified or None, there is no limit to the wait time.
If the future is cancelled before completing then CancelledError will be raised.
Attaches the callable fn to the future.
Added callables are called in the order that they were added and are always called in a thread belonging to
the process that added them.
If the callable raises an Exception subclass, it will be logged and ignored.
If
the callable raises a BaseException subclass, the behavior is undefined.
If the future has already completed or been cancelled, fn will be called immediately.
The following Future methods are meant for use in unit tests and Executor implementations.
This method should only be called by Executor implementations before executing the work associated with
the Future and by unit tests.
If the method returns False then the Future was cancelled, i.e. Future.cancel() was called and
returned True.
Any threads waiting on the Future completing (i.e. through as_completed() or
wait()) will be woken up.
If the method returns True then the Future was not cancelled and has been put in the running state, i.e.
calls to Future.running() will return True.
This method can only be called once and cannot be called after Future.set_result() or Future.
Sets the result of the work associated with the Future to result.
This method should only be used by Executor implementations and unit tests.
This method raises concurrent.futures.
InvalidStateError if the
Future is already done.
Sets the result of the work associated with the Future to the Exception exception.
This method should only be used by Executor implementations and unit tests.
This method raises concurrent.futures.
InvalidStateError if the
Future is already done.
Duplicate futures given to fs are removed and will be returned only once.
Returns a named 2-tuple of sets.
The first
set, named done, contains the futures that completed (finished or cancelled futures) before the wait completed.
The second set, named not_done, contains the futures that did not complete (pending or running futures).
If timeout is not specified or None, there is no limit to the wait time.
It must be one of the following constants:
Constant
Description
FIRST_COMPLETED
The function will return when any future finishes or is cancelled.
FIRST_EXCEPTION
The function will return when any future finishes by raising an exception.
If no
future raises an exception then it is equivalent to ALL_COMPLETED.
ALL_COMPLETED
The function will return when all futures finish or are cancelled.
concurrent.futures.as_completed(fs, timeout=None)
Returns an iterator over the Future instances (possibly created by different Executor instances) given by fs
that yields futures as they complete (finished or cancelled futures).
Any futures given by fs that are duplicated
will be returned once.
Any futures that completed before as_completed() is called will be yielded first.
If timeout is not specified or
None, there is no limit to the wait time.
See also:
PEP 3148 – futures - execute computations asynchronously The proposal which described this feature for inclusion
in the Python standard library.
CancelledError
Raised when a future is cancelled.
TimeoutError
A deprecated alias of TimeoutError, raised when a future operation exceeds the given timeout.
This class was made an alias of TimeoutError.
BrokenExecutor
Derived from RuntimeError, this exception class is raised when an executor is broken for some reason, and
cannot be used to submit or execute new tasks.
New in version 3.7.
InvalidStateError
Raised when an operation is performed on a future that is not allowed in the current state.
New in version 3.8.
exception concurrent.futures.thread.
BrokenThreadPool
Derived from BrokenExecutor,
this exception class is raised when one of the workers of a
ThreadPoolExecutor has failed initializing.
New in version 3.7.
BrokenProcessPool
Derived from BrokenExecutor (formerly RuntimeError), this exception class is raised when one of the
workers of a ProcessPoolExecutor has terminated in a non-clean fashion (for example, if it was killed from
the outside).
New in version 3.3.
See also:
PEP 324 – PEP proposing the subprocess module
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
For
more advanced use cases, the underlying Popen interface can be used directly.
Wait for command to complete, then return a CompletedProcess
instance.
The arguments shown above are merely the most common ones, described below in Frequently Used Arguments
(hence the use of keyword-only notation in the abbreviated signature).
The full function signature is largely the
same as that of the Popen constructor - most of the arguments to this function are passed through to that interface.
If capture_output is true, stdout and stderr will be captured.
When used, the internal Popen object is automatically
created with stdout=PIPE and stderr=PIPE.
The stdout and stderr arguments may not be supplied at the
same time as capture_output.
If you wish to capture and combine both streams into one, use stdout=PIPE and
stderr=STDOUT instead of capture_output.
A timeout may be specified in seconds, it is internally passed on to Popen.communicate().
If the timeout
expires, the child process will be killed and waited for.
The TimeoutExpired exception will be re-raised after
the child process has terminated.
The initial process creation itself cannot be interrupted on many platform APIs
so you are not guaranteed to see a timeout exception until at least after however long process creation takes.
The input argument is passed to Popen.communicate() and thus to the subprocess’s stdin.
If used it must be
a byte sequence, or a string if encoding or errors is specified or text is true.
When used, the internal Popen object
is automatically created with stdin=PIPE, and the stdin argument may not be used as well.
If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be
raised.
Attributes of that exception hold the arguments, the exit code, and stdout and stderr if they were captured.
If encoding or errors are specified, or text is true, file objects for stdin, stdout and stderr are opened in text mode
using the specified encoding and errors or the io.
TextIOWrapper default.
The universal_newlines argument is
equivalent to text and is provided for backwards compatibility.
By default, file objects are opened in binary mode.
If env is not None, it must be a mapping that defines the environment variables for the new process; these are
used instead of the default behavior of inheriting the current process’ environment.
It is passed directly to Popen.
This mapping can be str to str on any platform or bytes to bytes on POSIX platforms much like os.environ or
os.environb.
Examples:
# doesn't capture output
CompletedProcess(args=['ls', '-l'], returncode=0)
(continues on next page)
17.6.
Traceback (most recent call last):

subprocess.
New in version 3.5.
Changed in version 3.6: Added encoding and errors parameters
Changed in version 3.7: Added the text parameter, as a more understandable alias of universal_newlines.
Added
the capture_output parameter.
Changed in version 3.12: Changed Windows shell search order for shell=True.
The current directory and
%PATH% are replaced with %COMSPEC% and %SystemRoot%\System32\cmd.exe.
As a result, dropping
a malicious program named cmd.exe into a current directory no longer works.
CompletedProcess
The return value from run(), representing a process that has finished.
This may be a list or a string.
Typically, an exit status of 0 indicates that it ran successfully.
A negative value -N indicates that the child was terminated by signal N (POSIX only).
A bytes sequence, or a string if run() was called with an encoding,
errors, or text=True.
None if stdout was not captured.
If you ran the process with stderr=subprocess.
STDOUT, stdout and stderr will be combined in this
attribute, and stderr will be None.
A bytes sequence, or a string if run() was called with an encoding,
errors, or text=True.
None if stderr was not captured.
If returncode is non-zero, raise a CalledProcessError.
New in version 3.5.
subprocess.
DEVNULL
Special value that can be used as the stdin, stdout or stderr argument to Popen and indicates that the special file
os.devnull will be used.
New in version 3.3.
PIPE
Special value that can be used as the stdin, stdout or stderr argument to Popen and indicates that a pipe to the
standard stream should be opened.
STDOUT
Special value that can be used as the stderr argument to Popen and indicates that standard error should go into the
same handle as standard output.
SubprocessError
Base class for all other exceptions from this module.
New in version 3.3.
TimeoutExpired
Subclass of SubprocessError, raised when a timeout expires while waiting for a child process.
Otherwise, None.
This is
always bytes when any output was captured regardless of the text=True setting.
It may remain None
instead of b'' when no output was observed.
Otherwise, None.
This is always bytes
when stderr output was captured regardless of the text=True setting.
It may remain None instead of b''
when no stderr output was observed.
New in version 3.3.
Changed in version 3.5: stdout and stderr attributes added
exception subprocess.
CalledProcessError
Subclass of SubprocessError, raised when a process run by check_call(), check_output(), or
run() (with check=True) returns a non-zero exit status.
If the process exited due to a signal, this will be the negative signal number.
Otherwise, None.
stdout
Alias for output, for symmetry with stderr.
Otherwise, None.
Changed in version 3.5: stdout and stderr attributes added
17.6.
For most typical use cases, many of these arguments can be safely left at their default values.
The
arguments that are most commonly needed are:
args is required for all calls and should be a string, or a sequence of program arguments.
Providing a sequence
of arguments is generally preferred, as it allows the module to take care of any required escaping and quoting
of arguments (e.g. to permit spaces in file names).
If passing a single string, either shell must be True (see
below) or else the string must simply name the program to be executed without specifying any arguments.
Valid values are None, PIPE, DEVNULL, an existing file descriptor (a positive
integer), and an existing file object with a valid file descriptor.
With the default settings of None, no redi-
rection will occur.
PIPE indicates that a new pipe to the child should be created.
DEVNULL indicates that
the special file os.devnull will be used.
Additionally, stderr can be STDOUT, which indicates that the
stderr data from the child process should be captured into the same file handle as for stdout.
If encoding or errors are specified, or text (also known as universal_newlines) is true, the file objects stdin,
stdout and stderr will be opened in text mode using the encoding and errors specified in the call or the defaults
for io.
TextIOWrapper.
For stdin, line ending characters '\n' in the input will be converted to the default line separator os.
linesep.
For stdout and stderr, all line endings in the output will be converted to '\n'.
For more
information see the documentation of the io.
TextIOWrapper class when the newline argument to its
constructor is None.
If text mode is not used, stdin, stdout and stderr will be opened as binary streams.
No encoding or line ending
conversion is performed.
New in version 3.6: Added encoding and errors parameters.
Added the text parameter as an alias for universal_newlines.
Note: The newlines attribute of the file objects Popen.stdin, Popen.stdout and Popen.stderr
are not updated by the Popen.communicate() method.
If shell is True, the specified command will be executed through the shell.
This can be useful if you are
using Python primarily for the enhanced control flow it offers over most system shells and still want conve-
nient access to other shell features such as shell pipes, filename wildcards, environment variable expansion,
and expansion of ~ to a user’s home directory.
However, note that Python itself offers implementations
of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(),
os.path.expanduser(), and shutil).
Changed in version 3.3:
When universal_newlines is True, the class uses the encoding locale.
See the
io.
TextIOWrapper class for more information on this change.
Note: Read the Security Considerations section before using shell=True.
These options, along with all of the other options, are described in more detail in the Popen constructor documentation.
It offers a lot of flexibility
so that developers are able to handle the less common cases not covered by the convenience functions.
Popen(args, bufsize=- 1, executable=None, stdin=None, stdout=None, stderr=None,
preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None,
universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True,
start_new_session=False, pass_fds=(), *, group=None, extra_groups=None,
user=None, umask=- 1, encoding=None, errors=None, text=None, pipesize=- 1,
process_group=None)
Execute a child program in a new process.
On POSIX, the class uses os.execvpe()-like behavior to execute
the child program.
On Windows, the class uses the Windows CreateProcess() function.
The arguments to
Popen are as follows.
By default, the program
to execute is the first item in args if args is a sequence.
If args is a string, the interpretation is platform-dependent
and described below.
See the shell and executable arguments for additional differences from the default behavior.
Unless otherwise stated, it is recommended to pass args as a sequence.
Warning: For maximum reliability, use a fully qualified path for the executable.
To search for an unqualified
name on PATH, use shutil.which().
On all platforms, passing sys.executable is the recommended
way to launch the current Python interpreter again, and use the -m command-line format to launch an installed
module.
Resolving the path of executable (or the first item of args) is platform dependent.
For POSIX, see os.
execvpe(), and note that when resolving or searching for the executable path, cwd overrides the current
working directory and env can override the PATH environment variable.
For Windows, see the documenta-
tion of the lpApplicationName and lpCommandLine parameters of WinAPI CreateProcess, and
note that when resolving or searching for the executable path with shell=False, cwd does not override the
current working directory and env cannot override the PATH environment variable.
Using a full path avoids all
of these variations.
On POSIX, if args is a string, the string is interpreted as the name or path of the program to execute.
However,
this can only be done if not passing arguments to the program.
Note: It may not be obvious how to break a shell command into a sequence of arguments, especially in complex
cases.
On Windows, if args is a sequence, it will be converted to a string in a manner described in Converting an argument
sequence to a string on Windows.
This is because the underlying CreateProcess() operates on strings.
Changed in version 3.6: args parameter accepts a path-like object if shell is False and a sequence containing
path-like objects on POSIX.
Changed in version 3.8: args parameter accepts a path-like object if shell is False and a sequence containing bytes
and path-like objects on Windows.
The shell argument (which defaults to False) specifies whether to use the shell as the program to execute.
If shell
is True, it is recommended to pass args as a string rather than as a sequence.
On POSIX with shell=True, the shell defaults to /bin/sh.
If args is a string, the string specifies the command
to execute through the shell.
This means that the string must be formatted exactly as it would be when typed at
the shell prompt.
This includes, for example, quoting or backslash escaping filenames with spaces in them.
If args
is a sequence, the first item specifies the command string, and any additional items will be treated as additional
arguments to the shell itself.
On Windows with shell=True, the COMSPEC environment variable specifies the default shell.
The only time
you need to specify shell=True on Windows is when the command you wish to execute is built into the shell
(e.g. dir or copy).
You do not need shell=True to run a batch file or console-based executable.
Note: Read the Security Considerations section before using shell=True.
bufsize will be supplied as the corresponding argument to the open() function when creating the stdin/stdout/stderr
pipe file objects:
• 0 means unbuffered (read and write are one system call and can return short)
• 1 means line buffered (only usable if text=True or universal_newlines=True)
• any other positive value means use a buffer of approximately that size
• negative bufsize (the default) means the system default of io.DEFAULT_BUFFER_SIZE will be used.
Changed in version 3.3.1: bufsize now defaults to -1 to enable buffering by default to match the behavior that most
code expects.
In versions prior to Python 3.2.4 and 3.3.1 it incorrectly defaulted to 0 which was unbuffered and
allowed short reads.
This was unintentional and did not match the behavior of Python 2 as most code expected.
The executable argument specifies a replacement program to execute.
It is very seldom needed.
When
shell=False, executable replaces the program to execute specified by args.
However, the original args is still
passed to the program.
Most programs treat the program specified by args as the command name, which can then
be different from the program actually executed.
On POSIX, the args name becomes the display name for the
executable in utilities such as ps.
If shell=True, on POSIX the executable argument specifies a replacement
shell for the default /bin/sh.
Changed in version 3.6: executable parameter accepts a path-like object on POSIX.
Changed in version 3.8: executable parameter accepts a bytes and path-like object on Windows.
Changed in version 3.12: Changed Windows shell search order for shell=True.
The current directory and
%PATH% are replaced with %COMSPEC% and %SystemRoot%\System32\cmd.exe.
As a result, dropping
a malicious program named cmd.exe into a current directory no longer works.
With the default settings of None, no redirection will occur.
PIPE
indicates that a new pipe to the child should be created.
DEVNULL indicates that the special file os.devnull
will be used.
Additionally, stderr can be STDOUT, which indicates that the stderr data from the applications should
be captured into the same file handle as for stdout.
If preexec_fn is set to a callable object, this object will be called in the child process just before the child is executed.
The
child process could deadlock before exec is called.
Note:
If you need to modify the environment for the child use the env parameter rather than doing it in a pre-
exec_fn.
The start_new_session and process_group parameters should take the place of code using preexec_fn to
call os.setsid() or os.setpgid() in the child.
The preexec_fn parameter is no longer supported in subinterpreters.
The use of the pa-
rameter in a subinterpreter raises RuntimeError.
The new restriction may affect applications that are deployed
in mod_wsgi, uWSGI, and other embedded environments.
If close_fds is true, all file descriptors except 0, 1 and 2 will be closed before the child process is executed.
Otherwise when close_fds is false, file descriptors obey their inheritable flag as described in Inheritance of File
Descriptors.
On Windows, if close_fds is true then no handles will be inherited by the child process unless explicitly passed in
the handle_list element of STARTUPINFO.lpAttributeList, or by standard handle redirection.
The default for close_fds was changed from False to what is described above.
On Windows the default for close_fds was changed from False to True when redirecting
the standard handles.
It’s now possible to set close_fds to True when redirecting the standard handles.
Providing any
pass_fds forces close_fds to be True.
The pass_fds parameter was added.
If cwd is not None, the function changes the working directory to cwd before executing the child.
On POSIX, the function looks for executable (or for the first item in args) relative
to cwd if the executable path is a relative path.
Changed in version 3.6: cwd parameter accepts a path-like object on POSIX.
Changed in version 3.7: cwd parameter accepts a path-like object on Windows.
Changed in version 3.8: cwd parameter accepts a bytes object on Windows.
Currently this includes the SIGPIPE, SIGXFZ and SIGXFSZ signals.
If start_new_session is true the setsid() system call will be made in the child process prior to the execution of
the subprocess.
Availability: POSIX
Changed in version 3.2: start_new_session was added.
If process_group is a non-negative integer, the setpgid(0, value) system call will be made in the child
process prior to the execution of the subprocess.
If group is not None, the setregid() system call will be made in the child process prior to the execution of the
subprocess.
If the provided value is a string, it will be looked up via grp.getgrnam() and the value in gr_gid
will be used.
If the value is an integer, it will be passed verbatim.
If extra_groups is not None, the setgroups() system call will be made in the child process prior to the execution
of the subprocess.
Strings provided in extra_groups will be looked up via grp.getgrnam() and the values in
gr_gid will be used.
Integer values will be passed verbatim.
If user is not None, the setreuid() system call will be made in the child process prior to the execution of the
subprocess.
If the provided value is a string, it will be looked up via pwd.getpwnam() and the value in pw_uid
will be used.
If the value is an integer, it will be passed verbatim.
If umask is not negative, the umask() system call will be made in the child process prior to the execution of the
subprocess.
Availability: POSIX
New in version 3.9.
If env is not None, it must be a mapping that defines the environment variables for the new process; these are used
instead of the default behavior of inheriting the current process’ environment.
This mapping can be str to str on
any platform or bytes to bytes on POSIX platforms much like os.environ or os.environb.
Note: If specified, env must provide any variables required for the program to execute.
On Windows, in order to
run a side-by-side assembly the specified env must include a valid SystemRoot.
If encoding or errors are specified, or text is true, the file objects stdin, stdout and stderr are opened in text mode
with the specified encoding and errors, as described above in Frequently Used Arguments.
The universal_newlines
argument is equivalent to text and is provided for backwards compatibility.
By default, file objects are opened in
binary mode.
New in version 3.6: encoding and errors were added.
New in version 3.7: text was added as a more readable alias for universal_newlines.
If given, startupinfo will be a STARTUPINFO object, which is passed to the underlying CreateProcess func-
tion.
The size of the
pipe is only changed on platforms that support this (only Linux at this time of writing).
Other platforms will ignore
this parameter.
The pipesize parameter was added.
Popen and the other functions in this module that use it raise an auditing event subprocess.
Popen with ar-
guments executable, args, cwd, and env.
The value for args may be a single string or a list of strings,
depending on platform.
Changed in version 3.2: Added context manager support.
Popen destructor now emits a ResourceWarning warning if the child process is still
running.
Changed in version 3.8: Popen can use os.posix_spawn() in some cases for better performance.
On Windows
Subsystem for Linux and QEMU User Emulation, Popen constructor using os.posix_spawn() no longer raise
an exception on errors like missing program, but the child process fails with a non-zero returncode.
Exceptions
Exceptions raised in the child process, before the new program has started to execute, will be re-raised in the parent.
The most common exception raised is OSError.
This occurs, for example, when trying to execute a non-existent file.
Applications should prepare for OSError exceptions.
Note that, when shell=True, OSError will be raised by the
child only if the selected shell itself was not found.
To determine if the shell failed to find the requested application, it is
necessary to check the return code or output from the subprocess.
A ValueError will be raised if Popen is called with invalid arguments.
All of the functions and methods that accept a timeout parameter, such as run() and Popen.communicate() will
raise TimeoutExpired if the timeout expires before the process exits.
Exceptions defined in this module all inherit from SubprocessError.
The SubprocessError base class was added.
This means that all
characters, including shell metacharacters, can safely be passed to child processes.
If the shell is invoked explicitly,
via shell=True, it is the application’s responsibility to ensure that all whitespace and metacharacters are quoted ap-
propriately to avoid shell injection vulnerabilities.
On some platforms, it is possible to use shlex.quote() for this
escaping.
Set and return returncode attribute.
Otherwise, returns None.
Popen.wait(timeout=None)
Wait for child process to terminate.
Set and return returncode attribute.
If the process does not terminate after timeout seconds, raise a TimeoutExpired exception.
It is safe to catch
this exception and retry the wait.
This will deadlock when using stdout=PIPE or stderr=PIPE and the child process generates
enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data.
Use Popen.
Note:
The function is implemented using a busy loop (non-blocking call and short sleeps).
Use the asyncio
module for an asynchronous wait: see asyncio.create_subprocess_exec.
Changed in version 3.3: timeout was added.
Popen.communicate(input=None, timeout=None)
Interact with process: Send data to stdin.
Read data from stdout and stderr, until end-of-file is reached.
Wait for
process to terminate and set the returncode attribute.
The optional input argument should be data to be sent to
the child process, or None, if no data should be sent to the child.
If streams were opened in text mode, input must
be a string.
Otherwise, it must be bytes.
The data will be strings if streams
were opened in text mode; otherwise, bytes.
Note that if you want to send data to the process’s stdin, you need to create the Popen object with stdin=PIPE.
Similarly, to get anything other than None in the result tuple, you need to give stdout=PIPE and/or
stderr=PIPE too.
If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised.
Catching
this exception and retrying communication will not lose any output.
Note: The data read is buffered in memory, so do not use this method if the data size is large or unlimited.
Changed in version 3.3: timeout was added.
Popen.send_signal(signal)
Sends the signal signal to the child.
Do nothing if the process completed.
Note:
On
Windows,
SIGTERM
is
an
alias
for
terminate().
CTRL_C_EVENT
and
CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes
CREATE_NEW_PROCESS_GROUP.
Popen.terminate()
Stop the child.
On POSIX OSs the method sends SIGTERM to the child.
On Windows the Win32 API function
TerminateProcess() is called to stop the child.
Popen.kill()
Kills the child.
On POSIX OSs the function sends SIGKILL to the child.
On Windows kill() is an alias for
terminate().
The following attributes are also set by the class for you to access.
Reassigning them to new values is unsupported:
Popen.args
The args argument as it was passed to Popen – a sequence of program arguments or else a single string.
New in version 3.3.
Popen.stdin
If the stdin argument was PIPE, this attribute is a writeable stream object as returned by open().
If the encoding
or errors arguments were specified or the text or universal_newlines argument was True, the stream is a text stream,
otherwise it is a byte stream.
If the stdin argument was not PIPE, this attribute is None.
Popen.stdout
If the stdout argument was PIPE, this attribute is a readable stream object as returned by open().
Reading from
the stream provides output from the child process.
If the encoding or errors arguments were specified or the text
or universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream.
If the stdout
argument was not PIPE, this attribute is None.
If the stderr argument was PIPE, this attribute is a readable stream object as returned by open().
Reading from
the stream provides error output from the child process.
If the encoding or errors arguments were specified or the
text or universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream.
Note that if you set the shell argument to True, this is the process ID of the spawned shell.
Popen.returncode
The child return code.
Initially None, returncode is set by a call to the poll(), wait(), or
communicate() methods if they detect that the process has terminated.
A None value indicates that the process hadn’t yet terminated at the time of the last method call.
A negative value -N indicates that the child was terminated by signal N (POSIX only).
STARTUPINFO(*, dwFlags=0, hStdInput=None, hStdOutput=None, hStdError=None,
wShowWindow=0, lpAttributeList=None)
Partial support of the Windows STARTUPINFO structure is used for Popen creation.
The following attributes
can be set by passing them as keyword-only arguments.
Changed in version 3.7: Keyword-only argument support was added.
STARTF_USESHOWWINDOW
hStdInput
If dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard input handle for the pro-
cess.
If STARTF_USESTDHANDLES is not specified, the default for standard input is the keyboard buffer.
Otherwise, this attribute is ignored and the default for standard output is the console window’s
buffer.
Otherwise, this attribute is ignored and the default for standard error is the console window’s buffer.
Oth-
erwise, this attribute is ignored.
SW_HIDE is provided for this attribute.
It is used when Popen is called with shell=True.
Warning:
In a multithreaded process, use caution to avoid leaking handles that are marked in-
heritable when combining this feature with concurrent calls to other process creation functions that
inherit all handles such as os.system().
This also applies to standard handle redirection, which
temporarily creates inheritable handles.
New in version 3.7.
Windows Constants
The subprocess module exposes the following constants.
subprocess.
STD_INPUT_HANDLE
The standard input device.
Initially, this is the console input buffer, CONIN$.
subprocess.
STD_OUTPUT_HANDLE
The standard output device.
Initially, this is the active console screen buffer, CONOUT$.
subprocess.
STD_ERROR_HANDLE
The standard error device.
Initially, this is the active console screen buffer, CONOUT$.
subprocess.
SW_HIDE
Hides the window.
Another window will be activated.
Specifies that the STARTUPINFO.hStdInput, STARTUPINFO.hStdOutput, and STARTUPINFO.
STARTF_USESHOWWINDOW
Specifies that the STARTUPINFO.wShowWindow attribute contains additional information.
The new process has a new console, instead of inheriting its parent’s console (the default).
CREATE_NEW_PROCESS_GROUP
A Popen creationflags parameter to specify that a new process group will be created.
This flag is necessary
for using os.kill() on the subprocess.
This flag is ignored if CREATE_NEW_CONSOLE is specified.
ABOVE_NORMAL_PRIORITY_CLASS
A Popen creationflags parameter to specify that a new process will have an above average priority.
New in version 3.7.
BELOW_NORMAL_PRIORITY_CLASS
A Popen creationflags parameter to specify that a new process will have a below average priority.
New in version 3.7.
A Popen creationflags parameter to specify that a new process will have a high priority.
New in version 3.7.
IDLE_PRIORITY_CLASS
A Popen creationflags parameter to specify that a new process will have an idle (lowest) priority.
New in version 3.7.
NORMAL_PRIORITY_CLASS
A Popen creationflags parameter to specify that a new process will have an normal priority.
A Popen creationflags parameter to specify that a new process will have realtime priority.
You should
almost never use REALTIME_PRIORITY_CLASS, because this interrupts system threads that manage mouse
input, keyboard input, and background disk flushing.
This class can be appropriate for applications that “talk”
directly to hardware or that perform brief tasks that should have limited interruptions.
New in version 3.7.
CREATE_NO_WINDOW
A Popen creationflags parameter to specify that a new process will not create a window.
New in version 3.7.
A Popen creationflags parameter to specify that a new process will not inherit its parent’s console.
This
value cannot be used with CREATE_NEW_CONSOLE.
New in version 3.7.
A Popen creationflags parameter to specify that a new process does not inherit the error mode of the calling
process.
Instead, the new process gets the default error mode.
This feature is particularly useful for multithreaded
shell applications that run with hard errors disabled.
New in version 3.7.
A Popen creationflags parameter to specify that a new process is not associated with the job.
New in version 3.7.
Prior to Python 3.5, these three functions comprised the high level API to subprocess.
You can now use run() in many
cases, but lots of existing code calls these functions.
subprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None,
**other_popen_kwargs)
Run the command described by args.
Wait for command to complete, then return the returncode attribute.
To suppress stdout or stderr, supply a value of DEVNULL.
The arguments shown above are merely some common ones.
The full function signature is the same as that of
the Popen constructor - this function passes all supplied arguments other than timeout directly through to that
interface.
Note:
Do not use stdout=PIPE or stderr=PIPE with this function.
The child process will block if it
generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.
Changed in version 3.3: timeout was added.
Changed in version 3.12: Changed Windows shell search order for shell=True.
The current directory and
%PATH% are replaced with %COMSPEC% and %SystemRoot%\System32\cmd.exe.
As a result, dropping
a malicious program named cmd.exe into a current directory no longer works.
subprocess.check_call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None,
timeout=None, **other_popen_kwargs)
Run command with arguments.
Wait for command to complete.
If the return code was zero then return, oth-
erwise raise CalledProcessError.
The CalledProcessError object will have the return code in the
returncode attribute.
If check_call() was unable to start the process it will propagate the exception that
was raised.
Code needing to capture stdout or stderr should use run() instead:
run(..., check=True)
To suppress stdout or stderr, supply a value of DEVNULL.
The arguments shown above are merely some common ones.
The full function signature is the same as that of
the Popen constructor - this function passes all supplied arguments other than timeout directly through to that
interface.
Note:
Do not use stdout=PIPE or stderr=PIPE with this function.
The child process will block if it
generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.
Changed in version 3.3: timeout was added.
Changed in version 3.12: Changed Windows shell search order for shell=True.
The current directory and
%PATH% are replaced with %COMSPEC% and %SystemRoot%\System32\cmd.exe.
As a result, dropping
a malicious program named cmd.exe into a current directory no longer works.
subprocess.check_output(args, *, stdin=None, stderr=None, shell=False, cwd=None, encoding=None,
errors=None, universal_newlines=None, timeout=None, text=None,
**other_popen_kwargs)
Run command with arguments and return its output.
If the return code was non-zero it raises a CalledProcessError.
The CalledProcessError object will
have the return code in the returncode attribute and any output in the output attribute.
This is equivalent to:
run(..., check=True, stdout=PIPE).stdout
The arguments shown above are merely some common ones.
The full function signature is largely the same as
that of run() - most arguments are passed directly through to that interface.
One API deviation from run()
17.6.
By default, this function will return the data as encoded bytes.
The actual encoding of the output data may depend
on the command being invoked, so the decoding to text will often need to be handled at the application level.
This behaviour may be overridden by setting text, encoding, errors, or universal_newlines to True as described in
Frequently Used Arguments and run().
To also capture standard error in the result, use stderr=subprocess.
STDOUT:

"ls non_existent_file; exit 0",

stderr=subprocess.
Changed in version 3.4: Support for the input keyword argument was added.
Changed in version 3.6: encoding and errors were added.
See run() for details.
New in version 3.7: text was added as a more readable alias for universal_newlines.
Changed in version 3.12: Changed Windows shell search order for shell=True.
The current directory and
%PATH% are replaced with %COMSPEC% and %SystemRoot%\System32\cmd.exe.
As a result, dropping
a malicious program named cmd.exe into a current directory no longer works.
17.6.6 Replacing Older Functions with the subprocess Module
In this section, “a becomes b” means that b can be used as a replacement for a.
Note:
All “a” functions in this section fail (more or less) silently if the executed program cannot be found; the “b”
replacements raise OSError instead.
In addition, the replacements using check_output() will fail with a CalledProcessError if the requested
operation produces a non-zero return code.
The output is still available as the output attribute of the raised exception.
In the following examples, we assume that the relevant functions have already been imported from the subprocess
module.
Alternatively, for trusted input, the shell’s own pipeline support may still be used directly:
output=$(dmesg | grep hda)
becomes:
output = check_output("dmesg | grep hda", shell=True)
Replacing os.system()
sts = os.system("mycmd" + " myarg")
# becomes
retcode = call("mycmd" + " myarg", shell=True)
Notes:
• Calling the program through the shell is usually not required.
The call() return value is encoded differently to that of os.system().
The os.system() function ignores SIGINT and SIGQUIT signals while the command is running, but the caller
must do this separately when using the subprocess module.
A more realistic example would look like this:
try:
retcode = call("mycmd" + " myarg", shell=True)
if retcode < 0:
print("Child was terminated by signal", -retcode, file=sys.stderr)
else:
print("Child returned", retcode, file=sys.stderr)
except OSError as e:
print("Execution failed:", e, file=sys.stderr)
17.6.
P_NOWAIT, "/bin/mycmd", "mycmd", "myarg")
==>
pid = Popen(["/bin/mycmd", "myarg"]).pid
P_WAIT example:
retcode = os.spawnlp(os.
Vector example:
os.spawnvp(os.
Environment example:
os.spawnlpe(os.
Replacing functions from the popen2 module
Note: If the cmd argument to popen2 functions is a string, the command is executed through /bin/sh.
If it is a list, the
command is directly executed.
Popen raises an exception if the execution fails.
The capturestderr argument is replaced with the stderr argument.
Legacy Shell Invocation Functions
This module also provides the following legacy functions from the 2.x commands module.
These operations implicitly
invoke the system shell and none of the guarantees described above regarding security and exception handling consistency
are valid for these functions.
Execute the string cmd in a shell with Popen.check_output() and return a 2-tuple (exitcode,
output).
A trailing newline is stripped from the output.
The exit code for the command can be interpreted as the return code
of subprocess.
Example:
17.6.
The function now returns (exitcode, output) instead of (status, output) as it did in Python 3.3.3 and earlier.
New in version 3.11: Added encoding and errors arguments.
Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the
command’s output.
Example:
'/bin/ls'
Availability: Unix, Windows.
Changed in version 3.3.4: Windows support added
New in version 3.11: Added encoding and errors arguments.
Arguments are delimited by white space, which is either a space or a tab.
A string surrounded by double quotation marks is interpreted as a single argument, regardless of white space con-
tained within.
A quoted string can be embedded in an argument.
A double quotation mark preceded by a backslash is interpreted as a literal double quotation mark.
Backslashes are interpreted literally, unless they immediately precede a double quotation mark.
If backslashes immediately precede a double quotation mark, every pair of backslashes is interpreted as a literal
backslash.
If the number of backslashes is odd, the last backslash escapes the next double quotation mark as
described in rule 3.
See also:
shlex Module which provides function to parse and escape command lines.
On Linux, subprocess defaults to using the vfork() system call internally when it is safe to do so rather than
fork().
This greatly improves performance.
If you ever encounter a presumed highly unusual situation where you need to prevent vfork() from being used by
Python, you can set the subprocess._USE_VFORK attribute to a false value.
Setting this has no impact on use of posix_spawn() which could use vfork() internally within its libc implemen-
tation.
There is a similar subprocess._USE_POSIX_SPAWN attribute if you need to prevent use of that.
It is safe to set these to false on any Python version.
They will have no effect on older versions when unsupported.
Do
not assume the attributes are available to read.
Despite their names, a true value does not indicate that the corresponding
function will be used, only that it may be.
Please file issues any time you have to use these private knobs with a way to reproduce the issue you were seeing.
Link
to that issue from a comment in your code.
The scheduler class defines a generic interface to scheduling events.
It needs two functions to actually deal
with the “outside world” — timefunc should be callable without arguments, and return a number (the “time”, in
any units whatsoever).
The delayfunc function should be callable with one argument, compatible with the output
of timefunc, and should delay that many time units.
Changed in version 3.3: timefunc and delayfunc parameters are optional.
Changed in version 3.3: scheduler class can be safely used in multi-threaded environments.
Example:

print("From print_time", time.time(), a)


print(time.time())

s.enter(10, 1, print_time)

s.enter(5, 2, print_time, argument=('positional',))

# despite having higher priority, 'keyword' runs after 'positional' as␣
�→enter() is relative
(continues on next page)
17.7.
The time argument should be a numeric type compatible with the return value of the timefunc
function passed to the constructor.
Events scheduled for the same time will be executed in the order of their priority.
A lower number represents a higher priority.
Executing the event means executing action(*argument, **kwargs).
Return value is an event which may be used for later cancellation of the event (see cancel()).
Changed in version 3.3: argument parameter is optional.
Changed in version 3.3: kwargs parameter was added.
Changed in version 3.3: argument parameter is optional.
Changed in version 3.3: kwargs parameter was added.
If event is not an event currently in the queue, this method will raise a
ValueError.
This method will wait (using the delayfunc function passed to the constructor) for the
next event, then execute it and so on until there are no more scheduled events.
If blocking is false executes the scheduled events due to expire soonest (if any) and then return the deadline of the
next scheduled call in the scheduler (if any).
Either action or delayfunc can raise an exception.
In either case, the scheduler will maintain a consistent state
and propagate the exception.
If an exception is raised by action, the event will not be attempted in future calls to
run().
If a sequence of events takes longer to run than the time available before the next event, the scheduler will simply
fall behind.
No events will be dropped; the calling code is responsible for canceling events which are no longer
pertinent.
Changed in version 3.3: blocking parameter was added.
scheduler.queue
Read-only attribute returning a list of upcoming events in the order they will be run.
Each event is shown as a
named tuple with the following fields: time, priority, action, argument, kwargs.
17.8 queue — A synchronized queue class
Source code: Lib/queue.py
The queue module implements multi-producer, multi-consumer queues.
It is especially useful in threaded programming
when information must be exchanged safely between multiple threads.
The Queue class in this module implements all
the required locking semantics.
The module implements three types of queue, which differ only in the order in which the entries are retrieved.
In a FIFO
queue, the first tasks added are the first retrieved.
In a LIFO queue, the most recently added entry is the first retrieved
(operating like a stack).
With a priority queue, the entries are kept sorted (using the heapq module) and the lowest
valued entry is retrieved first.
Internally, those three types of queues use locks to temporarily block competing threads; however, they are not designed
to handle reentrancy within a thread.
In addition, the module implements a “simple” FIFO queue type, SimpleQueue, whose specific implementation pro-
vides additional guarantees in exchange for the smaller functionality.
The queue module defines the following classes and exceptions:
class queue.Queue(maxsize=0)
Constructor for a FIFO queue.
Insertion will block once this size has been reached, until queue items are consumed.
If
maxsize is less than or equal to zero, the queue size is infinite.
Constructor for a LIFO queue.
Insertion will block once this size has been reached, until queue items are consumed.
If maxsize is less than or equal to zero, the queue size is infinite.
Constructor for a priority queue.
Insertion will block once this size has been reached, until queue items are consumed.
If maxsize is less than or equal to zero, the queue size is infinite.
The lowest valued entries are retrieved first (the lowest valued entry is the one that would be returned by
min(entries)).
A typical pattern for entries is a tuple in the form: (priority_number, data).
If the data elements are not comparable, the data can be wrapped in a class that ignores the data item and only
compares the priority number:
from dataclasses import dataclass, field
from typing import Any
(continues on next page)
17.8.
SimpleQueue
Constructor for an unbounded FIFO queue.
Simple queues lack advanced functionality such as task tracking.
New in version 3.7.
exception queue.
Empty
Exception raised when non-blocking get() (or get_nowait()) is called on a Queue object which is empty.
Full
Exception raised when non-blocking put() (or put_nowait()) is called on a Queue object which is full.
Note, qsize() > 0 doesn’t guarantee that a subsequent get() will not block,
nor will qsize() < maxsize guarantee that put() will not block.
Queue.empty()
Return True if the queue is empty, False otherwise.
If empty() returns True it doesn’t guarantee that a subse-
quent call to put() will not block.
Return True if the queue is full, False otherwise.
If full() returns True it doesn’t guarantee that a subsequent
call to get() will not block.
Similarly, if full() returns False it doesn’t guarantee that a subsequent call to put()
will not block.
Queue.put(item, block=True, timeout=None)
Put item into the queue.
If optional args block is true and timeout is None (the default), block if necessary until
a free slot is available.
If timeout is a positive number, it blocks at most timeout seconds and raises the Full
exception if no free slot was available within that time.
Otherwise (block is false), put an item on the queue if a
free slot is immediately available, else raise the Full exception (timeout is ignored in that case).
Equivalent to put(item, block=False).
Queue.get(block=True, timeout=None)
Remove and return an item from the queue.
If optional args block is true and timeout is None (the default), block
if necessary until an item is available.
If timeout is a positive number, it blocks at most timeout seconds and raises
the Empty exception if no item was available within that time.
Otherwise (block is false), return an item if one is
immediately available, else raise the Empty exception (timeout is ignored in that case).
Prior to 3.0 on POSIX systems, and for all versions on Windows, if block is true and timeout is None, this operation
goes into an uninterruptible wait on an underlying lock.
This means that no exceptions can occur, and in particular
a SIGINT will not trigger a KeyboardInterrupt.
Two methods are offered to support tracking whether enqueued tasks have been fully processed by daemon consumer
threads.
Queue.task_done()
Indicate that a formerly enqueued task is complete.
Used by queue consumer threads.
For each get() used to
fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete.
If a join() is currently blocking, it will resume when all items have been processed (meaning that a
task_done() call was received for every item that had been put() into the queue).
Raises a ValueError if called more times than there were items placed in the queue.
Blocks until all items in the queue have been gotten and processed.
The count of unfinished tasks goes up whenever an item is added to the queue.
The count goes down whenever a
consumer thread calls task_done() to indicate that the item was retrieved and all work on it is complete.
When
the count of unfinished tasks drops to zero, join() unblocks.
Example of how to wait for enqueued tasks to be completed:
import threading
import queue
q = queue.
Queue()

item = q.get()
print(f'Working on {item}')
print(f'Finished {item}')
q.task_done()
# Turn-on the worker thread.
threading.
Thread(target=worker, daemon=True).start()
# Send thirty task requests to the worker.
SimpleQueue Objects
SimpleQueue objects provide the public methods described below.
SimpleQueue.qsize()
Return the approximate size of the queue.
Note, qsize() > 0 doesn’t guarantee that a subsequent get() will not block.
SimpleQueue.empty()
Return True if the queue is empty, False otherwise.
If empty() returns False it doesn’t guarantee that a
subsequent call to get() will not block.
17.8.
The method never blocks and always succeeds (except for potential low-level errors such
as failure to allocate memory).
The optional args block and timeout are ignored and only provided for compatibility
with Queue.put().
CPython implementation detail: This method has a C implementation which is reentrant.
That is, a put()
or get() call can be interrupted by another put() call in the same thread without deadlocking or corrupting
internal state inside the queue.
This makes it appropriate for use in destructors such as __del__ methods or
weakref callbacks.
Equivalent to put(item, block=False), provided for compatibility with Queue.put_nowait().
SimpleQueue.get(block=True, timeout=None)
Remove and return an item from the queue.
If optional args block is true and timeout is None (the default), block
if necessary until an item is available.
If timeout is a positive number, it blocks at most timeout seconds and raises
the Empty exception if no item was available within that time.
Otherwise (block is false), return an item if one is
immediately available, else raise the Empty exception (timeout is ignored in that case).
Equivalent to get(False).
See also:
Class multiprocessing.
Queue A queue class for use in a multi-processing (rather than multi-threading) context.
The ContextVar class is used to declare
and work with Context Variables.
The copy_context() function and the Context class should be used to manage
the current context in asynchronous frameworks.
Context managers that have state should use Context Variables instead of threading.local() to prevent their state
from bleeding to other code unexpectedly, when used in concurrent code.
See also PEP 567 for additional details.
New in version 3.7.
The required name parameter is used for introspection and debug purposes.
The optional keyword-only default parameter is returned by ContextVar.get() when no value for the variable
is found in the current context.
Context objects
hold strong references to context variables which prevents context variables from being properly garbage collected.
This is a read-only property.
New in version 3.7.1.
If there is no value for the variable in the current context, the method will:
• return the value of the default argument of the method, if provided; or
• return the default value for the context variable, if it was created with one; or
• raise a LookupError.
The required value argument is the new value for the context variable.
Returns a Token object that can be used to restore the variable to its previous value via the ContextVar.
For example:
var = ContextVar('var')
token = var.set('new value')
# code that uses 'var'; var.get() returns 'new value'.
var.reset(token)
# After the reset call the var has no value again, so
# var.get() would raise a LookupError.
Token
Token objects are returned by the ContextVar.set() method.
They can be passed to the ContextVar.
Points to the ContextVar object that created the token.
Set to the value the variable had before the ContextVar.set() method call that
created the token.
It points to Token.
MISSING if the variable was not set before the call.
MISSING
A marker object used by Token.old_value.
The function has an O(1) complexity, i.e. works equally fast for contexts with a few context variables and for
contexts that have a lot of them.
Context
A mapping of ContextVars to their values.
Context() creates an empty context with no values in it.
To get a copy of the current context use the
copy_context() function.
Every thread will have a different top-level Context object.
This means that a ContextVar object behaves in
a similar fashion to threading.local() when values are assigned in different threads.
Context implements the collections.abc.
Mapping interface.
Return
the result of the execution or propagate an exception if one occurred.
If the variable is not set in the context object, a
KeyError is raised.
Return default otherwise.
If default is not
given, return None.
iter(context)
Return an iterator over the variables stored in the context object.
For
example, here is a simple echo server, that uses a context variable to make the address of a remote client available in the
Task that handles that client:
import asyncio
import contextvars
client_addr_var = contextvars.
The address of the currently handled client can be accessed
# without passing it explicitly to this function.
In any code that we call is now possible to get
# client's address by calling 'client_addr_var.get()'.
For synchronization, simple locks (also called mutexes or
binary semaphores) are provided.
The threading module provides an easier to use and higher-level threading API
built on top of this module.
This module used to be optional, it is now always available.
This module defines the following constants and functions:
exception _thread.error
Raised on thread-specific errors.
Changed in version 3.3: This is now a synonym of the built-in RuntimeError.
This is the type of lock objects.
The thread executes the function function with the argument list args
(which must be a tuple).
The optional kwargs argument specifies a dictionary of keyword arguments.
When the function returns, the thread silently exits.
When the function terminates with an unhandled exception, sys.unraisablehook() is called to handle the
exception.
The object attribute of the hook argument is function.
By default, a stack trace is printed and then the
thread exits (but other threads continue to run).
When the function raises a SystemExit exception, it is silently ignored.
Raises an auditing event _thread.start_new_thread with arguments function, args, kwargs.
Changed in version 3.8: sys.unraisablehook() is now used to handle unhandled exceptions.
SIGINT, / )
Simulate the effect of a signal arriving in the main thread.
A thread can use this function to interrupt the main
thread, though there is no guarantee that the interruption will happen immediately.
If given, signum is the number of the signal to simulate.
If signum is not given, signal.
SIGINT is simulated.
If the given signal isn’t handled by Python (it was set to signal.SIG_DFL or signal.
The signum argument is added to customize the signal number.
Note: This does not emit the corresponding signal but schedules a call to the associated handler (if it exists).
If
you want to truly emit the signal, use signal.raise_signal().
_thread.exit()
Raise the SystemExit exception.
When not caught, this will cause the thread to exit silently.
Methods of locks are described below.
The lock is initially unlocked.
This is a nonzero integer.
Its value has no direct meaning; it is
intended as a magic cookie to be used e.g. to index a dictionary of thread-specific data.
Thread identifiers may be
recycled when a thread exits and another thread is created.
This is a non-negative integer.
Its
value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which
the value may be recycled by the OS).
Availability: Windows, FreeBSD, Linux, macOS, OpenBSD, NetBSD, AIX, DragonFlyBSD.
New in version 3.8.
The optional size argument specifies the stack size to
be used for subsequently created threads, and must be 0 (use platform or configured default) or a positive integer
value of at least 32,768 (32 KiB).
If size is not specified, 0 is used.
If changing the thread stack size is unsupported,
a RuntimeError is raised.
If the specified stack size is invalid, a ValueError is raised and the stack size
is unmodified.
Note that some platforms may have particular restrictions on values for the stack size, such
as requiring a minimum stack size > 32 KiB or requiring allocation in multiples of the system memory page size -
platform documentation should be referred to for more information (4 KiB pages are common; using multiples of
4096 for the stack size is the suggested approach in the absence of more specific information).
Availability: Windows, pthreads.
Unix platforms with POSIX threads support.
TIMEOUT_MAX
The maximum value allowed for the timeout parameter of Lock.acquire.
Specifying a timeout greater than
this value will raise an OverflowError.
New in version 3.2.
Lock objects have the following methods:
17.10.
If the blocking argument is present, the action depends on its value: if it is False, the lock is only acquired if it can
be acquired immediately without waiting, while if it is True, the lock is acquired unconditionally as above.
If the floating-point timeout argument is present and positive, it specifies the maximum wait time in seconds before
returning.
A negative timeout argument specifies an unbounded wait.
You cannot specify a timeout if blocking is
False.
The return value is True if the lock is acquired successfully, False if not.
The timeout parameter is new.
Changed in version 3.2: Lock acquires can now be interrupted by signals on POSIX.
The lock must have been acquired earlier, but not necessarily by the same thread.
Threads interact strangely with interrupts: the KeyboardInterrupt exception will be received by an arbitrary
thread.
It is not possible to interrupt the acquire() method on a lock — the KeyboardInterrupt exception will
happen after the lock has been acquired.
On most systems, they are killed
without executing try … finally clauses or executing object destructors.
The modules described in this chapter provide mechanisms for networking and inter-processes communication.
Some modules only work for two processes that are on the same machine, e.g. signal and mmap.
Other modules
support networking protocols that two or more processes can use to communicate across machines.
Hello World!
You can experiment with an asyncio concurrent context in the REPL:
$ python -m asyncio
asyncio REPL ...
Use "await" directly instead of "asyncio.run()".
Type "help", "copyright", "credits" or "license" for more information.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Reference
18.1.1 Runners
Source code: Lib/asyncio/runners.py
This section outlines high-level asyncio primitives to run asyncio code.
They are built on top of an event loop with the aim to simplify async code usage for common wide-spread scenarios.
This function runs the passed coroutine, taking care of managing the asyncio event loop, finalizing asynchronous
generators, and closing the executor.
This function cannot be called when another asyncio event loop is running in the same thread.
If debug is True, the event loop will be run in debug mode.
False disables debug mode explicitly.
None is used
to respect the global Debug Mode settings.
If loop_factory is not None, it is used to create a new event loop; otherwise asyncio.new_event_loop()
is used.
The loop is closed at the end.
This function should be used as a main entry point for asyncio programs,
and should ideally only be called once.
It is recommended to use loop_factory to configure the event loop instead
of policies.
The executor is given a timeout duration of 5 minutes to shutdown.
If the executor hasn’t finished within that
duration, a warning is emitted and the executor is closed.
New in version 3.7.
Changed in version 3.9: Updated to use loop.shutdown_default_executor().
Changed in version 3.10: debug is None by default to respect the global debug mode settings.
Changed in version 3.12: Added loop_factory parameter.
Runner context manager
class asyncio.
A context manager that simplifies multiple async function calls in the same context.
Sometimes several top-level async functions should be called in the same event loop and contextvars.
Context.
If debug is True, the event loop will be run in debug mode.
False disables debug mode explicitly.
None is used
to respect the global Debug Mode settings.
It is the responsibility of the loop_factory to set the
created loop as the current one.
By default asyncio.new_event_loop() is used and set as current event
loop with asyncio.set_event_loop() if loop_factory is None.
Basically, asyncio.run() example can be rewritten with the runner usage:
async def main():
await asyncio.sleep(1)
print('hello')
with asyncio.
New in version 3.11.
Return the coroutine’s result or raise its exception.
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the
coro to run in.
The runner’s default context is used if None.
This function cannot be called when another asyncio event loop is running in the same thread.
Finalize asynchronous generators, shutdown default executor, close the event loop and release embedded
contextvars.
Context.
get_loop()
Return the event loop associated with the runner instance.
Embedded loop and context are created at the with body entering or the first call of run() or get_loop().
Handling Keyboard Interruption
New in version 3.11.
When signal.
SIGINT is raised by Ctrl-C, KeyboardInterrupt exception is raised in the main thread by
default.
However this doesn’t work with asyncio because it can interrupt asyncio internals and can hang the program
from exiting.
To mitigate this issue, asyncio handles signal.
SIGINT as follows:
1. asyncio.Runner.run() installs a custom signal.
SIGINT handler before any user code is executed and
removes it when exiting from the function.
The Runner creates the main task for the passed coroutine for its execution.
When signal.
SIGINT is raised by Ctrl-C, the custom signal handler cancels the main task by calling
asyncio.Task.cancel() which raises asyncio.
CancelledError inside the main task.
This causes
the Python stack to unwind, try/except and try/finally blocks can be used for resource cleanup.
After
the main task is cancelled, asyncio.Runner.run() raises KeyboardInterrupt.
A user could write a tight loop which cannot be interrupted by asyncio.Task.cancel(), in which case the
second following Ctrl-C immediately raises the KeyboardInterrupt without cancelling the main task.
The following snippet of code will print “hello” after waiting for 1 second, and then print
“world” after waiting for another 2 seconds:
import asyncio
import time
async def say_after(delay, what):
await asyncio.sleep(delay)
print(what)
async def main():
print(f"started at {time.strftime('%X')}")
await say_after(1, 'hello')
await say_after(2, 'world')
print(f"finished at {time.strftime('%X')}")
asyncio.run(main())
Expected output:
started at 17:13:52
hello
world
finished at 17:13:55
18.1.
Note that expected output now shows that the snippet runs 1 second faster than before:
started at 17:14:32
hello
world
finished at 17:14:34
• The asyncio.
TaskGroup class provides a more modern alternative to create_task().
Using this API,
the last example becomes:
async def main():
async with asyncio.
The timing and output should be the same as for the previous version.
New in version 3.11: asyncio.
TaskGroup.
We say that an object is an awaitable object if it can be used in an await expression.
Many asyncio APIs are designed
to accept awaitables.
There are three main types of awaitable objects: coroutines, Tasks, and Futures.
Coroutines
Python coroutines are awaitables and therefore can be awaited from other coroutines:
import asyncio
async def nested():

async def main():
# Nothing happens if we just call "nested()".
Important: In this documentation the term “coroutine” can be used for two closely related concepts:
• a coroutine function: an async def function;
• a coroutine object: an object returned by calling a coroutine function.
Tasks
Tasks are used to schedule coroutines concurrently.
When a coroutine is wrapped into a Task with functions like asyncio.create_task() the coroutine is automatically
scheduled to run soon:
import asyncio
async def nested():

async def main():
# Schedule nested() to run soon concurrently
# with "main()".
When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place.
Future objects in asyncio are needed to allow callback-based code to be used with async/await.
Normally there is no need to create Future objects at the application level code.
A good example of a low-level function that returns a Future object is loop.run_in_executor().
Creating Tasks
Source code: Lib/asyncio/tasks.py
asyncio.create_task(coro, *, name=None, context=None)
Wrap the coro coroutine into a Task and schedule its execution.
Return the Task object.
If name is not None, it is set as the name of the task using Task.set_name().
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the coro
to run in.
The current context copy is created when no context is provided.
The task is executed in the loop returned by get_running_loop(), RuntimeError is raised if there is no
running loop in current thread.
Note:
asyncio.
TaskGroup.create_task() is a new alternative leveraging structural concurrency; it
allows for waiting for a group of related tasks with strong safety guarantees.
Important: Save a reference to the result of this function, to avoid a task disappearing mid-execution.
The event
loop only keeps weak references to tasks.
A task that isn’t referenced elsewhere may get garbage collected at any
time, even before it’s done.
This creates a strong reference.
Changed in version 3.8: Added the name parameter.
Added the context parameter.
Task Cancellation
Tasks can easily and safely be cancelled.
When a task is cancelled, asyncio.
CancelledError will be raised in the
task at the next opportunity.
It is recommended that coroutines use try/finally blocks to robustly perform clean-up logic.
In case asyncio.
CancelledError is explicitly caught, it should generally be propagated when clean-up is complete.
CancelledError directly subclasses BaseException so most code will not need to be aware of it.
The asyncio components that enable structured concurrency, like asyncio.
TaskGroup and asyncio.
CancelledError.
Similarly, user code should not generally call uncancel.
However, in cases when suppressing
asyncio.
CancelledError is truly desired, it is necessary to also call uncancel() to completely remove the
cancellation state.
Task Groups
Task groups combine a task creation API with a convenient and reliable way to wait for all tasks in the group to finish.
TaskGroup
An asynchronous context manager holding a group of tasks.
Tasks can be added to the group using
create_task().
All tasks are awaited when the context manager exits.
New in version 3.11.
The signature matches that of asyncio.create_task().
Example:
async def main():
async with asyncio.
The async with statement will wait for all tasks in the group to finish.
While waiting, new tasks may still be added to
the group (for example, by passing tg into one of the coroutines and calling tg.create_task() in that coroutine).
Once the last task has finished and the async with block is exited, no new tasks may be added to the group.
The first time any of the tasks belonging to the group fails with an exception other than asyncio.
CancelledError,
the remaining tasks in the group are cancelled.
No further tasks can then be added to the group.
At this point, if the body
of the async with statement is still active (i.e., __aexit__() hasn’t been called yet), the task directly containing
the async with statement is also cancelled.
The resulting asyncio.
CancelledError will interrupt an await,
but it will not bubble out of the containing async with statement.
Once all tasks have finished, if any tasks have failed with an exception other than asyncio.
CancelledError, those
exceptions are combined in an ExceptionGroup or BaseExceptionGroup (as appropriate; see their documen-
tation) which is then raised.
Two base exceptions are treated specially: If any task fails with KeyboardInterrupt or SystemExit, the task
group still cancels the remaining tasks and waits for them, but then the initial KeyboardInterrupt or SystemExit
is re-raised instead of ExceptionGroup or BaseExceptionGroup.
If the body of the async with statement exits with an exception (so __aexit__() is called with an exception
set), this is treated the same as if one of the tasks failed: the remaining tasks are cancelled and then waited for, and
non-cancellation exceptions are grouped into an exception group and raised.
The exception passed into __aexit__(),
unless it is asyncio.
CancelledError, is also included in the exception group.
The same special case is made for
KeyboardInterrupt and SystemExit as in the previous paragraph.
Sleeping
coroutine asyncio.sleep(delay, result=None)
Block for delay seconds.
If result is provided, it is returned to the caller when the coroutine completes.
Setting the delay to 0 provides an optimized path to allow other tasks to run.
This can be used by long-running
functions to avoid blocking the event loop for the full duration of the function call.
Removed the loop parameter.
Running Tasks Concurrently
awaitable asyncio.gather(*aws, return_exceptions=False)
Run awaitable objects in the aws sequence concurrently.
If any awaitable in aws is a coroutine, it is automatically scheduled as a Task.
If all awaitables are completed successfully, the result is an aggregate list of returned values.
The order of result
values corresponds to the order of awaitables in aws.
If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits
on gather().
Other awaitables in the aws sequence won’t be cancelled and will continue to run.
If gather() is cancelled, all submitted awaitables (that have not completed yet) are also cancelled.
If any Task or Future from the aws sequence is cancelled, it is treated as if it raised CancelledError – the
gather() call is not cancelled in this case.
This is to prevent the cancellation of one submitted Task/Future to
cause other Tasks/Futures to be cancelled.
Note:
A new alternative to create and run tasks concurrently and wait for their completion is asyncio.
TaskGroup.
TaskGroup provides stronger safety guarantees than gather for scheduling a nesting of subtasks:
if a task (or a subtask, a task scheduled by a task) raises an exception, TaskGroup will, while gather will not, cancel
the remaining scheduled tasks).
For instance, gather can be marked done after propagating an exception to the caller, therefore, calling
gather.cancel() after catching an exception (raised by one of the awaitables) from gather won’t cancel any
other awaitables.
If the gather itself is cancelled, the cancellation is propagated regardless of re-
turn_exceptions.
A task factory for eager task execution.
When using this factory (via loop.set_task_factory(asyncio.eager_task_factory)), corou-
tines begin execution synchronously during Task construction.
Tasks are only scheduled on the event loop if they
block.
This can be a performance improvement as the overhead of loop scheduling is avoided for coroutines that
complete synchronously.
Note:
Immediate execution of the coroutine is a semantic change.
If the coroutine returns or raises, the task
is never scheduled to the event loop.
If the coroutine execution blocks, the task is scheduled to the event loop.
This change may introduce behavior changes to existing applications.
For example, the application’s task execution
order is likely to change.
New in version 3.12.
The
callable must return a asyncio.
Task-compatible object.
This function returns a callable intended to be used as a task factory of an event loop via loop.
set_task_factory(factory)).
New in version 3.12.
Shielding From Cancellation
awaitable asyncio.shield(aw)
Protect an awaitable object from being cancelled.
If aw is a coroutine it is automatically scheduled as a Task.
The statement:
task = asyncio.create_task(something())
res = await shield(task)
is equivalent to:
res = await something()
except that if the coroutine containing it is cancelled, the Task running in something() is not cancelled.
From
the point of view of something(), the cancellation did not happen.
Although its caller is still cancelled, so the
“await” expression still raises a CancelledError.
If something() is cancelled by other means (i.e. from within itself) that would also cancel shield().
The
event loop only keeps weak references to tasks.
A task that isn’t referenced elsewhere may get garbage collected at
any time, even before it’s done.
Changed in version 3.10: Removed the loop parameter.
Deprecation warning is emitted if aw is not Future-like object and there is no
running event loop.
Timeouts
asyncio.timeout(delay)
Return an asynchronous context manager that can be used to limit the amount of time spent waiting on something.
If delay is None, no time limit will be applied;
this can be useful if the delay is unknown when the context manager is created.
In either case, the context manager can be rescheduled after creation using Timeout.reschedule().
If long_running_task takes more than 10 seconds to complete, the context manager will cancel the current
task and handle the resulting asyncio.
CancelledError internally, transforming it into a TimeoutError
which can be caught and handled.
Note:
The asyncio.timeout() context manager is what transforms the asyncio.
CancelledError
into a TimeoutError, which means the TimeoutError can only be caught outside of the context manager.
The context manager produced by asyncio.timeout() can be rescheduled to a different deadline and in-
spected.
An asynchronous context manager for cancelling overdue coroutines.
If when is None, the timeout will never trigger.
If when < loop.time(), the timeout will trigger on the next iteration of the event loop.
Timeout context managers can be safely nested.
Similar to asyncio.timeout(), except when is the absolute time to stop waiting, or None.
If aw is a coroutine it is automatically scheduled as a Task.
If timeout is None, block until the
future completes.
If a timeout occurs, it cancels the task and raises TimeoutError.
To avoid the task cancellation, wrap it in shield().
The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout.
If an
exception happens during cancellation, it is propagated.
If the wait is cancelled, the future aw is also cancelled.
Previ-
ously, it raised TimeoutError immediately.
Changed in version 3.10: Removed the loop parameter.
Raises TimeoutError instead of asyncio.
TimeoutError.
Run Future and Task instances in the aws iterable concurrently and block until the condition specified by
return_when.
The aws iterable must not be empty.
Returns two sets of Tasks/Futures: (done, pending).
Usage:
done, pending = await asyncio.wait(aws)
timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning.
Futures or Tasks that aren’t done when the timeout occurs
are simply returned in the second set.
It must be one of the following constants:
Constant
Description
FIRST_COMPLETED
The function will return when any future finishes or is cancelled.
FIRST_EXCEPTION
The function will return when any future finishes by raising an exception.
If no
future raises an exception then it is equivalent to ALL_COMPLETED.
ALL_COMPLETED
The function will return when all futures finish or are cancelled.
Unlike wait_for(), wait() does not cancel the futures when a timeout occurs.
Changed in version 3.10: Removed the loop parameter.
Passing coroutine objects to wait() directly is forbidden.
Changed in version 3.12: Added support for generators yielding tasks.
Return an iterator of coroutines.
Each coroutine returned
can be awaited to get the earliest next result from the iterable of the remaining awaitables.
Raises TimeoutError if the timeout occurs before all Futures are done.
Example:
for coro in as_completed(aws):
earliest_result = await coro
# ...
Changed in version 3.10: Removed the loop parameter.
Deprecation warning is emitted if not all awaitable objects in the aws iterable are
Future-like objects and there is no running event loop.
Changed in version 3.12: Added support for generators yielding tasks.
Running in Threads
coroutine asyncio.to_thread(func, /, *args, **kwargs)
Asynchronously run function func in a separate thread.
Any *args and **kwargs supplied for this function are directly passed to func.
Also, the current contextvars.
Context is propagated, allowing context variables from the event loop thread to be accessed in the separate
thread.
Return a coroutine that can be awaited to get the eventual result of func.
This coroutine function is primarily intended to be used for executing IO-bound functions/methods that would
otherwise block the event loop if they were run in the main thread.
Instead, by using asyncio.to_thread(), we can run it in a separate thread
without blocking the event loop.
Note: Due to the GIL, asyncio.to_thread() can typically only be used to make IO-bound functions non-
blocking.
However, for extension modules that release the GIL or alternative Python implementations that don’t
have one, asyncio.to_thread() can also be used for CPU-bound functions.
New in version 3.9.
Scheduling From Other Threads
asyncio.run_coroutine_threadsafe(coro, loop)
Submit a coroutine to the given event loop.
Thread-safe.
Return a concurrent.futures.
Future to wait for the result from another OS thread.
This function is meant to be called from a different OS thread than the one where the event loop is running.
Unlike other asyncio functions this function requires the loop argument to be passed explicitly.
New in version 3.5.1.
Introspection
asyncio.current_task(loop=None)
Return the currently running Task instance, or None if no task is running.
If loop is None get_running_loop() is used to get the current loop.
New in version 3.7.
If loop is None, get_running_loop() is used for getting current loop.
Return True if obj is a coroutine object.
New in version 3.4.
Task Object
class asyncio.
A Future-like object that runs a Python coroutine.
Not thread-safe.
Tasks are used to run coroutines in event loops.
If a coroutine awaits on a Future, the Task suspends the execution
of the coroutine and waits for the completion of the Future.
When the Future is done, the execution of the wrapped
coroutine resumes.
Event loops use cooperative scheduling: an event loop runs one Task at a time.
While a Task awaits for the
completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations.
Use the high-level asyncio.create_task() function to create Tasks,
or the low-level loop.
create_task() or ensure_future() functions.
Manual instantiation of Tasks is discouraged.
To cancel a running Task use the cancel() method.
Calling it will cause the Task to throw a
CancelledError exception into the wrapped coroutine.
If a coroutine is awaiting on a Future object dur-
ing cancellation, the Future object will be cancelled.
The method returns True if the wrapped corou-
tine did not suppress the CancelledError exception and was actually cancelled.
asyncio.
Task inherits from Future all of its APIs except Future.set_result() and Future.
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the coro
to run in.
If no context is provided, the Task copies the current context and later runs its coroutine in the copied
context.
An optional keyword-only eager_start argument allows eagerly starting the execution of the asyncio.
Task
at task creation time.
If set to True and the event loop is running, the task will start executing the coroutine
immediately, until the first time the coroutine blocks.
If the coroutine returns or raises without blocking, the task
will be finished eagerly and will skip scheduling to the event loop.
Changed in version 3.7: Added support for the contextvars module.
Changed in version 3.8: Added the name parameter.
Deprecation warning is emitted if loop is not specified and there is no running event
loop.
Added the context parameter.
Changed in version 3.12: Added the eager_start parameter.
A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was
cancelled.
If the Task has been cancelled, this method raises a CancelledError exception.
If the Task’s result isn’t yet available, this method raises a InvalidStateError exception.
If the wrapped coroutine raised an exception that exception is returned.
If the wrapped coroutine returned
normally this method returns None.
If the Task has been cancelled, this method raises a CancelledError exception.
If the Task isn’t done yet, this method raises an InvalidStateError exception.
This method should only be used in low-level callback-based code.
See the documentation of Future.add_done_callback() for more details.
remove_done_callback(callback)
Remove callback from the callbacks list.
This method should only be used in low-level callback-based code.
See the documentation of Future.remove_done_callback() for more details.
If the wrapped coroutine is not done, this returns the stack where it is suspended.
If the coroutine has
completed successfully or was cancelled, this returns an empty list.
If the coroutine was terminated by an
exception, this returns the list of traceback frames.
The frames are always ordered from oldest to newest.
Only one stack frame is returned for a suspended coroutine.
The optional limit argument sets the maximum number of frames to return; by default all available frames are
returned.
The ordering of the returned list differs depending on whether a stack or a traceback is returned:
the newest frames of a stack are returned, but the oldest frames of a traceback are returned.
This produces output similar to that of the traceback module for the frames retrieved by get_stack().
The limit argument is passed to get_stack() directly.
The file argument is an I/O stream to which the output is written; by default output is written to sys.stdout.
Note: This will return None for Tasks which have already completed eagerly.
See the Eager Task Factory.
New in version 3.8.
Changed in version 3.12: Newly added eager task execution means result may be None.
Context object associated with the task.
New in version 3.12.
If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a
default name during instantiation.
New in version 3.8.
The value argument can be any object, which is then converted to a string.
In the default Task implementation, the name will be visible in the repr() output of a task object.
New in version 3.8.
This arranges for a CancelledError exception to be thrown into the wrapped coroutine on the next cycle
of the event loop.
The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a
try … … except CancelledError … finally block.
Should the coroutine nevertheless decide to suppress
the cancellation, it needs to call Task.uncancel() in addition to catching the exception.
Added the msg parameter.
The msg parameter is propagated from cancelled task to its awaiter.
The following
example illustrates how coroutines can intercept the cancellation request:
async def cancel_me():
print('cancel_me(): before sleep')
try:
# Wait for 1 hour
await asyncio.sleep(3600)
except asyncio.
CancelledError:
print("main(): cancel_me is cancelled now")
asyncio.run(main())
# Expected output:
#
#
cancel_me(): before sleep
#
cancel_me(): cancel sleep
#
cancel_me(): after sleep
#
main(): cancel_me is cancelled now
cancelled()
Return True if the Task is cancelled.
The Task is cancelled when the cancellation was requested with cancel() and the wrapped coroutine
propagated the CancelledError exception thrown into it.
Note that once execution of a cancelled task completed, further calls to uncancel() are ineffective.
New in version 3.11.
This method is used by asyncio’s internals and isn’t expected to be used by end-user code.
In particular, if a
Task gets successfully uncancelled, this allows for elements of structured concurrency like Task Groups and
asyncio.timeout() to continue running, isolating cancellation to the respective structured block.
This is imple-
mented with uncancel().
TaskGroup context managers use uncancel() in a similar fashion.
If end-user code is, for some reason, suppresing cancellation by catching CancelledError, it needs to
call this method to remove the cancellation state.
Note that if this number is greater than zero but the Task is still executing, cancelled() will still return
False.
This is because this number can be lowered by calling uncancel(), which can lead to the task
not being cancelled after all if the cancellation requests go down to zero.
This method is used by asyncio’s internals and isn’t expected to be used by end-user code.
See uncancel()
for more details.
New in version 3.11.
Streams allow sending and re-
ceiving data without using callbacks or low-level protocols and transports.
See also the Examples section below.
Stream Functions
The following top-level asyncio functions can be used to create and work with streams:
coroutine asyncio.open_connection(host=None, port=None, *, limit=None, ssl=None, family=0,
proto=0, flags=0, sock=None, local_addr=None,
server_hostname=None, ssl_handshake_timeout=None,
ssl_shutdown_timeout=None, happy_eyeballs_delay=None,
interleave=None)
Establish a network connection and return a pair of (reader, writer) objects.
The returned reader and writer objects are instances of StreamReader and StreamWriter classes.
By default the limit is set to
64 KiB.
The rest of the arguments are passed directly to loop.create_connection().
Note: The sock argument transfers ownership of the socket to the StreamWriter created.
To close the socket,
call its close() method.
Added the ssl_handshake_timeout parameter.
New in version 3.8: Added happy_eyeballs_delay and interleave parameters.
Removed the loop parameter.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
AF_UNSPEC, flags=socket.
AI_PASSIVE, sock=None,
backlog=100, ssl=None, reuse_address=None, reuse_port=None,
ssl_handshake_timeout=None, ssl_shutdown_timeout=None,
start_serving=True)
Start a socket server.
The client_connected_cb callback is called whenever a new client connection is established.
It receives a (reader,
writer) pair as two arguments, instances of the StreamReader and StreamWriter classes.
By default the limit is set to
64 KiB.
The rest of the arguments are passed directly to loop.create_server().
Note:
The sock argument transfers ownership of the socket to the server created.
To close the socket, call the
server’s close() method.
Removed the loop parameter.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
Unix Sockets
coroutine asyncio.open_unix_connection(path=None, *, limit=None, ssl=None, sock=None,
server_hostname=None, ssl_handshake_timeout=None,
ssl_shutdown_timeout=None)
Establish a Unix socket connection and return a pair of (reader, writer).
Similar to open_connection() but operates on Unix sockets.
See also the documentation of loop.create_unix_connection().
Note: The sock argument transfers ownership of the socket to the StreamWriter created.
To close
the socket, call its close() method.
Availability: Unix.
Changed in version 3.7: Added the ssl_handshake_timeout parameter.
Removed the loop parameter.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
Similar to start_server() but works with Unix sockets.
See also the documentation of loop.create_unix_server().
Note:
The sock argument transfers ownership of the socket to the server created.
To close the socket, call the
server’s close() method.
Availability: Unix.
Changed in version 3.7: Added the ssl_handshake_timeout and start_serving parameters.
The path parameter can
now be a path-like object.
Changed in version 3.10: Removed the loop parameter.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
StreamReader
Represents a reader object that provides APIs to read data from the IO stream.
As an asynchronous iterable, the
object supports the async for statement.
It is not recommended to instantiate StreamReader objects directly;
use open_connection() and
start_server() instead.
coroutine read(n=- 1)
Read up to n bytes from the stream.
If n is not provided or set to -1, read until EOF, then return all read bytes.
If EOF was received and the
internal buffer is empty, return an empty bytes object.
If n is 0, return an empty bytes object immediately.
If n is positive, return at most n available bytes as soon as at least 1 byte is available in the internal buffer.
If EOF is received before any byte is read, return an empty bytes object.
If EOF is received and \n was not found, the method returns partially read data.
If EOF is received and the internal buffer is empty, return an empty bytes object.
Raise an IncompleteReadError
if EOF is reached before n can be read.
Use the
IncompleteReadError.partial attribute to get the partially read data.
Read data from the stream until separator is found.
On success, the data and separator will be removed from the internal buffer (consumed).
Returned data will
include the separator at the end.
If the amount of data read exceeds the configured stream limit, a LimitOverrunError exception is
raised, and the data is left in the internal buffer and can be read again.
If EOF is reached before the complete separator is found, an IncompleteReadError exception is raised,
and the internal buffer is reset.
The IncompleteReadError.partial attribute may contain a portion
of the separator.
New in version 3.5.2.
at_eof()
Return True if the buffer is empty and feed_eof() was called.
StreamWriter
Represents a writer object that provides APIs to write data to the IO stream.
The method attempts to write the data to the underlying socket immediately.
If that fails, the data is queued
in an internal write buffer until it can be sent.
The method writes a list (or any iterable) of bytes to the underlying socket immediately.
If that fails, the data
is queued in an internal write buffer until it can be sent.
The method closes the stream and the underlying socket.
Return True if the underlying transport supports the write_eof() method, False otherwise.
This is a flow control method that interacts with the underlying IO write buffer.
When the size of the buffer
reaches the high watermark, drain() blocks until the size of the buffer is drained down to the low watermark
and writing can be resumed.
Parameters:
• sslcontext: a configured instance of SSLContext.
New in version 3.11.
Changed in version 3.12: Added the ssl_shutdown_timeout parameter.
New in version 3.7.
coroutine wait_closed()
Wait until the stream is closed.
Should be called after close() to wait until the underlying connection is closed, ensuring that all data has
been flushed before e.g. exiting the program.
New in version 3.7.
See also:
The TCP echo server protocol example uses the loop.create_server() method.
See also:
The register an open socket to wait for data using a protocol example uses a low-level protocol and the loop.
The watch a file descriptor for read events example uses the low-level loop.add_reader() method to watch a file
descriptor.
Lock
Implements a mutex lock for asyncio tasks.
Not thread-safe.
An asyncio lock can be used to guarantee exclusive access to a shared resource.
The preferred way to use a Lock is an async with statement:
lock = asyncio.
Lock()
# ...
Lock()
# ...
Removed the loop parameter.
This method waits until the lock is unlocked, sets it to locked and returns True.
When more than one coroutine is blocked in acquire() waiting for the lock to be unlocked, only one
coroutine eventually proceeds.
Acquiring a lock is fair: the coroutine that proceeds will be the first coroutine that started waiting on the lock.
When the lock is locked, reset it to unlocked and return.
If the lock is unlocked, a RuntimeError is raised.
Event
class asyncio.
Event
An event object.
Not thread-safe.
An asyncio event can be used to notify multiple asyncio tasks that some event has happened.
An Event object manages an internal flag that can be set to true with the set() method and reset to false with the
clear() method.
The wait() method blocks until the flag is set to true.
The flag is set to false initially.
Removed the loop parameter.
Example:
18.1.
Event()
# Spawn a Task to wait until 'event' is set.
If the event is set, return True immediately.
Otherwise block until another task calls set().
All tasks waiting for event to be set will be immediately awakened.
Tasks awaiting on wait() will now block until the set() method is called again.
Condition
class asyncio.
Condition(lock=None)
A Condition object.
Not thread-safe.
An asyncio condition primitive can be used by a task to wait for some event to happen and then get exclusive access
to a shared resource.
In essence, a Condition object combines the functionality of an Event and a Lock.
It is possible to have multiple
Condition objects share one Lock, which allows coordinating exclusive access to a shared resource between different
tasks interested in particular states of that shared resource.
The optional lock argument must be a Lock object or None.
In the latter case a new Lock object is created
automatically.
Removed the loop parameter.
Condition()
# ...
Condition()
# ...
Acquire the underlying lock.
This method waits until the underlying lock is unlocked, sets it to locked and returns True.
notify(n=1)
Wake up at most n tasks (1 by default) waiting on this condition.
The method is no-op if no tasks are waiting.
The lock must be acquired before this method is called and released shortly after.
If called with an unlocked
lock a RuntimeError error is raised.
This method acts like notify(), but wakes up all waiting tasks.
The lock must be acquired before this method is called and released shortly after.
If called with an unlocked
lock a RuntimeError error is raised.
When invoked on an unlocked lock, a RuntimeError is raised.
If the calling task has not acquired the lock when this method is called, a RuntimeError is raised.
This method releases the underlying lock, and then blocks until it is awakened by a notify() or
notify_all() call.
Wait until a predicate becomes true.
The predicate must be a callable which result will be interpreted as a boolean value.
Not thread-safe.
A semaphore manages an internal counter which is decremented by each acquire() call and incremented by
each release() call.
The counter can never go below zero; when acquire() finds that it is zero, it blocks,
waiting until some task calls release().
The optional value argument gives the initial value for the internal counter (1 by default).
If the given value is less
than 0 a ValueError is raised.
Changed in version 3.10: Removed the loop parameter.
The preferred way to use a Semaphore is an async with statement:
sem = asyncio.
Semaphore(10)
# ... later
async with sem:
# work with shared resource
which is equivalent to:
sem = asyncio.
Semaphore(10)
# ... later
await sem.acquire()
try:
# work with shared resource
finally:
sem.release()
coroutine acquire()
Acquire a semaphore.
If the internal counter is greater than zero, decrement it by one and return True immediately.
If it is zero,
wait until a release() is called and return True.
Can wake up a task waiting to acquire the
semaphore.
Unlike BoundedSemaphore, Semaphore allows making more release() calls than acquire()
calls.
A bounded semaphore object.
Not thread-safe.
Bounded Semaphore is a version of Semaphore that raises a ValueError in release() if it increases the
internal counter above the initial value.
Removed the loop parameter.
Barrier
class asyncio.
A barrier object.
Not thread-safe.
A barrier is a simple synchronization primitive that allows to block until parties number of tasks are waiting on it.
Tasks can wait on the wait() method and would be blocked until the specified number of tasks end up waiting
on wait().
At that point all of the waiting tasks would unblock simultaneously.
The barrier can be reused any number of times.
Example:
async def example_barrier():
# barrier with 3 parties
b = asyncio.
Result of this example is:
<asyncio.locks.
Barrier object at 0x...
Barrier object at 0x...
Barrier object at 0x...
When all the tasks party to the barrier have called this function, they are all unblocked
simultaneously.
When a waiting or blocked task in the barrier is cancelled, this task exits the barrier which stays in the same
state.
If the state of the barrier is “filling”, the number of waiting task decreases by 1.
The return value is an integer in the range of 0 to parties-1, different for each task.
This method may raise a BrokenBarrierError exception if the barrier is broken or reset while a task
is waiting.
It could raise a CancelledError if a task is cancelled.
Any tasks waiting on it will receive the
BrokenBarrierError exception.
If a barrier is broken it may be better to just leave it and create a new one.
This causes any active or future calls to wait() to fail with the
BrokenBarrierError.
Use this for example if one of the tasks needs to abort, to avoid infinite waiting
tasks.
BrokenBarrierError
This exception, a subclass of RuntimeError, is raised when the Barrier object is reset or broken.
Changed in version 3.9: Acquiring a lock using await lock or yield from lock and/or with statement (with
await lock, with (yield from lock)) was removed.
Use async with lock instead.
PIPE,
stderr=asyncio.subprocess.
No such file or directory
Because all asyncio subprocess functions are asynchronous and asyncio provides many tools to work with such functions,
it is easy to execute and monitor multiple subprocesses in parallel.
See also the Examples subsection.
Creating Subprocesses
coroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None,
limit=None, **kwds)
Create a subprocess.
The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.
PIPE is passed to stdout and stderr arguments).
Return a Process instance.
See the documentation of loop.subprocess_exec() for other parameters.
Removed the loop parameter.
The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.
PIPE is passed to stdout and stderr arguments).
Return a Process instance.
See the documentation of loop.subprocess_shell() for other parameters.
The shlex.quote() function can be used to properly
escape whitespace and special shell characters in strings that are going to be used to construct shell commands.
Removed the loop parameter.
Note: Subprocesses are available for Windows if a ProactorEventLoop is used.
See Subprocess Support on Win-
dows for details.
See also:
asyncio also has the following low-level APIs to work with subprocesses: loop.subprocess_exec(), loop.
Constants
asyncio.subprocess.
PIPE
Can be passed to the stdin, stdout or stderr parameters.
If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance.
If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will
point to StreamReader instances.
asyncio.subprocess.
STDOUT
Special value that can be used as the stderr argument and indicates that standard error should be redirected into
standard output.
DEVNULL
Special value that can be used as the stdin, stdout or stderr argument to process creation functions.
It indicates that
the special file os.devnull will be used for the corresponding subprocess stream.
Interacting with Subprocesses
Both create_subprocess_exec() and create_subprocess_shell() functions return instances of the
Process class.
Process is a high-level wrapper that allows communicating with subprocesses and watching for their com-
pletion.
Process
An
object
that
wraps
OS
processes
created
by
the
create_subprocess_exec()
and
create_subprocess_shell() functions.
This class is designed to have a similar API to the subprocess.
This class is not thread safe.
See also the Subprocess and Threads section.
Set and return the returncode attribute.
Note:
This method can deadlock when using stdout=PIPE or stderr=PIPE and the child pro-
cess generates so much output that it blocks waiting for the OS pipe buffer to accept more data.
Use the
communicate() method when using pipes to avoid this condition.
coroutine communicate(input=None)
Interact with process:
1.
The optional input argument is the data (bytes object) that will be sent to the child process.
Return a tuple (stdout_data, stderr_data).
If either BrokenPipeError or ConnectionResetError exception is raised when writing input into
stdin, the exception is ignored.
This condition occurs when the process exits before all data are written into
stdin.
If it is desired to send data to the process’ stdin, the process needs to be created with stdin=PIPE.
Similarly,
to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and/or
stderr=PIPE arguments.
Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited.
Changed in version 3.12: stdin gets closed when input=None too.
Note:
On Windows,
SIGTERM is an alias for terminate().
CTRL_C_EVENT and
CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes
CREATE_NEW_PROCESS_GROUP.
terminate()
Stop the child process.
On POSIX systems this method sends signal.
SIGTERM to the child process.
On Windows the Win32 API function TerminateProcess() is called to stop the child process.
On POSIX systems this method sends SIGKILL to the child process.
On Windows this method is an alias for terminate().
This avoids deadlocks due to
streams pausing reading or writing and blocking the child process.
Note that for processes created by the create_subprocess_shell() function, this attribute is the
PID of the spawned shell.
A None value indicates that the process has not terminated yet.
A negative value -N indicates that the child was terminated by signal N (POSIX only).
Subprocess and Threads
Standard asyncio event loop supports running subprocesses from different threads by default.
On Windows subprocesses are provided by ProactorEventLoop only (default), SelectorEventLoop has no
subprocess support.
On UNIX child watchers are used for subprocess finish waiting, see Process Watchers for more info.
UNIX switched to use ThreadedChildWatcher for spawning subprocesses from different
threads without any limitation.
Spawning a subprocess with inactive current child watcher raises RuntimeError.
Note that alternative event loop implementations might have own limitations; please refer to their documentation.
See also:
The Concurrency and multithreading in asyncio section.
Examples
An example using the Process class to control a subprocess and the StreamReader class to read from its standard
output.
PIPE)
# Read one line of output.
Although asyncio queues are not thread-safe,
they are designed to be used specifically in async/await code.
Note that methods of asyncio queues don’t have a timeout parameter; use asyncio.wait_for() function to do queue
operations with a timeout.
See also the Examples section below.
A first in, first out (FIFO) queue.
If maxsize is less than or equal to zero, the queue size is infinite.
If it is an integer greater than 0, then await
put() blocks when the queue reaches maxsize until an item is removed by get().
Unlike the standard library threading queue, the size of the queue is always known and can be returned by calling
the qsize() method.
Changed in version 3.10: Removed the loop parameter.
This class is not thread safe.
If the queue was initialized with maxsize=0 (the default), then full() never returns True.
If queue is empty, wait until an item is available.
The count of unfinished tasks goes up whenever an item is added to the queue.
The count goes down whenever
a consumer coroutine calls task_done() to indicate that the item was retrieved and all work on it is
complete.
When the count of unfinished tasks drops to zero, join() unblocks.
If the queue is full, wait until a free slot is available before adding the item.
put_nowait(item)
Put an item into the queue without blocking.
If no free slot is immediately available, raise QueueFull.
Used by queue consumers.
For each get() used to fetch a task, a subsequent call to task_done() tells
the queue that the processing on the task is complete.
If a join() is currently blocking, it will resume when all items have been processed (meaning that a
task_done() call was received for every item that had been put() into the queue).
Raises ValueError if called more times than there were items placed in the queue.
Priority Queue
class asyncio.
PriorityQueue
A variant of Queue; retrieves entries in priority order (lowest first).
Entries are typically tuples of the form (priority_number, data).
LifoQueue
A variant of Queue that retrieves most recently added entries first (last in, first out).
Exceptions
exception asyncio.
QueueEmpty
This exception is raised when the get_nowait() method is called on an empty queue.
QueueFull
Exception raised when the put_nowait() method is called on a queue that has reached its maxsize.
Examples
Queues can be used to distribute workload between several concurrent tasks:
import asyncio
import random
import time
async def worker(name, queue):
while True:
# Get a "work item" out of the queue.
Queue()
# Generate random timings and put them into the queue.
This class was made an alias of TimeoutError.
CancelledError
The operation has been cancelled.
This exception can be caught to perform custom operations when asyncio Tasks are cancelled.
In almost all situa-
tions the exception must be re-raised.
CancelledError is now a subclass of BaseException rather than Exception.
InvalidStateError
Invalid internal state of Task or Future.
Can be raised in situations like setting a result value for a Future object that already has a result value set.
SendfileNotAvailableError
The “sendfile” syscall is not available for the given socket or file type.
A subclass of RuntimeError.
IncompleteReadError
The requested read operation did not complete fully.
Raised by the asyncio stream APIs.
This exception is a subclass of EOFError.
expected
The total number (int) of expected bytes.
LimitOverrunError
Reached the buffer size limit while looking for a separator.
Raised by the asyncio stream APIs.
The event loop is the core of every asyncio application.
Event loops run asynchronous tasks and callbacks, perform
network IO operations, and run subprocesses.
Application developers should typically use the high-level asyncio functions, such as asyncio.run(), and should
rarely need to reference the loop object or call its methods.
This section is intended mostly for authors of lower-level
code, libraries, and frameworks, who need finer control over the event loop behavior.
Obtaining the Event Loop
The following low-level functions can be used to get, set, or create an event loop:
asyncio.get_running_loop()
Return the running event loop in the current OS thread.
Raise a RuntimeError if there is no running event loop.
This function can only be called from a coroutine or a callback.
New in version 3.7.
asyncio.get_event_loop()
Get the current event loop.
If there is no running event loop set, the function will return the result of the get_event_loop_policy().
Because this function has rather complex behavior (especially when custom event loop policies are in use), using
the get_running_loop() function is preferred to get_event_loop() in coroutines and callbacks.
As noted above, consider using the higher-level asyncio.run() function, instead of using these lower level
functions to manually create and close an event loop.
Deprecated since version 3.12: Deprecation warning is emitted if there is no current event loop.
In some future
Python release this will become an error.
Note that the behaviour of get_event_loop(), set_event_loop(), and new_event_loop() functions can
be altered by setting a custom event loop policy.
The Examples section showcases how to work with some event loop APIs.
If the argument is a coroutine object it is implicitly scheduled to run as a asyncio.
Task.
Return the Future’s result or raise its exception.
If stop() is called before run_forever() is called, the loop will poll the I/O selector once with a timeout of
zero, run all callbacks scheduled in response to I/O events (and those that were already scheduled), and then exit.
If stop() is called while run_forever() is running, the loop will run the current batch of callbacks and then
exit.
Note that new callbacks scheduled by callbacks will not run in this case; instead, they will run the next time
run_forever() or run_until_complete() is called.
The loop must not be running when this function is called.
Any pending callbacks will be discarded.
This method clears all queues and shuts down the executor, but does not wait for the executor to finish.
This method is idempotent and irreversible.
No other methods should be called after the event loop is closed.
After calling this
method, the event loop will issue a warning if a new asynchronous generator is iterated.
This should be used to
reliably finalize all scheduled asynchronous generators.
Note that there is no need to call this function when asyncio.run() is used.
Example:
try:
loop.run_forever()
finally:
loop.run_until_complete(loop.shutdown_asyncgens())
loop.close()
New in version 3.6.
coroutine loop.shutdown_default_executor(timeout=None)
Schedule the closure of the default executor and wait for it to join all of the threads in the
ThreadPoolExecutor.
Once this method has been called, using the default executor with loop.
run_in_executor() will raise a RuntimeError.
With the default, None, the executor is allowed an unlimited amount of time.
If the timeout is reached, a RuntimeWarning is emitted and the default executor is terminated without waiting
for its threads to finish joining.
Note:
Do not call this method when using asyncio.run(), as the latter handles default executor shutdown
automatically.
New in version 3.9.
Changed in version 3.12: Added the timeout parameter.
Scheduling callbacks
loop.call_soon(callback, *args, context=None)
Schedule the callback callback to be called with args arguments at the next iteration of the event loop.
Return an instance of asyncio.
Handle, which can be used later to cancel the callback.
Callbacks are called in the order in which they are registered.
Each callback will be called exactly once.
The optional keyword-only context argument specifies a custom contextvars.
Context for the callback to
run in.
Callbacks use the current context when no context is provided.
Unlike call_soon_threadsafe(), this method is not thread-safe.
A thread-safe variant of call_soon().
When scheduling callbacks from another thread, this function must be
used, since call_soon() is not thread-safe.
Raises RuntimeError if called on a loop that’s been closed.
This can happen on a secondary thread when the
main application is shutting down.
See the concurrency and multithreading section of the documentation.
The context keyword-only parameter was added.
See PEP 567 for more details.
Note:
Most asyncio scheduling functions don’t allow passing keyword arguments.
To do that, use functools.
Using partial objects is usually more convenient than using lambdas, as asyncio can render partial objects better in debug
and error messages.
None)
Schedule callback to be called after the given delay number of seconds (can be either an int or a float).
An instance of asyncio.
TimerHandle is returned which can be used to cancel the callback.
If two callbacks are scheduled for exactly the same time, the order in which
they are called is undefined.
The optional positional args will be passed to the callback when it is called.
If you want the callback to be called
with keyword arguments use functools.partial().
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the call-
back to run in.
The current context is used when no context is provided.
The context keyword-only parameter was added.
See PEP 567 for more details.
In Python 3.7 and earlier with the default event loop implementation, the delay could not
exceed one day.
This has been fixed in Python 3.8.
loop.call_at(when, callback, *args, context=None)
Schedule callback to be called at the given absolute timestamp when (an int or a float), using the same time reference
as loop.time().
This method’s behavior is the same as call_later().
An instance of asyncio.
TimerHandle is returned which can be used to cancel the callback.
The context keyword-only parameter was added.
See PEP 567 for more details.
In Python 3.7 and earlier with the default event loop implementation, the difference between
when and the current time could not exceed one day.
This has been fixed in Python 3.8.
In Python 3.7 and earlier timeouts (relative delay or absolute when) should not exceed
one day.
This has been fixed in Python 3.8.
See also:
The asyncio.sleep() function.
Creating Futures and Tasks
loop.create_future()
Create an asyncio.
Future object attached to the event loop.
This is the preferred way to create Futures in asyncio.
This lets third-party event loops provide alternative imple-
mentations of the Future object (with better performance or instrumentation).
New in version 3.5.2.
Return a Task object.
Third-party event loops can use their own subclass of Task for interoperability.
In this case, the result type is a
subclass of Task.
If the name argument is provided and not None, it is set as the name of the task using Task.set_name().
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the coro
to run in.
The current context copy is created when no context is provided.
Changed in version 3.8: Added the name parameter.
Added the context parameter.
loop.set_task_factory(factory)
Set a task factory that will be used by loop.create_task().
If factory is None the default task factory will be set.
Otherwise, factory must be a callable with the signature
matching (loop, coro, context=None), where loop is a reference to the active event loop, and coro is a
coroutine object.
The callable must return a asyncio.
Future-compatible object.
Opening network connections
coroutine loop.create_connection(protocol_factory, host=None, port=None, *, ssl=None, family=0,
proto=0, flags=0, sock=None, local_addr=None,
server_hostname=None, ssl_handshake_timeout=None,
ssl_shutdown_timeout=None, happy_eyeballs_delay=None,
interleave=None, all_errors=False)
Open a streaming transport connection to a given address specified by host and port.
The socket family can be either AF_INET or AF_INET6 depending on host (or the family argument, if provided).
The socket type will be SOCK_STREAM.
This method will try to establish the connection in the background.
When successful, it returns a (transport,
protocol) pair.
The chronological synopsis of the underlying operation is as follows:
1.
The connection is established and a transport is created for it.
The protocol instance is coupled with the transport by calling its connection_made() method.
A (transport, protocol) tuple is returned on success.
The created transport is an implementation-dependent bidirectional stream.
Other arguments:
• ssl: if given and not false, a SSL/TLS transport is created (by default a plain TCP transport is created).
If ssl
is a ssl.
Should only be passed if ssl is not None.
By default the value of the host argument is used.
If host is empty,
there is no default and you must pass a value for server_hostname.
If server_hostname is an empty string,
hostname matching is disabled (which is a serious security risk, allowing for potential man-in-the-middle
attacks).
If given, these should all be integers from the corresponding socket module constants.
It should be a floating-point
number representing the amount of time in seconds to wait for a connection attempt to complete, before
starting the next attempt in parallel.
This is the “Connection Attempt Delay” as defined in RFC 8305.
A
sensible default value recommended by the RFC is 0.25 (250 milliseconds).
If 0 or unspecified,
no reordering is done, and addresses are tried in the order returned by getaddrinfo().
If a positive
integer is specified, the addresses are interleaved by address family, and the given integer is interpreted as
“First Address Family Count” as defined in RFC 8305.
The default is 0 if happy_eyeballs_delay is not
specified, and 1 if it is.
• sock, if given, should be an existing, already connected socket.socket object to be used by the transport.
If sock is given, none of host, port, family, proto, flags, happy_eyeballs_delay, interleave and local_addr should
be specified.
Note:
The sock argument transfers ownership of the socket to the transport created.
To close the socket,
call the transport’s close() method.
The
local_host and local_port are looked up using getaddrinfo(), similarly to host and port.
By default, only a
single Exception is raised: the first exception if there is only one or all errors have same message, or a
single OSError with the error messages combined.
When all_errors is True, an ExceptionGroup
will be raised containing all exceptions (even if there is only one).
Changed in version 3.5: Added support for SSL/TLS in ProactorEventLoop.
The socket option socket.
TCP_NODELAY is set by default for all TCP connections.
Added the ssl_handshake_timeout parameter.
Added the happy_eyeballs_delay and interleave parameters.
Success with Dual-Stack Hosts.
When a server’s IPv4 path and protocol are working,
but the server’s IPv6 path and protocol are not working, a dual-stack client application experiences significant
connection delay compared to an IPv4-only client.
This is undesirable because it causes the dual-stack client to
have a worse user experience.
This document specifies requirements for algorithms that reduce this user-visible
delay and provides an algorithm.
For more information: https://datatracker.ietf.org/doc/html/rfc6555
18.1.
Changed in version 3.12: all_errors was added.
See also:
The open_connection() function is a high-level alternative API.
It returns a pair of (StreamReader,
StreamWriter) that can be used directly in async/await code.
The socket family can be either AF_INET, AF_INET6, or AF_UNIX, depending on host (or the family argument,
if provided).
The socket type will be SOCK_DGRAM.
A tuple of (transport, protocol) is returned on success.
Other arguments:
• local_addr, if given, is a (local_host, local_port) tuple used to bind the socket locally.
The
local_host and local_port are looked up using getaddrinfo().
The remote_host and remote_port are looked up using getaddrinfo().
If given, these should all be integers from the corresponding socket
module constants.
This option is not supported on Windows and
some Unixes.
If the socket.
SO_REUSEPORT constant is not defined then this capability is unsupported.
If specified, local_addr and remote_addr should be omitted (must be None).
Note:
The sock argument transfers ownership of the socket to the transport created.
To close the socket,
call the transport’s close() method.
See UDP echo client protocol and UDP echo server protocol examples.
The family, proto, flags, reuse_address, reuse_port, allow_broadcast, and sock parameters
were added.
Changed in version 3.8.1: The reuse_address parameter is no longer supported, as using socket.
SO_REUSEADDR
poses a significant security concern for UDP.
Explicitly passing reuse_address=True will raise an exception.
When multiple processes with differing UIDs assign sockets to an identical UDP socket address with
SO_REUSEADDR, incoming packets can become randomly distributed among the sockets.
For supported platforms, reuse_port can be used as a replacement for similar functionality.
With reuse_port,
socket.
SO_REUSEPORT is used instead, which specifically prevents processes with differing UIDs from assign-
ing sockets to the same socket address.
Changed in version 3.8: Added support for Windows.
The socket family will be AF_UNIX; socket type will be SOCK_STREAM.
A tuple of (transport, protocol) is returned on success.
Abstract Unix
sockets, str, bytes, and Path paths are supported.
See the documentation of the loop.create_connection() method for information about arguments to this
method.
Availability: Unix.
Changed in version 3.7: Added the ssl_handshake_timeout parameter.
The path parameter can now be a path-like
object.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
Creating network servers
coroutine loop.create_server(protocol_factory, host=None, port=None, *, family=socket.
AF_UNSPEC,
flags=socket.
AI_PASSIVE, sock=None, backlog=100, ssl=None,
reuse_address=None, reuse_port=None, ssl_handshake_timeout=None,
ssl_shutdown_timeout=None, start_serving=True)
Create a TCP server (socket type SOCK_STREAM) listening on port of the host address.
Returns a Server object.
Arguments:
• protocol_factory must be a callable returning a protocol implementation.
The host parameter can be set to several types which determine where the server would be listening:
– If host is a string, the TCP server is bound to a single network interface specified by host.
The port parameter can be set to specify which port the server should listen on.
If 0 or None (the default),
a random unused port will be selected (note that if host resolves to multiple network interfaces, a different
random port will be selected for each interface).
AF_INET or AF_INET6 to force the socket to use IPv4 or IPv6.
If
not set, the family will be determined from host name (defaults to AF_UNSPEC).
If specified, host and port must
not be specified.
To close the socket, call
the server’s close() method.
If not specified will automatically be set to True on Unix.
This option is not supported on Windows.
When set to False, the user should await on Server.start_serving() or Server.
Changed in version 3.5: Added support for SSL/TLS in ProactorEventLoop.
Changed in version 3.5.1: The host parameter can be a sequence of strings.
Changed in version 3.6:
Added ssl_handshake_timeout and start_serving parameters.
The socket option
socket.
TCP_NODELAY is set by default for all TCP connections.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
See also:
The start_server() function is a higher-level alternative API that returns a pair of StreamReader and
StreamWriter that can be used in an async/await code.
Abstract Unix
sockets, str, bytes, and Path paths are supported.
See the documentation of the loop.create_server() method for information about arguments to this
method.
Availability: Unix.
Changed in version 3.7: Added the ssl_handshake_timeout and start_serving parameters.
The path parameter can
now be a Path object.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
This method can be used by servers that accept connections outside of asyncio but that use asyncio to handle them.
Note:
The sock argument transfers ownership of the socket to the transport created.
To close the socket,
call the transport’s close() method.
Returns a (transport, protocol) pair.
New in version 3.5.3.
Added the ssl_handshake_timeout parameter.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
Transferring files
coroutine loop.sendfile(transport, file, offset=0, count=None, *, fallback=True)
Send a file over a transport.
Return the total number of bytes sent.
The method uses high-performance os.sendfile() if available.
file must be a regular file object opened in binary mode.
If specified, count is the total number of bytes to transmit as opposed
to sending the file until EOF is reached.
File position is always updated, even when this method raises an error, and
file.tell() can be used to obtain the actual number of bytes sent.
Raise SendfileNotAvailableError if the system does not support the sendfile syscall and fallback is
False.
New in version 3.7.
TLS Upgrade
coroutine loop.start_tls(transport, protocol, sslcontext, *, server_side=False, server_hostname=None,
ssl_handshake_timeout=None, ssl_shutdown_timeout=None)
Upgrade an existing transport-based connection to TLS.
Create a TLS coder/decoder instance and insert it between the transport and the protocol.
The coder/decoder
implements both transport-facing protocol and protocol-facing transport.
Return the created two-interface instance.
After await, the protocol must stop using the original transport and
communicate with the returned object only because the coder caches protocol-side data and sporadically exchanges
extra TLS session packets with transport.
New in version 3.7.
Changed in version 3.11: Added the ssl_shutdown_timeout parameter.
Watching file descriptors
loop.add_reader(fd, callback, *args)
Start monitoring the fd file descriptor for read availability and invoke callback with the specified arguments once
fd is available for reading.
Returns True if fd was previously being monitored for
reads.
Use functools.partial() to pass keyword arguments to callback.
loop.remove_writer(fd)
Stop monitoring the fd file descriptor for write availability.
Returns True if fd was previously being monitored
for writes.
See also Platform Support section for some limitations of these methods.
Working with socket objects directly
In general, protocol implementations that use transport-based APIs such as loop.create_connection() and
loop.create_server() are faster than implementations that work with sockets directly.
However, there are some
use cases when performance is not critical, and working with socket objects directly is more convenient.
Asynchronous version of socket.recv().
Return the received data as a bytes object.
Since Python 3.7 this is an async def method.
Modeled after the blocking socket.recv_into() method.
Return the number of bytes written to the buffer.
New in version 3.7.
coroutine loop.sock_recvfrom(sock, bufsize)
Receive a datagram of up to bufsize from sock.
Asynchronous version of socket.recvfrom().
Return a tuple of (received data, remote address).
New in version 3.11.
Asynchronous version of socket.recvfrom_into().
Return a tuple of (number of bytes received, remote address).
New in version 3.11.
Asynchronous version of socket.sendall().
This method continues to send to the socket until either all data in data has been sent or an error occurs.
None is
returned on success.
On error, an exception is raised.
Additionally, there is no way to determine how much data,
if any, was successfully processed by the receiving end of the connection.
Changed in version 3.7: Even though the method was always documented as a coroutine method, before Python
3.7 it returned a Future.
Since Python 3.7, this is an async def method.
Asynchronous version of socket.sendto().
Return the number of bytes sent.
New in version 3.11.
Asynchronous version of socket.connect().
Changed in version 3.5.2: address no longer needs to be resolved.
If not, loop.getaddrinfo() will be used
to resolve the address.
See also:
loop.create_connection() and asyncio.open_connection().
Modeled after the blocking socket.accept() method.
The socket must be bound to an address and listening for connections.
The return value is a pair (conn,
address) where conn is a new socket object usable to send and receive data on the connection, and address
is the address bound to the socket on the other end of the connection.
Changed in version 3.7: Even though the method was always documented as a coroutine method, before Python
3.7 it returned a Future.
Since Python 3.7, this is an async def method.
See also:
loop.create_server() and start_server().
Return the total number of bytes sent.
Asynchronous version of socket.sendfile().
If specified, count is the total number of bytes to transmit as opposed
to sending the file until EOF is reached.
File position is always updated, even when this method raises an error, and
file.tell() can be used to obtain the actual number of bytes sent.
Raise SendfileNotAvailableError if the system does not support sendfile syscall and fallback is False.
Asynchronous version of socket.getaddrinfo().
Changed in version 3.7: Both getaddrinfo and getnameinfo methods were always documented to return a coroutine, but
prior to Python 3.7 they were, in fact, returning asyncio.
Future objects.
Starting with Python 3.7 both methods are
coroutines.
Return pair (transport, protocol), where transport supports the ReadTransport interface and pro-
tocol is an object instantiated by the protocol_factory.
With SelectorEventLoop event loop, the pipe is set to non-blocking mode.
Return pair (transport, protocol), where transport supports WriteTransport interface and protocol
is an object instantiated by the protocol_factory.
With SelectorEventLoop event loop, the pipe is set to non-blocking mode.
Note:
SelectorEventLoop does not support the above methods on Windows.
Use ProactorEventLoop
instead for Windows.
See also:
The loop.subprocess_exec() and loop.subprocess_shell() methods.
Set callback as the handler for the signum signal.
The callback will be invoked by loop, along with other queued callbacks and runnable coroutines of that event loop.
Unlike signal handlers registered using signal.signal(), a callback registered with this function is allowed
to interact with the event loop.
Raise ValueError if the signal number is invalid or uncatchable.
Raise RuntimeError if there is a problem
setting up the handler.
Use functools.partial() to pass keyword arguments to callback.
Like signal.signal(), this function must be invoked in the main thread.
Return True if the signal handler was removed, or False if no handler was set for the given signal.
Availability: Unix.
See also:
The signal module.
The executor argument should be an concurrent.futures.
Executor instance.
The default executor is
used if executor is None.
Example:
import asyncio
import concurrent.futures

# File operations (such as logging) can block the
# event loop: run them in a thread pool.
Run in a custom thread pool:
with concurrent.futures.
Run in a custom process pool:
with concurrent.futures.
See Safe importing of main
module.
This method returns a asyncio.
Future object.
Use functools.partial() to pass keyword arguments to func.
Changed in version 3.11: executor must be an instance of ThreadPoolExecutor.
Error Handling API
Allows customizing how exceptions are handled in the event loop.
If handler is None, the default exception handler will be set.
Otherwise, handler must be a callable with the
signature matching (loop, context), where loop is a reference to the active event loop, and context is
a dict object containing the details of the exception (see call_exception_handler() documentation for
details about context).
If the handler is called on behalf of a Task or Handle, it is run in the contextvars.
Context of that task
or callback handle.
Changed in version 3.12: The handler may be called in the Context of the task or handle where the exception
originated.
New in version 3.5.2.
This is called when an exception occurs and no exception handler is set.
This can be called by a custom exception
handler that wants to defer to the default handler behavior.
Future instance;
• ‘task’ (optional): asyncio.
Task instance;
• ‘handle’ (optional): asyncio.
Protocol instance;
• ‘transport’ (optional): Transport instance;
• ‘socket’ (optional): socket.socket instance;
• ‘asyncgen’ (optional): Asynchronous generator that caused the exception.
For custom exception handling, use the
set_exception_handler() method.
Enabling debug mode
loop.get_debug()
Get the debug mode (bool) of the event loop.
The default value is True if the environment variable PYTHONASYNCIODEBUG is set to a non-empty string,
False otherwise.
The new Python Development Mode can now also be used to enable the debug mode.
When debug
mode is enabled, “slow” callbacks are logged.
Default value is 100 milliseconds.
See also:
The debug mode of asyncio.
Running Subprocesses
Methods described in this subsections are low-level.
In regular async/await code consider using the high-level asyncio.
Note:
On Windows,
the default event loop ProactorEventLoop supports subprocesses,
whereas
SelectorEventLoop does not.
See Subprocess Support on Windows for details.
PIPE, stdout=subprocess.
PIPE,
stderr=subprocess.
PIPE, **kwargs)
Create a subprocess from one or more string arguments specified by args.
The first string specifies the program executable, and the remaining strings specify the arguments.
Together, string
arguments form the argv of the program.
This is similar to the standard library subprocess.
Popen class called with shell=False and the list of
strings passed as the first argument; however, where Popen takes a single argument which is list of strings, sub-
process_exec takes multiple string arguments.
The protocol_factory must be a callable returning a subclass of the asyncio.
SubprocessProtocol class.
PIPE constant (default) which will create a new pipe and connect it,
– the value None which will make the subprocess inherit the file descriptor from this process
– the subprocess.
DEVNULL constant which indicates that the special os.devnull file will be used
• stdout can be any of these:
– a file-like object
– the subprocess.
PIPE constant (default) which will create a new pipe and connect it,
– the value None which will make the subprocess inherit the file descriptor from this process
– the subprocess.
DEVNULL constant which indicates that the special os.devnull file will be used
• stderr can be any of these:
– a file-like object
– the subprocess.
PIPE constant (default) which will create a new pipe and connect it,
– the value None which will make the subprocess inherit the file descriptor from this process
– the subprocess.
DEVNULL constant which indicates that the special os.devnull file will be used
– the subprocess.
All other keyword arguments are passed to subprocess.
Popen without interpretation, except for bufsize,
universal_newlines, shell, text, encoding and errors, which should not be specified at all.
The asyncio subprocess API does not support decoding the streams as text.
If a file-like object passed as stdin, stdout or stderr represents a pipe, then the other side of this pipe should be
registered with connect_write_pipe() or connect_read_pipe() for use with the event loop.
See the constructor of the subprocess.
Popen class for documentation on other arguments.
Returns
a
pair
of
(transport, protocol),
where
transport
conforms
to
the
asyncio.
SubprocessTransport base class and protocol is an object instantiated by the protocol_factory.
PIPE,
stdout=subprocess.
PIPE, stderr=subprocess.
PIPE, **kwargs)
Create a subprocess from cmd, which can be a str or a bytes string encoded to the filesystem encoding, using
the platform’s “shell” syntax.
This is similar to the standard library subprocess.
Popen class called with shell=True.
The protocol_factory must be a callable returning a subclass of the SubprocessProtocol class.
See subprocess_exec() for more details about the remaining arguments.
Returns a pair of (transport, protocol), where transport conforms to the SubprocessTransport
base class and protocol is an object instantiated by the protocol_factory.
The shlex.quote() function can be used to properly escape whitespace and
special characters in strings that are going to be used to construct shell commands.
Callback Handles
class asyncio.
Handle
A callback wrapper object returned by loop.call_soon(), loop.call_soon_threadsafe().
Context object associated with the handle.
New in version 3.12.
cancel()
Cancel the callback.
If the callback has already been canceled or executed, this method has no effect.
cancelled()
Return True if the callback was cancelled.
New in version 3.7.
class asyncio.
TimerHandle
A callback wrapper object returned by loop.call_later(), and loop.call_at().
This class is a subclass of Handle.
The time is an absolute timestamp, using the same time reference as loop.time().
New in version 3.7.
Server Objects
Server
objects
are
created
by
loop.create_server(),
loop.create_unix_server(),
start_server(), and start_unix_server() functions.
Do not instantiate the Server class directly.
Server
Server objects are asynchronous context managers.
When used in an async with statement, it’s guaranteed that
the Server object is closed and not accepting new connections when the async with statement is completed:
srv = await loop.create_server(...)
async with srv:
# some code
# At this point, srv is closed and no longer accepts new connections.
Changed in version 3.7: Server object is an asynchronous context manager since Python 3.7.
This class was exposed publicly as asyncio.
The sockets that represent existing incoming client connections are left open.
The server is closed asynchronously; use the wait_closed() coroutine to wait until the server is closed
(and no more connections are active).
New in version 3.7.
coroutine start_serving()
Start accepting connections.
This method is idempotent, so it can be called when the server is already serving.
The
start_serving
keyword-only
parameter
to
loop.create_server()
and
asyncio.
start_server() allows creating a Server object that is not accepting connections initially.
In
this case Server.start_serving(), or Server.serve_forever() can be used to make the
Server start accepting connections.
New in version 3.7.
coroutine serve_forever()
Start accepting connections until the coroutine is cancelled.
Cancellation of serve_forever task causes
the server to be closed.
This method can be called if the server is already accepting connections.
Only one serve_forever task
can exist per one Server object.
Example:
async def client_connected(reader, writer):
# Communicate with the client with
# reader/writer streams.
New in version 3.7.
New in version 3.7.
coroutine wait_closed()
Wait until the close() method completes and all active connections have finished.
TransportSocket, which the server is listening on.
Changed in version 3.7: Prior to Python 3.7 Server.sockets used to return an internal list of server
sockets directly.
In 3.7 a copy of that list is returned.
By default asyncio is configured to use SelectorEventLoop on Unix and ProactorEventLoop on Windows.
An event loop based on the selectors module.
Uses the most efficient selector available for the given platform.
Availability: Unix, Windows.
An event loop for Windows that uses “I/O Completion Ports” (IOCP).
Availability: Windows.
See also:
MSDN documentation on I/O Completion Ports.
AbstractEventLoop
Abstract base class for asyncio-compliant event loops.
The Event Loop Methods section lists all methods that an alternative implementation of AbstractEventLoop
should have defined.
Examples
Note that all examples in this section purposefully show how to use the low-level event loop APIs, such as loop.
Modern asyncio applications rarely need to be written this way; con-
sider using the high-level functions like asyncio.run().
An example using the loop.call_soon() method to schedule a callback.
An example of a callback displaying the current date every second.
A similar example using transports, protocols, and the loop.create_connection() method.
Future Functions
asyncio.isfuture(obj)
Return True if obj is either of:
• an instance of asyncio.
Future,
• an instance of asyncio.
Task,
• a Future-like object with a _asyncio_future_blocking attribute.
New in version 3.5.
If obj is neither of the above a TypeError is raised.
Important: See also the create_task() function which is the preferred way for creating new Tasks.
Save a reference to the result of this function, to avoid a task disappearing mid-execution.
Changed in version 3.5.1: The function accepts any awaitable object.
Future object in a asyncio.
Future object.
Deprecation warning is emitted if future is not a Future-like object and loop is not
specified and there is no running event loop.
Future Object
class asyncio.
A Future represents an eventual result of an asynchronous operation.
Not thread-safe.
Future is an awaitable object.
Coroutines can await on Future objects until they either have a result or an exception
set, or until they are cancelled.
A Future can be awaited multiple times and the result is same.
Typically Futures are used to enable low-level callback-based code (e.g. in protocols implemented using asyncio
transports) to interoperate with high-level async/await code.
The rule of thumb is to never expose Future objects in user-facing APIs, and the recommended way to create a
Future object is to call loop.create_future().
This way alternative event loop implementations can inject
their own optimized implementations of a Future object.
Changed in version 3.7: Added support for the contextvars module.
Deprecation warning is emitted if loop is not specified and there is no running event
loop.
If the Future is done and has a result set by the set_result() method, the result value is returned.
If the Future is done and has an exception set by the set_exception() method, this method raises the
exception.
If the Future has been cancelled, this method raises a CancelledError exception.
If the Future’s result isn’t yet available, this method raises a InvalidStateError exception.
set_result(result)
Mark the Future as done and set its result.
Raises a InvalidStateError error if the Future is already done.
Mark the Future as done and set an exception.
Raises a InvalidStateError error if the Future is already done.
Return True if the Future was cancelled.
The callback is called with the Future object as its only argument.
If the Future is already done when this method is called, the callback is scheduled with loop.
An optional keyword-only context argument allows specifying a custom contextvars.
Context for the
callback to run in.
The current context is used when no context is provided.
See PEP 567 for more details.
remove_done_callback(callback)
Remove callback from the callbacks list.
Returns the number of callbacks removed, which is typically 1, unless a callback was added more than once.
If the Future is already done or cancelled, return False.
Otherwise, change the Future’s state to cancelled,
schedule the callbacks, and return True.
Added the msg parameter.
The exception (or None if no exception was set) is returned only if the Future is done.
If the Future has been cancelled, this method raises a CancelledError exception.
If the Future isn’t done yet, this method raises an InvalidStateError exception.
New in version 3.7.
This example creates a Future object, creates and schedules an asynchronous Task to set result for the Future, and waits
until the Future has a result:
async def set_after(fut, delay, value):
# Sleep for *delay* seconds.
Important: The Future object was designed to mimic concurrent.futures.
Future.
Key differences include:
• unlike asyncio Futures, concurrent.futures.
Future instances cannot be awaited.
Future.add_done_callback() are not called immediately.
They
are scheduled with loop.call_soon() instead.
Future.cancel() does not.
18.1.10 Transports and Protocols
Preface
Transports and Protocols are used by the low-level event loop APIs such as loop.create_connection().
They
use callback-based programming style and enable high-performance implementations of network or IPC protocols (e.g.
HTTP).
Essentially, transports and protocols should only be used in libraries and frameworks and never in high-level asyncio
applications.
A different way of saying the same thing: a transport is an abstraction for a socket (or similar I/O endpoint) while a
protocol is an abstraction for an application, from the transport’s point of view.
Yet another view is the transport and protocol interfaces together define an abstract interface for using network I/O and
interprocess I/O.
There is always a 1:1 relationship between transport and protocol objects: the protocol calls transport methods to send
data, while the transport calls protocol methods to pass it data that has been received.
Most of connection oriented event loop methods (such as loop.create_connection()) usually accept a proto-
col_factory argument used to create a Protocol object for an accepted connection, represented by a Transport object.
Such methods usually return a tuple of (transport, protocol).
The Transports section documents asyncio BaseTransport, ReadTransport, WriteTransport,
Transport, DatagramTransport, and SubprocessTransport classes.
• The
Protocols
section
documents
asyncio
BaseProtocol,
Protocol,
BufferedProtocol,
DatagramProtocol, and SubprocessProtocol classes.
The Examples section showcases how to work with transports, protocols, and low-level event loop APIs.
Transports
Source code: Lib/asyncio/transports.py
Transports are classes provided by asyncio in order to abstract various kinds of communication channels.
Transport objects are always instantiated by an asyncio event loop.
The methods available on a transport depend
on the transport’s kind.
The transport classes are not thread safe.
Transports Hierarchy
class asyncio.
BaseTransport
Base class for all transports.
Contains methods that all asyncio transports share.
A base transport for write-only connections.
Instances of the WriteTransport class are returned from the loop.connect_write_pipe() event loop
method and are also used by subprocess-related methods like loop.subprocess_exec().
A base transport for read-only connections.
Instances of the ReadTransport class are returned from the loop.connect_read_pipe() event loop method
and are also used by subprocess-related methods like loop.subprocess_exec().
Interface representing a bidirectional transport, such as a TCP connection.
The user does not instantiate a transport directly; they call a utility function, passing it a protocol factory and other
information necessary to create the transport and protocol.
Instances of the Transport
class are returned from or used by event loop methods like loop.
A transport for datagram (UDP) connections.
Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint()
event loop method.
An abstraction to represent a connection between a parent and its child OS process.
Instances
of
the
SubprocessTransport
class
are
returned
from
event
loop
methods
loop.
Base Transport
BaseTransport.close()
Close the transport.
If the transport has a buffer for outgoing data, buffered data will be flushed asynchronously.
No more data will be
received.
After all buffered data is flushed, the protocol’s protocol.connection_lost() method will be
called with None as its argument.
The transport should not be used once it is closed.
BaseTransport.is_closing()
Return True if the transport is closing or is closed.
BaseTransport.get_extra_info(name, default=None)
Return information about the transport or underlying resources it uses.
SSLContext instance
– 'ssl_object': ssl.SSLObject or ssl.
SSLSocket instance
• pipe:
– 'pipe': pipe object
• subprocess:
– 'subprocess': subprocess.
Popen instance
BaseTransport.set_protocol(protocol)
Set a new protocol.
Switching protocol should only be done when both protocols are documented to support the switch.
BaseTransport.get_protocol()
Return the current protocol.
Read-only Transports
ReadTransport.is_reading()
Return True if the transport is receiving new data.
New in version 3.7.
Pause the receiving end of the transport.
No data will be passed to the protocol’s protocol.
The method is idempotent, i.e. it can be called when the transport is already paused or
closed.
ReadTransport.resume_reading()
Resume the receiving end.
The protocol’s protocol.data_received() method will be called once again
if some data is available for reading.
The method is idempotent, i.e. it can be called when the transport is already reading.
Buffered data will be lost.
No more data will be received.
The protocol’s protocol.connection_lost() method will eventually be
called with None as its argument.
WriteTransport.can_write_eof()
Return True if the transport supports write_eof(), False if not.
WriteTransport.get_write_buffer_size()
Return the current size of the output buffer used by the transport.
WriteTransport.get_write_buffer_limits()
Get the high and low watermarks for write flow control.
Return a tuple (low, high) where low and high are
positive number of bytes.
Use set_write_buffer_limits() to set the limits.
New in version 3.4.2.
WriteTransport.set_write_buffer_limits(high=None, low=None)
Set the high and low watermarks for write flow control.
These two values (measured in number of bytes) control when the protocol’s protocol.pause_writing()
and protocol.resume_writing() methods are called.
If specified, the low watermark must be less than
or equal to the high watermark.
Neither high nor low can be negative.
If writing has
been paused, resume_writing() is called when the buffer size becomes less than or equal to the low value.
The defaults are implementation-specific.
If only the high watermark is given, the low watermark defaults to an
implementation-specific value less than or equal to the high watermark.
Setting high to zero forces low to zero as
well, and causes pause_writing() to be called whenever the buffer becomes non-empty.
Setting low to zero
causes resume_writing() to be called only once the buffer is empty.
Use of zero for either limit is generally
sub-optimal as it reduces opportunities for doing I/O and computation concurrently.
Use get_write_buffer_limits() to get the limits.
WriteTransport.write(data)
Write some data bytes to the transport.
This method does not block; it buffers the data and arranges for it to be sent out asynchronously.
WriteTransport.writelines(list_of_data)
Write a list (or any iterable) of data bytes to the transport.
This is functionally equivalent to calling write() on
each element yielded by the iterable, but may be implemented more efficiently.
WriteTransport.write_eof()
Close the write end of the transport after flushing all buffered data.
Data may still be received.
If addr is None, the
data is sent to the target address given on transport creation.
This method does not block; it buffers the data and arranges for it to be sent out asynchronously.
DatagramTransport.abort()
Close the transport immediately, without waiting for pending operations to complete.
Buffered data will be lost.
No more data will be received.
The protocol’s protocol.connection_lost() method will eventually be
called with None as its argument.
Subprocess Transports
SubprocessTransport.get_pid()
Return the subprocess process id as an integer.
SubprocessTransport.get_pipe_transport(fd)
Return the transport for the communication pipe corresponding to the integer file descriptor fd:
• 0: readable streaming transport of the standard input (stdin), or None if the subprocess was not created with
stdin=PIPE
• 1: writable streaming transport of the standard output (stdout), or None if the subprocess was not created
with stdout=PIPE
• 2: writable streaming transport of the standard error (stderr), or None if the subprocess was not created with
stderr=PIPE
• other fd: None
SubprocessTransport.get_returncode()
Return the subprocess return code as an integer or None if it hasn’t returned, which is similar to the
subprocess.Popen.returncode attribute.
SubprocessTransport.kill()
Kill the subprocess.
On POSIX systems, the function sends SIGKILL to the subprocess.
On Windows, this method is an alias for
terminate().
See also subprocess.Popen.kill().
SubprocessTransport.send_signal(signal)
Send the signal number to the subprocess, as in subprocess.
Popen.send_signal().
SubprocessTransport.terminate()
Stop the subprocess.
On POSIX systems, this method sends SIGTERM to the subprocess.
On Windows, the Windows API function
TerminateProcess() is called to stop the subprocess.
See also subprocess.Popen.terminate().
SubprocessTransport.close()
Kill the subprocess by calling the kill() method.
If the subprocess hasn’t returned yet, and close transports of stdin, stdout, and stderr pipes.
Those classes are
meant to be used together with transports.
Subclasses of abstract base protocol classes may implement some or all methods.
All these methods are callbacks: they
are called by transports on certain events, for example when some data is received.
A base protocol method should be
called by the corresponding transport.
Base Protocols
class asyncio.
BaseProtocol
Base protocol with methods that all protocols share.
The base class for implementing streaming protocols (TCP, Unix sockets, etc).
BufferedProtocol(BaseProtocol)
A base class for implementing streaming protocols with manual control of the receive buffer.
The base class for implementing datagram (UDP) protocols.
The base class for implementing protocols communicating with child processes (unidirectional pipes).
Base Protocol
All asyncio protocols can implement Base Protocol callbacks.
Connection Callbacks
Connection callbacks are called on all protocols, exactly once per a successful connection.
All other protocol callbacks
can only be called between those two methods.
BaseProtocol.connection_made(transport)
Called when a connection is made.
The transport argument is the transport representing the connection.
The protocol is responsible for storing the
reference to its transport.
BaseProtocol.connection_lost(exc)
Called when the connection is lost or closed.
The argument is either an exception object or None.
See the documentation of the set_write_buffer_limits() method for more details.
BaseProtocol.pause_writing()
Called when the transport’s buffer goes over the high watermark.
BaseProtocol.resume_writing()
Called when the transport’s buffer drains below the low watermark.
If the buffer size equals the high watermark, pause_writing() is not called: the buffer size must go strictly over.
Conversely, resume_writing() is called when the buffer size is equal or lower than the low watermark.
These end
conditions are important to ensure that things go as expected when either mark is zero.
Streaming Protocols
Event
methods,
such
as
loop.create_server(),
loop.create_unix_server(),
loop.create_connection(),
loop.create_unix_connection(),
loop.
connect_accepted_socket(), loop.connect_read_pipe(), and loop.connect_write_pipe()
accept factories that return streaming protocols.
Protocol.data_received(data)
Called when some data is received.
Whether the data is buffered, chunked or reassembled depends on the transport.
In general, you shouldn’t rely on
specific semantics and instead make your parsing generic and flexible.
However, data is always received in the
correct order.
The method can be called an arbitrary number of times while a connection is open.
However, protocol.eof_received() is called at most once.
Once eof_received() is called,
data_received() is not called anymore.
Protocol.eof_received()
Called when the other end signals it won’t send any more data (for example by calling transport.
This method may return a false value (including None), in which case the transport will close itself.
Conversely,
if this method returns a true value, the protocol used determines whether to close the transport.
Since the default
implementation returns None, it implicitly closes the connection.
Some transports, including SSL, don’t support half-closed connections, in which case returning true from this
method will result in the connection being closed.
State machine:
start -> connection_made
[-> data_received]*
[-> eof_received]?
-> connection_lost -> end
18.1.
Buffered Protocols can be used with any event loop method that supports Streaming Protocols.
BufferedProtocol implementations allow explicit manual allocation and control of the receive buffer.
Event loops
can then use the buffer provided by the protocol to avoid unnecessary data copies.
This can result in noticeable perfor-
mance improvement for protocols that receive big amounts of data.
Sophisticated protocol implementations can signifi-
cantly reduce the number of buffer allocations.
The following callbacks are called on BufferedProtocol instances:
BufferedProtocol.get_buffer(sizehint)
Called to allocate a new receive buffer.
It is acceptable to return smaller or larger
buffers than what sizehint suggests.
When set to -1, the buffer size can be arbitrary.
It is an error to return a buffer
with a zero size.
BufferedProtocol.eof_received()
See the documentation of the protocol.eof_received() method.
However, protocol.
eof_received() is called at most once and, if called, get_buffer() and buffer_updated() won’t be called
after it.
State machine:
start -> connection_made
[-> get_buffer
[-> buffer_updated]?
]*
[-> eof_received]?
-> connection_lost -> end
Datagram Protocols
Datagram
Protocol
instances
should
be
constructed
by
protocol
factories
passed
to
the
loop.
create_datagram_endpoint() method.
DatagramProtocol.datagram_received(data, addr)
Called when a datagram is received.
This method is called in rare conditions, when the transport (e.g. UDP) detects that a datagram could not be
delivered to its recipient.
The socket always appears ‘ready’ and excess packets are dropped.
An OSError with errno set to errno.
ENOBUFS
may or may not be raised; if it is raised, it will be reported to DatagramProtocol.error_received() but
otherwise ignored.
Subprocess Protocols
Subprocess Protocol instances should be constructed by protocol factories passed to the loop.subprocess_exec()
and loop.subprocess_shell() methods.
SubprocessProtocol.pipe_data_received(fd, data)
Called when the child process writes data into its stdout or stderr pipe.
SubprocessProtocol.pipe_connection_lost(fd, exc)
Called when one of the pipes communicating with the child process is closed.
SubprocessProtocol.process_exited()
Called when the child process has exited.
It can be called before pipe_data_received() and pipe_connection_lost() methods.
Examples
TCP Echo Server
Create a TCP echo server using the loop.create_server() method, send back received data, and close the con-
nection:
import asyncio
class EchoServerProtocol(asyncio.
See also:
The TCP echo server using streams example uses the high-level asyncio.start_server() function.
TCP Echo Client
A TCP echo client using the loop.create_connection() method, sends data, and waits until the connection is
closed:
import asyncio
class EchoClientProtocol(asyncio.
See also:
The TCP echo client using streams example uses the high-level asyncio.open_connection() function.
One protocol instance will be created to serve all
# client requests.
We are done: close the transport;
# connection_lost() will be called automatically.
See also:
The watch a file descriptor for read events example uses the low-level loop.add_reader() method to register an
FD.
The register an open socket to wait for data using streams example uses high-level streams created by the
18.1.
The subprocess is created by the loop.subprocess_exec() method:
import asyncio
import sys
class DateProtocol(asyncio.
SubprocessProtocol):

self.output = bytearray()
self.pipe_closed = False
self.exited = False

self.check_for_exit()


# process_exited() method can be called before
# pipe_connection_lost() method: wait until both methods are
# called.
Future(loop=loop)
# Create the subprocess controlled by DateProtocol;
# redirect the standard output into a pipe.
transport, protocol = await loop.subprocess_exec(
lambda: DateProtocol(exit_future),
sys.executable, '-c', code,
stdin=None, stderr=None)
# Wait for the subprocess exit using the process_exited()
# method of the protocol.
The
default policy can be replaced with built-in alternatives to use different event loop implementations, or substituted by a
custom policy that can override these behaviors.
The policy object gets and sets a separate event loop per context.
This is per-thread by default, though custom policies
could define context differently.
Custom event loop policies can control the behavior of get_event_loop(), set_event_loop(), and
new_event_loop().
Policy objects should implement the APIs defined in the AbstractEventLoopPolicy abstract base class.
The following functions can be used to get and set the policy for the current process:
asyncio.get_event_loop_policy()
Return the current process-wide policy.
If policy is set to None, the default policy is restored.
Policy Objects
The abstract event loop policy base class is defined as follows:
class asyncio.
AbstractEventLoopPolicy
An abstract base class for asyncio policies.
Return an event loop object implementing the AbstractEventLoop interface.
This method should never return None.
Changed in version 3.6.
set_event_loop(loop)
Set the event loop for the current context to loop.
This method should never return None.
get_child_watcher()
Get a child process watcher object.
Return a watcher object implementing the AbstractChildWatcher interface.
This function is Unix specific.
Set the current child process watcher to watcher.
This function is Unix specific.
Deprecated since version 3.12.
The default asyncio policy.
Uses SelectorEventLoop on Unix and ProactorEventLoop on Windows.
There is no need to install the default policy manually.
Changed in version 3.8: On Windows, ProactorEventLoop is now used by default.
Deprecated since version 3.12: The get_event_loop() method of the default asyncio policy now emits a
DeprecationWarning if there is no current event loop set and it decides to create one.
In some future Python
release this will become an error.
An alternative event loop policy that uses the SelectorEventLoop event loop implementation.
Availability: Windows.
An alternative event loop policy that uses the ProactorEventLoop event loop implementation.
Availability: Windows.
Process Watchers
A process watcher allows customization of how an event loop monitors child processes on Unix.
Specifically, the event
loop needs to know when a child process has exited.
In asyncio, child processes are created with create_subprocess_exec() and loop.subprocess_exec()
functions.
See also the Subprocess and Threads section.
Deprecated since version 3.12.
Deprecated since version 3.12.
Note:
Third-party event loops implementations might not support custom child watchers.
For such event loops, using
set_child_watcher() might be prohibited or have no effect.
AbstractChildWatcher
add_child_handler(pid, callback, *args)
Register a new child handler.
Arrange for callback(pid, returncode, *args) to be called when a process with PID equal to
pid terminates.
Specifying another callback for the same process replaces the previous handler.
Removes the handler for process with PID equal to pid.
The function returns True if the handler was successfully removed, False if there was nothing to remove.
If the watcher was previously attached to an event loop, then it is first detached before attaching to the new
loop.
Note: loop may be None.
is_active()
Return True if the watcher is ready to use.
Spawning a subprocess with inactive current child watcher raises RuntimeError.
New in version 3.8.
Close the watcher.
This method has to be called to ensure that underlying resources are cleaned-up.
Deprecated since version 3.12.
ThreadedChildWatcher
This implementation starts a new waiting thread for every subprocess spawn.
It works reliably even when the asyncio event loop is run in a non-main OS thread.
There is no noticeable overhead when handling a big number of children (O(1) each time a child terminates), but
starting a thread per process requires extra memory.
This watcher is used by default.
New in version 3.8.
This implementation registers a SIGCHLD signal handler on instantiation.
That can break third-party code that
installs a custom handler for SIGCHLD signal.
The watcher avoids disrupting other code spawning processes by polling every process explicitly on a SIGCHLD
signal.
There is no limitation for running subprocesses from different threads once the watcher is installed.
The solution is safe but it has a significant overhead when handling a big number of processes (O(n) each time a
SIGCHLD is received).
New in version 3.8.
Deprecated since version 3.12.
This implementation uses active event loop from the main thread to handle SIGCHLD signal.
If the main thread
has no running event loop another thread cannot spawn a subprocess (RuntimeError is raised).
The watcher avoids disrupting other code spawning processes by polling every process explicitly on a SIGCHLD
signal.
This solution is as safe as MultiLoopChildWatcher and has the same O(N) complexity but requires a running
event loop in the main thread to work.
Deprecated since version 3.12.
This implementation reaps every terminated processes by calling os.waitpid(-1) directly, possibly breaking
other code spawning processes and waiting for their termination.
There is no noticeable overhead when handling a big number of children (O(1) each time a child terminates).
This solution requires a running event loop in the main thread to work, as SafeChildWatcher.
Deprecated since version 3.12.
PidfdChildWatcher
This implementation polls process file descriptors (pidfds) to await child process termination.
In some respects,
PidfdChildWatcher is a “Goldilocks” child watcher implementation.
It doesn’t require signals or threads,
doesn’t interfere with any processes launched outside the event loop, and scales linearly with the number of sub-
processes launched by the event loop.
The main disadvantage is that pidfds are specific to Linux, and only work on
recent (5.3+) kernels.
New in version 3.9.
This may be None or an instance of EventLoop.
All Platforms
• loop.add_reader() and loop.add_writer() cannot be used to monitor file I/O.
Windows
Source code: Lib/asyncio/proactor_events.py, Lib/asyncio/windows_events.py, Lib/asyncio/windows_utils.py
Changed in version 3.8: On Windows, ProactorEventLoop is now the default event loop.
All event loops on Windows do not support the following methods:
• loop.create_unix_connection() and loop.create_unix_server() are not supported.
The
socket.
AF_UNIX socket family is specific to Unix.
SelectorEventLoop has the following limitations:
• SelectSelector is used to wait on socket events: it supports sockets and is limited to 512 sockets.
The loop.add_reader() and loop.add_writer() methods are not supported.
The resolution of the monotonic clock on Windows is usually around 15.6 milliseconds.
The best resolution is 0.5 mil-
liseconds.
The resolution depends on the hardware (availability of HPET) and on the Windows configuration.
The policy.set_child_watcher() function is also not supported, as ProactorEventLoop has a different
mechanism to watch child processes.
KqueueSelector, which does not
support character devices on these versions.
The SelectorEventLoop can be manually configured to use
SelectSelector or PollSelector to support character devices on these older versions of macOS.
Example:
import asyncio
import selectors
selector = selectors.
SelectSelector()
loop = asyncio.
Extending
The main direction for asyncio extending is writing custom event loop classes.
Asyncio has helpers that could be used
to simplify this task.
Note:
Third-parties should reuse existing asyncio code with caution, a new Python version is free to break backward
compatibility in internal part of API.
Writing a Custom Event Loop
asyncio.
AbstractEventLoop declares very many methods.
Implementing all them from scratch is a tedious job.
A loop can get many common methods implementation for free by inheriting from asyncio.
BaseEventLoop.
In turn, the successor should implement a bunch of private methods declared but not implemented in asyncio.
BaseEventLoop.
For
example,
loop.create_connection()
checks
arguments,
resolves
DNS
addresses,
and
calls
loop._make_socket_transport()
that
should
be
implemented
by
inherited
class.
Future and asyncio.
Task should be never created directly, please use corresponding loop.
However, third-party event loops may reuse built-in future and task implementations for the sake of getting a complex and
highly optimized code for free.
Task.__init__(coro, *, loop=None, name=None, context=None)
Create a built-in task instance.
The rest of arguments are described in loop.create_task() descrip-
tion.
Changed in version 3.11: context argument is added.
Register a new task as managed by asyncio.
Call the function from a task constructor.
Unregister a task from asyncio internal structures.
The function should be called when a task is about to finish.
Call the function just before executing a portion of embedded coroutine (coroutine.send() or coroutine.
throw()).
asyncio._leave_task(loop, task)
Switch the current task back from task to None.
Call the function just after coroutine.send() or coroutine.throw() execution.
Tasks
Utilities to run asyncio programs, create Tasks, and await on multiple things with timeouts.
Runner
A context manager that simplifies multiple async function
calls.
Task
Task object.
TaskGroup
A context manager that holds a group of tasks.
Provides
a convenient and reliable way to wait for all tasks in the
group to finish.
Useful in cases when wait_for is
not suitable.
Examples
• Using asyncio.gather() to run things in parallel.
Cancellation.
Using asyncio.sleep().
Queues
Queues should be used to distribute work amongst multiple asyncio Tasks, implement connection pools, and pub/sub
patterns.
Queue
A FIFO queue.
PriorityQueue
A priority queue.
LifoQueue
A LIFO queue.
Queue to distribute workload between several Tasks.
Subprocesses
Utilities to spawn subprocesses and run shell commands.
Examples
• Executing a shell command.
Streams
High-level APIs to work with network IO.
StreamReader
High-level async/await object to receive network data.
StreamWriter
High-level async/await object to send network data.
Examples
• Example TCP client.
Synchronization
Threading-like synchronization primitives that can be used in Tasks.
Lock
A mutex lock.
Event
An event object.
Condition
A condition object.
Semaphore
A semaphore.
BoundedSemaphore
A bounded semaphore.
Barrier
A barrier object.
Event.
Barrier.
Exceptions
asyncio.
CancelledError
Raised when a Task is cancelled.
See also Task.
cancel().
asyncio.
BrokenBarrierError
Raised when a Barrier is broken.
See also Barrier.
wait().
Examples
• Handling CancelledError to run code on cancellation request.
The preferred function to get the running event loop.
asyncio.get_event_loop()
Get an event loop instance (running or current via the cur-
rent policy).
Examples
• Using asyncio.get_running_loop().
Event Loop Methods
See also the main documentation section about the Event Loop Methods.
Debugging
loop.set_debug()
Enable or disable the debug mode.
Scheduling Callbacks
loop.call_soon()
Invoke a callback soon.
Thread/Process Pool
await loop.run_in_executor()
Run a CPU-bound or other blocking function in a
concurrent.futures executor.
Tasks and Futures
loop.create_future()
Create a Future object.
Networking and IPC
await loop.create_connection()
Open a TCP connection.
Create a TCP server.
Sockets
await loop.sock_recv()
Receive data from the socket.
Send data to the socket.
Send a file over the socket.
Unix Signals
loop.add_signal_handler()
Add a handler for a signal.
Error Handling
loop.call_exception_handler()
Call the exception handler.
loop.set_exception_handler()
Set a new exception handler.
The default exception handler implementation.
Examples
• Using asyncio.new_event_loop() and loop.run_forever().
Transports
All transports implement the following methods:
transport.close()
Close the transport.
Transports that can receive data (TCP and Unix connections, pipes, etc).
Returned from methods like loop.
Transports that can Send data (TCP and Unix connections, pipes, etc).
Returned from methods like loop.
Transports returned by loop.create_datagram_endpoint():
Datagram Transports
transport.sendto()
Send data to the remote peer.
Low-level transport abstraction over subprocesses.
Returned by loop.subprocess_exec() and loop.
Protocols
Protocol classes can implement the following callback methods:
callback connection_made()
Called when a connection is made.
callback connection_lost()
Called when the connection is lost or closed.
Buffered Streaming Protocols
callback get_buffer()
Called to allocate a new receive buffer.
Datagram Protocols
callback datagram_received()
Called when a datagram is received.
Subprocess Protocols
callback pipe_data_received()
Called when the child process writes data into its stdout
or stderr pipe.
It can
be called before pipe_data_received() and
pipe_connection_lost() methods.
Event Loop Policies
Policies is a low-level mechanism to alter the behavior of functions like asyncio.get_event_loop().
See also
the main policies section for more details.
Accessing Policies
asyncio.get_event_loop_policy()
Return the current process-wide policy.
AbstractEventLoopPolicy
Base class for policy objects.
This page lists common mistakes and traps and explains how to avoid them.
Debug Mode
By default asyncio runs in production mode.
In order to ease the development asyncio has a debug mode.
There are several ways to enable asyncio debug mode:
• Setting the PYTHONASYNCIODEBUG environment variable to 1.
• Using the Python Development Mode.
Passing debug=True to asyncio.run().
Calling loop.set_debug().
In addition to enabling the debug mode, consider also:
• setting the log level of the asyncio logger to logging.
DEBUG, for example the following snippet of code can be
run at startup of the application:
logging.basicConfig(level=logging.
DEBUG)
• configuring the warnings module to display ResourceWarning warnings.
One way of doing that is by using
the -W default command line option.
When the debug mode is enabled:
• asyncio checks for coroutines that were not awaited and logs them; this mitigates the “forgotten await” pitfall.
The execution time of the I/O selector is logged if it takes too long to perform an I/O operation.
The loop.slow_callback_duration attribute
can be used to set the minimum execution duration in seconds that is considered “slow”.
Concurrency and Multithreading
An event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread.
While a Task
is running in the event loop, no other Tasks can run in the same thread.
When a Task executes an await expression, the
running Task gets suspended, and the event loop executes the next Task.
To schedule a callback from another OS thread, the loop.call_soon_threadsafe() method should be used.
Almost all asyncio objects are not thread safe, which is typically not a problem unless there is code that works with
them from outside of a Task or a callback.
To schedule a coroutine object from a different OS thread, the run_coroutine_threadsafe() function should
be used.
It returns a concurrent.futures.
Future to access the result:
async def coro_func():

# Later in another OS thread:
future = asyncio.run_coroutine_threadsafe(coro_func(), loop)
# Wait for the result:
result = future.result()
To handle signals the event loop must be run in the main thread.
The
loop.run_in_executor()
method
can
be
used
with
a
concurrent.futures.
ThreadPoolExecutor to execute blocking code in a different OS thread without blocking the OS thread
that the event loop runs in.
There is currently no way to schedule coroutines or callbacks directly from a different process (such as one started with
multiprocessing).
The Event Loop Methods section lists APIs that can read from pipes and watch file descriptors
without blocking the event loop.
In addition, asyncio’s Subprocess APIs provide a way to start a process and communicate
with it from the event loop.
Lastly, the aforementioned loop.run_in_executor() method can also be used with
a concurrent.futures.
ProcessPoolExecutor to execute code in a different process.
Running Blocking Code
Blocking (CPU-bound) code should not be called directly.
For example, if a function performs a CPU-intensive calcula-
tion for 1 second, all concurrent asyncio Tasks and IO operations would be delayed by 1 second.
An executor can be used to run a task in a different thread or even in a different process to avoid blocking the OS thread
with the event loop.
See the loop.run_in_executor() method for more details.
Logging
asyncio uses the logging module and all logging is performed via the "asyncio" logger.
The default log level is logging.
INFO, which can be easily adjusted:
logging.getLogger("asyncio").setLevel(logging.
Network logging can block the event loop.
It is recommended to use a separate thread for handling logs or use non-
blocking IO.
For example, see blocking-handlers.
The usual fix is to either await the coroutine or call the asyncio.create_task() function:
async def main():
await test()
Detect never-retrieved exceptions
If a Future.set_exception() is called but the Future object is never awaited on, the exception would never be
propagated to the user code.
In this case, asyncio would emit a log message when the Future object is garbage collected.
Exception: not consumed
Note: The source code for asyncio can be found in Lib/asyncio/.
18.2 socket — Low-level networking interface
Source code: Lib/socket.py
This module provides access to the BSD socket interface.
It is available on all modern Unix systems, Windows, MacOS,
and probably additional platforms.
Note: Some behavior may be platform dependent, since calls are made to the operating system socket APIs.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The Python interface is a straightforward transliteration of the Unix system call and library interface for sockets to Python’s
object-oriented style: the socket() function returns a socket object whose methods implement the various socket system
calls.
Parameter types are somewhat higher-level than in the C interface: as with read() and write() operations on
Python files, buffer allocation on receive operations is automatic, and buffer length is implicit on send operations.
See also:
Module socketserver Classes that simplify writing network servers.
Module ssl A TLS/SSL wrapper for socket objects.
The address format required by a particular socket object is automatically selected based on the address family specified
when the socket object was created.
The address of an AF_UNIX socket bound to a file system node is represented as a string, using the file system
encoding and the 'surrogateescape' error handler (see PEP 383).
An address in Linux’s abstract namespace
is returned as a bytes-like object with an initial null byte; note that sockets in this namespace can communicate with
normal file system sockets, so programs intended to run on Linux may need to deal with both types of address.
A
string or bytes-like object can be used for either type of address when passing it as an argument.
Changed in version 3.3: Previously, AF_UNIX socket paths were assumed to use UTF-8 encoding.
Writable bytes-like object is now accepted.
A pair (host, port) is used for the AF_INET address family, where host is a string representing either a
hostname in internet domain notation like 'daring.cwi.nl' or an IPv4 address like '100.50.200.5',
and port is an integer.
This behavior is not compatible with IPv6, therefore, you may want to avoid these if you intend to support
IPv6 with your Python programs.
For AF_INET6 address family, a four-tuple (host, port, flowinfo, scope_id) is used, where
flowinfo and scope_id represent the sin6_flowinfo and sin6_scope_id members in struct
sockaddr_in6 in C. For socket module methods, flowinfo and scope_id can be omitted just for backward
compatibility.
Note, however, omission of scope_id can cause problems in manipulating scoped IPv6 addresses.
Changed in version 3.7: For multicast addresses (with scope_id meaningful) address may not contain %scope_id
(or zone id) part.
This information is superfluous and may be safely omitted (recommended).
TIPC is an open, non-IP based
networked protocol designed for use in clustered computer environments.
Addresses are represented by a tuple, and
the fields depend on the address type.
If addr_type is TIPC_ADDR_NAMESEQ, then v1 is the server type, v2 is the lower port number, and v3 is
the upper port number.
If addr_type is TIPC_ADDR_ID, then v1 is the node, v2 is the reference, and v3 should be set to 0.
A tuple (interface, ) is used for the AF_CAN address family, where interface is a string representing a
network interface name like 'can0'.
The network interface name '' can be used to receive packets from all
network interfaces of this family.
The string is the name of a kernel control using a dynamically assigned ID.
The tuple can be used if ID and unit
number of the kernel control are known or if a registered ID is used.
New in version 3.3.
• AF_BLUETOOTH supports the following protocols and address formats:
– BTPROTO_L2CAP accepts (bdaddr, psm) where bdaddr is the Bluetooth address as a string and psm
is an integer.
– BTPROTO_RFCOMM accepts (bdaddr, channel) where bdaddr is the Bluetooth address as a string
and channel is an integer.
– BTPROTO_HCI accepts (device_id,) where device_id is either an integer or a string with the Blue-
tooth address of the interface.
Changed in version 3.2: NetBSD and DragonFlyBSD support added.
– BTPROTO_SCO accepts bdaddr where bdaddr is a bytes object containing the Bluetooth address in a
string format.
This protocol is not supported under FreeBSD.
An algorithm socket is configured with a
tuple of two to four elements (type, name [, feat [, mask]]), where:
– type is the algorithm type as string, e.g. aead, hash, skcipher or rng.
– name is the algorithm name and operation mode as string, e.g. sha256, hmac(sha256), cbc(aes) or
drbg_nopr_ctr_aes256.
Availability: Linux >= 2.6.38.
Some algorithm types require more recent Kernels.
New in version 3.6.
The sockets are represented as a
(CID, port) tuple where the context ID or CID and port are integers.
Availability: Linux >= 3.9
See vsock(7)
New in version 3.7.
The packets are represented by the tuple
(ifname, proto[, pkttype[, hatype[, addr]]]) where:
– ifname - String specifying the device name.
– proto - The Ethernet protocol number.
May be ETH_P_ALL to capture all protocols, one of the ETHER-
TYPE_* constants or any other Ethernet protocol number.
Packet addressed to the local host.
Availability: Linux >= 2.2.
The address family is represented as a (node, port) tuple where the node and port
are non-negative integers.
Availability: Linux >= 4.7.
New in version 3.8.
It adds two socket options that you can change.
In both cases length should be in range(8, 2**16, 8).
Such a socket should be constructed with socket(AF_INET, SOCK_DGRAM, IPPROTO_UDPLITE) for
IPv4 or socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDPLITE) for IPv6.
Availability: Linux >= 2.6.20, FreeBSD >= 10.1
New in version 3.9.
• AF_HYPERV is a Windows-only socket based interface for communicating with Hyper-V hosts and guests.
The
address family is represented as a (vm_id, service_id) tuple where the vm_id and service_id are
UUID strings.
The vm_id is the virtual machine identifier or a set of known VMID values if the target is not a specific virtual
machine.
Known VMID constants defined on socket are:
– HV_GUID_ZERO
– HV_GUID_BROADCAST
– HV_GUID_WILDCARD - Used to bind on itself and accept connections from all partitions.
When used as an
address target it will connect to the parent partition.
The service_id is the service identifier of the registered service.
New in version 3.12.
If you use a hostname in the host portion of IPv4/v6 socket address, the program may show a nondeterministic behavior,
as Python uses the first address returned from the DNS resolution.
The socket address will be resolved differently into an
actual IPv4/v6 address, depending on the results from DNS resolution and/or the host configuration.
For deterministic
behavior use a numeric address in host portion.
All errors raise exceptions.
The normal exceptions for invalid argument types and out-of-memory conditions can be
raised.
Errors related to socket or address semantics raise OSError or one of its subclasses.
Non-blocking mode is supported through setblocking().
A generalization of this based on timeouts is supported
through settimeout().
Exceptions
exception socket.error
A deprecated alias of OSError.
Changed in version 3.3: Following PEP 3151, this class was made an alias of OSError.
The accompanying value is
a pair (h_errno, string) representing an error returned by a library call.
Changed in version 3.3: This class was made a subclass of OSError.
The accompanying value is a pair (error, string) representing an error returned by
a library call.
The
numeric error value will match one of the EAI_* constants defined in this module.
Changed in version 3.3: This class was made a subclass of OSError.
A subclass of OSError, this exception is raised when a timeout occurs on a socket which has had timeouts enabled
via a prior call to settimeout() (or implicitly through setdefaulttimeout()).
The accompanying value
is a string whose value is currently always “timed out”.
Changed in version 3.3: This class was made a subclass of OSError.
This class was made an alias of TimeoutError.
Constants
The AF_* and SOCK_* constants are now AddressFamily and SocketKind IntEnum collections.
New in version 3.4.
socket.
AF_UNIX
socket.
AF_INET
socket.
These constants represent the address (and protocol) families, used for the first argument to socket().
If the
AF_UNIX constant is not defined then this protocol is unsupported.
More constants may be available depending
on the system.
AF_UNSPEC
AF_UNSPEC means that getaddrinfo() should return socket addresses for any address family (either IPv4,
IPv6, or any other) that can be used.
socket.
SOCK_STREAM
socket.SOCK_DGRAM
18.2.
SOCK_RAW
socket.
SOCK_RDM
socket.
SOCK_SEQPACKET
These constants represent the socket types, used for the second argument to socket().
More constants may be
available depending on the system.
SOCK_CLOEXEC
socket.
SOCK_NONBLOCK
These two constants, if defined, can be combined with the socket types and allow you to set some flags atomically
(thus avoiding possible race conditions and the need for separate calls).
See also:
Secure File Descriptor Handling for a more thorough explanation.
Availability: Linux >= 2.6.27.
New in version 3.2.
SO_*
socket.
SOMAXCONN
MSG_*
SOL_*
SCM_*
IPPROTO_*
IPPORT_*
INADDR_*
IP_*
IPV6_*
EAI_*
AI_*
NI_*
TCP_*
Many constants of these forms, documented in the Unix documentation on sockets and/or the IP protocol, are also
defined in the socket module.
They are generally used in arguments to the setsockopt() and getsockopt()
methods of socket objects.
In most cases, only those symbols that are defined in the Unix header files are defined;
for a few symbols, default values are provided.
Changed
in
version
3.6:
SO_DOMAIN,
SO_PROTOCOL,
SO_PEERSEC,
SO_PASSSEC,
TCP_USER_TIMEOUT, TCP_CONGESTION were added.
Changed in version 3.6.5: On Windows, TCP_FASTOPEN, TCP_KEEPCNT appear if run-time Windows sup-
ports.
Changed in version 3.7: TCP_NOTSENT_LOWAT was added.
On Windows, TCP_KEEPIDLE, TCP_KEEPINTVL appear if run-time Windows supports.
Changed in version 3.10: IP_RECVTOS was added.
Added TCP_KEEPALIVE.
On MacOS this constant can be
used in the same way that TCP_KEEPIDLE is used on Linux.
Changed in version 3.11: Added TCP_CONNECTION_INFO.
On MacOS this constant can be used in the same
way that TCP_INFO is used on Linux and BSD.
Changed in version 3.12: Added SO_RTABLE and SO_USER_COOKIE.
On OpenBSD and FreeBSD respec-
tively those constants can be used in the same way that SO_MARK is used on Linux.
Added
IP_PKTINFO,
IP_UNBLOCK_SOURCE,
IP_BLOCK_SOURCE,
IP_ADD_SOURCE_MEMBERSHIP,
IP_DROP_SOURCE_MEMBERSHIP.
socket.
AF_CAN
socket.
PF_CAN
SOL_CAN_*
CAN_*
Many constants of these forms, documented in the Linux documentation, are also defined in the socket module.
Availability: Linux >= 2.6.25, NetBSD >= 8.
New in version 3.3.
Changed in version 3.11: NetBSD support was added.
socket.
CAN_BCM
CAN_BCM_*
CAN_BCM, in the CAN protocol family, is the broadcast manager (BCM) protocol.
Broadcast manager constants,
documented in the Linux documentation, are also defined in the socket module.
Availability: Linux >= 2.6.25.
Note: The CAN_BCM_CAN_FD_FRAME flag is only available on Linux >= 4.8.
New in version 3.4.
socket.
Enables CAN FD support in a CAN_RAW socket.
This is disabled by default.
This allows your application to send
both CAN and CAN FD frames; however, you must accept both CAN and CAN FD frames when reading from
the socket.
This constant is documented in the Linux documentation.
Availability: Linux >= 3.6.
New in version 3.5.
socket.
CAN_RAW_JOIN_FILTERS
Joins the applied CAN filters such that only CAN frames that match all given CAN filters are passed to user space.
This constant is documented in the Linux documentation.
Availability: Linux >= 4.1.
New in version 3.9.
socket.
CAN_ISOTP
CAN_ISOTP, in the CAN protocol family, is the ISO-TP (ISO 15765-2) protocol.
ISO-TP constants, documented
in the Linux documentation.
Availability: Linux >= 2.6.25.
New in version 3.7.
CAN_J1939
CAN_J1939, in the CAN protocol family, is the SAE J1939 protocol.
J1939 constants, documented in the Linux
documentation.
Availability: Linux >= 5.4.
New in version 3.9.
socket.
AF_DIVERT
socket.
PF_DIVERT
These two constants, documented in the FreeBSD divert(4) manual page, are also defined in the socket module.
Availability: FreeBSD >= 14.0.
New in version 3.12.
socket.
AF_PACKET
socket.
PF_PACKET
PACKET_*
Many constants of these forms, documented in the Linux documentation, are also defined in the socket module.
Availability: Linux >= 2.2.
socket.
ETH_P_ALL
ETH_P_ALL can be used in the socket constructor as proto for the AF_PACKET family in order to capture
every packet, regardless of protocol.
For more information, see the packet(7) manpage.
Availability: Linux.
New in version 3.12.
socket.
AF_RDS
socket.
PF_RDS
socket.
SOL_RDS
RDS_*
Many constants of these forms, documented in the Linux documentation, are also defined in the socket module.
Availability: Linux >= 2.6.30.
New in version 3.3.
socket.
SIO_RCVALL
socket.
SIO_KEEPALIVE_VALS
socket.
SIO_LOOPBACK_FAST_PATH
RCVALL_*
Constants for Windows’ WSAIoctl().
The constants are used as arguments to the ioctl() method of socket
objects.
Changed in version 3.6: SIO_LOOPBACK_FAST_PATH was added.
TIPC_*
TIPC related constants, matching the ones exported by the C socket API.
See the TIPC documentation for more
information.
socket.
Availability: Linux >= 2.6.38.
New in version 3.6.
socket.
AF_VSOCK
socket.
IOCTL_VM_SOCKETS_GET_LOCAL_CID
VMADDR*
SO_VM*
Constants for Linux host/guest communication.
Availability: Linux >= 4.8.
New in version 3.7.
socket.
AF_LINK
Availability: BSD, macOS.
New in version 3.4.
This constant contains a boolean value which indicates if IPv6 is supported on this platform.
socket.
BDADDR_ANY
socket.
These are string constants containing Bluetooth addresses with special meanings.
For example, BDADDR_ANY can
be used to indicate any address when specifying the binding socket with BTPROTO_RFCOMM.
socket.
HCI_FILTER
socket.
HCI_TIME_STAMP
socket.
HCI_DATA_DIR
For
use
with
BTPROTO_HCI.
HCI_FILTER
is
not
available
for
NetBSD
or
DragonFlyBSD.
HCI_TIME_STAMP and HCI_DATA_DIR are not available for FreeBSD, NetBSD, or DragonFlyBSD.
AF_QIPCRTR
Constant for Qualcomm’s IPC router protocol, used to communicate with service providing remote processors.
Availability: Linux >= 4.7.
socket.
SCM_CREDS2
socket.
LOCAL_CREDS
socket.
LOCAL_CREDS_PERSISTENT
LOCAL_CREDS and LOCAL_CREDS_PERSISTENT can be used with SOCK_DGRAM, SOCK_STREAM
sockets, equivalent to Linux/DragonFlyBSD SO_PASSCRED, while LOCAL_CREDS sends the credentials at
first read, LOCAL_CREDS_PERSISTENT sends for each read, SCM_CREDS2 must be then used for the latter
for the message type.
New in version 3.11.
Availability: FreeBSD.
socket.
SO_INCOMING_CPU
Constant to optimize CPU locality, to be used in conjunction with SO_REUSEPORT.
New in version 3.11.
Availability: Linux >= 3.9
18.2.
AF_HYPERV
socket.
HV_PROTOCOL_RAW
socket.
HVSOCKET_CONNECT_TIMEOUT
socket.
HVSOCKET_CONNECT_TIMEOUT_MAX
socket.
HVSOCKET_CONNECTED_SUSPEND
socket.
HVSOCKET_ADDRESS_FLAG_PASSTHRU
socket.
HV_GUID_ZERO
socket.
HV_GUID_WILDCARD
socket.
HV_GUID_BROADCAST
socket.
HV_GUID_CHILDREN
socket.
HV_GUID_LOOPBACK
socket.
HV_GUID_PARENT
Constants for Windows Hyper-V sockets for host/guest communications.
Availability: Windows.
New in version 3.12.
socket.
ETHERTYPE_ARP
socket.
ETHERTYPE_IP
socket.
ETHERTYPE_IPV6
socket.
ETHERTYPE_VLAN
IEEE 802.3 protocol number.
Availability: Linux, FreeBSD, macOS.
New in version 3.12.
Functions
Creating sockets
The following functions all create socket objects.
The address family should be
AF_INET (the default), AF_INET6, AF_UNIX, AF_CAN, AF_PACKET, or AF_RDS.
The socket type should
be SOCK_STREAM (the default), SOCK_DGRAM, SOCK_RAW or perhaps one of the other SOCK_ constants.
The
protocol number is usually zero and may be omitted or in the case where the address family is AF_CAN the protocol
should be one of CAN_RAW, CAN_BCM, CAN_ISOTP or CAN_J1939.
If fileno is specified, the values for family, type, and proto are auto-detected from the specified file descriptor.
Auto-detection can be overruled by calling the function with explicit family, type, or proto arguments.
This only
affects how Python represents e.g. the return value of socket.getpeername() but not the actual OS resource.
Unlike socket.fromfd(), fileno will return the same socket and not a duplicate.
This may help close a detached
socket using socket.close().
The newly created socket is non-inheritable.
The AF_CAN family was added.
The AF_RDS family was added.
The CAN_BCM protocol was added.
The returned socket is now non-inheritable.
The CAN_ISOTP protocol was added.
Changed in version 3.7: When SOCK_NONBLOCK or SOCK_CLOEXEC bit flags are applied to type they are
cleared, and socket.type will not reflect them.
They are still passed to the underlying system socket() call.
Therefore,
sock = socket.socket(
socket.
The CAN_J1939 protocol was added.
The IPPROTO_MPTCP protocol was added.
Address
family, socket type, and protocol number are as for the socket() function above.
The default family is AF_UNIX
if defined on the platform; otherwise, the default is AF_INET.
The newly created sockets are non-inheritable.
The returned socket objects now support the whole socket API, rather than a subset.
The returned sockets are now non-inheritable.
Changed in version 3.5: Windows support added.
This is a higher-level function than socket.connect(): if host is a non-numeric hostname, it will try
to resolve it for both AF_INET and AF_INET6, and then try to connect to all possible addresses in turn until a
connection succeeds.
This makes it easy to write clients that are compatible to both IPv4 and IPv6.
Passing the optional timeout parameter will set the timeout on the socket instance before attempting to connect.
If
no timeout is supplied, the global default timeout setting returned by getdefaulttimeout() is used.
If supplied, source_address must be a 2-tuple (host, port) for the socket to bind to as its source address
before connecting.
If host or port are ‘’ or 0 respectively the OS default behavior will be used.
When a connection cannot be created, an exception is raised.
By default, it is the exception from the last address
in the list.
If all_errors is True, it is an ExceptionGroup containing the errors of all attempts.
Changed in version 3.2: source_address was added.
Changed in version 3.11: all_errors was added.
If dualstack_ipv6 is true and the platform supports it the socket will be able to accept both IPv4 and IPv6 con-
nections, else it will raise ValueError.
Most POSIX platforms and Windows are supposed to support this
functionality.
When this functionality is enabled the address returned by socket.getpeername() when an
18.2.
If dualstack_ipv6 is
false it will explicitly disable this functionality on platforms that enable it by default (e.g. Linux).
This parameter
can be used in conjunction with has_dualstack_ipv6():
import socket
addr = ("", 8080)
# all interfaces, port 8080
if socket.has_dualstack_ipv6():
s = socket.create_server(addr, family=socket.
Note:
On POSIX platforms the SO_REUSEADDR socket option is set in order to immediately reuse previous
sockets which were bound on the same address and remained in TIME_WAIT state.
New in version 3.8.
New in version 3.8.
Address family, socket type and protocol number are as for the socket() function above.
The file descriptor should refer to a socket, but this is not checked — subsequent operations on the object may fail
if the file descriptor is invalid.
This function is rarely needed, but can be used to get or set socket options on a
socket passed to a program as standard input or output (such as a server started by the Unix inet daemon).
The
socket is assumed to be in blocking mode.
The newly created socket is non-inheritable.
The returned socket is now non-inheritable.
The socket is assumed to be in
blocking mode.
Availability: Windows.
New in version 3.3.
socket.
This is a Python type object that represents the socket object type.
It is the same as type(socket(...)).
Other functions
The socket module also offers various network-related services:
socket.close(fd)
Close a socket file descriptor.
This is like os.close(), but for sockets.
On some platforms (most noticeable
Windows) os.close() does not work for socket file descriptors.
By passing None as the value of
host and port, you can pass NULL to the underlying C API.
The family, type and proto arguments can be optionally specified in order to narrow the list of addresses returned.
Passing zero as a value for each of these arguments selects the full range of results.
The flags argument can be
one or several of the AI_* constants, and will influence how results are computed and returned.
For example,
AI_NUMERICHOST will disable domain name resolution and will raise an error if host is a domain name.
In these tuples, family, type, proto are all integers and are meant to be passed to the socket() function.
Raises an auditing event socket.getaddrinfo with arguments host, port, family, type, protocol.
The following example fetches address information for a hypothetical TCP connection to example.org on port
80 (results may differ on your system if IPv6 isn’t enabled):
[(socket.AF_INET6, socket.
SOCK_STREAM,
6, '', ('2606:2800:220:1:248:1893:25c8:1946', 80, 0, 0)),
(socket.
AF_INET, socket.
SOCK_STREAM,
6, '', ('93.184.216.34', 80))]
Changed in version 3.2: parameters can now be passed using keyword arguments.
Changed in version 3.7: for IPv6 multicast addresses, string representing an address will not contain %scope_id
part.
If name is omitted or empty, it is interpreted as the local host.
To
find the fully qualified name, the hostname returned by gethostbyaddr() is checked, followed by aliases for
the host, if available.
The first name which includes a period is selected.
In case no fully qualified domain name
is available and name was provided, it is returned unchanged.
If name was empty or equal to '0.0.0.0', the
hostname from gethostname() is returned.
The IPv4 address is returned as a string, such as '100.50.200.
5'.
If the host name is an IPv4 address itself it is returned unchanged.
See gethostbyname_ex() for a more
complete interface.
Raises an auditing event socket.gethostbyname with argument hostname.
Availability: not WASI.
Return a 3-tuple (hostname, aliaslist,
ipaddrlist) where hostname is the host’s primary host name, aliaslist is a (possibly empty) list of alternative
host names for the same address, and ipaddrlist is a list of IPv4 addresses for the same interface on the same host
(often but not always a single address).
Availability: not WASI.
socket.gethostname()
Return a string containing the hostname of the machine where the Python interpreter is currently executing.
Raises an auditing event socket.gethostname with no arguments.
Note: gethostname() doesn’t always return the fully qualified domain name; use getfqdn() for that.
Availability: not WASI.
socket.gethostbyaddr(ip_address)
Return a 3-tuple (hostname, aliaslist, ipaddrlist) where hostname is the primary host name re-
sponding to the given ip_address, aliaslist is a (possibly empty) list of alternative host names for the same address,
and ipaddrlist is a list of IPv4/v6 addresses for the same interface on the same host (most likely containing only a
single address).
To find the fully qualified domain name, use the function getfqdn().
Depending on the settings of flags, the result
can contain a fully qualified domain name or numeric address representation in host.
Similarly, port can contain a
string port name or a numeric port number.
For IPv6 addresses, %scope_id is appended to the host part if sockaddr contains meaningful scope_id.
Usually
this happens for multicast addresses.
For more information about flags you can consult getnameinfo(3).
Raises an auditing event socket.getnameinfo with argument sockaddr.
Availability: not WASI.
socket.getprotobyname(protocolname)
Translate an internet protocol name (for example, 'icmp') to a constant suitable for passing as the (optional) third
argument to the socket() function.
This is usually only needed for sockets opened in “raw” mode (SOCK_RAW);
for the normal socket modes, the correct protocol is chosen automatically if the protocol is omitted or zero.
The optional protocol
name, if given, should be 'tcp' or 'udp', otherwise any protocol will match.
Raises an auditing event socket.getservbyname with arguments servicename, protocolname.
Availability: not WASI.
socket.getservbyport(port[, protocolname])
Translate an internet port number and protocol name to a service name for that service.
The optional protocol
name, if given, should be 'tcp' or 'udp', otherwise any protocol will match.
Raises an auditing event socket.getservbyport with arguments port, protocolname.
Availability: not WASI.
Convert 32-bit positive integers from network to host byte order.
Convert 16-bit positive integers from network to host byte order.
On machines where the host byte order is the
same as network byte order, this is a no-op; otherwise, it performs a 2-byte swap operation.
Raises OverflowError if x does not fit in a 16-bit unsigned integer.
socket.htonl(x)
Convert 32-bit positive integers from host to network byte order.
Convert 16-bit positive integers from host to network byte order.
On machines where the host byte order is the
same as network byte order, this is a no-op; otherwise, it performs a 2-byte swap operation.
Raises OverflowError if x does not fit in a 16-bit unsigned integer.
socket.inet_aton(ip_string)
Convert an IPv4 address from dotted-quad string format (for example, ‘123.45.67.89’) to 32-bit packed binary
format, as a bytes object four characters in length.
This is useful when conversing with a program that uses the
standard C library and needs objects of type in_addr, which is the C type for the 32-bit packed binary this
function returns.
If the IPv4 address string passed to this function is invalid, OSError will be raised.
Note that exactly what is
valid depends on the underlying C implementation of inet_aton().
Convert a 32-bit packed IPv4 address (a bytes-like object four bytes in length) to its standard dotted-quad string
representation (for example, ‘123.45.67.89’).
This is useful when conversing with a program that uses the standard
C library and needs objects of type in_addr, which is the C type for the 32-bit packed binary data this function
takes as an argument.
If the byte sequence passed to this function is not exactly 4 bytes in length, OSError will be raised.
Writable bytes-like object is now accepted.
Supported values for address_family are currently AF_INET and AF_INET6.
If the IP address string ip_string is
invalid, OSError will be raised.
Note that exactly what is valid depends on both the value of address_family and
the underlying implementation of inet_pton().
Availability: Unix, Windows.
Changed in version 3.4: Windows support added
18.2.
Supported values for address_family are currently AF_INET and AF_INET6.
If the bytes object packed_ip is not
the correct length for the specified address family, ValueError will be raised.
OSError is raised for errors
from the call to inet_ntop().
Writable bytes-like object is now accepted.
CMSG_LEN(length)
Return the total length, without trailing padding, of an ancillary data item with associated data of the given length.
This value can often be used as the buffer size for recvmsg() to receive a single item of ancillary data, but RFC
3542 requires portable applications to use CMSG_SPACE() and thus include space for padding, even when the
item will be the last in the buffer.
Raises OverflowError if length is outside the permissible range of values.
Availability: Unix, not Emscripten, not WASI.
Most Unix platforms.
New in version 3.3.
socket.
CMSG_SPACE(length)
Return the buffer size needed for recvmsg() to receive an ancillary data item with associated data of the
given length, along with any trailing padding.
The buffer space needed to receive multiple items is the sum of
the CMSG_SPACE() values for their associated data lengths.
Raises OverflowError if length is outside the
permissible range of values.
Note that some systems might support ancillary data without providing this function.
Also note that setting the buffer
size using the results of this function may not precisely limit the amount of ancillary data that can be received, since
additional data may be able to fit into the padding area.
Availability: Unix, not Emscripten, not WASI.
New in version 3.3.
A value of None indicates that new socket
objects have no timeout.
When the socket module is first imported, the default is None.
socket.setdefaulttimeout(timeout)
Set the default timeout in seconds (float) for new socket objects.
When the socket module is first imported, the
default is None.
See settimeout() for possible values and their respective meanings.
socket.sethostname(name)
Set the machine’s hostname to name.
This will raise an OSError if you don’t have enough rights.
Raises an auditing event socket.sethostname with argument name.
Availability: Unix.
OSError if the system call fails.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.3.
Windows support was added.
OSError if no interface with the
given name exists.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.3.
Windows support was added.
See also:
“Interface name” is a name as documented in if_nameindex().
OSError if no interface with the
given index exists.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.3.
Windows support was added.
See also:
“Interface name” is a name as documented in if_nameindex().
The fds parameter is a sequence of file descrip-
tors.
Consult sendmsg() for the documentation of these parameters.
Availability: Unix, Windows, not Emscripten, not WASI.
Unix platforms supporting sendmsg() and SCM_RIGHTS mechanism.
New in version 3.9.
Return (msg, list(fds), flags,
addr).
Consult recvmsg() for the documentation of these parameters.
Availability: Unix, Windows, not Emscripten, not WASI.
New in version 3.9.
Note: Any truncated integers at the end of the list of file descriptors.
Except for makefile(), these correspond to Unix system calls applicable
to sockets.
Changed in version 3.2: Support for the context manager protocol was added.
Exiting the context manager is equivalent
to calling close().
The socket must be bound to an address and listening for connections.
The return value is a
pair (conn, address) where conn is a new socket object usable to send and receive data on the connection,
and address is the address bound to the socket on the other end of the connection.
The newly created socket is non-inheritable.
The socket is now non-inheritable.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
The socket must not already be bound.
Raises an auditing event socket.bind with arguments self, address.
Availability: not WASI.
The underlying system resource (e.g. a file descriptor) is also closed when all file objects
from makefile() are closed.
Once that happens, all future operations on the socket object will fail.
The remote
end will receive no more data (after queued data is flushed).
Sockets are automatically closed when they are garbage-collected, but it is recommended to close() them ex-
plicitly, or to use a with statement around them.
Changed in version 3.6: OSError is now raised if an error occurs when the underlying close() call is made.
Note: close() releases the resource associated with a connection but does not necessarily close the connection
immediately.
If you want to close the connection in a timely fashion, call shutdown() before close().
socket.connect(address)
Connect to a remote socket at address.
If the connection is interrupted by a signal, the method waits until the connection completes, or raise a
TimeoutError on timeout, if the signal handler doesn’t raise an exception and the socket is blocking or has
a timeout.
For non-blocking sockets, the method raises an InterruptedError exception if the connection is
interrupted by a signal (or the exception raised by the signal handler).
The method now waits until the connection completes instead of raising an
InterruptedError exception if the connection is interrupted by a signal, the signal handler doesn’t raise
an exception and the socket is blocking or has a timeout (see the PEP 475 for the rationale).
Availability: not WASI.
The error indicator
is 0 if the operation succeeded, otherwise the value of the errno variable.
This is useful to support, for example,
asynchronous connects.
Raises an auditing event socket.connect with arguments self, address.
Availability: not WASI.
The file descriptor is
returned, and can be reused for other purposes.
New in version 3.2.
The newly created socket is non-inheritable.
The socket is now non-inheritable.
Availability: not WASI.
socket.fileno()
Return the socket’s file descriptor (a small integer), or -1 on failure.
This is useful with select.select().
Under Windows the small integer returned by this method cannot be used where a file descriptor can be used (such
as os.fdopen()).
Unix does not have this limitation.
New in version 3.4.
This is useful to find out the port number of a remote
IPv4/v6 socket, for instance.
On
some systems this function is not supported.
This is useful to find out the port number of an IPv4/v6 socket, for instance.
The needed symbolic
constants (SO_* etc.) are defined in this module.
If buflen is absent, an integer option is assumed and its integer
value is returned by the function.
If buflen is present, it specifies the maximum length of the buffer used to receive
the option in, and this buffer is returned as a bytes object.
It is up to the caller to decode the contents of the buffer
(see the optional built-in module struct for a way to decode C structures encoded as byte strings).
Availability: not WASI.
This is equivalent to checking socket.gettimeout() !
New in version 3.7.
This reflects
the last call to setblocking() or settimeout().
Please refer to the Win32 docu-
mentation for more information.
On other platforms, the generic fcntl.fcntl() and fcntl.ioctl() functions may be used; they accept a
socket object as their first argument.
Currently only the following control codes are supported:
SIO_RCVALL, SIO_KEEPALIVE_VALS, and
SIO_LOOPBACK_FAST_PATH.
Changed in version 3.6: SIO_LOOPBACK_FAST_PATH was added.
If backlog is specified, it must be at least 0 (if it is lower, it is set to 0); it
specifies the number of unaccepted connections that the system will allow before refusing new connections.
If not
specified, a default reasonable value is chosen.
Availability: not WASI.
Changed in version 3.5: The backlog parameter is now optional.
socket.makefile(mode=’r’, buffering=None, *, encoding=None, errors=None, newline=None)
Return a file object associated with the socket.
The exact returned type depends on the arguments given to
makefile().
These arguments are interpreted the same way as by the built-in open() function, except the
only supported mode values are 'r' (default), 'w' and 'b'.
The socket must be in blocking mode; it can have a timeout, but the file object’s internal buffer may end up in an
inconsistent state if a timeout occurs.
Closing the file object returned by makefile() won’t close the original socket unless all other file objects have
been closed and socket.close() has been called on the socket object.
Note: On Windows, the file-like object created by makefile() cannot be used where a file object with a file
descriptor is expected, such as the stream arguments of subprocess.
Popen().
Receive data from the socket.
The return value is a bytes object representing the data received.
The maximum
amount of data to be received at once is specified by bufsize.
See the Unix manual page recv(2) for the meaning
of the optional argument flags; it defaults to zero.
The return value is a pair (bytes, address) where bytes is a bytes object
representing the data received and address is the address of the socket sending the data.
See the Unix manual page
recv(2) for the meaning of the optional argument flags; it defaults to zero.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Changed in version 3.7: For multicast IPv6 address, first item of address does not contain %scope_id part
anymore.
In order to get full IPv6 address use getnameinfo().
The ancbufsize argument sets the size
in bytes of the internal buffer used to receive the ancillary data; it defaults to 0, meaning that no ancillary data will be
received.
Appropriate buffer sizes for ancillary data can be calculated using CMSG_SPACE() or CMSG_LEN(),
and items which do not fit into the buffer might be truncated or discarded.
The flags argument defaults to 0 and
has the same meaning as for recv().
The return value is a 4-tuple: (data, ancdata, msg_flags, address).
The data item is a bytes
object holding the non-ancillary data received.
The ancdata item is a list of zero or more tuples (cmsg_level,
cmsg_type, cmsg_data) representing the ancillary data (control messages) received: cmsg_level and
cmsg_type are integers specifying the protocol level and protocol-specific type respectively, and cmsg_data is a
bytes object holding the associated data.
The msg_flags item is the bitwise OR of various flags indicating condi-
tions on the received message; see your system documentation for details.
If the receiving socket is unconnected,
address is the address of the sending socket, if available; otherwise, its value is unspecified.
On some systems, sendmsg() and recvmsg() can be used to pass file descriptors between processes over an
AF_UNIX socket.
When this facility is used (it is often restricted to SOCK_STREAM sockets), recvmsg() will
return, in its ancillary data, items of the form (socket.
SOL_SOCKET, socket.
SCM_RIGHTS, fds),
where fds is a bytes object representing the new file descriptors as a binary array of the native C int type.
If recvmsg() raises an exception after the system call returns, it will first attempt to close any file descriptors
received via this mechanism.
Some systems do not indicate the truncated length of ancillary data items which have been only partially received.
If an item appears to extend beyond the end of the buffer, recvmsg() will issue a RuntimeWarning, and will

data.
On systems which support the SCM_RIGHTS mechanism, the following function will receive up to maxfds file
descriptors, returning the message data and a list containing the descriptors (while ignoring unexpected conditions
such as unrelated control messages being received).
See also sendmsg().
SOL_SOCKET and cmsg_type == socket.
SCM_RIGHTS:
# Append data, ignoring any truncated integers at the end.
Availability: Unix.
Most Unix platforms.
New in version 3.3.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
The buffers argument must be an
iterable of objects that export writable buffers (e.g. bytearray objects); these will be filled with successive
chunks of the non-ancillary data until it has all been written or there are no more buffers.
The operating system
may set a limit (sysconf() value SC_IOV_MAX) on the number of buffers that can be used.
The ancbufsize
and flags arguments have the same meaning as for recvmsg().
The return value is a 4-tuple: (nbytes, ancdata, msg_flags, address), where nbytes is the total
number of bytes of non-ancillary data written into the buffers, and ancdata, msg_flags and address are the same as
for recvmsg().
Availability: Unix.
Most Unix platforms.
New in version 3.3.
The return value is a pair
(nbytes, address) where nbytes is the number of bytes received and address is the address of the socket
sending the data.
See the Unix manual page recv(2) for the meaning of the optional argument flags; it defaults
to zero.
If nbytes is not specified (or 0), receive up to the size available in the given buffer.
Returns the number of bytes
received.
See the Unix manual page recv(2) for the meaning of the optional argument flags; it defaults to zero.
The socket must be connected to a remote socket.
The optional flags argument has the
same meaning as for recv() above.
Returns the number of bytes sent.
Applications are responsible for checking
that all data has been sent; if only some of the data was transmitted, the application needs to attempt delivery of
the remaining data.
For further information on this topic, consult the socket-howto.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Send data to the socket.
The socket must be connected to a remote socket.
The optional flags argument has the
same meaning as for recv() above.
Unlike send(), this method continues to send data from bytes until either
all data has been sent or an error occurs.
None is returned on success.
On error, an exception is raised, and there
is no way to determine how much data, if any, was successfully sent.
The socket timeout is no longer reset each time data is sent successfully.
The socket
timeout is now the maximum total duration to send all data.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
The socket should not be connected to a remote socket, since the destination socket is
specified by address.
The optional flags argument has the same meaning as for recv() above.
Return the number
of bytes sent.
Raises an auditing event socket.sendto with arguments self, address.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
The buffers argument specifies the non-ancillary data as an iterable of bytes-like
objects (e.g. bytes objects); the operating system may set a limit (sysconf() value SC_IOV_MAX) on the
number of buffers that can be used.
The ancdata argument specifies the ancillary data (control messages) as an iter-
able of zero or more tuples (cmsg_level, cmsg_type, cmsg_data), where cmsg_level and cmsg_type
are integers specifying the protocol level and protocol-specific type respectively, and cmsg_data is a bytes-like ob-
ject holding the associated data.
Note that some systems (in particular, systems without CMSG_SPACE()) might
support sending only one control message per call.
The flags argument defaults to 0 and has the same meaning as
for send().
If address is supplied and not None, it sets a destination address for the message.
The return value
is the number of bytes of non-ancillary data sent.
The following function sends the list of file descriptors fds over an AF_UNIX socket, on systems which support
the SCM_RIGHTS mechanism.
See also recvmsg().
SOL_SOCKET, socket.
SCM_RIGHTS, array.
Availability: Unix, not WASI.
Most Unix platforms.
New in version 3.3.
If the system call is interrupted and the signal handler does not raise an exception, the
method now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the
rationale).
Set mode, IV, AEAD associated data length and flags
for AF_ALG socket.
Availability: Linux >= 2.6.38.
New in version 3.6.
socket.sendfile(file, offset=0, count=None)
Send a file until EOF is reached by using high-performance os.sendfile and return the total number of bytes
which were sent.
If os.sendfile is not available (e.g.
Windows) or file is not a regular file send() will be used instead.
If specified, count is the total number of bytes to transmit as opposed to sending the file until EOF is reached.
File
position is updated on return or also in case of error in which case file.tell() can be used to figure out the
number of bytes which were sent.
The socket must be of SOCK_STREAM type.
Non-blocking sockets are not
supported.
New in version 3.5.
Set blocking or non-blocking mode of the socket: if flag is false, the socket is set to non-blocking, else to blocking
mode.
The method no longer applies SOCK_NONBLOCK flag on socket.type.
socket.settimeout(value)
Set a timeout on blocking socket operations.
The value argument can be a nonnegative floating point number
expressing seconds, or None.
If a non-zero value is given, subsequent socket operations will raise a timeout
exception if the timeout period value has elapsed before the operation has completed.
If zero is given, the socket
is put in non-blocking mode.
If None is given, the socket is put in blocking mode.
For further information, please consult the notes on socket timeouts.
The method no longer toggles SOCK_NONBLOCK flag on socket.type.
socket.setsockopt(level, optname, value: int)
socket.setsockopt(level, optname, value: buffer)
socket.setsockopt(level, optname, None, optlen: int)
Set the value of the given socket option (see the Unix manual page setsockopt(2)).
The needed symbolic
constants are defined in the socket module (SO_* etc.).
The value can be an integer, None or a bytes-like object
representing a buffer.
When value is set to
None, optlen argument is required.
It’s equivalent to call setsockopt() C function with optval=NULL and
optlen=optlen.
Writable bytes-like object is now accepted.
Changed in version 3.6: setsockopt(level, optname, None, optlen: int) form added.
Availability: not WASI.
socket.shutdown(how)
Shut down one or both halves of the connection.
If how is SHUT_RD, further receives are disallowed.
If how is
SHUT_WR, further sends are disallowed.
If how is SHUT_RDWR, further sends and receives are disallowed.
Availability: not WASI.
The target process must be provided with
process_id.
The resulting bytes object can then be passed to the target process using some form of interprocess
communication and the socket can be recreated there using fromshare().
Once this method has been called,
it is safe to close the socket since the operating system has already duplicated it for the target process.
Availability: Windows.
New in version 3.3.
Note that there are no methods read() or write(); use recv() and send() without flags argument instead.
Socket objects also have these (read-only) attributes that correspond to the values given to the socket constructor.
Sockets are by default always created
in blocking mode, but this can be changed by calling setdefaulttimeout().
In blocking mode, operations block until complete or the system returns an error (such as connection timed out).
In non-blocking mode, operations fail (with an error that is unfortunately system-dependent) if they cannot be
completed immediately: functions from the select module can be used to know when and whether a socket is
available for reading or writing.
In timeout mode, operations fail if they cannot be completed within the timeout specified for the socket (they raise
a timeout exception) or if the system returns an error.
Note: At the operating system level, sockets in timeout mode are internally set in non-blocking mode.
Also, the blocking
and timeout modes are shared between file descriptors and socket objects that refer to the same network endpoint.
This
implementation detail can have visible consequences if e.g. you decide to use the fileno() of a socket.
However,
the system network stack may also return a connection timeout error of its own regardless of any Python socket timeout
setting.
Timeouts and the accept method
If getdefaulttimeout() is not None, sockets returned by the accept() method inherit that timeout.
Otherwise,
the behaviour depends on settings of the listening socket:
• if the listening socket is in blocking mode or in timeout mode, the socket returned by accept() is in blocking
mode;
• if the listening socket is in non-blocking mode, whether the socket returned by accept() is in blocking or non-
blocking mode is operating system-dependent.
If you want to ensure cross-platform behaviour, it is recommended
you manually override this setting.
Note that a server must perform the sequence socket(), bind(),
listen(), accept() (possibly repeating the accept() to service more than one client), while a client only needs
the sequence socket(), connect().
Also note that the server does not sendall()/recv() on the socket it is
listening on but on the new socket returned by accept().
The first two examples support IPv4 only.
The same port as used by the server
with socket.socket(socket.
The next two examples are identical to the above two, but support both IPv4 and IPv6.
The server side will listen to the
first address family available (it should listen to both instead).
On most of IPv6-ready systems, IPv6 will take precedence
and the server may not accept IPv4 traffic.
The client side will try to connect to all the addresses returned as a result of
the name resolution, and sends traffic to the first one connected successfully.
AF_UNSPEC,
socket.
SOCK_STREAM, 0, socket.
AF_UNSPEC, socket.
SOCK_STREAM):
af, socktype, proto, canonname, sa = res
try:
s = socket.socket(af, socktype, proto)
except OSError as msg:
s = None
continue
try:
s.connect(sa)
except OSError as msg:
s.close()
(continues on next page)
18.2.
The next example shows how to write a very simple network sniffer with raw sockets on Windows.
The example requires
administrator privileges to modify the interface:
import socket
# the public network interface
HOST = socket.gethostbyname(socket.gethostname())
# create a raw socket and bind it to the public interface
s = socket.socket(socket.
AF_INET, socket.
SOCK_RAW, socket.IPPROTO_IP)
s.bind((HOST, 0))
# Include IP headers
s.setsockopt(socket.IPPROTO_IP, socket.
IP_HDRINCL, 1)
# receive all packets
s.ioctl(socket.
SIO_RCVALL, socket.
RCVALL_ON)
# receive a packet
print(s.recvfrom(65565))
# disabled promiscuous mode
s.ioctl(socket.
SIO_RCVALL, socket.
The next example shows how to use the socket interface to communicate to a CAN network using the raw socket protocol.
To use CAN with the broadcast manager protocol instead, open a socket with:
socket.socket(socket.
AF_CAN, socket.SOCK_DGRAM, socket.
After binding (CAN_RAW) or connecting (CAN_BCM) the socket, you can use the socket.send() and socket.
AF_CAN, socket.
There is a socket flag to set, in order to prevent this, socket.
SO_REUSEADDR:
s = socket.socket(socket.
AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.
SOL_SOCKET, socket.
SO_REUSEADDR, 1)
s.bind((HOST, PORT))
the SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state, without waiting for its natural
timeout to expire.
An Introductory 4.3BSD Interprocess Communication Tutorial, by Stuart Sechrest
• An Advanced 4.3BSD Interprocess Communication Tutorial, by Samuel J. Leffler et al,
both in the UNIX Programmer’s Manual, Supplementary Documents 1 (sections PS1:7 and PS1:8).
The platform-specific
reference material for the various socket-related system calls are also a valuable source of information on the details of
socket semantics.
For Unix, refer to the manual pages; for Windows, see the WinSock (or Winsock 2) specification.
For
IPv6-ready APIs, readers may want to refer to RFC 3493 titled Basic Socket Interface Extensions for IPv6.
This module uses the OpenSSL library.
It
is available on all modern Unix systems, Windows, macOS, and probably additional platforms, as long as OpenSSL is
installed on that platform.
Note: Some behavior may be platform dependent, since calls are made to the operating system socket APIs.
The installed
version of OpenSSL may also cause variations in behavior.
For example, TLSv1.3 with OpenSSL version 1.1.1.
Warning: Don’t use this module without reading the Security considerations.
Doing so may lead to a false sense of
security, as the default settings of the ssl module are not necessarily appropriate for your application.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
This section documents the objects and functions in the ssl module; for more general information about TLS, SSL, and
certificates, the reader is referred to the documents in the “See Also” section at the bottom.
This module provides a class, ssl.SSLSocket, which is derived from the socket.socket type, and provides a
socket-like wrapper that also encrypts and decrypts the data going over the socket with SSL.
It supports additional methods
such as getpeercert(), which retrieves the certificate of the other side of the connection, and cipher(), which
retrieves the cipher being used for the secure connection.
For more sophisticated applications, the ssl.
SSLContext class helps manage settings and certificates, which can then
be inherited by SSL sockets created through the SSLContext.wrap_socket() method.
Changed in version 3.5.3: Updated to support linking with OpenSSL 1.1.0
Changed in version 3.6: OpenSSL 0.9.8, 1.0.0 and 1.0.1 are deprecated and no longer supported.
In the future the ssl
module will require at least OpenSSL 1.0.2 or 1.1.0.
PEP 644 has been implemented.
The ssl module requires OpenSSL 1.1.1 or newer.
Use of deprecated constants and functions result in deprecation warnings.
The helper function
create_default_context() returns a new context with secure default settings.
Client socket example with custom context and IPv4:
hostname = 'www.python.org'
# PROTOCOL_TLS_CLIENT requires valid cert chain and hostname
context = ssl.SSLContext(ssl.
PROTOCOL_TLS_CLIENT)
context.load_verify_locations('path/to/cabundle.pem')
with socket.socket(socket.
AF_INET, socket.
SOCK_STREAM, 0) as sock:
with context.wrap_socket(sock, server_hostname=hostname) as ssock:
print(ssock.version())
Server socket example listening on localhost IPv4:
context = ssl.SSLContext(ssl.
AF_INET, socket.
A convenience function helps create SSLContext objects for common purposes.
ssl.create_default_context(purpose=Purpose.SERVER_AUTH, cafile=None, capath=None, cadata=None)
Return a new SSLContext object with default settings for the given purpose.
The settings are chosen by the ssl
module, and usually represent a higher security level than when calling the SSLContext constructor directly.
If all three are None, this function can choose to trust the system’s default
CA certificates instead.
The settings are:
PROTOCOL_TLS_CLIENT
or PROTOCOL_TLS_SERVER, OP_NO_SSLv2,
and
OP_NO_SSLv3 with high encryption cipher suites without RC4 and without unauthenticated cipher suites.
Passing SERVER_AUTH as purpose sets verify_mode to CERT_REQUIRED and either loads CA certificates
(when at least one of cafile, capath or cadata is given) or uses SSLContext.load_default_certs() to
load default CA certificates.
When
keylog_filename
is
supported
and
the
environment
variable
SSLKEYLOGFILE
is
set,
create_default_context() enables key logging.
Note: The protocol, options, cipher and other settings may change to more restrictive values anytime without prior
deprecation.
The values represent a fair balance between compatibility and security.
If your application needs specific settings, you should create a SSLContext and apply the settings yourself.
SSL3.0 is widely considered to be completely
broken.
If you still wish to continue to use this function but still allow SSL 3.0 connections you can re-enable them
using:
ctx = ssl.create_default_context(Purpose.CLIENT_AUTH)
ctx.options &= ~ssl.
OP_NO_SSLv3
New in version 3.4.
Changed in version 3.4.4: RC4 was dropped from the default cipher string.
Changed in version 3.6: ChaCha20/Poly1305 was added to the default cipher string.
Changed in version 3.8: Support for key logging to SSLKEYLOGFILE was added.
The context now uses PROTOCOL_TLS_CLIENT or PROTOCOL_TLS_SERVER pro-
tocol instead of generic PROTOCOL_TLS.
Exceptions
exception ssl.
SSLError
Raised to signal an error from the underlying SSL implementation (currently provided by the OpenSSL library).
This signifies some problem in the higher-level encryption and authentication layer that’s superimposed on the
underlying network connection.
This error is a subtype of OSError.
The error code and message of SSLError
instances are provided by the OpenSSL library.
Changed in version 3.3: SSLError used to be a subtype of socket.error.
The range of possible values depends on the OpenSSL version.
New in version 3.3.
The range of possible values depends on the OpenSSL version.
New in version 3.3.
SSLZeroReturnError
A subclass of SSLError raised when trying to read or write and the SSL connection has been closed cleanly.
Note that this doesn’t mean that the underlying transport (read TCP) has been closed.
New in version 3.3.
SSLWantReadError
A subclass of SSLError raised by a non-blocking SSL socket when trying to read or write data, but more data
needs to be received on the underlying TCP transport before the request can be fulfilled.
New in version 3.3.
Unfortunately, there is no easy way to inspect the original errno number.
New in version 3.3.
SSLEOFError
A subclass of SSLError raised when the SSL connection has been terminated abruptly.
Generally, you shouldn’t
try to reuse the underlying transport when this error is encountered.
New in version 3.3.
New in version 3.7.
CertificateError
An alias for SSLCertVerificationError.
The exception is now an alias for SSLCertVerificationError.
Random generation
ssl.
RAND_bytes(num)
Return num cryptographically strong pseudo-random bytes.
Raises an SSLError if the PRNG has not been
seeded with enough data or if the operation is not supported by the current RAND method.
RAND_status()
can be used to check the status of the PRNG and RAND_add() can be used to seed the PRNG.
For almost all applications os.urandom() is preferable.
Read the Wikipedia article, Cryptographically secure pseudorandom number generator (CSPRNG), to get the
requirements of a cryptographically strong generator.
New in version 3.3.
ssl.
RAND_status()
Return True if the SSL pseudo-random number generator has been seeded with ‘enough’ randomness, and False
otherwise.
You can use ssl.RAND_egd() and ssl.RAND_add() to increase the randomness of the pseudo-
random number generator.
ssl.RAND_add(bytes, entropy)
Mix the given bytes into the SSL pseudo-random number generator.
The parameter entropy (a float) is a lower
bound on the entropy contained in string (so you can always use 0.0).
See RFC 1750 for more information on
sources of entropy.
Writable bytes-like object is now accepted.
Y %Z" strptime format (C locale).
Here’s an example:
5 09:34:43 2018 GMT")

2018-01-05 09:34:43
“notBefore” or “notAfter” dates must use GMT (RFC 5280).
Changed in version 3.5: Interpret the input time as a time in UTC as specified by ‘GMT’ timezone in the input
string.
Local timezone was used previously.
Given the address addr of an SSL-protected server, as a (hostname, port-number) pair, fetches the server’s certifi-
cate, and returns it as a PEM-encoded string.
If ssl_version is specified, uses that version of the SSL protocol
to attempt to connect to the server.
If ca_certs is specified, it should be a file containing a list of root certificates, the
same format as used for the cafile parameter in SSLContext.load_verify_locations().
The call will
attempt to validate the server certificate against that set of root certificates, and will fail if the validation attempt
fails.
A timeout can be specified with the timeout parameter.
Changed in version 3.3: This function is now IPv6-compatible.
The default ssl_version is changed from PROTOCOL_SSLv3 to PROTOCOL_TLS for
maximum compatibility with modern servers.
Given a certificate as a DER-encoded blob of bytes, returns a PEM-encoded string version of the same certificate.
ssl.
Given a certificate as an ASCII PEM string, returns a DER-encoded sequence of bytes for that same certificate.
ssl.get_default_verify_paths()
Returns a named tuple with paths to OpenSSL’s default cafile and capath.
The paths are the same
as used by SSLContext.set_default_verify_paths().
Retrieve certificates from Windows’ system cert store.
Windows may
provide additional cert stores, too.
The function returns a list of (cert_bytes, encoding_type, trust) tuples.
The encoding_type specifies the encoding
of cert_bytes.
It is either x509_asn for X.509 ASN.1 data or pkcs_7_asn for PKCS#7 ASN.1 data.
Trust
specifies the purpose of the certificate as a set of OIDS or exactly True if the certificate is trustworthy for all
purposes.
Retrieve CRLs from Windows’ system cert store.
Windows may
provide additional cert stores, too.
The function returns a list of (cert_bytes, encoding_type, trust) tuples.
The encoding_type specifies the encoding
of cert_bytes.
It is either x509_asn for X.509 ASN.1 data or pkcs_7_asn for PKCS#7 ASN.1 data.
Availability: Windows.
New in version 3.4.
Constants
All constants are now enum.
IntEnum or enum.
IntFlag collections.
New in version 3.6.
ssl.CERT_NONE
Possible value for SSLContext.verify_mode.
Except for PROTOCOL_TLS_CLIENT, it is the default
mode.
With client-side sockets, just about any cert is accepted.
Validation errors, such as untrusted or expired
cert, are ignored and do not abort the TLS/SSL handshake.
In server mode, no certificate is requested from the client, so the client does not send any for client cert authenti-
cation.
Possible value for SSLContext.verify_mode.
In client mode, CERT_OPTIONAL has the same meaning as
CERT_REQUIRED.
It is recommended to use CERT_REQUIRED for client-side sockets instead.
In server mode, a client certificate request is sent to the client.
The client may either ignore the request or send a
certificate in order perform TLS client cert authentication.
If the client chooses to send a certificate, it is verified.
Any verification error immediately aborts the TLS handshake.
Use
of
this
setting
requires
a
valid
set
of
CA
certificates
to
be
passed
to
SSLContext.
load_verify_locations().
In this mode, certificates are required from the other side of
the socket connection; an SSLError will be raised if no certificate is provided, or if its validation fails.
This mode
is not sufficient to verify a certificate in client mode as it does not match hostnames.
PROTOCOL_TLS_CLIENT uses CERT_REQUIRED and
enables check_hostname by default.
With server socket, this mode provides mandatory TLS client cert authentication.
A client certificate request is sent
to the client and the client must provide a valid and trusted certificate.
Use
of
this
setting
requires
a
valid
set
of
CA
certificates
to
be
passed
to
SSLContext.
load_verify_locations().
VerifyMode
enum.
IntEnum collection of CERT_* constants.
New in version 3.6.
ssl.VERIFY_DEFAULT
Possible value for SSLContext.verify_flags.
In this mode, certificate revocation lists (CRLs) are not
checked.
By default OpenSSL does neither require nor verify CRLs.
New in version 3.4.
ssl.VERIFY_CRL_CHECK_LEAF
Possible value for SSLContext.verify_flags.
In this mode, only the peer cert is checked but none of
the intermediate CA certificates.
The mode requires a valid CRL that is signed by the peer cert’s issuer (its direct
ancestor CA).
If no proper CRL has been loaded with SSLContext.load_verify_locations, validation
will fail.
New in version 3.4.
ssl.VERIFY_CRL_CHECK_CHAIN
Possible value for SSLContext.verify_flags.
In this mode, CRLs of all certificates in the peer cert chain
are checked.
New in version 3.4.
ssl.
VERIFY_X509_STRICT
Possible value for SSLContext.verify_flags to disable workarounds for broken X.509 certificates.
New in version 3.4.
ssl.
VERIFY_ALLOW_PROXY_CERTS
Possible value for SSLContext.verify_flags to enables proxy certificate verification.
New in version 3.10.
ssl.
VERIFY_X509_TRUSTED_FIRST
Possible value for SSLContext.verify_flags.
It instructs OpenSSL to prefer trusted certificates when
building the trust chain to validate a certificate.
This flag is enabled by default.
New in version 3.4.4.
ssl.
VERIFY_X509_PARTIAL_CHAIN
Possible value for SSLContext.verify_flags.
It instructs OpenSSL to accept intermediate CAs in the trust
store to be treated as trust-anchors, in the same way as the self-signed root CA certificates.
This makes it possible
to trust certificates issued by an intermediate CA without having to trust its ancestor root CA.
VerifyFlags
enum.
IntFlag collection of VERIFY_* constants.
New in version 3.6.
ssl.
PROTOCOL_TLS
Selects the highest protocol version that both the client and server support.
Despite the name, this option can select
both “SSL” and “TLS” protocols.
New in version 3.6.
TLS clients and servers require different default settings for secure com-
munication.
The generic TLS protocol constant is deprecated in favor of PROTOCOL_TLS_CLIENT and
PROTOCOL_TLS_SERVER.
PROTOCOL_TLS_CLIENT
Auto-negotiate the highest protocol version that both the client and server support, and configure the context client-
side connections.
The protocol enables CERT_REQUIRED and check_hostname by default.
New in version 3.6.
ssl.PROTOCOL_TLS_SERVER
Auto-negotiate the highest protocol version that both the client and server support, and configure the context server-
side connections.
New in version 3.6.
ssl.PROTOCOL_SSLv23
Alias for PROTOCOL_TLS.
Deprecated since version 3.6: Use PROTOCOL_TLS instead.
This protocol is not available if OpenSSL is compiled with the no-ssl3 option.
Warning: SSL version 3 is insecure.
Its use is highly discouraged.
Deprecated since version 3.6: OpenSSL has deprecated all version specific protocols.
OpenSSL has deprecated all version specific protocols.
PROTOCOL_TLSv1_1
Selects TLS version 1.1 as the channel encryption protocol.
Available only with openssl version 1.0.1+.
New in version 3.4.
Deprecated since version 3.6: OpenSSL has deprecated all version specific protocols.
PROTOCOL_TLSv1_2
Selects TLS version 1.2 as the channel encryption protocol.
Available only with openssl version 1.0.1+.
New in version 3.4.
Deprecated since version 3.6: OpenSSL has deprecated all version specific protocols.
This option is set by default.
It does
not necessarily set the same flags as OpenSSL’s SSL_OP_ALL constant.
New in version 3.2.
ssl.
OP_NO_SSLv2
Prevents an SSLv2 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents
the peers from choosing SSLv2 as the protocol version.
New in version 3.2.
SSLv2 is deprecated
ssl.
OP_NO_SSLv3
Prevents an SSLv3 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents
the peers from choosing SSLv3 as the protocol version.
New in version 3.2.
SSLv3 is deprecated
ssl.
OP_NO_TLSv1
Prevents a TLSv1 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents the
peers from choosing TLSv1 as the protocol version.
New in version 3.2.
The option is deprecated since OpenSSL 1.1.0, use the new SSLContext.
minimum_version and SSLContext.maximum_version instead.
OP_NO_TLSv1_1
Prevents a TLSv1.1 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents
the peers from choosing TLSv1.1 as the protocol version.
Available only with openssl version 1.0.1+.
New in version 3.4.
The option is deprecated since OpenSSL 1.1.0.
ssl.
OP_NO_TLSv1_2
Prevents a TLSv1.2 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents
the peers from choosing TLSv1.2 as the protocol version.
Available only with openssl version 1.0.1+.
New in version 3.4.
The option is deprecated since OpenSSL 1.1.0.
ssl.OP_NO_TLSv1_3
Prevents a TLSv1.3 connection.
This option is only applicable in conjunction with PROTOCOL_TLS.
It prevents
the peers from choosing TLSv1.3 as the protocol version.
TLS 1.3 is available with OpenSSL 1.1.1 or later.
When
Python has been compiled against an older version of OpenSSL, the flag defaults to 0.
New in version 3.7.
The option is deprecated since OpenSSL 1.1.0.
It was added to 2.7.15, 3.6.3 and
3.7.0 for backwards compatibility with OpenSSL 1.0.2.
Disable all renegotiation in TLSv1.2 and earlier.
Do not send HelloRequest messages, and ignore renegotiation
requests via ClientHello.
This option is only available with OpenSSL 1.1.0h and later.
This option has no effect on client sockets and
SSLv2 server sockets.
New in version 3.3.
ssl.
OP_SINGLE_DH_USE
Prevents re-use of the same DH key for distinct SSL sessions.
This improves forward secrecy but requires more
computational resources.
This option only applies to server sockets.
New in version 3.3.
ssl.OP_SINGLE_ECDH_USE
Prevents re-use of the same ECDH key for distinct SSL sessions.
This improves forward secrecy but requires more
computational resources.
This option only applies to server sockets.
New in version 3.3.
ssl.OP_ENABLE_MIDDLEBOX_COMPAT
Send dummy Change Cipher Spec (CCS) messages in TLS 1.3 handshake to make a TLS 1.3 connection look more
like a TLS 1.2 connection.
This option is only available with OpenSSL 1.1.1 and later.
Disable compression on the SSL channel.
This is useful if the application protocol supports its own compression
scheme.
New in version 3.3.
IntFlag collection of OP_* constants.
New in version 3.6.
ssl.OP_IGNORE_UNEXPECTED_EOF
Ignore unexpected shutdown of TLS connections.
This option is only available with OpenSSL 3.0.0 and later.
New in version 3.10.
ssl.OP_ENABLE_KTLS
Enable the use of the kernel TLS.
To benefit from the feature, OpenSSL must have been compiled with support
for it, and the negotiated cipher suites and extensions must be supported by it (a list of supported ones may vary by
platform and kernel version).
Note that with enabled kernel TLS some cryptographic operations are performed by the kernel directly and not via
any available OpenSSL Providers.
This might be undesirable if, for example, the application requires all crypto-
graphic operations to be performed by the FIPS provider.
This option is only available with OpenSSL 3.0.0 and later.
New in version 3.12.
OP_LEGACY_SERVER_CONNECT
Allow legacy insecure renegotiation between OpenSSL and unpatched servers only.
New in version 3.12.
ssl.HAS_ALPN
Whether the OpenSSL library has built-in support for the Application-Layer Protocol Negotiation TLS extension as
described in RFC 7301.
New in version 3.5.
ssl.
HAS_NEVER_CHECK_COMMON_NAME
Whether the OpenSSL library has built-in support not checking subject common name and SSLContext.
hostname_checks_common_name is writeable.
New in version 3.7.
ssl.
HAS_ECDH
Whether the OpenSSL library has built-in support for the Elliptic Curve-based Diffie-Hellman key exchange.
This
should be true unless the feature was explicitly disabled by the distributor.
New in version 3.3.
ssl.HAS_SNI
Whether the OpenSSL library has built-in support for the Server Name Indication extension (as defined in RFC
6066).
Whether the OpenSSL library has built-in support for the Next Protocol Negotiation as described in the Application
Layer Protocol Negotiation.
When true, you can use the SSLContext.set_npn_protocols() method to
advertise which protocols you want to support.
New in version 3.3.
ssl.
Whether the OpenSSL library has built-in support for the SSL 2.0 protocol.
New in version 3.7.
ssl.HAS_SSLv3
Whether the OpenSSL library has built-in support for the SSL 3.0 protocol.
Whether the OpenSSL library has built-in support for the TLS 1.0 protocol.
New in version 3.7.
ssl.HAS_TLSv1_1
Whether the OpenSSL library has built-in support for the TLS 1.1 protocol.
Whether the OpenSSL library has built-in support for the TLS 1.2 protocol.
Whether the OpenSSL library has built-in support for the TLS 1.3 protocol.
New in version 3.7.
ssl.CHANNEL_BINDING_TYPES
List of supported TLS channel binding types.
Strings in this list can be used as arguments to SSLSocket.
A tuple of five integers representing version information about the OpenSSL library:
(1, 0, 2, 11, 15)
New in version 3.2.
ssl.OPENSSL_VERSION_NUMBER
The raw version number of the OpenSSL library, as a single integer:

'0x100020bf'
New in version 3.2.
ssl.ALERT_DESCRIPTION_HANDSHAKE_FAILURE
ssl.ALERT_DESCRIPTION_INTERNAL_ERROR
ALERT_DESCRIPTION_*
Alert Descriptions from RFC 5246 and others.
The IANA TLS Alert Registry contains this list and references to
the RFCs where their meaning is defined.
Used as the return value of the callback function in SSLContext.set_servername_callback().
New in version 3.4.
AlertDescription
enum.
IntEnum collection of ALERT_DESCRIPTION_* constants.
New in version 3.6.
Purpose.SERVER_AUTH
Option for create_default_context() and SSLContext.load_default_certs().
This value
indicates that the context may be used to authenticate web servers (therefore, it will be used to create client-side
sockets).
New in version 3.4.
CLIENT_AUTH
Option for create_default_context() and SSLContext.load_default_certs().
This value
indicates that the context may be used to authenticate web clients (therefore, it will be used to create server-side
sockets).
New in version 3.4.
IntEnum collection of SSL_ERROR_* constants.
New in version 3.6.
IntEnum collection of SSL and TLS versions for SSLContext.maximum_version and
SSLContext.minimum_version.
New in version 3.7.
TLSVersion.MINIMUM_SUPPORTED
TLSVersion.MAXIMUM_SUPPORTED
The minimum or maximum supported SSL or TLS version.
These are magic constants.
Their values don’t reflect
the lowest and highest available TLS/SSL versions.
TLSVersion.
SSLv3
TLSVersion.
TLSv1
TLSVersion.
TLSv1_1
TLSVersion.
TLSv1_2
TLSVersion.
TLSv1_3
SSL 3.0 to TLS 1.3.
Deprecated since version 3.10:
All TLSVersion members except TLSVersion.
TLSv1_2 and
TLSVersion.
TLSv1_3 are deprecated.
However, since the SSL (and TLS) protocol has its own framing atop of TCP, the SSL sockets abstraction can, in
certain respects, diverge from the specification of normal, OS-level sockets.
See especially the notes on non-blocking
sockets.
Instances of SSLSocket must be created using the SSLContext.wrap_socket() method.
The sendfile() method was added.
Changed in version 3.5: The shutdown() does not reset the socket timeout each time bytes are received or sent.
The socket timeout is now the maximum total duration of the shutdown.
It is deprecated to create a SSLSocket instance directly, use SSLContext.
wrap_socket() to wrap a socket.
Changed in version 3.7: SSLSocket instances must to created with wrap_socket().
In earlier versions, it
was possible to create instances directly.
This was never documented or officially supported.
Python now uses SSL_read_ex and SSL_write_ex internally.
The functions sup-
port reading and writing of data larger than 2 GB.
Writing zero-length data no longer fails with a protocol violation
error.
SSL sockets also have the following additional methods and attributes:
SSLSocket.read(len=1024, buffer=None)
Read up to len bytes of data from the SSL socket and return the result as a bytes instance.
If buffer is specified,
then read into the buffer instead, and return the number of bytes read.
Raise SSLWantReadError or SSLWantWriteError if the socket is non-blocking and the read would block.
As at any time a re-negotiation is possible, a call to read() can also cause write operations.
The socket timeout is no longer reset each time bytes are received or sent.
The socket
timeout is now the maximum total duration to read up to len bytes.
Use recv() instead of read().
SSLSocket.write(buf)
Write buf to the SSL socket and return the number of bytes written.
The buf argument must be an object supporting
the buffer interface.
Raise SSLWantReadError or SSLWantWriteError if the socket is non-blocking and the write would block.
As at any time a re-negotiation is possible, a call to write() can also cause read operations.
The socket timeout is no longer reset each time bytes are received or sent.
The socket
timeout is now the maximum total duration to write buf.
Deprecated since version 3.6: Use send() instead of write().
Note:
The read() and write() methods are the low-level methods that read and write unencrypted, application-
level data and decrypt/encrypt it to encrypted, wire-level data.
These methods require an active SSL connection, i.e. the
handshake was completed and SSLSocket.unwrap() was not called.
Normally you should use the socket API methods like recv() and send() instead of these methods.
18.3.
The handshake method also performs match_hostname() when the
check_hostname attribute of the socket’s context is true.
The socket timeout is no longer reset each time bytes are received or sent.
The socket
timeout is now the maximum total duration of the handshake.
Changed in version 3.7: Hostname or IP address is matched by OpenSSL during handshake.
The function
match_hostname() is no longer used.
In case OpenSSL refuses a hostname or IP address, the handshake
is aborted early and a TLS alert message is sent to the peer.
SSLSocket.getpeercert(binary_form=False)
If there is no certificate for the peer on the other end of the connection, return None.
If the SSL handshake hasn’t
been done yet, raise ValueError.
If the binary_form parameter is False, and a certificate was received from the peer, this method returns a
dict instance.
If the certificate was not validated, the dict is empty.
If the certificate was validated, it returns a
dict with several keys, amongst them subject (the principal for which the certificate was issued) and issuer
(the principal issuing the certificate).
If a certificate contains an instance of the Subject Alternative Name extension
(see RFC 3280), there will also be a subjectAltName key in the dictionary.
The subject and issuer fields are tuples containing the sequence of relative distinguished names (RDNs)
given in the certificate’s data structure for the respective fields, and each RDN is a sequence of name-value pairs.
Here is a real-world example:
{'issuer': ((('countryName', 'IL'),),
(('organizationName', 'StartCom Ltd.'),),
(('organizationalUnitName',
'Secure Digital Certificate Signing'),),
(('commonName',
'StartCom Class 2 Primary Intermediate Server CA'),)),
'notAfter': 'Nov 22 08:15:19 2013 GMT',
'notBefore': 'Nov 21 03:09:52 2011 GMT',
'serialNumber': '95F0',
'subject': ((('description', '571208-SLe257oHY9fVQ07Z'),),
(('countryName', 'US'),),
(('stateOrProvinceName', 'California'),),
(('localityName', 'San Francisco'),),
(('organizationName', 'Electronic Frontier Foundation, Inc.'),),
(('commonName', '*.eff.org'),),
(('emailAddress', 'hostmaster@eff.org'),)),
'subjectAltName': (('DNS', '*.eff.org'), ('DNS', 'eff.org')),
'version': 3}
If the binary_form parameter is True, and a certificate was provided, this method returns the DER-encoded
form of the entire certificate as a sequence of bytes, or None if the peer did not provide a certificate.
Whether the
peer provides a certificate depends on the SSL socket’s role:
• for a client SSL socket, the server will always provide a certificate, regardless of whether validation was
required;
• for a server SSL socket, the client will only provide a certificate when requested by the server; there-
fore getpeercert() will return None if you used CERT_NONE (rather than CERT_OPTIONAL or
CERT_REQUIRED).
See also SSLContext.check_hostname.
The returned dictionary includes
additional X509v3 extension items such as crlDistributionPoints, caIssuers and OCSP URIs.
IPv6 address strings no longer have a trailing new line.
SSLSocket.cipher()
Returns a three-value tuple containing the name of the cipher being used, the version of the SSL protocol that
defines its use, and the number of secret bits being used.
If no connection has been established, returns None.
SSLSocket.shared_ciphers()
Return the list of ciphers available in both the client and server.
Each entry of the returned list is a three-value tuple
containing the name of the cipher, the version of the SSL protocol that defines its use, and the number of secret
bits the cipher uses.
New in version 3.5.
SSLSocket.compression()
Return the compression algorithm being used as a string, or None if the connection isn’t compressed.
If the higher-level protocol supports its own compression mechanism, you can use OP_NO_COMPRESSION to
disable SSL-level compression.
New in version 3.3.
SSLSocket.get_channel_binding(cb_type=’tls-unique’)
Get channel binding data for current connection, as a bytes object.
Returns None if not connected or the handshake
has not been completed.
The cb_type parameter allow selection of the desired channel binding type.
Valid channel binding types are listed
in the CHANNEL_BINDING_TYPES list.
Currently only the ‘tls-unique’ channel binding, defined by RFC 5929,
is supported.
ValueError will be raised if an unsupported channel binding type is requested.
New in version 3.3.
SSLSocket.selected_alpn_protocol()
Return the protocol that was selected during the TLS handshake.
If SSLContext.set_alpn_protocols()
was not called, if the other party does not support ALPN, if this socket does not support any of the client’s proposed
protocols, or if the handshake has not happened yet, None is returned.
New in version 3.5.
SSLSocket.selected_npn_protocol()
Return the higher-level protocol that was selected during the TLS/SSL handshake.
If SSLContext.
set_npn_protocols() was not called, or if the other party does not support NPN, or if the handshake has
not yet happened, this will return None.
New in version 3.3.
Performs the SSL shutdown handshake, which removes the TLS layer from the underlying socket, and returns the
underlying socket object.
This can be used to go from encrypted operation over a connection to unencrypted.
The
returned socket should always be used for further communication with the other side of the connection, rather than
the original socket.
SSLSocket.verify_client_post_handshake()
Requests post-handshake authentication (PHA) from a TLS 1.3 client.
PHA can only be initiated for a TLS 1.3
connection from a server-side socket, after the initial TLS handshake and with PHA enabled on both sides, see
SSLContext.post_handshake_auth.
The method does not perform a cert exchange immediately.
The server-side sends a CertificateRequest during the
next write event and expects the client to respond with a certificate on the next read event.
If any precondition isn’t met (e.g. not TLS 1.3, PHA not enabled), an SSLError is raised.
Note:
Only available with OpenSSL 1.1.1 and TLS 1.3 enabled.
Without TLS 1.3 support, the method raises
NotImplementedError.
New in version 3.8.
SSLSocket.version()
Return the actual SSL protocol version negotiated by the connection as a string, or None if no secure connection
is established.
As of this writing, possible return values include "SSLv2", "SSLv3", "TLSv1", "TLSv1.1"
and "TLSv1.2".
Recent OpenSSL versions may define more return values.
New in version 3.5.
SSLSocket.pending()
Returns the number of already decrypted bytes available for read, pending on the connection.
SSLSocket.context
The SSLContext object this SSL socket is tied to.
New in version 3.2.
SSLSocket.server_side
A boolean which is True for server-side sockets and False for client-side sockets.
New in version 3.2.
SSLSocket.server_hostname
Hostname of the server: str type, or None for server-side socket or if the hostname was not specified in the
constructor.
New in version 3.2.
Changed in version 3.7: The attribute is now always ASCII text.
When server_hostname is an internation-
alized domain name (IDN), this attribute now stores the A-label form ("xn--pythn-mua.org"), rather than
the U-label form ("pythön.org").
SSLSocket.session
The SSLSession for this SSL connection.
The session is available for client and server side sockets after the
TLS handshake has been performed.
For client sockets the session can be set before do_handshake() has been
called to reuse a session.
New in version 3.6.
An SSL context holds various data longer-lived than single SSL connections, such as SSL configuration options, certifi-
cate(s) and private key(s).
It also manages a cache of SSL sessions for server-side sockets, in order to speed up repeated
connections from the same clients.
SSLContext(protocol=None)
Create a new SSL context.
You may pass protocol which must be one of the PROTOCOL_* constants defined in this
module.
The parameter specifies which version of the SSL protocol to use.
Typically, the server chooses a particular
protocol version, and the client must adapt to the server’s choice.
Most of the versions are not interoperable with
the other versions.
If not specified, the default is PROTOCOL_TLS; it provides the most compatibility with other
versions.
Here’s a table showing which versions in a client (down the side) can connect to which versions in a server (along
the top):
client / server
SSLv2
SSLv3
TLS3
TLSv1
TLSv1.1
TLSv1.2
SSLv2
yes
no
no1
no
no
no
SSLv3
no
yes
no2
no
no
no
TLS (SSLv23)?
The context is created with secure default values.
The options OP_NO_COMPRESSION,
OP_CIPHER_SERVER_PREFERENCE,
OP_SINGLE_DH_USE,
OP_SINGLE_ECDH_USE,
OP_NO_SSLv2, and OP_NO_SSLv3 (except for PROTOCOL_SSLv3) are set by default.
The initial
cipher suite list contains only HIGH ciphers, no NULL ciphers and no MD5 ciphers.
Deprecated since version 3.10: SSLContext without protocol argument is deprecated.
The context class will
either require PROTOCOL_TLS_CLIENT or PROTOCOL_TLS_SERVER protocol in the future.
The default cipher suites now include only secure AES and ChaCha20 ciphers with
forward secrecy and security level 2. RSA and DH keys with less than 2048 bits and ECC keys with less than 224
bits are prohibited.
PROTOCOL_TLS, PROTOCOL_TLS_CLIENT, and PROTOCOL_TLS_SERVER use TLS
1.2 as minimum TLS version.
SSLContext objects have the following methods and attributes:
SSLContext.cert_store_stats()
Get statistics about quantities of loaded X.509 certificates, count of X.509 certificates flagged as CA certificates
and certificate revocation lists as dictionary.
Example for a context with one CA cert and one other cert:
{'crl': 0, 'x509_ca': 1, 'x509': 2}
New in version 3.4.
There is no dedicated PROTOCOL constant for just TLS 1.3.
The certfile string must be the path to a single file in PEM
format containing the certificate as well as any number of CA certificates needed to establish the certificate’s au-
thenticity.
The keyfile string, if present, must point to a file containing the private key.
Otherwise the private key
will be taken from certfile as well.
See the discussion of Certificates for more information on how the certificate is
stored in the certfile.
The password argument may be a function to call to get the password for decrypting the private key.
It will only
be called if the private key is encrypted and a password is necessary.
It will be called with no arguments, and it
should return a string, bytes, or bytearray.
If the return value is a string it will be encoded as UTF-8 before using
it to decrypt the key.
Alternatively a string, bytes, or bytearray value may be supplied directly as the password
argument.
It will be ignored if the private key is not encrypted and no password is needed.
If the password argument is not specified and a password is required, OpenSSL’s built-in password prompting
mechanism will be used to interactively prompt the user for a password.
An SSLError is raised if the private key doesn’t match with the certificate.
Changed in version 3.3: New optional argument password.
On Windows
it loads CA certs from the CA and ROOT system stores.
On all systems it calls SSLContext.
In the future the method may load CA certificates from other locations,
too.
The purpose flag specifies what kind of CA certificates are loaded.
The default settings Purpose.SERVER_AUTH
loads certificates, that are flagged and trusted for TLS web server authentication (client side sockets).
Purpose.
CLIENT_AUTH loads CA certificates for client certificate verification on the server side.
New in version 3.4.
At least one of cafile or capath must be specified.
This method can also load certification revocation lists (CRLs) in PEM or DER format.
In order to make use of
CRLs, SSLContext.verify_flags must be configured properly.
The cafile string, if present, is the path to a file of concatenated CA certificates in PEM format.
See the discussion
of Certificates for more information about how to arrange the certificates in this file.
The capath string, if present, is the path to a directory containing several CA certificates in PEM format, following
an OpenSSL specific layout.
The cadata object, if present, is either an ASCII string of one or more PEM-encoded certificates or a bytes-like
object of DER-encoded certificates.
Like with capath extra lines around PEM-encoded certificates are ignored but
at least one certificate must be present.
New optional argument cadata
SSLContext.get_ca_certs(binary_form=False)
Get a list of loaded “certification authority” (CA) certificates.
If the binary_form parameter is False each list
entry is a dict like the output of SSLSocket.getpeercert().
Otherwise the method returns a list of DER-
encoded certificates.
The returned list does not contain certificates from capath unless a certificate was requested
and loaded by a SSL connection.
Note: Certificates in a capath directory aren’t loaded unless they have been used at least once.
The list is in order of cipher priority.
See SSLContext.set_ciphers().
Unfortunately, there’s no easy way to know whether this method succeeds: no error is returned
if no certificates are to be found.
When the OpenSSL library is provided as part of the operating system, though,
it is likely to be configured properly.
SSLContext.set_ciphers(ciphers)
Set the available ciphers for sockets created with this context.
It should be a string in the OpenSSL cipher list
format.
If no cipher can be selected (because compile-time options or other configuration forbids use of all the
specified ciphers), an SSLError will be raised.
Note:
when connected, the SSLSocket.cipher() method of SSL sockets will give the currently selected
cipher.
TLS 1.3 cipher suites cannot be disabled with set_ciphers().
SSLContext.set_alpn_protocols(protocols)
Specify which protocols the socket should advertise during the SSL/TLS handshake.
It should be a list of ASCII
strings, like ['http/1.1', 'spdy/2'], ordered by preference.
The selection of a protocol will happen
18.3.
After a successful handshake, the SSLSocket.
This method will raise NotImplementedError if HAS_ALPN is False.
New in version 3.5.
SSLContext.set_npn_protocols(protocols)
Specify which protocols the socket should advertise during the SSL/TLS handshake.
It should be a list of strings,
like ['http/1.1', 'spdy/2'], ordered by preference.
The selection of a protocol will happen during the
handshake, and will play out according to the Application Layer Protocol Negotiation.
After a successful handshake,
the SSLSocket.selected_npn_protocol() method will return the agreed-upon protocol.
This method will raise NotImplementedError if HAS_NPN is False.
New in version 3.3.
Deprecated since version 3.10: NPN has been superseded by ALPN
SSLContext.sni_callback
Register a callback function that will be called after the TLS Client Hello handshake message has been received by
the SSL/TLS server when the TLS client specifies a server name indication.
The server name indication mechanism
is specified in RFC 6066 section 3 - Server Name Indication.
Only one callback can be set per SSLContext.
If sni_callback is set to None then the callback is disabled.
Calling this function a subsequent time will disable the previously registered callback.
The callback function will be called with three arguments; the first being the ssl.
SSLSocket, the second is a
string that represents the server name that the client is intending to communicate (or None if the TLS Client Hello
does not contain a server name) and the third argument is the original SSLContext.
The server name argument
is text.
For internationalized domain name, the server name is an IDN A-label ("xn--pythn-mua.org").
A typical use of this callback is to change the ssl.
SSLSocket’s SSLSocket.context attribute to a new
object of type SSLContext representing a certificate chain that matches the server name.
Due to the early negotiation phase of the TLS connection, only limited methods and attributes are usable
like SSLSocket.selected_alpn_protocol() and SSLSocket.context.
The SSLSocket.
Hello and therefore will not return meaningful values nor
can they be called safely.
The sni_callback function must return None to allow the TLS negotiation to continue.
If a TLS failure is required,
a constant ALERT_DESCRIPTION_* can be returned.
Other return values will result in a TLS fatal error with
ALERT_DESCRIPTION_INTERNAL_ERROR.
If an exception is raised from the sni_callback function the TLS connection will terminate with a fatal TLS alert
message ALERT_DESCRIPTION_HANDSHAKE_FAILURE.
This method will raise NotImplementedError if the OpenSSL library had OPENSSL_NO_TLSEXT defined
when it was built.
This is a legacy API retained for backwards compatibility.
When possible, you should use sni_callback
instead.
The given server_name_callback is similar to sni_callback, except that when the server hostname is an
IDN-encoded internationalized domain name, the server_name_callback receives a decoded U-label ("pythön.
org").
Using DH key exchange improves
forward secrecy at the expense of computational resources (both on the server and on the client).
The dhfile
parameter should be the path to a file containing DH parameters in PEM format.
This setting doesn’t apply to client sockets.
You can also use the OP_SINGLE_DH_USE option to further improve
security.
New in version 3.3.
SSLContext.set_ecdh_curve(curve_name)
Set the curve name for Elliptic Curve-based Diffie-Hellman (ECDH) key exchange.
ECDH is significantly faster
than regular DH while arguably as secure.
The curve_name parameter should be a string describing a well-known
elliptic curve, for example prime256v1 for a widely supported curve.
This setting doesn’t apply to client sockets.
You can also use the OP_SINGLE_ECDH_USE option to further
improve security.
This method is not available if HAS_ECDH is False.
New in version 3.3.
See also:
SSL/TLS & Perfect Forward Secrecy Vincent Bernat.
SSLContext.wrap_socket(sock, server_side=False, do_handshake_on_connect=True,
suppress_ragged_eofs=True, server_hostname=None, session=None)
Wrap an existing Python socket sock and return an instance of SSLContext.sslsocket_class (default
SSLSocket).
The returned SSL socket is tied to the context, its settings and certificates.
The parameter server_side is a boolean which identifies whether server-side or client-side behavior is desired
from this socket.
For client-side sockets, the context construction is lazy; if the underlying socket isn’t connected yet, the context
construction will be performed after connect() is called on the socket.
For server-side sockets, if the socket has
no remote peer, it is assumed to be a listening socket, and the server-side SSL wrapping is automatically performed
on client connections accepted via the accept() method.
The method may raise SSLError.
On client connections, the optional parameter server_hostname specifies the hostname of the service which we
are connecting to.
This allows a single server to host multiple SSL-based services with distinct certificates, quite
similarly to HTTP virtual hosts.
Specifying server_hostname will raise a ValueError if server_side is true.
The parameter do_handshake_on_connect specifies whether to do the SSL handshake automatically af-
ter doing a socket.connect(), or whether the application program will call it explicitly, by invoking the
SSLSocket.do_handshake() method.
Calling SSLSocket.do_handshake() explicitly gives the
program control over the blocking behavior of the socket I/O involved in the handshake.
The parameter suppress_ragged_eofs specifies how the SSLSocket.recv() method should signal un-
expected EOF from the other end of the connection.
If specified as True (the default), it returns a normal EOF
(an empty bytes object) in response to unexpected EOF errors raised from the underlying socket; if False, it will
raise the exceptions back to the caller.
session, see session.
Changed in version 3.5: Always allow a server_hostname to be passed, even if OpenSSL does not have SNI.
Changed in version 3.6: session argument was added.
SSLContext.sslsocket_class
The return type of SSLContext.wrap_socket(), defaults to SSLSocket.
The attribute can be overridden
on instance of class in order to return a custom subclass of SSLSocket.
New in version 3.7.
SSLContext.wrap_bio(incoming, outgoing, server_side=False, server_hostname=None, session=None)
Wrap the BIO objects incoming and outgoing and return an instance of SSLContext.sslobject_class
(default SSLObject).
The SSL routines will read input data from the incoming BIO and write data to the outgoing
BIO.
The server_side, server_hostname and session parameters have the same meaning as in SSLContext.
wrap_socket().
Changed in version 3.6: session argument was added.
The method returns an instance of SSLContext.sslobject_class instead of
hard-coded SSLObject.
SSLContext.sslobject_class
The return type of SSLContext.wrap_bio(), defaults to SSLObject.
The attribute can be overridden on
instance of class in order to return a custom subclass of SSLObject.
New in version 3.7.
SSLContext.session_stats()
Get statistics about the SSL sessions created or managed by this context.
A dictionary is returned which maps the
names of each piece of information to their numeric values.
For example, here is the total number of hits and
misses in the session cache since the context was created:
(0, 0)
SSLContext.check_hostname
Whether to match the peer cert’s hostname in SSLSocket.do_handshake().
The context’s
verify_mode must be set to CERT_OPTIONAL or CERT_REQUIRED, and you must pass server_hostname
to wrap_socket() in order to match the hostname.
Enabling hostname checking automatically sets
verify_mode from CERT_NONE to CERT_REQUIRED.
It cannot be set back to CERT_NONE as long as
hostname checking is enabled.
The PROTOCOL_TLS_CLIENT protocol enables hostname checking by default.
With other protocols, hostname checking must be enabled explicitly.
Example:
import socket, ssl
context = ssl.SSLContext(ssl.
PROTOCOL_TLSv1_2)
context.verify_mode = ssl.CERT_REQUIRED
context.check_hostname = True
context.load_default_certs()
s = socket.socket(socket.
Previously the same operation would have failed with a
ValueError.
SSLContext.keylog_filename
Write TLS keys to a keylog file, whenever key material is generated or received.
The keylog file is designed for
debugging purposes only.
The file format is specified by NSS and used by many traffic analyzers such as Wireshark.
The log file is opened in append-only mode.
Writes are synchronized between threads, but not between processes.
New in version 3.8.
SSLContext.maximum_version
A TLSVersion enum member representing the highest supported TLS version.
The value defaults to
TLSVersion.MAXIMUM_SUPPORTED.
The attribute is read-only for protocols other than PROTOCOL_TLS,
PROTOCOL_TLS_CLIENT, and PROTOCOL_TLS_SERVER.
The attributes maximum_version, minimum_version and SSLContext.options all affect the sup-
ported SSL and TLS versions of the context.
The implementation does not prevent invalid combination.
For exam-
ple a context with OP_NO_TLSv1_2 in options and maximum_version set to TLSVersion.
TLSv1_2
will not be able to establish a TLS 1.2 connection.
New in version 3.7.
SSLContext.minimum_version
Like SSLContext.maximum_version except it is the lowest supported version or TLSVersion.
MINIMUM_SUPPORTED.
New in version 3.7.
SSLContext.num_tickets
Control the number of TLS 1.3 session tickets of a PROTOCOL_TLS_SERVER context.
The setting has no impact
on TLS 1.0 to 1.2 connections.
New in version 3.8.
SSLContext.options
An integer representing the set of SSL options enabled on this context.
The default value is OP_ALL, but you can
specify other options such as OP_NO_SSLv2 by ORing them together.
Changed in version 3.6: SSLContext.options returns Options flags:
<Options.
OP_ALL|OP_NO_SSLv3|OP_NO_SSLv2|OP_NO_COMPRESSION: 2197947391>
Deprecated since version 3.7: All OP_NO_SSL* and OP_NO_TLS* options have been deprecated since Python
3.7.
Use SSLContext.minimum_version and SSLContext.maximum_version instead.
SSLContext.post_handshake_auth
Enable TLS 1.3 post-handshake client authentication.
Post-handshake auth is disabled by default and a server can
only request a TLS client certificate during the initial handshake.
When enabled, a server may request a TLS client
certificate at any time after the handshake.
When enabled on client-side sockets, the client signals the server that it supports post-handshake authentication.
When enabled on server-side sockets, SSLContext.verify_mode must be set to CERT_OPTIONAL
or
CERT_REQUIRED,
too.
The
actual
client
cert
exchange
is
delayed
until
SSLSocket.
New in version 3.8.
The protocol version chosen when constructing the context.
This attribute is read-only.
SSLContext.hostname_checks_common_name
Whether check_hostname falls back to verify the cert’s subject common name in the absence of a subject
alternative name extension (default: true).
New in version 3.7.
The flag had no effect with OpenSSL before version 1.1.1k.
Python 3.8.9, 3.9.3, and
3.10 include workarounds for previous versions.
An integer representing the security level for the context.
This attribute is read-only.
New in version 3.10.
SSLContext.verify_flags
The flags for certificate verification operations.
You can set flags like VERIFY_CRL_CHECK_LEAF by ORing
them together.
By default OpenSSL does neither require nor verify certificate revocation lists (CRLs).
New in version 3.4.
SSLContext.verify_flags returns VerifyFlags flags:
<VerifyFlags.
VERIFY_X509_TRUSTED_FIRST: 32768>
SSLContext.verify_mode
Whether to try to verify other peers’ certificates and how to behave if verification fails.
This attribute must be one
of CERT_NONE, CERT_OPTIONAL or CERT_REQUIRED.
SSLContext.verify_mode returns VerifyMode enum:
<VerifyMode.CERT_REQUIRED: 2>
18.3.4 Certificates
Certificates in general are part of a public-key / private-key system.
In this system, each principal, (which may be a
machine, or a person, or an organization) is assigned a unique two-part encryption key.
One part of the key is public, and
is called the public key; the other part is kept secret, and is called the private key.
The two parts are related, in that if you
encrypt a message with one of the parts, you can decrypt it with the other part, and only with the other part.
A certificate contains information about two principals.
It contains the name of a subject, and the subject’s public key.
It also contains a statement by a second principal, the issuer, that the subject is who they claim to be, and that this is
indeed the subject’s public key.
The issuer’s statement is signed with the issuer’s private key, which only the issuer knows.
However, anyone can verify the issuer’s statement by finding the issuer’s public key, decrypting the statement with it, and
comparing it to the other information in the certificate.
The certificate also contains information about the time period
over which it is valid.
This is expressed as two fields, called “notBefore” and “notAfter”.
In the Python use of certificates, a client or server can use a certificate to prove who they are.
The other side of a network
connection can also be required to produce a certificate, and that certificate can be validated to the satisfaction of the
client or server that requires such validation.
The connection attempt can be set to raise an exception if the validation
fails.
Validation is done automatically, by the underlying OpenSSL framework; the application need not concern itself
with its mechanics.
But the application does usually need to provide sets of certificates to allow this process to take place.
They should be formatted as “PEM” (see RFC 1422), which is a base-64 encoded
form wrapped with a header line and a footer line:
-----BEGIN CERTIFICATE-----
... (certificate in base64 PEM encoding) ...
This chain should start with the specific certificate for the principal who “is” the client or server, and then the certificate
for the issuer of that certificate, and then the certificate for the issuer of that certificate, and so on up the chain till you
get to a certificate which is self-signed, that is, a certificate which has the same subject and issuer, sometimes called a
root certificate.
The certificates should just be concatenated together in the certificate file.
For example, suppose we had
a three certificate chain, from our server certificate to the certificate of the certification authority that signed our server
certificate, to the root certificate of the agency which issued the certification authority’s certificate:
-----BEGIN CERTIFICATE-----
... (certificate for your server)...
Again, this file just contains these chains
concatenated together.
For validation, Python will use the first chain it finds in the file which matches.
The platform’s
certificates file can be used by calling SSLContext.load_default_certs(), this is done automatically with
create_default_context().
Combined key and certificate
Often the private key is stored in the same file as the certificate; in this case, only the certfile parameter to
SSLContext.load_cert_chain() needs to be passed.
If the private key is stored with the certificate, it should
come before the first certificate in the certificate chain:
-----BEGIN RSA PRIVATE KEY-----
... (private key in base64 encoding) ...
There are many ways of acquiring appropriate certificates, such as buying one from a certification authority.
Another common practice is to generate a self-signed certificate.
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
For some fields there will be a default value,
If you enter '.
The disadvantage of a self-signed certificate is that it is its own root certificate, and no one else will have it in their cache
of known (and trusted) root certificates.
This example creates a SSL context with the recommended security settings for client sockets, including automatic cer-
tificate verification:
If you prefer to tune security settings yourself, you might create a context from scratch (but beware that you might not
get the settings right):
(this snippet assumes your operating system places a bundle of all CA certificates in /etc/ssl/certs/ca-bundle.
The PROTOCOL_TLS_CLIENT protocol configures the context for cert validation and hostname verification.
All other protocols create SSL
contexts with insecure defaults.
You may then fetch the certificate:
Visual inspection shows that the certificate does identify the desired service (that is, the HTTPS host www.python.
Server-side operation
For server operation, typically you’ll need to have a server certificate, and private key, each in a file.
You’ll first create a
context holding the key and the certificate, so that clients can check your authenticity.
Then you’ll open a socket, bind it
to a port, call listen() on it, and start waiting for clients to connect:
import socket, ssl
context = ssl.create_default_context(ssl.
When a client connects, you’ll call accept() on the socket to get the new socket from the other end, and use the
context’s SSLContext.wrap_socket() method to create a server-side SSL socket for the connection:
while True:
newsocket, fromaddr = bindsocket.accept()
connstream = context.wrap_socket(newsocket, server_side=True)
try:
deal_with_client(connstream)
finally:
connstream.shutdown(socket.
SHUT_RDWR)
connstream.close()
Then you’ll read data from the connstream and do something with it till you are finished with the client (or the client
is finished with you):

# empty data means the client is finished with us
while data:
if not do_something(connstream, data):
# we'll assume do_something returns False
# when we're finished with client
break
data = connstream.recv(1024)
# finished with client
And go back to listening for new client connections (of course, a real server would probably handle each client connection
in a separate thread, or put the sockets in non-blocking mode and use an event loop).
When working with non-blocking
sockets, there are thus several things you need to be aware of:
• Most SSLSocket methods will raise either SSLWantWriteError or SSLWantReadError instead of
BlockingIOError if an I/O operation would block.
SSLWantReadError will be raised if a read oper-
ation on the underlying socket is necessary, and SSLWantWriteError for a write operation on the underlying
socket.
Note that attempts to write to an SSL socket may require reading from the underlying socket first, and
attempts to read from the SSL socket may require a prior write to the underlying socket.
In earlier Python versions, the SSLSocket.send() method returned zero instead of
raising SSLWantWriteError or SSLWantReadError.
For example, only part of an SSL frame might have arrived.
Therefore, you must be ready to handle SSLSocket.recv() and SSLSocket.send() failures, and retry
after another call to select().
Therefore, you should first call SSLSocket.recv() to drain any potentially
available data, and then only block on a select() call if still necessary.
The SSL handshake itself will be non-blocking: the SSLSocket.do_handshake() method has to be retried
until it returns successfully.
Here is a synopsis using select() to wait for the socket’s readiness:
18.3.
It polls for events using the
selectors module and handles SSLWantWriteError, SSLWantReadError and BlockingIOError ex-
ceptions.
It runs the SSL handshake asynchronously as well.
Memory BIO Support
New in version 3.5.
Ever since the SSL module was introduced in Python 2.6, the SSLSocket class has provided two related but distinct
areas of functionality:
• SSL protocol handling
• Network IO
The network IO API is identical to that provided by socket.socket, from which SSLSocket also inherits.
This
allows an SSL socket to be used as a drop-in replacement for a regular socket, making it very easy to add SSL support to
an existing application.
Combining SSL protocol handling and network IO usually works well, but there are some cases where it doesn’t.
An
example is async IO frameworks that want to use a different IO multiplexing model than the “select/poll on a file descriptor”
(readiness based) model that is assumed by socket.socket and by the internal OpenSSL socket IO routines.
This is
mostly relevant for platforms like Windows where this model is not efficient.
For this purpose, a reduced scope variant
of SSLSocket called SSLObject is provided.
SSLObject
A reduced-scope variant of SSLSocket representing an SSL protocol instance that does not contain any network
IO methods.
This class is typically used by framework authors that want to implement asynchronous IO for SSL
through memory buffers.
This class implements an interface on top of a low-level SSL object as implemented by OpenSSL.
This object
captures the state of an SSL connection but does not provide any network IO itself.
IO needs to be performed
through separate “BIO” objects which are OpenSSL’s IO abstraction layer.
This class has no public constructor.
An SSLObject instance must be created using the wrap_bio() method.
This method will create the SSLObject instance and bind it to a pair of BIOs.
The incoming BIO is used to pass
data from Python to the SSL protocol instance, while the outgoing BIO is used to pass data the other way around.
When compared to SSLSocket, this object lacks the following features:
• Any form of network IO; recv() and send() read and write only to the underlying MemoryBIO buffers.
You must always manually call do_handshake() to
start the handshake.
All end-of-file conditions that are in violation of the protocol
are reported via the SSLEOFError exception.
The method unwrap() call does not return anything, unlike for an SSL socket where it returns the underlying
socket.
The server_name_callback callback passed to SSLContext.set_servername_callback() will
get an SSLObject instance instead of a SSLSocket instance as its first parameter.
Some notes related to the use of SSLObject:
• All IO on an SSLObject is non-blocking.
This means that for example read() will raise an
SSLWantReadError if it needs more data than the incoming BIO has available.
An SSLObject is always
created via an SSLContext.
Changed in version 3.7: SSLObject instances must to created with wrap_bio().
In earlier versions, it was
possible to create instances directly.
This was never documented or officially supported.
An SSLObject communicates with the outside world using memory buffers.
The class MemoryBIO provides a memory
buffer that can be used for this purpose.
It wraps an OpenSSL memory BIO (Basic IO) object:
class ssl.MemoryBIO
A memory buffer that can be used to pass data between Python and an SSL protocol instance.
If n is not specified or negative, all bytes are returned.
The buf argument must be an object supporting the buffer
protocol.
The return value is the number of bytes written, which is always equal to the length of buf.
After this method has been called, it is illegal to call write().
The attribute eof will become true after all data currently in the buffer has been read.
SSLSession
Session object used by session.
It will load the system’s trusted CA
certificates, enable certificate validation and hostname checking, and try to choose reasonably secure protocol and cipher
settings.
For example, here is how you would use the smtplib.
If a client certificate is needed for the connection, it can be added with SSLContext.load_cert_chain().
By contrast, if you create the SSL context by calling the SSLContext constructor yourself, it will not have certificate
validation nor hostname checking enabled by default.
Since it does not authenticate the
other peer, it can be insecure, especially in client mode where most of time you would like to ensure the authenticity
of the server you’re talking to.
Therefore, when in client mode, it is highly recommended to use CERT_REQUIRED.
However, it is in itself not sufficient; you also have to check that the server certificate, which can be obtained by calling
SSLSocket.getpeercert(), matches the desired service.
For many protocols and applications, the service can be
identified by the hostname.
This common check is automatically performed when SSLContext.check_hostname
is enabled.
Changed in version 3.7:
Hostname matchings is now performed by OpenSSL.
Python no longer uses
match_hostname().
In server mode, if you want to authenticate your clients using the SSL layer (rather than using a higher-level authentication
mechanism), you’ll also have to specify CERT_REQUIRED and similarly check the client certificate.
Protocol versions
SSL versions 2 and 3 are considered insecure and are therefore dangerous to use.
If you want maximum compatibility
between clients and servers, it is recommended to use PROTOCOL_TLS_CLIENT or PROTOCOL_TLS_SERVER as
the protocol version.
SSLv2 and SSLv3 are disabled by default.
The SSL context created above will only allow TLSv1.3 and later (if supported by your system) connections to a server.
PROTOCOL_TLS_CLIENT implies certificate validation and hostname checks by default.
You have to load certificates
into the context.
Cipher selection
If you have advanced security requirements, fine-tuning of the ciphers enabled when negotiating a SSL session is possible
through the SSLContext.set_ciphers() method.
Starting from Python 3.2.3, the ssl module disables certain
weak ciphers by default, but you may want to further restrict the cipher choice.
Be sure to read OpenSSL’s documentation
about the cipher list format.
If you want to check which ciphers are enabled by a given cipher list, use SSLContext.
get_ciphers() or the openssl ciphers command on your system.
Multi-processing
If using this module as part of a multi-processed application (using, for example the multiprocessing or
concurrent.futures modules), be aware that OpenSSL’s internal random number generator does not properly
handle forked processes.
Applications must change the PRNG state of the parent process if they use any SSL feature
with os.fork().
Any successful call of RAND_add() or RAND_bytes() is sufficient.
The TLS 1.3 protocol behaves slightly differently than previous version of TLS/SSL.
Some new TLS 1.3 features are not
yet available.
All AES-GCM and ChaCha20 cipher suites are enabled by de-
fault.
The method SSLContext.set_ciphers() cannot enable or disable any TLS 1.3 ciphers yet, but
SSLContext.get_ciphers() returns them.
SSLSocket.
session and SSLSession are not compatible with TLS 1.3.
A server can request a certificate at
any time.
Clients process certificate requests while they send or receive application data from the server.
The Transport Layer Security (TLS) Protocol Version 1.2 T. Dierks et.
Recommendations for Secure Use of Transport Layer Security (TLS) and Datagram Transport Layer Security (DTL
IETF
Mozilla’s Server Side TLS recommendations Mozilla
18.4 select — Waiting for I/O completion
This module provides access to the select() and poll() functions available in most operating systems, devpoll()
available on Solaris and derivatives, epoll() available on Linux 2.5+ and kqueue() available on most BSD.
Note
that on Windows, it only works for sockets; on other operating systems, it also works for other file types (in particular, on
Unix, it works on pipes).
It cannot be used on regular files to determine whether a file has grown since it was last read.
Note:
The selectors module allows high-level and efficient I/O multiplexing, built upon the select module
primitives.
Users are encouraged to use the selectors module instead, unless they want precise control over the
OS-level primitives used.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The module defines the following:
exception select.error
A deprecated alias of OSError.
Changed in version 3.3: Following PEP 3151, this class was made an alias of OSError.
Returns a /dev/poll polling object; see section /dev/poll Polling
Objects below for the methods supported by devpoll objects.
If your
program reduces this value, devpoll() will fail.
If your program increases this value, devpoll() may return
an incomplete list of active file descriptors.
The new file descriptor is non-inheritable.
New in version 3.3.
The new file descriptor is now non-inheritable.
Return an edge polling object, which can be used as Edge or Level
Triggered interface for I/O events.
It must be positive, or -1 to use the
default.
It is only used on older systems where epoll_create1() is not available; otherwise it has no effect
(though its value is still checked).
However, when supplied, its value must be 0 or select.
EPOLL_CLOEXEC, otherwise OSError is raised.
See the Edge and Level Trigger Polling (epoll) Objects section below for the methods supported by epolling objects.
The new file descriptor is non-inheritable.
Changed in version 3.3: Added the flags parameter.
Changed in version 3.4: Support for the with statement was added.
The new file descriptor is now non-inheritable.
The flags parameter.
Use
os.set_inheritable() to make the file descriptor inheritable.
Returns a polling object, which supports registering and unregistering
file descriptors, and then polling them for I/O events; see section Polling Objects below for the methods supported
by polling objects.
Returns a kernel queue object; see section Kqueue Objects below for the methods sup-
ported by kqueue objects.
The new file descriptor is non-inheritable.
The new file descriptor is now non-inheritable.
This is a straightforward interface to the Unix select() system call.
The first three arguments are iterables
of ‘waitable objects’: either integers representing file descriptors or objects with a parameterless method named
fileno() returning such an integer:
• rlist: wait until ready for reading
• wlist: wait until ready for writing
• xlist: wait for an “exceptional condition” (see the manual page for what your system considers such a condition)
Empty iterables are allowed, but acceptance of three empty iterables is platform-dependent.
The optional timeout argument specifies a time-out as a floating point number in
seconds.
When the timeout argument is omitted the function blocks until at least one file descriptor is ready.
A
time-out value of zero specifies a poll and never blocks.
The return value is a triple of lists of objects that are ready: subsets of the first three arguments.
When the time-out
is reached without a file descriptor becoming ready, three empty lists are returned.
Among the acceptable object types in the iterables are Python file objects (e.g. sys.stdin, or objects returned
by open() or os.popen()), socket objects returned by socket.socket().
You may also define a wrapper
class yourself, as long as it has an appropriate fileno() method (that really returns a file descriptor, not just a
random integer).
Note:
File objects on Windows are not acceptable, but sockets are.
On Windows, the underlying select()
function is provided by the WinSock library, and does not handle file descriptors that don’t originate from WinSock.
The function is now retried with a recomputed timeout when interrupted by a signal, except
if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
The minimum number of bytes which can be written without blocking to a pipe when the pipe has been reported
as ready for writing by select(), poll() or another interface in this module.
This doesn’t apply to other kind
of file-like objects such as sockets.
This value is guaranteed by POSIX to be at least 512.
Availability: Unix
New in version 3.2.
While select() is O(highest file descriptor) and poll() is O(number of
file descriptors), /dev/poll is O(active file descriptors).
New in version 3.4.
New in version 3.4.
Future calls to the poll() method will then check whether the
file descriptor has any pending I/O events.
File objects implement fileno(), so they can also be used as the argument.
The constants are the same
that with poll() object.
The default value is a combination of the constants POLLIN, POLLPRI, and POLLOUT.
Warning: Registering a file descriptor that’s already registered is not an error, but the result is undefined.
The
appropriate action is to unregister or modify it first.
This is an important difference compared with poll().
This method does an unregister() followed by a register().
It is (a bit) more efficient that doing the
same explicitly.
Just like the register() method, fd can be an
integer or an object with a fileno() method that returns an integer.
Attempting to remove a file descriptor that was never registered is safely ignored.
Polls the set of registered file descriptors, and returns a possibly empty list containing (fd, event) 2-tuples for
the descriptors that have events or errors to report.
An empty list indicates that the call timed out and no file descriptors had any events
to report.
If timeout is given, it specifies the length of time in milliseconds which the system will wait for events
before returning.
If timeout is omitted, -1, or None, the call will block until there is an event for this poll object.
The function is now retried with a recomputed timeout when interrupted by a signal, except
if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
Edge and Level Trigger Polling (epoll) Objects
https://linux.die.net/man/4/epoll
eventmask
18.4.
After one event is pulled out, the fd is internally disabled
EPOLLEXCLUSIVE
Wake only one epoll object when the associated fd has an event.
The default (if this flag
is not set) is to wake all epoll objects polling on a fd.
EPOLLRDHUP Stream socket peer closed connection or shut down writing half of connection.
EPOLLRDNORMEquivalent to EPOLLIN
EPOLLRDBANDPriority data band can be read.
EPOLLWRNORMEquivalent to EPOLLOUT
EPOLLWRBANDPriority data may be written.
EPOLLMSG
Ignored.
New in version 3.6: EPOLLEXCLUSIVE was added.
It’s only supported by Linux Kernel 4.5 or later.
The method no longer ignores the EBADF error.
The function is now retried with a recomputed timeout when interrupted by a signal, except
if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
Future calls to the poll() method will then check whether the
file descriptor has any pending I/O events.
File objects implement fileno(), so they can also be used as the argument.
If not specified, the default value
used will check for all 3 types of events.
Constant
Meaning
POLLIN
There is data to read
POLLPRI
There is urgent data to read
POLLOUT
Ready for output: writing will not block
POLLERR
Error condition of some sort
POLLHUP
Hung up
POLLRDHUP
Stream socket peer closed connection, or shut down writing half of connection
POLLNVAL
Invalid request: descriptor not open
Registering a file descriptor that’s already registered is not an error, and has the same effect as registering the
descriptor exactly once.
This has the same effect as register(fd, eventmask).
Attempting to
modify a file descriptor that was never registered causes an OSError exception with errno ENOENT to be raised.
Just like the register() method, fd can be an
integer or an object with a fileno() method that returns an integer.
Attempting to remove a file descriptor that was never registered causes a KeyError exception to be raised.
Polls the set of registered file descriptors, and returns a possibly empty list containing (fd, event) 2-tuples for
the descriptors that have events or errors to report.
An empty list indicates that the call timed out and no file descriptors had any events
to report.
If timeout is given, it specifies the length of time in milliseconds which the system will wait for events
before returning.
If timeout is omitted, negative, or None, the call will block until there is an event for this poll
object.
The function is now retried with a recomputed timeout when interrupted by a signal, except
if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
Kqueue Objects
kqueue.close()
Close the control file descriptor of the kqueue object.
The function is now retried with a recomputed timeout when interrupted by a signal, except
if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
Kevent Objects
https://man.freebsd.org/cgi/man.cgi?query=kqueue&sektion=2
kevent.ident
Value used to identify the event.
The interpretation depends on the filter but it’s usually the file descriptor.
In the
constructor ident can either be an int or an object with a fileno() method.
Constant
Meaning
KQ_FILTER_READ
Takes a descriptor and returns whenever there is data available to read
KQ_FILTER_WRITE
Takes a descriptor and returns whenever there is data available to write
KQ_FILTER_AIO
AIO requests
KQ_FILTER_VNODE
Returns when one or more of the requested events watched in fflag occurs
KQ_FILTER_PROC
Watch for events on a process id
KQ_FILTER_NETDEV
Watch for events on a network device [not available on macOS]
KQ_FILTER_SIGNAL
Returns whenever the watched signal is delivered to the process
KQ_FILTER_TIMER
Establishes an arbitrary timer
kevent.flags
Filter action.
Source code: Lib/selectors.py
18.5.1 Introduction
This module allows high-level and efficient I/O multiplexing, built upon the select module primitives.
Users are
encouraged to use this module instead, unless they want precise control over the OS-level primitives used.
It defines a BaseSelector abstract base class, along with several concrete implementations (KqueueSelector,
EpollSelector…), that can be used to wait for I/O readiness notification on multiple file objects.
In the following,
“file object” refers to any object with a fileno() method, or a raw file descriptor.
See file object.
DefaultSelector is an alias to the most efficient implementation available on the current platform: this should be
the default choice for most users.
Note:
The type of file objects supported depends on the platform: on Windows, sockets are supported, but not pipes,
whereas on Unix, both are supported (some other types may be supported as well, such as fifos or special file devices).
See also:
select Low-level I/O multiplexing module.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
SelectorKey
A SelectorKey is a namedtuple used to associate a file object to its underlying file descriptor, selected event
mask and attached data.
It is returned by several BaseSelector methods.
BaseSelector
A BaseSelector is used to wait for I/O event readiness on multiple file objects.
It supports file stream reg-
istration, unregistration, and a method to wait for I/O events on those streams, with an optional timeout.
It’s an
abstract base class, so cannot be instantiated.
Use DefaultSelector instead, or one of SelectSelector,
KqueueSelector etc.
if you want to specifically use an implementation, and your platform supports it.
BaseSelector and its concrete implementations support the context manager protocol.
It may either be an integer file descriptor or an object with a fileno()
method.
This returns a new SelectorKey instance, or raises a ValueError in case of invalid event mask or file
descriptor, or KeyError if the file object is already registered.
A file object shall be unregistered prior
to being closed.
This returns the associated SelectorKey instance, or raises a KeyError if fileobj is not registered.
It
will raise ValueError if fileobj is invalid (e.g. it has no fileno() method or its fileno() method
has an invalid return value).
This is equivalent to BaseSelector.unregister(fileobj) followed by BaseSelector.
This returns a new SelectorKey instance, or raises a ValueError in case of invalid event mask or file
descriptor, or KeyError if the file object is not registered.
If timeout > 0, this specifies the maximum wait time, in seconds.
If timeout <= 0, the call won’t
block, and will report the currently ready file objects.
If timeout is None, the call will block until a monitored
file object becomes ready.
This returns a list of (key, events) tuples, one for each ready file object.
Note: This method can return before any file object becomes ready or the timeout has elapsed if the current
process receives a signal: in this case, an empty list will be returned.
Changed in version 3.5: The selector is now retried with a recomputed timeout when interrupted by a signal
if the signal handler did not raise an exception (see PEP 475 for the rationale), instead of returning an empty
list of events before the timeout.
This must be called to make sure that any underlying resource is freed.
The selector shall not be used once it
has been closed.
This returns the SelectorKey instance associated to this file object, or raises KeyError if the file object
is not registered.
This returns a Mapping instance mapping registered file objects to their associated SelectorKey in-
stance.
DefaultSelector
The default selector class, using the most efficient implementation available on the current platform.
This should
be the default choice for most users.
SelectSelector
select.select()-based selector.
PollSelector
select.poll()-based selector.
EpollSelector
select.epoll()-based selector.
This returns the file descriptor used by the underlying select.epoll() object.
DevpollSelector
select.devpoll()-based selector.
This returns the file descriptor used by the underlying select.devpoll() object.
New in version 3.5.
class selectors.
KqueueSelector
select.kqueue()-based selector.
This returns the file descriptor used by the underlying select.kqueue() object.
General rules
The signal.signal() function allows defining custom handlers to be executed when a signal is received.
A small
number of default handlers are installed: SIGPIPE is ignored (so write errors on pipes and sockets can be reported as
ordinary Python exceptions) and SIGINT is translated into a KeyboardInterrupt exception if the parent process
has not changed it.
A handler for a particular signal, once set, remains installed until it is explicitly reset (Python emulates the BSD style
interface regardless of the underlying implementation), with the exception of the handler for SIGCHLD, which follows
the underlying implementation.
On WebAssembly platforms wasm32-emscripten and wasm32-wasi, signals are emulated and therefore behave
differently.
Several functions and signals are not available on these platforms.
Execution of Python signal handlers
A Python signal handler does not get executed inside the low-level (C) signal handler.
Instead, the low-level signal handler
sets a flag which tells the virtual machine to execute the corresponding Python signal handler at a later point(for example
at the next bytecode instruction).
This has consequences:
• It makes little sense to catch synchronous errors like SIGFPE or SIGSEGV that are caused by an invalid operation
in C code.
Python will return from the signal handler to the C code, which is likely to raise the same signal again,
causing Python to apparently hang.
From Python 3.3 onwards, you can use the faulthandler module to report
on synchronous errors.
A long-running calculation implemented purely in C (such as regular expression matching on a large body of text)
may run uninterrupted for an arbitrary amount of time, regardless of any signals received.
The Python signal
handlers will be called when the calculation finishes.
If the handler raises an exception, it will be raised “out of thin air” in the main thread.
See the note below for a
discussion.
Signals and threads
Python signal handlers are always executed in the main Python thread of the main interpreter, even if the signal was
received in another thread.
This means that signals can’t be used as a means of inter-thread communication.
You can use
the synchronization primitives from the threading module instead.
Besides, only the main thread of the main interpreter is allowed to set a new signal handler.
The signal module defines three enums:
class signal.
Signals
enum.
IntEnum collection of SIG* constants and the CTRL_* constants.
New in version 3.5.
class signal.
Handlers
enum.
IntEnum collection the constants SIG_DFL and SIG_IGN.
New in version 3.5.
class signal.
Sigmasks
enum.
IntEnum collection the constants SIG_BLOCK, SIG_UNBLOCK and SIG_SETMASK.
Availability: Unix.
See the man page sigprocmask(2) and pthread_sigmask(3) for further information.
New in version 3.5.
The variables defined in the signal module are:
signal.
This is one of two standard signal handling options; it will simply perform the default function for the signal.
For
example, on most systems the default action for SIGQUIT is to dump core and exit, while the default action for
SIGCHLD is to simply ignore it.
signal.
This is another standard signal handler, which will simply ignore the given signal.
signal.
SIGABRT
Abort signal from abort(3).
SIGALRM
Timer signal from alarm(2).
Availability: Unix.
signal.
SIGBREAK
Interrupt from keyboard (CTRL + BREAK).
Availability: Windows.
signal.
SIGBUS
Bus error (bad memory access).
Availability: Unix.
signal.
SIGCHLD
Child process stopped or terminated.
Availability: Unix.
18.6.
SIGCLD
Alias to SIGCHLD.
signal.
SIGCONT
Continue the process if it is currently stopped
Availability: Unix.
signal.
SIGFPE
Floating-point exception.
For example, division by zero.
See also:
ZeroDivisionError is raised when the second argument of a division or modulo operation is zero.
SIGHUP
Hangup detected on controlling terminal or death of controlling process.
Availability: Unix.
signal.
SIGILL
Illegal instruction.
signal.
SIGINT
Interrupt from keyboard (CTRL + C).
Default action is to raise KeyboardInterrupt.
signal.
SIGKILL
Kill signal.
It cannot be caught, blocked, or ignored.
Availability: Unix.
signal.
SIGPIPE
Broken pipe: write to pipe with no readers.
Default action is to ignore the signal.
Availability: Unix.
signal.
SIGSEGV
Segmentation fault: invalid memory reference.
signal.
SIGSTKFLT
Stack fault on coprocessor.
The Linux kernel does not raise this signal: it can only be raised in user
space.
Availability: Linux.
On architectures where the signal is available.
See the man page signal(7) for further information.
New in version 3.11.
signal.
SIGTERM
Termination signal.
signal.
SIGUSR1
User-defined signal 1.
SIGUSR2
User-defined signal 2.
Availability: Unix.
signal.
SIGWINCH
Window resize signal.
Availability: Unix.
SIG*
All the signal numbers are defined symbolically.
For example, the hangup signal is defined as signal.
SIGHUP;
the variable names are identical to the names used in C programs, as found in <signal.h>.
The Unix man
page for ‘signal()’ lists the existing signals (on some systems this is signal(2), on others the list is in
signal(7)).
Note that not all systems define the same set of signal names; only those names defined by the
system are defined by this module.
CTRL_C_EVENT
The signal corresponding to the Ctrl+C keystroke event.
This signal can only be used with os.kill().
Availability: Windows.
New in version 3.2.
signal.
The signal corresponding to the Ctrl+Break keystroke event.
This signal can only be used with os.kill().
Availability: Windows.
New in version 3.2.
signal.
NSIG
One more than the number of the highest signal number.
Use valid_signals() to get valid signal numbers.
signal.
ITIMER_REAL
Decrements interval timer in real time, and delivers SIGALRM upon expiration.
ITIMER_VIRTUAL
Decrements interval timer only when the process is executing, and delivers SIGVTALRM upon expiration.
ITIMER_PROF
Decrements interval timer both when the process executes and when the system is executing on behalf of the
process.
Coupled with ITIMER_VIRTUAL, this timer is usually used to profile the time spent by the application
in user and kernel space.
SIGPROF is delivered upon expiration.
signal.
SIG_BLOCK
A possible value for the how parameter to pthread_sigmask() indicating that signals are to be blocked.
New in version 3.3.
signal.
SIG_UNBLOCK
A possible value for the how parameter to pthread_sigmask() indicating that signals are to be unblocked.
New in version 3.3.
signal.
SIG_SETMASK
A possible value for the how parameter to pthread_sigmask() indicating that the signal mask is to be re-
placed.
New in version 3.3.
The signal module defines one exception:
18.6.
ItimerError
Raised to signal an error from the underlying setitimer() or getitimer() implementation.
Expect this er-
ror if an invalid interval timer or a negative time is passed to setitimer().
This error is a subtype of OSError.
This error used to be a subtype of IOError, which is now an alias of OSError.
If time is non-zero, this function requests that a SIGALRM signal be sent to the process in time seconds.
Any
previously scheduled alarm is canceled (only one alarm can be scheduled at any time).
The returned value is then
the number of seconds before any previously set alarm was to have been delivered.
If time is zero, no alarm is
scheduled, and any scheduled alarm is canceled.
If the return value is zero, no alarm is currently scheduled.
Availability: Unix.
See the man page alarm(2) for further information.
signal.getsignal(signalnum)
Return the current signal handler for the signal signalnum.
The returned value may be a callable Python object, or
one of the special values signal.
SIG_IGN, signal.SIG_DFL or None.
Here, signal.SIG_IGN means
that the signal was previously ignored, signal.SIG_DFL means that the default way of handling the signal was
previously in use, and None means that the previous signal handler was not installed from Python.
signal.strsignal(signalnum)
Returns the description of signal signalnum, such as “Interrupt” for SIGINT.
Returns None if signalnum has no
description.
Raises ValueError if signalnum is invalid.
New in version 3.8.
This can be less than range(1, NSIG) if some signals
are reserved by the system for internal use.
New in version 3.8.
Returns nothing.
Availability: Unix.
See the man page signal(2) for further information.
Sends a signal to the calling process.
Returns nothing.
New in version 3.8.
Python does not currently support the siginfo
parameter; it must be None.
The flags argument is provided for future extensions; no flag values are currently
defined.
See the pidfd_send_signal(2) man page for more information.
The target
thread can be executing any code (Python or not).
However, if the target thread is executing the Python inter-
preter, the Python signal handlers will be executed by the main thread of the main interpreter.
Therefore, the
only point of sending a signal to a particular Python thread would be to force a running system call to fail with
InterruptedError.
Use threading.get_ident() or the ident attribute of threading.
Thread objects to get a suitable
value for thread_id.
If signalnum is 0, then no signal is sent, but error checking is still performed; this can be used to check if the target
thread is still running.
Raises an auditing event signal.pthread_kill with arguments thread_id, signalnum.
Availability: Unix.
See the man page pthread_kill(3) for further information.
See also os.kill().
New in version 3.3.
The signal mask is the set of signals whose delivery is
currently blocked for the caller.
Return the old signal mask as a set of signals.
The behavior of the call is dependent on the value of how, as follows.
It is permissible
to attempt to unblock a signal which is not blocked.
SIGINT, signal.
SIGTERM}).
Use valid_signals()
for a full mask including all signals.
For example, signal.pthread_sigmask(signal.
SIG_BLOCK, []) reads the signal mask of the call-
ing thread.
SIGKILL and SIGSTOP cannot be blocked.
Availability: Unix.
See the man page sigprocmask(2) and pthread_sigmask(3) for further information.
See also pause(), sigpending() and sigwait().
New in version 3.3.
ITIMER_REAL, signal.ITIMER_VIRTUAL or signal.
ITIMER_PROF) specified by which to fire after seconds (float is accepted, different from alarm()) and after
that every interval seconds (if interval is non-zero).
The interval timer specified by which can be cleared by setting
seconds to zero.
When an interval timer fires, a signal is sent to the process.
The signal sent is dependent on the timer being
used; signal.ITIMER_REAL will deliver SIGALRM, signal.
ITIMER_VIRTUAL sends SIGVTALRM,
and signal.ITIMER_PROF will deliver SIGPROF.
The old values are returned as a tuple: (delay, interval).
Attempting to pass an invalid interval timer will cause an ItimerError.
Availability: Unix.
When a signal is received, the signal number is written as a single byte into the
fd.
This can be used by a library to wakeup a poll or select call, allowing the signal to be fully processed.
The old wakeup fd is returned (or -1 if file descriptor wakeup was not enabled).
If fd is -1, file descriptor wakeup
is disabled.
If not -1, fd must be non-blocking.
It is up to the library to remove any bytes from fd before calling
poll or select again.
When threads are enabled, this function can only be called from the main thread of the main interpreter; attempting
to call it from other threads will cause a ValueError exception to be raised.
There are two common ways to use this function.
In both approaches, you use the fd to wake up when a signal
arrives, but then they differ in how they determine which signal or signals have arrived.
In the first approach, we read the data out of the fd’s buffer, and the byte values give you the signal numbers.
This
is simple, but in rare cases it can run into a problem: generally the fd will have a limited amount of buffer space,
and if too many signals arrive too quickly, then the buffer may become full, and some signals may be lost.
If you
use this approach, then you should set warn_on_full_buffer=True, which will at least cause a warning to
be printed to stderr when signals are lost.
In the second approach, we use the wakeup fd only for wakeups, and ignore the actual byte values.
In this case, all
we care about is whether the fd’s buffer is empty or non-empty; a full buffer doesn’t indicate a problem at all.
If you
use this approach, then you should set warn_on_full_buffer=False, so that your users are not confused
by spurious warning messages.
Changed in version 3.5: On Windows, the function now also supports socket handles.
Change system call restart behaviour: if flag is False, system calls will be restarted when interrupted by signal
signalnum, otherwise system calls will be interrupted.
Returns nothing.
Availability: Unix.
See the man page siginterrupt(3) for further information.
Note that installing a signal handler with signal() will reset the restart behaviour to interruptible by implicitly
calling siginterrupt() with a true flag value for the given signal.
signal.signal(signalnum, handler)
Set the handler for signal signalnum to the function handler.
The previous sig-
nal handler will be returned (see the description of getsignal() above).
When threads are enabled, this function can only be called from the main thread of the main interpreter; attempting
to call it from other threads will cause a ValueError exception to be raised.
The handler is called with two arguments: the signal number and the current stack frame (None or a frame object;
for a description of frame objects, see the description in the type hierarchy or see the attribute descriptions in the
inspect module).
A ValueError will be raised in any other case.
Note that not all systems de-
fine the same set of signal names; an AttributeError will be raised if a signal name is not defined as SIG*
module level constant.
Return the set of the pending signals.
Availability: Unix.
See the man page sigpending(2) for further information.
See also pause(), pthread_sigmask() and sigwait().
Suspend execution of the calling thread until the delivery of one of the signals specified in the signal set sigset.
The
function accepts the signal (removes it from the pending list of signals), and returns the signal number.
Availability: Unix.
See the man page sigwait(3) for further information.
See also pause(), pthread_sigmask(), sigpending(), sigwaitinfo() and sigtimedwait().
New in version 3.3.
Suspend execution of the calling thread until the delivery of one of the signals specified in the signal set sigset.
The
function accepts the signal and removes it from the pending list of signals.
If one of the signals in sigset is already
pending for the calling thread, the function will return immediately with information about that signal.
The signal
handler is not called for the delivered signal.
The function raises an InterruptedError if it is interrupted by
a signal that is not in sigset.
The return value is an object representing the data contained in the siginfo_t structure, namely: si_signo,
si_code, si_errno, si_pid, si_uid, si_status, si_band.
Availability: Unix.
See the man page sigwaitinfo(2) for further information.
See also pause(), sigwait() and sigtimedwait().
New in version 3.3.
The function is now retried if interrupted by a signal not in sigset and the signal handler
does not raise an exception (see PEP 475 for the rationale).
Like sigwaitinfo(), but takes an additional timeout argument specifying a timeout.
If timeout is specified as
0, a poll is performed.
Returns None if a timeout occurs.
Availability: Unix.
See the man page sigtimedwait(2) for further information.
See also pause(), sigwait() and sigwaitinfo().
New in version 3.3.
The function is now retried with the recomputed timeout if interrupted by a signal not in
sigset and the signal handler does not raise an exception (see PEP 475 for the rationale).
It uses the alarm() function to limit the time spent waiting to open a file; this is
useful if the file is for a serial device that may not be turned on, which would normally cause the os.open() to hang
indefinitely.
The solution is to set a 5-second alarm before opening the file; if the operation takes too long, the alarm
signal will be sent, and the handler raises an exception.
Signals(signum).name
print(f'Signal handler called with signal {signame} ({signum})')
raise OSError("Couldn't open device!")
# Set the signal handler and a 5-second alarm
signal.signal(signal.
Doing that would cause your
program to exit unexpectedly whenever any socket connection is interrupted while your program is still writing to it.
Note on Signal Handlers and Exceptions
If a signal handler raises an exception, the exception will be propagated to the main thread and may be raised after any
bytecode instruction.
Most notably, a KeyboardInterrupt may appear at any point during execution.
Most Python
code, including the standard library, cannot be made robust against this, and so a KeyboardInterrupt (or any other
exception resulting from a signal handler) may on rare occasions put the program in an unexpected state.
For many programs, especially those that merely want to exit on KeyboardInterrupt, this is not a problem, but
applications that are complex or require high reliability should avoid raising exceptions from signal handlers.
They should
also avoid catching KeyboardInterrupt as a means of gracefully shutting down.
Instead, they should install their
own SIGINT handler.
Below is an example of an HTTP server that avoids KeyboardInterrupt:
import signal
import socket
from selectors import DefaultSelector, EVENT_READ
from http.server import HTTPServer, SimpleHTTPRequestHandler
interrupt_read, interrupt_write = socket.socketpair()

interrupt_write.send(b'\0')
signal.signal(signal.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Memory-mapped file objects behave like both bytearray and like file objects.
You can use mmap objects in most
places where bytearray are expected; for example, you can use the re module to search through a memory-mapped
file.
You can also read and write data starting at the current file position, and seek() through
the file to different positions.
A memory-mapped file is created by the mmap constructor, which is different on Unix and on Windows.
In either case
you must provide a file descriptor for a file opened for update.
If you wish to map an existing Python file object, use
its fileno() method to obtain the correct value for the fileno parameter.
Otherwise, you can open the file using the
os.open() function, which returns a file descriptor directly (the file still needs to be closed when done).
Note: If you want to create a memory-mapping for a writable, buffered file, you should flush() the file first.
This is
necessary to ensure that local modifications to the buffers are actually available to the mapping.
For both the Unix and Windows versions of the constructor, access may be specified as an optional keyword parameter.
If access is not specified, Windows mmap returns a write-through mapping.
The initial memory
values for all three access types are taken from the specified file.
Assignment to an ACCESS_READ memory map raises
a TypeError exception.
Assignment to an ACCESS_WRITE memory map affects both memory and the underlying
file.
Assignment to an ACCESS_COPY memory map affects memory but does not update the underlying file.
Changed in version 3.7: Added ACCESS_DEFAULT constant.
To map anonymous memory, -1 should be passed as the fileno along with the length.
Maps length bytes from the file specified by the file handle fileno, and creates a mmap object.
If length is larger than the current size of the file, the file is extended to contain length bytes.
If length is 0, the
maximum length of the map is the current size of the file, except that if the file is empty Windows raises an exception
(you cannot create an empty mapping on Windows).
Windows allows you to have
many different mappings against the same file.
If you specify the name of an existing tag, that tag is opened,
otherwise a new tag of this name is created.
If this parameter is omitted or None, the mapping is created without a
name.
Avoiding the use of the tag parameter will assist in keeping your code portable between Unix and Windows.
Raises an auditing event mmap.__new__ with arguments fileno, length, access, offset.
Maps length bytes from the file specified by the file descriptor fileno, and returns a mmap object.
If length is 0, the maximum length of the map will be the current size of the file when mmap is called.
The default value is MAP_SHARED.
Some systems
have additional possible flags with the full list specified in MAP_* constants.
It is an error to specify both
flags, prot and access.
See the description of access above for information on how to use this parameter.
To ensure validity of the created memory mapping the file specified by the descriptor fileno is internally automati-
cally synchronized with the physical backing store on macOS.
New in version 3.2: Context manager support.
Memory-mapped file objects support the following methods:
close()
Closes the mmap.
Subsequent calls to other methods of the object will result in a ValueError exception being
raised.
This will not close the open file.
New in version 3.2.
Optional arguments start and end are interpreted as in slice notation.
Returns -1 on failure.
Writable bytes-like object is now accepted.
Flushes changes made to the in-memory copy of a file back to disk.
Without use of this call there is no
guarantee that changes are written back before the object is destroyed.
If offset and size are specified, only
changes to the given range of bytes will be flushed to disk; otherwise, the whole extent of the mapping is
flushed.
None is returned to indicate success.
An exception is raised when the call failed.
Changed in version 3.8: Previously, a nonzero value was returned on success; zero was returned on error
under Windows.
A zero value was returned on success; an exception was raised on error under Unix.
If start and length are omitted, the
entire mapping is spanned.
On some systems (including Linux), start must be a multiple of the PAGESIZE.
Availability: Systems with the madvise() system call.
New in version 3.8.
If the mmap was created with
ACCESS_READ, then calls to move will raise a TypeError exception.
If the argument is omitted,
None or negative, return all bytes from the current file position to the end of the mapping.
The file position
is updated to point after the bytes that were returned.
The file position is updated
to point after the bytes that were returned.
If the mmap was created with ACCESS_READ or
ACCESS_COPY, resizing the map will raise a TypeError exception.
On Windows: Resizing the map will raise an OSError if there are other maps against the same named
file.
Resizing an anonymous map (ie against the pagefile) will silently create a new map with the original data
copied over up to the length of the new size.
Optional arguments start and end are interpreted as in slice notation.
Returns -1 on failure.
Writable bytes-like object is now accepted.
SEEK_SET or 0 (absolute file
positioning); other values are os.
SEEK_CUR or 1 (seek relative to the current position) and os.
SEEK_END
or 2 (seek relative to the file’s end).
The file
position is updated to point after the bytes that were written.
If the mmap was created with ACCESS_READ,
then writing to it will raise a TypeError exception.
Writable bytes-like object is now accepted.
The number of bytes written is now returned.
If the mmap was created with ACCESS_READ, then writing to it will raise a TypeError exception.
Constants
mmap.
MADV_NORMAL
mmap.
MADV_RANDOM
mmap.
MADV_SEQUENTIAL
mmap.
MADV_WILLNEED
mmap.
MADV_DONTNEED
mmap.MADV_REMOVE
mmap.
MADV_DONTFORK
18.7.
MADV_DOFORK
mmap.
MADV_HWPOISON
mmap.
MADV_MERGEABLE
mmap.MADV_UNMERGEABLE
mmap.
MADV_SOFT_OFFLINE
mmap.
MADV_HUGEPAGE
mmap.
MADV_NOHUGEPAGE
mmap.
MADV_DONTDUMP
mmap.
MADV_DODUMP
mmap.MADV_FREE
mmap.
MADV_NOSYNC
mmap.
MADV_AUTOSYNC
mmap.
MADV_NOCORE
mmap.MADV_CORE
mmap.
MADV_PROTECT
mmap.
MADV_FREE_REUSABLE
mmap.
These options can be passed to mmap.madvise().
Not every option will be present on every system.
Availability: Systems with the madvise() system call.
New in version 3.8.
Constants
mmap.
MAP_SHARED
mmap.
MAP_PRIVATE
mmap.
MAP_DENYWRITE
mmap.
MAP_EXECUTABLE
mmap.
MAP_ANON
mmap.
MAP_ANONYMOUS
mmap.
MAP_POPULATE
mmap.
MAP_STACK
mmap.
MAP_ALIGNED_SUPER
mmap.
MAP_CONCEAL
These are the various flags that can be passed to mmap.mmap().
MAP_ALIGNED_SUPER is only available at
FreeBSD and MAP_CONCEAL is only available at OpenBSD.
Note that some options might not be present on some
systems.
Changed in version 3.10: Added MAP_POPULATE constant.
New in version 3.11: Added MAP_STACK constant.
New in version 3.12: Added MAP_ALIGNED_SUPER constant.
Networking and Interprocess Communication
CHAPTER
NINETEEN
INTERNET DATA HANDLING
This chapter describes modules which support handling data formats commonly used on the internet.
It is specifically not designed to do any sending of
email messages to SMTP (RFC 2821), NNTP, or other servers; those are functions of modules such as smtplib and
nntplib.
The email package attempts to be as RFC-compliant as possible, supporting RFC 5322 and RFC 6532,
as well as such MIME-related RFCs as RFC 2045, RFC 2046, RFC 2047, RFC 2183, and RFC 2231.
The overall structure of the email package can be divided into three major components, plus a fourth component that
controls the behavior of the other components.
The central component of the package is an “object model” that represents email messages.
An application interacts with
the package primarily through the object model interface defined in the message sub-module.
The application can use
this API to ask questions about an existing email, to construct a new email, or to add or remove email subcomponents
that themselves use the same object model interface.
That is, following the nature of email messages and their MIME
subcomponents, the email object model is a tree structure of objects that all provide the EmailMessage API.
The other two major components of the package are the parser and the generator.
The parser takes the serialized
version of an email message (a stream of bytes) and converts it into a tree of EmailMessage objects.
The generator
takes an EmailMessage and turns it back into a serialized byte stream.
The control component is the policy module.
Every EmailMessage, every generator, and every parser has
an associated policy object that controls its behavior.
Usually an application only needs to specify the policy when an
EmailMessage is created, either by directly instantiating an EmailMessage to create a new email, or by parsing
an input stream using a parser.
But the policy can be changed when the message is serialized using a generator.
This allows, for example, a generic email message to be parsed from disk, but to serialize it using standard SMTP settings
when sending it to an email server.
The email package does its best to hide the details of the various governing RFCs from the application.
Conceptually the
application should be able to treat the email message as a structured tree of unicode text and binary attachments, without
having to worry about how these are represented when serialized.
In practice, however, it is often necessary to be aware of
at least some of the rules governing MIME messages and their structure, specifically the names and nature of the MIME
“content types” and how they identify multipart documents.
For the most part this knowledge should only be required
for more complex applications, and even then it should only be the high level structure in question, and not the details of
how those structures are represented.
Since MIME content types are used widely in modern internet software (not just
email), this will be a familiar concept to many programmers.
The following sections describe the functionality of the email package.
We start with the message object model,
which is the primary interface an application will use, and follow that with the parser and generator components.
Then we cover the policy controls, which completes the treatment of the main components of the library.
The next three sections cover the exceptions the package may raise and the defects (non-compliance with the RFCs) that
the parser may detect.
Then we cover the headerregistry and the contentmanager sub-components, which
provide tools for doing more detailed manipulation of headers and payloads, respectively.
Both of these components
contain features relevant to consuming and producing non-trivial messages, but also document their extensibility APIs,
which will be of interest to advanced applications.
Following those is a set of examples of using the fundamental parts of the APIs covered in the preceding sections.
The foregoing represent the modern (unicode friendly) API of the email package.
The remaining sections, starting with the
Message class, cover the legacy compat32 API that deals much more directly with the details of how email messages
are represented.
The compat32 API does not hide the details of the RFCs from the application, but for applications
that need to operate at that level, they can be useful tools.
This documentation is also relevant for applications that are
still using the compat32 API for backward compatibility reasons.
Changed in version 3.6: Docs reorganized and rewritten to promote the new EmailMessage/EmailPolicy API.
Lib/email/message.py
New in version 3.6:1
The central class in the email package is the EmailMessage class, imported from the email.message module.
It
is the base class for the email object model.
EmailMessage provides the core functionality for setting and querying
header fields, for accessing message bodies, and for creating or modifying structured messages.
An email message consists of headers and a payload (which is also referred to as the content).
Headers are RFC 5322 or
RFC 6532 style field names and values, where the field name and value are separated by a colon.
The colon is not part
of either the field name or the field value.
The payload may be a simple text message, or a binary object, or a structured
sequence of sub-messages each with their own set of headers and their own payload.
The latter type of payload is indicated
by the message having a MIME type such as multipart/* or message/rfc822.
The conceptual model provided by an EmailMessage object is that of an ordered dictionary of headers coupled with
a payload that represents the RFC 5322 body of the message, which might be a list of sub-EmailMessage objects.
In addition to the normal dictionary methods for accessing the header names and values, there are methods for access-
ing specialized information from the headers (for example the MIME content type), for operating on the payload, for
generating a serialized version of the message, and for recursively walking over the object tree.
The EmailMessage dictionary-like interface is indexed by the header names, which must be ASCII values.
The values
of the dictionary are strings with some extra methods.
Headers are stored and returned in case-preserving form, but field
names are matched case-insensitively.
Unlike a real dict, there is an ordering to the keys, and there can be duplicate keys.
Additional methods are provided for working with headers that have duplicate keys.
The payload is either a string or bytes object, in the case of simple message objects, or a list of EmailMessage objects,
for MIME container documents such as multipart/* and message/rfc822 message objects.
Originally added in 3.4 as a provisional module.
Docs for legacy message class moved to email.message.
If policy is specified use the rules it specifies to update and serialize the representation of the message.
If policy
is not set, use the default policy, which follows the rules of the email RFCs except for line endings (instead of
the RFC mandated \r\n, it uses the Python standard \n line endings).
For more information see the policy
documentation.
When optional unixfrom is true, the envelope header is included
in the returned string.
For backward compatibility with the base Message
class maxheaderlen is accepted, but defaults to None, which means that by default the line length is controlled
by the max_line_length of the policy.
The policy argument may be used to override the default policy
obtained from the message instance.
This can be used to control some of the formatting produced by the
method, since the specified policy will be passed to the Generator.
Flattening the message may trigger changes to the EmailMessage if defaults need to be filled in to complete
the transformation to a string (for example, MIME boundaries may be generated or modified).
Note that this method is provided as a convenience and may not be the most useful way to serialize mes-
sages in your application, especially if you are dealing with multiple messages.
See email.generator.
Generator for a more flexible API for serializing messages.
Note also that this method is restricted to
producing messages serialized as “7 bit clean” when utf8 is False, which is the default.
Changed in version 3.6: the default behavior when maxheaderlen is not specified was changed from defaulting
to 0 to defaulting to the value of max_line_length from the policy.
Equivalent to as_string(policy=self.policy.clone(utf8=True)).
Allows str(msg) to
produce a string containing the serialized message in a readable format.
Changed in version 3.4: the method was changed to use utf8=True, thus producing an RFC 6531-like
message representation, instead of being a direct alias for as_string().
When optional unixfrom is true, the envelope header is
included in the returned string.
The policy argument may be used to override
the default policy obtained from the message instance.
This can be used to control some of the formatting
produced by the method, since the specified policy will be passed to the BytesGenerator.
Flattening the message may trigger changes to the EmailMessage if defaults need to be filled in to complete
the transformation to a string (for example, MIME boundaries may be generated or modified).
Note that this method is provided as a convenience and may not be the most useful way to serialize mes-
sages in your application, especially if you are dealing with multiple messages.
See email.generator.
BytesGenerator for a more flexible API for serializing messages.
Allows bytes(msg) to produce a bytes object containing the serialized
message.
When is_multipart() returns False, the payload should be a string object (which might be a CTE
encoded binary payload).
Note that is_multipart() returning True does not necessarily mean that
“msg.get_content_maintype() == ‘multipart’” will return the True.
Defaults to None if the envelope header was never set.
The following methods implement the mapping-like interface for accessing the message’s headers.
Note that there
are some semantic differences between these methods and a normal mapping (i.e. dictionary) interface.
For ex-
ample, in a dictionary there are no duplicate keys, but here there may be duplicate message headers.
Also, in
dictionaries there is no guaranteed order to the keys returned by keys(), but in an EmailMessage object,
headers are always returned in the order they appeared in the original message, or in which they were added to the
message later.
Any header deleted and then re-added is always appended to the end of the header list.
These semantic differences are intentional and are biased toward convenience in the most common use cases.
Note that in all cases, any envelope header present in the message is not included in the mapping interface.
Matching is done without regard to case and
name does not include the trailing colon.
Used for the in operator.
For example:
if 'message-id' in myMessage:
print('Message-ID:', myMessage['message-id'])
__getitem__(name)
Return the value of the named header field.
If the header is
missing, None is returned; a KeyError is never raised.
Note that if the named field appears more than once in the message’s headers, exactly which of those field
values will be returned is undefined.
Use the get_all() method to get the values of all the extant headers
named name.
Using the standard (non-compat32) policies, the returned value is an instance of a subclass of email.
headerregistry.
BaseHeader.
__setitem__(name, val)
Add a header to the message with field name name and value val.
The field is appended to the end of the
message’s existing headers.
Note that this does not overwrite or delete any existing header with the same name.
If the policy defines certain headers to be unique (as the standard policies do), this method may raise a
ValueError when an attempt is made to assign a value to such a header when one already exists.
Delete all occurrences of the field with name name from the message’s headers.
No exception is raised if the
named field isn’t present in the headers.
This is identical to __getitem__() except that optional failobj
is returned if the named header is missing (failobj defaults to None).
Here are some additional useful header related methods:
get_all(name, failobj=None)
Return a list of all the values for the field named name.
If there are no such named headers in the message,
failobj is returned (defaults to None).
This method is similar to __setitem__() except that additional header param-
eters can be provided as keyword arguments.
For each item in the keyword argument dictionary _params, the key is taken as the parameter name, with
underscores converted to dashes (since dashes are illegal in Python identifiers).
Normally, the parameter will
be added as key="value" unless the value is None, in which case only the key will be added.
If the value contains non-ASCII characters, the charset and language may be explicitly controlled by specifying
the value as a three tuple in the format (CHARSET, LANGUAGE, VALUE), where CHARSET is a string
naming the charset to be used to encode the value, LANGUAGE can usually be set to None or the empty string
(see RFC 2231 for other possibilities), and VALUE is the string value containing non-ASCII code points.
If
a three tuple is not passed and the value contains non-ASCII characters, it is automatically encoded in RFC
2231 format using a CHARSET of utf-8 and a LANGUAGE of None.
This will add a header that looks like
Content-Disposition: attachment; filename="bud.gif"
An example of the extended interface with non-ASCII characters:
msg.add_header('Content-Disposition', 'attachment',
filename=('iso-8859-1', '', 'Fußballer.ppt'))
replace_header(_name, _value)
Replace a header.
Replace the first header found in the message that matches _name, retaining header order
and field name case of the original header.
If no matching header is found, raise a KeyError.
If there is
no Content-Type header in the message return the value returned by get_default_type().
If the
Content-Type header is invalid, return text/plain.
RFC 2045 defines a message’s default type to be text/plain unless it appears inside a
multipart/digest container, in which case it would be message/rfc822.
This is the maintype part of the string returned by
get_content_type().
This is the subtype part of the string returned by
get_content_type().
Most messages have a default content type of text/plain, except for
messages that are subparts of multipart/digest containers.
Such subparts have a default content type
of message/rfc822.
set_default_type(ctype)
Set the default content type.
The default content type is not stored in the Content-Type header, so it only affects
the return value of the get_content_type methods when no Content-Type header is present in the
message.
If the parameter already exists in the header, replace its value
with value.
When header is Content-Type (the default) and the header does not yet exist in the message,
add it, set its value to text/plain, and append the new parameter value.
Optional header specifies an
alternative header to Content-Type.
If the value contains non-ASCII characters, the charset and language may be explicitly specified using the
optional charset and language parameters.
Optional language specifies the RFC 2231 language, defaulting
to the empty string.
Both charset and language should be strings.
The default is to use the utf8 charset and
None for the language.
If replace is False (the default) the header is moved to the end of the list of headers.
If replace is True,
the header will be updated in place.
Use of the requote parameter with EmailMessage objects is deprecated.
Note that existing parameter values of headers may be accessed through the params attribute of the header
value (for example, msg['Content-Type'].params['charset']).
Changed in version 3.4: replace keyword was added.
The header will be re-written
in place without the parameter or its value.
Optional header specifies an alternative to Content-Type.
Use of the requote parameter with EmailMessage objects is deprecated.
If
the header does not have a filename parameter, this method falls back to looking for the name parameter
on the Content-Type header.
If neither is found, or the header is missing, then failobj is returned.
The
returned string will always be unquoted as per email.utils.unquote().
The returned string will always be unquoted
as per email.utils.unquote().
A HeaderParseError is raised if the message object has no
Content-Type header.
Note that using this method is subtly different from deleting the old Content-Type header and adding a
new one with the new boundary via add_header(), because set_boundary() preserves the order of
the Content-Type header in the list of headers.
If there is no
Content-Type header, or if that header has no charset parameter, failobj is returned.
If the message is a multipart, then the
list will contain one element for each subpart in the payload, otherwise, it will be a list of length 1.
Each item in the list will be a string which is the value of the charset parameter in the Content-Type
header for the represented subpart.
If the subpart has no Content-Type header, no charset parameter,
or is not of the text main MIME type, then that item in the returned list will be failobj.
Changed in version 3.4.2: is_attachment is now a method instead of a property, for consistency with
is_multipart().
The possible values for this method are inline, attachment or None if the message follows
RFC 2183.
New in version 3.5.
The following methods relate to interrogating and manipulating the content (payload) of the message.
You will typically use walk() as the iterator in a
for loop; each iteration returns the next subpart.
Here’s an example that prints the MIME type of every part of a multipart message structure:

print(part.get_content_type())
multipart/report
text/plain
message/delivery-status
text/plain
text/plain
message/rfc822
text/plain
walk iterates over the subparts of any part where is_multipart() returns True, even though msg.
We can see this in our example
by making use of the _structure debug helper function:
19.1.
True True
False False
False True
False False
False False
False True
False False
multipart/report
text/plain
message/delivery-status
text/plain
text/plain
message/rfc822
text/plain
Here the message parts are not multiparts, but they do contain subparts.
Return the MIME part that is the best candidate to be the “body” of the message.
Start looking for candidate matches with the object on which the get_body method is called.
If related is not included in preferencelist, consider the root part (or subpart of the root part) of any related
encountered as a candidate if the (sub-)part matches a preference.
When encountering a multipart/related, check the start parameter and if a part with a matching
Content-ID is found, consider only it when looking for candidate matches.
Otherwise consider only the
first (default root) part of the multipart/related.
If a part has a Content-Disposition header, only consider the part a candidate match if the value of
the header is inline.
If none of the candidates matches any of the preferences in preferencelist, return None.
Notes: (1) For most applications the only preferencelist combinations that really make sense are ('plain',
), ('html', 'plain'), and the default ('related', 'html', 'plain').
That is, skip the first occurrence of each of text/plain, text/html, multipart/
related, or multipart/alternative (unless they are explicitly marked as attachments via
Content-Disposition: attachment), and return all remaining parts.
When applied directly to
a multipart/related, return an iterator over the all the related parts except the root part (ie: the
part pointed to by the start parameter, or the first part if there is no start parameter or the start
parameter doesn’t match the Content-ID of any of the parts).
If content_manager is not specified, use the
content_manager specified by the current policy.
set_content(*args, content_manager=None, **kw)
Call the set_content() method of the content_manager, passing self as the message object, and passing
along any other arguments or keywords as additional arguments.
If content_manager is not specified, use the
content_manager specified by the current policy.
If boundary is specified, use it
as the boundary string in the multipart, otherwise leave the boundary to be automatically created when it is
needed (for example, when the message is serialized).
If boundary is
specified, use it as the boundary string in the multipart, otherwise leave the boundary to be automatically
created when it is needed (for example, when the message is serialized).
If boundary is specified, use it as the boundary string in the multipart, otherwise leave the
boundary to be automatically created when it is needed (for example, when the message is serialized).
If the message is a multipart/related, create a new message object, pass all of the arguments to its
set_content() method, and attach() it to the multipart.
If the message is a non-multipart,
call make_related() and then proceed as above.
If the message is any other type of multipart, raise
a TypeError.
If content_manager is not specified, use the content_manager specified by the current
policy.
If the added part has no Content-Disposition header, add one with the value inline.
If the message is a multipart/alternative, create a new message object, pass all of the argu-
ments to its set_content() method, and attach() it to the multipart.
If the message is a non-
multipart or multipart/related, call make_alternative() and then proceed as above.
If
the message is any other type of multipart, raise a TypeError.
If content_manager is not specified,
use the content_manager specified by the current policy.
If the message is a multipart/mixed, create a new message object, pass all of the arguments to its
set_content() method, and attach() it to the multipart.
If the message is a non-multipart,
multipart/related, or multipart/alternative, call make_mixed() and then proceed as
above.
If content_manager is not specified, use the content_manager specified by the current policy.
If the added part has no Content-Disposition header, add one with the value attachment.
This method can be used both for explicit attachments (Content-Disposition: attachment)
and inline attachments (Content-Disposition: inline), by passing appropriate options to the
content_manager.
Content- headers, leaving all other headers intact and in their original
order.
EmailMessage objects have the following instance attributes:
preamble
The format of a MIME document allows for some text between the blank line following the headers, and the
first multipart boundary string.
Normally, this text is never visible in a MIME-aware mail reader because it
falls outside the standard MIME armor.
However, when viewing the raw text of the message, or when viewing
the message in a non-MIME aware reader, this text can become visible.
The preamble attribute contains this leading extra-armor text for MIME documents.
When the Parser
discovers some text after the headers but before the first boundary string, it assigns this text to the message’s
preamble attribute.
When the Generator is writing out the plain text representation of a MIME message,
and it finds the message has a preamble attribute, it will write this text in the area between the headers and
the first boundary.
See email.parser and email.generator for details.
Note that if the message object has no preamble, the preamble attribute will be None.
As with the preamble, if there is no epilog text this
attribute will be None.
defects
The defects attribute contains a list of all the problems found when parsing this message.
See email.
This class represents a subpart of a MIME message.
It is identical to EmailMessage, except that no
MIME-Version headers are added when set_content() is called, since sub-parts do not need their own
MIME-Version headers.
The email package provides a standard parser that understands most email document structures, including MIME
documents.
You can pass the parser a bytes, string or file object, and the parser will return to you the root
EmailMessage instance of the object structure.
For simple, non-MIME messages the payload of this root object
will likely be a string containing the text of the message.
For MIME messages, the root object will return True
from its is_multipart() method, and the subparts can be accessed via the payload manipulation methods, such
as get_body(), iter_parts(), and walk().
There are actually two parser interfaces available for use, the Parser API and the incremental FeedParser API.
The Parser API is most useful if you have the entire text of the message in memory, or if the entire message lives in a
file on the file system.
The FeedParser can consume and
parse the message incrementally, and only returns the root object when you close the parser.
Note that the parser can be extended in limited ways, and of course you can implement your own parser completely from
scratch.
All of the logic that connects the email package’s bundled parser and the EmailMessage class is embodied
in the Policy class, so a custom parser can create message object trees any way it finds necessary by implementing
custom versions of the appropriate Policy methods.
FeedParser API
The BytesFeedParser, imported from the email.feedparser module, provides an API that is conducive to
incremental parsing of email messages, such as would be necessary when reading the text of an email message from a
source that can block (such as a socket).
The BytesFeedParser can of course be used to parse an email message
fully contained in a bytes-like object, string, or file, but the BytesParser API may be more convenient for such use
cases.
The semantics and results of the two parser APIs are identical.
The BytesFeedParser’s API is simple; you create an instance, feed it a bunch of bytes until there’s no more to feed it,
then close the parser to retrieve the root message object.
The BytesFeedParser is extremely accurate when parsing
standards-compliant messages, and it does a very good job of parsing non-compliant messages, providing information
about how a message was deemed broken.
It will populate a message object’s defects attribute with a list of any
problems it found in a message.
See the email.errors module for the list of defects that it can find.
Here is the API for the BytesFeedParser:
class email.parser.
BytesFeedParser(_factory=None, *, policy=policy.compat32)
Create a BytesFeedParser instance.
Optional _factory is a no-argument callable; if not specified use the
message_factory from the policy.
Call _factory whenever a new message object is needed.
If policy is specified use the rules it specifies to update the representation of the message.
If policy is not set, use
the compat32 policy, which maintains backward compatibility with the Python 3.2 version of the email package
and provides Message as the default factory.
All other policies provide EmailMessage as the default _factory.
For more information on what else policy controls, see the policy documentation.
Note: The policy keyword should always be specified; The default will change to email.policy.default
in a future version of Python.
New in version 3.2.
Added the policy keyword.
Feed the parser some more data.
The lines can
be partial and the parser will stitch such partial lines together properly.
The lines can have any of the three
common line endings: carriage return, newline, or carriage return and newline (they can even be mixed).
It is undefined what
happens if feed() is called after this method has been called.
FeedParser(_factory=None, *, policy=policy.compat32)
Works like BytesFeedParser except that the input to the feed() method must be a string.
This is of limited
utility, since the only way for such a message to be valid is for it to contain only ASCII text or, if utf8 is True,
no binary attachments.
Added the policy keyword.
The email.
BytesHeaderParser
and HeaderParser can be much faster in these situations, since they do not attempt to parse the message body, instead
setting the payload to the raw body.
BytesParser(_class=None, *, policy=policy.compat32)
Create a BytesParser instance.
The _class and policy arguments have the same meaning and semantics as the
_factory and policy arguments of BytesFeedParser.
Note: The policy keyword should always be specified; The default will change to email.policy.default
in a future version of Python.
Removed the strict argument that was deprecated in 2.4.
Added the policy keyword.
The bytes contained in fp must be formatted as a block of RFC 5322 (or, if utf8 is True, RFC
6532) style headers and header continuation lines, optionally preceded by an envelope header.
The
header block is terminated either by the end of the data or by a blank line.
Following the header
block is the body of the message (which may contain MIME-encoded subparts, including subparts with a
Content-Transfer-Encoding of 8bit).
Optional headersonly is a flag specifying whether to stop parsing after reading the headers or not.
The default
is False, meaning it parses the entire contents of the file.
Calling
this method on a bytes-like object is equivalent to wrapping bytes in a BytesIO instance first and calling
parse().
Optional headersonly is as with the parse() method.
New in version 3.2.
BytesHeaderParser(_class=None, *, policy=policy.compat32)
Exactly like BytesParser, except that headersonly defaults to True.
New in version 3.3.
This class is parallel to BytesParser, but handles string input.
Removed the strict argument.
Added the policy keyword.
Other than the text mode requirement, this method operates like BytesParser.parse().
Calling this
method on a string is equivalent to wrapping text in a StringIO instance first and calling parse().
Optional headersonly is as with the parse() method.
HeaderParser(_class=None, *, policy=policy.compat32)
Exactly like Parser, except that headersonly defaults to True.
Since creating a message object structure from a string or a file object is such a common task, four functions are provided
as a convenience.
They are available in the top-level email package namespace.
This is equivalent to BytesParser().
parsebytes(s).
Optional _class and policy are interpreted as with the BytesParser class constructor.
New in version 3.2.
Changed in version 3.3: Removed the strict argument.
Added the policy keyword.
This is equivalent to BytesParser().
parse(fp).
New in version 3.2.
Changed in version 3.3: Removed the strict argument.
Added the policy keyword.
This is equivalent to Parser().parsestr(s).
Removed the strict argument.
Added the policy keyword.
This is equivalent to Parser().parse(fp).
Removed the strict argument.
Added the policy keyword.
Here’s an example of how you might use message_from_bytes() at an interactive Python prompt:
Additional notes
Here are some notes on the parsing semantics:
• Most non-multipart type messages are parsed as a single message object with a string payload.
These objects
will return False for is_multipart(), and iter_parts() will yield an empty list.
The outer container message will return True for is_multipart(), and iter_parts()
will yield a list of subparts.
Their is_multipart()
method will return True.
The single element yielded by iter_parts() will be a sub-message object.
Such
messages may have a Content-Type header of type multipart, but their is_multipart() method
may return False.
If such messages were parsed with the FeedParser, they will have an instance of the
MultipartInvariantViolationDefect class in their defects attribute list.
See email.errors for
details.
Generating MIME documents
Source code: Lib/email/generator.py
One of the most common tasks is to generate the flat (serialized) version of the email message represented by a message
object structure.
You will need to do this if you want to send your message via smtplib.SMTP.sendmail() or the
nntplib module, or print the message on the console.
Taking a message object structure and producing a serialized
representation is the job of the generator classes.
As with the email.parser module, you aren’t limited to the functionality of the bundled generator; you could write
one from scratch yourself.
However the bundled generator knows how to generate most email in a standards-compliant
way, should handle MIME and non-MIME email messages just fine, and is designed so that the bytes-oriented pars-
ing and generation operations are inverses, assuming the same non-transforming policy is used for both.
That is,
parsing the serialized byte stream via the BytesParser class and then regenerating the serialized byte stream us-
ing BytesGenerator should produce output identical to the input1.
The Generator class can be used to flatten a message into a text (as opposed to binary) serialized representation, but
since Unicode cannot represent binary data directly, the message is of necessity transformed into something that contains
only ASCII characters, using the standard email RFC Content Transfer Encoding techniques for encoding email messages
for transport over channels that are not “8 bit clean”.
To accommodate reproducible processing of SMIME-signed messages Generator disables header folding for message
parts of type multipart/signed and all subparts.
BytesGenerator(outfp, mangle_from_=None, maxheaderlen=None, *,
policy=None)
Return a BytesGenerator object that will write any message provided to the flatten() method, or any
surrogateescape encoded text provided to the write() method, to the file-like object outfp.
If optional mangle_from_ is True, put a > character in front of any line in the body that starts with the exact string
"From ", that is From followed by a space at the beginning of a line.
If maxheaderlen is not None, refold any header lines that are longer than maxheaderlen, or if 0, do not rewrap
any headers.
If manheaderlen is None (the default), wrap headers and other message lines according to the policy
settings.
If policy is specified, use that policy to control message generation.
If policy is None (the default), use the policy
associated with the Message or EmailMessage object passed to flatten to control the message generation.
See email.policy for details on what policy controls.
This statement assumes that you use the appropriate setting for unixfrom, and that there are no email.policy settings calling for automatic
adjustments (for example, refold_source must be none, which is not the default).
It is also not 100% true, since if the message does not conform
to the RFC standards occasionally information about the exact original text is lost during parsing error recovery.
The default behavior of the mangle_from_ and maxheaderlen parameters is to follow the
policy.
If the policy option cte_type is 8bit (the default), copy any headers in the original parsed
message that have not been modified to the output with any bytes with the high bit set reproduced
as in the original, and preserve the non-ASCII Content-Transfer-Encoding of any body parts
that have them.
If cte_type is 7bit, convert the bytes with the high bit set as needed using
an ASCII-compatible Content-Transfer-Encoding.
That is, transform parts with non-ASCII
Content-Transfer-Encoding (Content-Transfer-Encoding: 8bit) to an ASCII com-
patible Content-Transfer-Encoding, and encode RFC-invalid non-ASCII bytes in headers using
the MIME unknown-8bit character set, thus rendering them RFC-compliant.
If unixfrom is True, print the envelope header delimiter used by the Unix mailbox format (see mailbox)
before the first of the RFC 5322 headers of the root message object.
If the root object has no envelope header,
craft a standard one.
The default is False.
Note that for subparts, no envelope header is ever printed.
If linesep is not None, use it as the separator character between all the lines of the flattened message.
If
linesep is None (the default), use the value specified in the policy.
Encode s using the ASCII codec and the surrogateescape error handler, and pass it to the write method
of the outfp passed to the BytesGenerator’s constructor.
As a convenience, EmailMessage provides the methods as_bytes() and bytes(aMessage) (a.k.a.
__bytes__()), which simplify the generation of a serialized binary representation of a message object.
For more
detail, see email.message.
Because strings cannot represent binary data, the Generator class must convert any binary data in any message it flattens
to an ASCII compatible format, by converting them to an ASCII compatible Content-Transfer_Encoding.
Using
the terminology of the email RFCs, you can think of this as Generator serializing to an I/O stream that is not “8 bit
clean”.
In other words, most applications will want to be using BytesGenerator, and not Generator.
Generator(outfp, mangle_from_=None, maxheaderlen=None, *, policy=None)
Return a Generator object that will write any message provided to the flatten() method, or any text pro-
vided to the write() method, to the file-like object outfp.
If optional mangle_from_ is True, put a > character in front of any line in the body that starts with the exact string
"From ", that is From followed by a space at the beginning of a line.
If maxheaderlen is not None, refold any header lines that are longer than maxheaderlen, or if 0, do not rewrap
any headers.
If manheaderlen is None (the default), wrap headers and other message lines according to the policy
settings.
If policy is specified, use that policy to control message generation.
If policy is None (the default), use the policy
associated with the Message or EmailMessage object passed to flatten to control the message generation.
See email.policy for details on what policy controls.
Added the policy keyword.
The default behavior of the mangle_from_ and maxheaderlen parameters is to follow the
policy.
If the policy option cte_type is 8bit, generate the message as if the option were set to 7bit.
Convert any bytes with the high bit set as needed
using an ASCII-compatible Content-Transfer-Encoding.
That is, transform parts with non-ASCII
Content-Transfer-Encoding (Content-Transfer-Encoding: 8bit) to an ASCII com-
patible Content-Transfer-Encoding, and encode RFC-invalid non-ASCII bytes in headers using
the MIME unknown-8bit character set, thus rendering them RFC-compliant.
If unixfrom is True, print the envelope header delimiter used by the Unix mailbox format (see mailbox)
before the first of the RFC 5322 headers of the root message object.
If the root object has no envelope header,
craft a standard one.
The default is False.
Note that for subparts, no envelope header is ever printed.
If linesep is not None, use it as the separator character between all the lines of the flattened message.
If
linesep is None (the default), use the value specified in the policy.
Changed in version 3.2: Added support for re-encoding 8bit message bodies, and the linesep argument.
This provides just enough
file-like API for Generator instances to be used in the print() function.
For more de-
tail, see email.message.
The email.generator module also provides a derived class, DecodedGenerator, which is like the Generator
base class, except that non-text parts are not serialized, but are instead represented in the output stream by a string
derived from a template filled in with information about the part.
DecodedGenerator(outfp, mangle_from_=None, maxheaderlen=None,
fmt=None, *, policy=None)
Act like Generator, except that for any subpart of the message passed to Generator.flatten(), if the
subpart is of main type text, print the decoded payload of the subpart, and if the main type is not text, instead
of printing it fill in the string fmt using information from the part and print the resulting filled-in string.
If fmt is None, use the following default fmt:
“[Non-text (%(type)s) part of message omitted, filename %(filename)s]”
Optional _mangle_from_ and maxheaderlen are as with the Generator base class.
Source code: Lib/email/policy.py
The email package’s prime focus is the handling of email messages as described by the various email and MIME RFCs.
However, the general format of email messages (a block of header fields each consisting of a name followed by a colon
followed by a value, the whole block followed by a blank line and an arbitrary ‘body’), is a format that has found utility
outside of the realm of email.
Some of these uses conform fairly closely to the main email RFCs, some do not.
Even
when working with email, there are times when it is desirable to break strict compliance with the RFCs, such as generating
emails that interoperate with email servers that do not themselves follow the standards, or that implement extensions you
want to use in ways that violate the standards.
Policy objects give the email package the flexibility to handle all these disparate use cases.
A Policy object encapsulates a set of attributes and methods that control the behavior of various components of the
email package during use.
Policy instances can be passed to various classes and methods in the email package to alter
the default behavior.
The settable values and their defaults are described below.
There is a default policy used by all classes in the email package.
For all of the parser classes and the related conve-
nience functions, and for the Message class, this is the Compat32 policy, via its corresponding pre-defined instance
compat32.
This policy provides for complete backward compatibility (in some cases, including bug compatibility) with
the pre-Python3.3 version of the email package.
This default value for the policy keyword to EmailMessage is the EmailPolicy policy, via its pre-defined instance
default.
When a Message or EmailMessage object is created, it acquires a policy.
If the message is created by a parser,
a policy passed to the parser will be the policy used by the message it creates.
If the message is created by the program,
then the policy can be specified when it is created.
When a message is passed to a generator, the generator uses the
policy from the message by default, but you can also pass a specific policy to the generator that will override the one stored
on the message object.
The default value for the policy keyword for the email.parser classes and the parser convenience functions will be
changing in a future version of Python.
Therefore you should always specify explicitly which policy you want to use
when calling any of the classes and functions described in the parser module.
The first part of this documentation covers the features of Policy, an abstract base class that defines the features that
are common to all policy objects, including compat32.
This includes certain hook methods that are called internally
by the email package, which a custom policy could override to obtain different behavior.
The second part describes the
concrete classes EmailPolicy and Compat32, which implement the hooks that provide the standard behavior and
the backward compatible behavior and features, respectively.
Policy instances are immutable, but they can be cloned, accepting the same keyword arguments as the class constructor
and returning a new Policy instance that is a copy of the original but with the specified attributes values changed.
As an example, the following code could be used to read an email message from a file on disk and pass it to the system
sendmail program on a Unix system:
19.1.
Here we are telling BytesGenerator to use the RFC correct line separator characters when creating the binary string
to feed into sendmail's stdin, where the default policy would use \n line separators.
Some email package methods accept a policy keyword argument, allowing the policy to be overridden for that method.
This is the abstract base class for all policy classes.
It provides default implementations for a couple of trivial
methods, as well as the implementation of the immutability property, the clone() method, and the constructor
semantics.
The constructor of a policy class can be passed various keyword arguments.
The arguments that may be specified
are any non-method properties on this class, plus any additional non-method properties on the concrete class.
A
value specified in the constructor will override the default value for the corresponding attribute.
Default
is 78, per RFC 5322.
The default is \n because that’s the internal
end-of-line discipline used by Python, though \r\n is required by the RFCs.
The possible values
are:
7bit
all data must be “7 bit clean” (ASCII-only).
This means that where necessary data will be
encoded using either quoted-printable or base64 encoding.
Data in headers is still required to be ASCII-only and
so will be encoded (see fold_binary() and utf8 below for exceptions), but body parts
may use the 8bit CTE.
A cte_type value of 8bit only works with BytesGenerator, not Generator, because strings
cannot contain binary data.
If a Generator is operating under a policy that specifies cte_type=8bit,
it will act as if cte_type is 7bit.
If False (the default), defects will be passed to
the register_defect() method.
This parameter
is used when the message is being serialized by a generator.
False.
The mangle_from_ parameter.
Used by the parser when building messages.
Defaults to None, in which case Message is used.
New in version 3.6.
The following Policy method is intended to be called by code using the email library to create policy instances
with custom settings:
clone(**kw)
Return a new Policy instance whose attributes have the same values as the current instance, except where
those attributes are given new values by the keyword arguments.
The remaining Policy methods are called by the email package code, and are not intended to be called by an
application using the email package.
A custom policy must implement all of these methods.
Handle a defect found on obj.
When the email package calls this method, defect will always be a subclass of
Defect.
The default implementation checks the raise_on_defect flag.
If it is True, defect is raised as an
exception.
If it is False (the default), obj and defect are passed to register_defect().
Register a defect on obj.
In the email package, defect will always be a subclass of Defect.
The default implementation calls the append method of the defects attribute of obj.
When the email
package calls handle_defect, obj will normally have a defects attribute that has an append method.
Custom object types used with the email package (for example, custom Message objects) should also provide
such an attribute, otherwise defects in parsed messages will raise unexpected errors.
Called when a header is added to an EmailMessage or Message object.
If the returned value is not 0
or None, and there are already a number of headers with the name name greater than or equal to the value
returned, a ValueError is raised.
Because the default behavior of Message.__setitem__ is to append the value to the list of headers, it
is easy to create duplicate headers without realizing it.
This method allows certain headers to be limited in
the number of instances of that header that may be added to a Message programmatically.
The email package calls this method with a list of strings, each string ending with the line separation characters
found in the source being parsed.
The first line includes the field header name and separator.
All whitespace
in the source is preserved.
The method should return the (name, value) tuple that is to be stored in the
Message to represent the parsed header.
If an implementation wishes to retain compatibility with the existing email package policies, name should be
the case preserved name (all characters up to the ‘:’ separator), while value should be the unfolded value (all
line separator characters removed, but whitespace kept intact), stripped of leading whitespace.
The email package calls this method with the name and value provided by the application program when
the application program is modifying a Message programmatically (as opposed to a Message created by
a parser).
The method should return the (name, value) tuple that is to be stored in the Message to
represent the header.
If an implementation wishes to retain compatibility with the existing email package policies, the name and
value should be strings or string subclasses that do not change the content of the passed in arguments.
There is no default implementation
header_fetch_parse(name, value)
The email package calls this method with the name and value currently stored in the Message when that
header is requested by the application program, and whatever the method returns is what is passed back to the
application as the value of the header being retrieved.
Note that there may be more than one header with the
same name stored in the Message; the method is passed the specific name and value of the header destined
to be returned to the application.
There should be no surrogateescaped binary data in the
value returned by the method.
The email package calls this method with the name and value currently stored in the Message for a given
header.
The method should return a string that represents that header “folded” correctly (according to the
policy settings) by composing the name with the value and inserting linesep characters at the appropriate
places.
See RFC 5322 for a discussion of the rules for folding email headers.
There should be no surrogateescaped binary data in the
string returned by the method.
The same as fold(), except that the returned value should be a bytes object rather than a string.
These could be converted back into binary data in the
returned bytes object.
This concrete Policy provides behavior that is intended to be fully compliant with the current email RFCs.
These
include (but are not limited to) RFC 5322, RFC 2047, and the current MIME RFCs.
This policy adds new header parsing and folding algorithms.
Instead of simple strings, headers are str subclasses
with attributes that depend on the type of the field.
The parsing and folding algorithm fully implement RFC 2047
and RFC 5322.
The default value for the message_factory attribute is EmailMessage.
In addition to the settable attributes listed above that apply to all policies, this policy adds the following additional
attributes:
New in version 3.6:1
utf8
If False, follow RFC 5322, supporting non-ASCII characters in headers by encoding them as “encoded
words”.
If True, follow RFC 6532 and use utf-8 encoding for headers.
Messages formatted in this way
may be passed to SMTP servers that support the SMTPUTF8 extension (RFC 6531).
The possible values are:
none
all source values use original folding
long
source values that have any line that is longer than max_line_length will be refolded
all
all values are refolded.
The default is long.
A default
header_factory (see headerregistry) is provided that supports custom parsing for the various
address and date RFC 5322 header field types, and the major MIME header field stypes.
Support for addi-
tional custom parsing will be added in the future.
When the get_content() or
set_content() method of an EmailMessage object is called, it calls the corresponding method of
this object, passing it the message object as its first argument, and any arguments or keywords that were
passed to it as additional arguments.
By default content_manager is set to raw_data_manager.
New in version 3.4.
The class provides the following concrete implementations of the abstract methods of Policy:
header_max_count(name)
Returns the value of the max_count attribute of the specialized class used to represent the header with the
given name.
Originally added in 3.3 as a provisional feature.
The name is parsed as everything up to the ‘:’ and returned unmodified.
The value is determined by stripping
leading whitespace off the remainder of the first line, joining all subsequent lines together, and stripping any
trailing carriage return or linefeed characters.
The name is returned unchanged.
If the input value has a name attribute and it matches name ignoring case,
the value is returned unchanged.
Otherwise the name and value are passed to header_factory, and the
resulting header object is returned as the value.
In this case a ValueError is raised if the input value
contains CR or LF characters.
If the value has a name attribute, it is returned to unmodified.
Otherwise the name, and the value with any
CR or LF characters removed, are passed to the header_factory, and the resulting header object is
returned.
Any surrogateescaped bytes get turned into the unicode unknown-character glyph.
A value is considered to be a ‘source
value’ if and only if it does not have a name attribute (having a name attribute means it is a header object of
some sort).
If a source value needs to be refolded according to the policy, it is converted into a header object
by passing the name and the value with any CR and LF characters removed to the header_factory.
Folding of a header object is done by calling its fold method with the current policy.
Source values are split into lines using splitlines().
If the value is not to be refolded, the lines are
rejoined using the linesep from the policy and returned.
The exception is lines containing non-ascii binary
data.
In that case the value is refolded regardless of the refold_source setting, which causes the binary
data to be CTE encoded using the unknown-8bit charset.
If cte_type is 8bit, non-ASCII binary data is converted back into bytes.
Headers with binary data are
not refolded, regardless of the refold_header setting, since there is no way to know whether the binary
data consists of single byte characters or multibyte characters.
The following instances of EmailPolicy provide defaults suitable for specific application domains.
Note that in the
future the behavior of these instances (in particular the HTTP instance) may be adjusted to conform even more closely to
the RFCs relevant to their domains.
This policy uses the standard Python \n line endings
rather than the RFC-correct \r\n.
email.policy.
SMTP
Suitable for serializing messages in conformance with the email RFCs.
Like default, but with linesep set to
\r\n, which is RFC compliant.
SMTPUTF8
The same as SMTP except that utf8 is True.
Useful for serializing messages to a message store without using
encoded words in the headers.
Should only be used for SMTP transmission if the sender or recipient addresses
have non-ASCII characters (the smtplib.
SMTP.send_message() method handles this automatically).
HTTP
Suitable for serializing headers with for use in HTTP traffic.
Like SMTP except that max_line_length is set
to None (unlimited).
The same as default except that raise_on_defect is set to True.
From the application view, this means that any header obtained through the EmailMessage is a header object with
extra attributes, whose string value is the fully decoded unicode value of the header.
Likewise, a header may be assigned
a new value, or a new header created, using a unicode string, and the policy will take care of converting the unicode string
into the correct RFC encoded form.
The header objects and their attributes are described in headerregistry.
This concrete Policy is the backward compatibility policy.
It replicates the behavior of the email package in
Python 3.2.
The policy module also defines an instance of this class, compat32, that is used as the default
policy.
Thus the default behavior of the email package is to maintain compatibility with Python 3.2.
The default is True.
The name is parsed as everything up to the ‘:’ and returned unmodified.
The value is determined by stripping
leading whitespace off the remainder of the first line, joining all subsequent lines together, and stripping any
trailing carriage return or linefeed characters.
The name and value are returned unmodified.
If the value contains binary data, it is converted into a Header object using the unknown-8bit charset.
Otherwise it is returned unmodified.
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value,
and wraps each resulting line to the max_line_length.
Non-ASCII binary data are CTE encoded using
the unknown-8bit charset.
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value,
and wraps each resulting line to the max_line_length.
If cte_type is 7bit, non-ascii binary data
is CTE encoded using the unknown-8bit charset.
Otherwise the original source header is used, with its
existing line breaks and any (RFC invalid) binary data it may contain.
MessageError
This is the base class for all exceptions that the email package can raise.
It is derived from the standard
Exception class and defines no additional methods.
This is the base class for exceptions raised by the Parser class.
It is derived from MessageError.
This class
is also used internally by the parser used by headerregistry.
HeaderParseError
Raised under some error conditions when parsing the RFC 5322 headers of a message, this class is derived from
MessageParseError.
The set_boundary() method will raise this error if the content type is unknown
when the method is called.
Header may raise this error for certain base64 decoding errors, and when an attempt
is made to create a header that appears to contain an embedded header (that is, there is what is supposed to be a
continuation line that has no leading whitespace and looks like a header).
BoundaryError
Deprecated and no longer used.
MultipartConversionError
Raised when a payload is added to a Message object using add_payload(), but the payload is al-
ready a scalar and the message’s Content-Type main type is not either multipart or missing.
MultipartConversionError multiply inherits from MessageError and the built-in TypeError.
Since Message.add_payload() is deprecated, this exception is rarely raised in practice.
However the
exception may also be raised if the attach() method is called on an instance of a class derived from
MIMENonMultipart (e.g. MIMEImage).
Here is the list of the defects that the FeedParser can find while parsing messages.
Note that the defects are added to
the message where the problem was found, so for example, if a message nested inside a multipart/alternative
had a malformed header, that nested message object would have a defect, but the containing messages would not.
All defect classes are subclassed from email.errors.
MessageDefect.
New in version 3.3.
Parsing continues assuming that the line represents the first line of the body.
New in version 3.3.
A header was found that was missing a colon, or was otherwise malformed.
This defect has not been used for several Python versions.
A message claimed to be a multipart, but no subparts
were found.
Note that when a message has this defect, its is_multipart() method may return False even
though its content type claims to be multipart.
Enough padding is added to perform the decode, but the resulting decoded bytes may be invalid.
The characters are ignored, but the resulting decoded bytes may be invalid.
The encoded block was kept as-is.
The original value is kept as-is.
The particular class used to represent a given header is
determined by the header_factory of the policy in effect when the headers are created.
This section documents
the particular header_factory implemented by the email package for handling RFC 5322 compliant email messages,
which not only provides customized header objects for various header types, but also provides an extension mechanism
for applications to add their own custom header types.
When using any of the policy objects derived from EmailPolicy, all headers are produced by HeaderRegistry
and have BaseHeader as their last base class.
Each header class has an additional base class that is determined by
the type of the header.
For example, many headers have the class UnstructuredHeader as their other base class.
The specialized second class for a header is determined by the name of the header, using a lookup table stored in the
HeaderRegistry.
All of this is managed transparently for the typical application program, but interfaces are provided
for modifying the default behavior for use by more complex applications.
The sections below first document the header base classes and their attributes, followed by the API for modifying the
behavior of HeaderRegistry, and finally the support classes used to represent the data parsed from structured headers.
BaseHeader(name, value)
name and value are passed to BaseHeader from the header_factory call.
The string value of any header
object is the value fully decoded to unicode.
This base class defines the following read-only properties:
name
The name of the header (the portion of the field before the ‘:’).
This is exactly the value passed in the
header_factory call for name; that is, case is preserved.
The
email package tries to be complete about detecting compliance issues.
See the errors module for a discus-
sion of the types of defects that may be reported.
A value of None means
unlimited.
The BaseHeader value for this attribute is None; it is expected that specialized header classes
will override this value as needed.
BaseHeader also provides the following method, which is called by the email library code and should not in
general be called by application programs:
fold(*, policy)
Return a string containing linesep characters as required to correctly fold the header according to policy.
A cte_type of 8bit will be treated as if it were 7bit, since headers may not contain arbitrary binary
data.
If utf8 is False, non-ASCII data will be RFC 2047 encoded.
BaseHeader by itself cannot be used to create a header object.
It defines a protocol that each specialized header
cooperates with in order to produce the header object.
Specifically, BaseHeader requires that the specialized
class provide a classmethod() named parse.
This method is called as follows:
parse(string, kwds)
kwds is a dictionary containing one pre-initialized key, defects.
The parse method
should append any detected defects to this list.
On return, the kwds dictionary must contain values for at least the
keys decoded and defects.
The parse method should assume that string may contain content-transfer-encoded parts, but
should correctly handle all valid unicode characters as well so that it can parse un-encoded header values.
BaseHeader’s __new__ then creates the header instance, and calls its init method.
The specialized class only
needs to provide an init method if it wishes to set additional attributes beyond those provided by BaseHeader
itself.
That is, anything extra that the specialized class puts in to the kwds dictionary should be removed and handled,
and the remaining contents of kw (and args) passed to the BaseHeader init method.
UnstructuredHeader
An “unstructured” header is the default type of header in RFC 5322.
Any header that does not have a specified
syntax is treated as unstructured.
The classic example of an unstructured header is the Subject header.
In RFC 5322, an unstructured header is a run of arbitrary text in the ASCII character set.
RFC 2047, how-
ever, has an RFC 5322 compatible mechanism for encoding non-ASCII text as ASCII characters within a header
value.
When a value containing encoded words is passed to the constructor, the UnstructuredHeader parser
converts such encoded words into unicode, following the RFC 2047 rules for unstructured text.
The parser uses
heuristics to attempt to decode certain non-compliant encoded words.
Defects are registered in such cases, as well
as defects for issues such as invalid characters within the encoded words or the non-encoded text.
This header type provides no additional attributes.
DateHeader
RFC 5322 specifies a very specific format for dates within email headers.
The DateHeader parser recognizes
that date format, as well as recognizing a number of variant forms that are sometimes found “in the wild”.
If the timezone of the input date is specified as -0000 (in-
dicating it is in UTC but contains no information about the source timezone), then datetime will be a
naive datetime.
If a specific timezone offset is found (including +0000), then datetime will contain
an aware datetime that uses datetime.timezone to record the timezone offset.
When creating a DateHeader, value may be datetime instance.
Because this is a naive datetime it will be interpreted as a UTC timestamp, and the resulting value will have a
timezone of -0000.
This example sets the date header to the current time and date using the current timezone offset.
AddressHeader
Address headers are one of the most complex structured header types.
The AddressHeader class provides a
generic interface to any address header.
This header type provides the following additional attributes:
groups
A tuple of Group objects encoding the addresses and groups found in the header value.
Addresses that are
not part of a group are represented in this list as single-address Groups whose display_name is None.
addresses
A tuple of Address objects encoding all of the individual addresses from the header value.
If the header
value contains any groups, the individual addresses from the group are included in the list at the point where
the group occurs in the value (that is, the list of addresses is “flattened” into a one dimensional list).
The decoded value of the header will have all encoded words decoded to unicode.
The decoded value is set by joining the str value of the elements of the groups
attribute with ', '.
A list of Address and Group objects in any combination may be used to set the value of an address header.
Group objects whose display_name is None will be interpreted as single addresses, which allows an address
list to be copied with groups intact by using the list obtained from the groups attribute of the source header.
SingleAddressHeader
A subclass of AddressHeader that adds one additional attribute:
address
The single address encoded by the header value.
If the header value actually contains more than one address
(which would be a violation of the RFC under the default policy), accessing this attribute will result in a
ValueError.
Many of the above classes also have a Unique variant (for example, UniqueUnstructuredHeader).
The only
difference is that in the Unique variant, max_count is set to 1.
19.1.
MIMEVersionHeader
There is really only one valid value for the MIME-Version header, and that is 1.0.
For future proofing, this
header class supports other valid version numbers.
If a version number has a valid value per RFC 2045, then the
header object will have non-None values for the following attributes:
version
The version number as a string, with any whitespace and/or comments removed.
ParameterizedMIMEHeader
MIME headers all start with the prefix ‘Content-’.
Each specific header has a certain value, described under the
class for that header.
Some can also take a list of supplemental parameters, which have a common format.
This
class serves as a base for all the MIME headers that take parameters.
ContentTypeHeader
A ParameterizedMIMEHeader class that handles the Content-Type header.
ContentDispositionHeader
A ParameterizedMIMEHeader class that handles the Content-Disposition header.
ContentTransferEncoding
Handles the Content-Transfer-Encoding header.
See RFC 2045 for more information.
This is the factory used by EmailPolicy by default.
HeaderRegistry builds the class used to create a
header instance dynamically, using base_class and a specialized class retrieved from a registry that it holds.
When
a given header name does not appear in the registry, the class specified by default_class is used as the specialized
class.
When use_default_map is True (the default), the standard mapping of header names to classes is copied in
to the registry during initialization.
It will be converted to lower case in the registry.
Construct and return a class to handle creating a name header.
The following classes are the classes used to represent data parsed from structured headers and can, in general, be used
by an application program to construct structured values to assign to specific headers.
The class used to represent an email address.
As a convenience addr_spec can be specified instead of username and domain, in which case username and domain
will be parsed from the addr_spec.
An addr_spec must be a properly RFC quoted string; if it is not Address will
raise an error.
Unicode characters are allowed and will be property encoded when serialized.
However, per the
RFCs, unicode is not allowed in the username portion of the address.
If the address does not have a
display name, this attribute will be an empty string.
This attribute is not mutable.
The str value of the object is the address quoted according to RFC 5322 rules, but with no Content Transfer
Encoding of any non-ASCII characters.
To support SMTP (RFC 5321), Address handles one special case: if username and domain are both the
empty string (or None), then the string value of the Address is <>.
Group(display_name=None, addresses=None)
The class used to represent an address group.
If it is None and there is exactly one Address in addresses, then
the Group represents a single address that is not in a group.
The str value of a Group is formatted according to RFC 5322, but with no Content Transfer Encoding of
any non-ASCII characters.
ContentManager
Base class for content managers.
Provides the standard registry mechanisms to register converters between MIME
content and other representations, as well as the get_content and set_content dispatch methods.
get_content(msg, *args, **kw)
Look up a handler function based on the mimetype of msg (see next paragraph), call it, passing through all
arguments, and return the result of the call.
The expectation is that the handler will extract the payload from
msg and return an object that encodes information about the extracted data.
If the maintype is multipart, raise a TypeError; otherwise look up a handler function based on
the type of obj (see next paragraph), call clear_content() on the msg, and call the handler function,
passing through all arguments.
The expectation is that the handler will transform and store obj into msg,
possibly making other changes to msg as well, such as adding various MIME headers to encode information
needed to interpret the stored data.
If none of the above match, repeat all of the checks above for each of the types in the MRO (typ.__mro__).
Finally, if no other key yields a handler, check for a handler for the key None.
If there is no handler for None,
raise a KeyError for the fully qualified name of the type.
Record the function handler as the handler for key.
For the possible values of key, see get_content().
For the possible values of typekey, see set_content().
Originally added in 3.4 as a provisional module
19.1.
Nevertheless, it provides significant advantages compared to the
base API: get_content on a text part will return a unicode string without the application needing to manually
decode it, set_content provides a rich set of options for controlling the headers added to a part and controlling
the content transfer encoding, and it enables the use of the various add_ methods, thereby simplifying the creation
of multipart messages.
Raise a KeyError
if called on a multipart.
If the part is a text part and errors is specified, use it as the error handler when
decoding the payload to unicode.
The default error handler is replace.
For bytes, use the specified maintype and subtype, or raise a TypeError if they are not specified.
For EmailMessage objects, set the maintype to message, and set the subtype to subtype if it is
specified or rfc822 if it is not.
If subtype is partial, raise an error (bytes objects must be used
to construct message/partial parts).
If charset is provided (which is valid only for str), encode the string to bytes using the specified character
set.
The default is utf-8.
If the specified charset is a known alias for a standard MIME charset name, use
the standard charset instead.
If
cte
is
set,
encode
the
payload
using
the
specified
content
transfer
encoding,
and
set
the
Content-Transfer-Encoding
header
to
that
value.
Possible
values
for
cte
are
quoted-printable, base64, 7bit, 8bit, and binary.
If the input cannot be encoded in
the specified encoding (for example, specifying a cte of 7bit for an input that contains non-ASCII values),
raise a ValueError.
For str objects, if cte is not set use heuristics to determine the most compact encoding.
For
message/rfc822, use 8bit if cte is not specified.
The EmailMessage object as modified by
set_content is correct, but BytesGenerator does not serialize it correctly.
If disposition is set, use it as the value of the Content-Disposition header.
If not specified, and
filename is specified, add the header with the value attachment.
If disposition is not specified and filename
is also not specified, do not add the header.
The only valid values for disposition are attachment and
inline.
If filename is specified, use it as the value of the filename parameter of the Content-Disposition
header.
If cid is specified, add a Content-ID header with cid as its value.
If params is specified, iterate its items method and use the resulting (key, value) pairs to set additional
parameters on the Content-Type header.
If headers is specified and is a list of strings of the form headername: headervalue or a list of
header objects (distinguished from strings by having a name attribute), add the headers to msg.
You can also omit the subtype
# if you want MIMEImage to guess it.
Unless the -o option is given, the email is sent by forwarding to your local
SMTP server, which then does the normal delivery process.
Your local machine
must be running an SMTP server.
Only the regular
files in the directory are sent, and we don't recurse to
subdirectories.
Encoding
# will be ignored, although we should check for simple things like
# gzip'd or compressed files.
Here’s an example of how to unpack a MIME message like the one above, into a directory of files:
#!/usr/bin/env python3
"""Unpack a MIME message into a directory of files.
Here’s an example of how to create an HTML message with an alternative plain text version.
To make things a bit more
interesting, we include a related image in the html part, and we save a copy of what we are going to send to disk, as well
as sending it.
This converts the message into a multipart/alternative
# container, with the original text message as the first part and the new html
# message as the second part.
Rewrite the href="cid:...." attributes to point to the filenames in partfiles.
Though not trivial, this should be possible using html.parser.
In a real program you'd get the filename from the arguments.
Of course,
# if the message has no plain text part printing the first three lines of html
# is probably useless, but this is just a conceptual example.
NamedTemporaryFile(suffix=extension, delete=False) as f:
f.write(part.get_content())
# again strip the <> to go from email form of cid to html form.
NamedTemporaryFile(mode='w', delete=False) as f:
f.write(magic_html_parser(body.get_content(), partfiles))
webbrowser.open(f.name)
os.remove(f.name)
for fn in partfiles.values():
os.remove(fn)
# Of course, there are lots of email messages that could break this simple
# minded program, but it will handle the most common ones.
Cela ressemble à un excellent recipie[1] déjeuner.
Legacy API:
19.1.9 email.message.
Message:
Representing an email message using the
compat32 API
The Message class is very similar to the EmailMessage class, without the methods added by that class, and with
the default behavior of certain other methods being slightly different.
We also document here some methods that, while
supported by the EmailMessage class, are not recommended unless you are dealing with legacy code.
The philosophy and structure of the two classes is otherwise the same.
This document describes the behavior under the default (for Message) policy Compat32.
If you are going to use
another policy, you should be using the EmailMessage class instead.
An email message consists of headers and a payload.
Headers must be RFC 5322 style names and values, where the
field name and value are separated by a colon.
The colon is not part of either the field name or the field value.
The
payload may be a simple text message, or a binary object, or a structured sequence of sub-messages each with their own
set of headers and their own payload.
The latter type of payload is indicated by the message having a MIME type such
as multipart/* or message/rfc822.
The conceptual model provided by a Message object is that of an ordered dictionary of headers with additional methods
for accessing both specialized information from the headers, for accessing the payload, for generating a serialized version
of the message, and for recursively walking over the object tree.
Note that duplicate headers are supported but special
methods must be used to access them.
The Message pseudo-dictionary is indexed by the header names, which must be ASCII values.
The values of the
dictionary are strings that are supposed to contain only ASCII characters; there is some special handling for non-ASCII
input, but it doesn’t always produce the correct results.
Headers are stored and returned in case-preserving form, but field
names are matched case-insensitively.
There may also be a single envelope header, also known as the Unix-From header
or the From_ header.
The payload is either a string or bytes, in the case of simple message objects, or a list of Message
objects, for MIME container documents (e.g. multipart/* and message/rfc822).
Here are the methods of the Message class:
class email.message.
If policy is specified (it must be an instance of a policy class) use the rules it specifies to update and serialize the
representation of the message.
If policy is not set, use the compat32 policy, which maintains backward compat-
ibility with the Python 3.2 version of the email package.
For more information see the policy documentation.
The policy keyword argument was added.
When optional unixfrom is true, the envelope header is
included in the returned string.
For backward compatibility reasons, maxhead-
erlen defaults to 0, so if you want a different value you must override it explicitly (the value specified for
max_line_length in the policy will be ignored by this method).
The policy argument may be used to override
the default policy obtained from the message instance.
This can be used to control some of the formatting
produced by the method, since the specified policy will be passed to the Generator.
Flattening the message may trigger changes to the Message if defaults need to be filled in to complete the
transformation to a string (for example, MIME boundaries may be generated or modified).
Note that this method is provided as a convenience and may not always format the message the way you
want.
For more flexibility, instantiate a Generator instance and use its flatten()
method directly.
If the message object contains binary data that is not encoded according to RFC standards, the non-
compliant data will be replaced by unicode “unknown character” code points.
Changed in version 3.4: the policy keyword argument was added.
Equivalent to as_string().
Allows str(msg) to produce a string containing the formatted message.
When optional unixfrom is true, the envelope header is
included in the returned string.
The policy argument may be used to override
the default policy obtained from the message instance.
This can be used to control some of the formatting
produced by the method, since the specified policy will be passed to the BytesGenerator.
Flattening the message may trigger changes to the Message if defaults need to be filled in to complete the
transformation to a string (for example, MIME boundaries may be generated or modified).
Note that this method is provided as a convenience and may not always format the message the way you want.
For example, by default it does not do the mangling of lines that begin with From that is required by the
Unix mbox format.
For more flexibility, instantiate a BytesGenerator instance and use its flatten()
method directly.
For example:
from io import BytesIO
from email.generator import BytesGenerator
fp = BytesIO()
g = BytesGenerator(fp, mangle_from_=True, maxheaderlen=60)
g.flatten(msg)
text = fp.getvalue()
New in version 3.4.
__bytes__()
Equivalent to as_bytes().
Allows bytes(msg) to produce a bytes object containing the formatted
message.
New in version 3.4.
When
is_multipart() returns False, the payload should be a string object (which might be a CTE en-
coded binary payload).
Defaults to None if the envelope header was never set.
attach(payload)
Add the given payload to the current payload, which must be None or a list of Message objects before the
call.
After the call, the payload will always be a list of Message objects.
If you want to set the payload to a
scalar object (e.g. a string), use set_payload() instead.
This is a legacy method.
On the EmailMessage class its functionality is replaced by set_content()
and the related make and add methods.
If the payload is a list and you mutate the list object, you
modify the message’s payload in place.
With optional argument i, get_payload() will return the i-th element of the payload, counting from zero,
if is_multipart() is True.
An IndexError will be raised if i is less than 0 or greater than or equal
to the number of items in the payload.
If the payload is a string (i.e. is_multipart() is False) and i
is given, a TypeError is raised.
Optional decode is a flag indicating whether the payload should be decoded or not, according to the
Content-Transfer-Encoding header.
When True and the message is not a multipart, the pay-
load will be decoded if this header’s value is quoted-printable or base64.
If some other encoding is
used, or Content-Transfer-Encoding header is missing, the payload is returned as-is (undecoded).
In all cases the returned value is binary data.
If the message is a multipart and the decode flag is True,
then None is returned.
If the payload is base64 and it was not perfectly formed (missing padding, charac-
ters outside the base64 alphabet), then an appropriate defect will be added to the message’s defect property
(InvalidBase64PaddingDefect or InvalidBase64CharactersDefect, respectively).
When decode is False (the default) the body is returned as a string without decoding the
Content-Transfer-Encoding.
However, for a Content-Transfer-Encoding of 8bit, an at-
tempt is made to decode the original bytes using the charset specified by the Content-Type header,
using the replace error handler.
If no charset is specified, or if the charset given is not recognized
by the email package, the body is decoded using the default ASCII charset.
This is a legacy method.
On the EmailMessage class its functionality is replaced by get_content()
and iter_parts().
It is the client’s responsibility to ensure the payload
invariants.
Optional charset sets the message’s default character set; see set_charset() for details.
This is a legacy method.
On the EmailMessage class its functionality is replaced by set_content().
If it is a string, it will be converted to a Charset
instance.
If charset is None, the charset parameter will be removed from the Content-Type header
(the message will not be otherwise modified).
Anything else will generate a TypeError.
If there is no existing MIME-Version header one will be added.
If there is no existing Content-Type
header, one will be added with a value of text/plain.
Whether the Content-Type header al-
ready exists or not, its charset parameter will be set to charset.output_charset.
If charset.input_charset
and charset.output_charset differ, the payload will be re-encoded to the output_charset.
If there is
no existing Content-Transfer-Encoding header, then the payload will be transfer-encoded, if
needed, using the specified Charset, and a header with the appropriate value will be added.
This is a legacy method.
On the EmailMessage class its functionality is replaced by the charset parameter
of the email.emailmessage.
EmailMessage.set_content() method.
This is a legacy method.
On the EmailMessage class it always returns None.
The following methods implement a mapping-like interface for accessing the message’s RFC 2822 headers.
Note
that there are some semantic differences between these methods and a normal mapping (i.e. dictionary) interface.
For example, in a dictionary there are no duplicate keys, but here there may be duplicate message headers.
Also,
in dictionaries there is no guaranteed order to the keys returned by keys(), but in a Message object, headers
are always returned in the order they appeared in the original message, or were added to the message later.
Any
header deleted and then re-added are always appended to the end of the header list.
These semantic differences are intentional and are biased toward maximal convenience.
Note that in all cases, any envelope header present in the message is not included in the mapping interface.
In a model generated from bytes, any header values that (in contravention of the RFCs) contain non-ASCII bytes
will, when retrieved through this interface, be represented as Header objects with a charset of unknown-8bit.
Matching is done case-insensitively and name
should not include the trailing colon.
Used for the in operator, e.g.:
if 'message-id' in myMessage:
print('Message-ID:', myMessage['message-id'])
__getitem__(name)
Return the value of the named header field.
If the header
is missing, None is returned; a KeyError is never raised.
Note that if the named field appears more than once in the message’s headers, exactly which of those field
values will be returned is undefined.
Use the get_all() method to get the values of all the extant named
headers.
__setitem__(name, val)
Add a header to the message with field name name and value val.
The field is appended to the end of the
message’s existing fields.
Note that this does not overwrite or delete any existing header with the same name.
Delete all occurrences of the field with name name from the message’s headers.
No exception is raised if the
named field isn’t present in the headers.
This is identical to __getitem__() except that optional failobj
is returned if the named header is missing (defaults to None).
Here are some additional useful methods:
get_all(name, failobj=None)
Return a list of all the values for the field named name.
If there are no such named headers in the message,
failobj is returned (defaults to None).
This method is similar to __setitem__() except that additional header param-
eters can be provided as keyword arguments.
For each item in the keyword argument dictionary _params, the key is taken as the parameter name, with
underscores converted to dashes (since dashes are illegal in Python identifiers).
Normally, the parameter will
be added as key="value" unless the value is None, in which case only the key will be added.
If the value
contains non-ASCII characters, it can be specified as a three tuple in the format (CHARSET, LANGUAGE,
VALUE), where CHARSET is a string naming the charset to be used to encode the value, LANGUAGE can
usually be set to None or the empty string (see RFC 2231 for other possibilities), and VALUE is the string
value containing non-ASCII code points.
If a three tuple is not passed and the value contains non-ASCII
characters, it is automatically encoded in RFC 2231 format using a CHARSET of utf-8 and a LANGUAGE
of None.
Replace a header.
Replace the first header found in the message that matches _name, retaining header order
and field name case.
If no matching header was found, a KeyError is raised.
The returned string is coerced to lower case of the form maintype/
subtype.
Since according to RFC 2045, messages always have a de-
fault type, get_content_type() will always return a value.
RFC 2045 defines a message’s default type to be text/plain unless it appears inside a multipart/
digest container, in which case it would be message/rfc822.
If the Content-Type header has an
invalid type specification, RFC 2045 mandates that the default type be text/plain.
This is the maintype part of the string returned by
get_content_type().
This is the subtype part of the string returned by
get_content_type().
Most messages have a default content type of text/plain, except for
messages that are subparts of multipart/digest containers.
Such subparts have a default content type
of message/rfc822.
set_default_type(ctype)
Set the default content type.
The default content type is not stored in the Content-Type header.
The elements of the returned list are 2-tuples
of key/value pairs, as split on the '=' sign.
The left hand side of the '=' is the key, while the right hand
side is the value.
If there is no '=' sign in the parameter the value is the empty string, otherwise the value is
as described in get_param() and is unquoted if optional unquote is True (the default).
Optional failobj is the object to return if there is no Content-Type header.
Optional header is the header
to search instead of Content-Type.
This is a legacy method.
On the EmailMessage class its functionality is replaced by the params property
of the individual header objects returned by the header access methods.
If the message has no
Content-Type header or if there is no such parameter, then failobj is returned (defaults to None).
Optional header if given, specifies the message header to use instead of Content-Type.
Parameter keys are always compared case insensitively.
The return value can either be a string, or a 3-
tuple if the parameter was RFC 2231 encoded.
When it’s a 3-tuple, the elements of the value are of the
form (CHARSET, LANGUAGE, VALUE).
Note that both CHARSET and LANGUAGE can be None, in
which case you should consider VALUE to be encoded in the us-ascii charset.
You can usually ignore
LANGUAGE.
If your application doesn’t care whether the parameter was encoded as in RFC 2231, you can collapse the
parameter value by calling email.utils.collapse_rfc2231_value(), passing in the return value
from get_param().
This will return a suitably decoded Unicode string when the value is a tuple, or the
original string unquoted if it isn’t.
In any case, the parameter value (either the returned string, or the VALUE item in the 3-tuple) is always
unquoted, unless unquote is set to False.
This is a legacy method.
On the EmailMessage class its functionality is replaced by the params property
of the individual header objects returned by the header access methods.
If the parameter already exists in the header, its value will
be replaced with value.
If the Content-Type header as not yet been defined for this message, it will be
set to text/plain and the new parameter value will be appended as per RFC 2045.
Optional header specifies an alternative header to Content-Type, and all parameters will be quoted as
necessary unless optional requote is False (the default is True).
If optional charset is specified, the parameter will be encoded according to RFC 2231.
Optional language
specifies the RFC 2231 language, defaulting to the empty string.
Both charset and language should be strings.
If replace is False (the default) the header is moved to the end of the list of headers.
If replace is True,
the header will be updated in place.
Changed in version 3.4: replace keyword was added.
The header will be re-written
in place without the parameter or its value.
All values will be quoted as necessary unless requote is False
(the default is True).
Optional header specifies an alternative to Content-Type.
set_type(type, header=’Content-Type’, requote=True)
Set the main type and subtype for the Content-Type header.
This method replaces the Content-Type header, keeping all the parameters in place.
If requote is False,
this leaves the existing header’s quoting as is, otherwise the parameters will be quoted (the default).
An alternative header can be specified in the header argument.
When the Content-Type header is set a
MIME-Version header is also added.
This is a legacy method.
On the EmailMessage class its functionality is replaced by the make_ and add_
methods.
If
the header does not have a filename parameter, this method falls back to looking for the name parameter
on the Content-Type header.
If neither is found, or the header is missing, then failobj is returned.
The
returned string will always be unquoted as per email.utils.unquote().
The returned string will always be unquoted
as per email.utils.unquote().
A HeaderParseError is raised if the message object has no
Content-Type header.
Note that using this method is subtly different than deleting the old Content-Type header and adding a
new one with the new boundary via add_header(), because set_boundary() preserves the order
of the Content-Type header in the list of headers.
However, it does not preserve any continuation lines
which may have been present in the original Content-Type header.
If there is no
Content-Type header, or if that header has no charset parameter, failobj is returned.
Note that this method differs from get_charset() which returns the Charset instance for the default
encoding of the message body.
If the message is a multipart, then the
list will contain one element for each subpart in the payload, otherwise, it will be a list of length 1.
Each item in the list will be a string which is the value of the charset parameter in the Content-Type
header for the represented subpart.
However, if the subpart has no Content-Type header, no charset
parameter, or is not of the text main MIME type, then that item in the returned list will be failobj.
The possible values for this method are inline, attachment or None if the message follows
RFC 2183.
New in version 3.5.
walk()
The walk() method is an all-purpose generator which can be used to iterate over all the parts and subparts
of a message object tree, in depth-first traversal order.
You will typically use walk() as the iterator in a
for loop; each iteration returns the next subpart.
Here’s an example that prints the MIME type of every part of a multipart message structure:

print(part.get_content_type())
multipart/report
text/plain
message/delivery-status
text/plain
text/plain
message/rfc822
text/plain
walk iterates over the subparts of any part where is_multipart() returns True, even though msg.
True True
False False
False True
False False
False False
False True
False False
multipart/report
text/plain
message/delivery-status
text/plain
(continues on next page)
19.1.
Message objects can also optionally contain two instance attributes, which can be used when generating the plain
text of a MIME message.
Normally, this text is never visible in a MIME-aware mail reader because it
falls outside the standard MIME armor.
However, when viewing the raw text of the message, or when viewing
the message in a non-MIME aware reader, this text can become visible.
The preamble attribute contains this leading extra-armor text for MIME documents.
When the Parser
discovers some text after the headers but before the first boundary string, it assigns this text to the message’s
preamble attribute.
When the Generator is writing out the plain text representation of a MIME message,
and it finds the message has a preamble attribute, it will write this text in the area between the headers and
the first boundary.
See email.parser and email.generator for details.
Note that if the message object has no preamble, the preamble attribute will be None.
You do not need to set the epilogue to the empty string in order for the Generator to print a newline at
the end of the file.
See email.
Lib/email/mime/
This module is part of the legacy (Compat32) email API.
Its functionality is partially replaced by the
contentmanager in the new API, but in certain applications these classes may still be useful, even in non-legacy
code.
Ordinarily, you get a message object structure by passing a file or some text to a parser, which parses the text and returns
the root message object.
However you can also build a complete message structure from scratch, or even individual
Message objects by hand.
In fact, you can also take an existing structure and add new Message objects, move them
around, etc.
This makes a very convenient interface for slicing-and-dicing MIME messages.
You can create a new object structure by creating Message instances, adding attachments and all the appropriate headers
manually.
For MIME messages though, the email package provides some convenient subclasses to make things easier.
This is the base class for all the MIME-specific subclasses of Message.
Ordinarily you won’t create instances
specifically of MIMEBase, although you could.
MIMEBase is provided primarily as a convenient base class for
more specific MIME-aware subclasses.
If policy is specified, (defaults to the compat32 policy) it will be passed to Message.
The MIMEBase class always adds a Content-Type header (based on _maintype, _subtype, and _params), and
a MIME-Version header (always set to 1.0).
Changed in version 3.6: Added policy keyword-only parameter.
Module: email.mime.nonmultipart
A subclass of MIMEBase, this is an intermediate base class for MIME messages that are not multipart.
The primary purpose of this class is to prevent the use of the attach() method, which only makes sense for
multipart messages.
If attach() is called, a MultipartConversionError exception is raised.
Module: email.mime.multipart
A subclass of MIMEBase, this is an intermediate base class for MIME messages that are multipart.
Optional
_subtype defaults to mixed, but can be used to specify the subtype of the message.
A Content-Type header of
multipart/_subtype will be added to the message object.
A MIME-Version header will also be added.
Optional boundary is the multipart boundary string.
When None (the default), the boundary is calculated when
needed (for example, when the message is serialized).
It must be possible to convert this sequence to a list.
You can always attach new subparts to the message by using the Message.attach method.
Optional policy argument defaults to compat32.
Additional parameters for the Content-Type header are taken from the keyword arguments, or passed into the
_params argument, which is a keyword dictionary.
Changed in version 3.6: Added policy keyword-only parameter.
Module: email.mime.application
A subclass of MIMENonMultipart, the MIMEApplication class is used to represent MIME message objects
of major type application.
Optional _subtype specifies
the MIME subtype and defaults to octet-stream.
Optional _encoder is a callable (i.e.
function) which will perform the actual encoding of the data for
transport.
This callable takes one argument, which is the MIMEApplication instance.
It should use
get_payload() and set_payload() to change the payload to encoded form.
It should also add any
Content-Transfer-Encoding or other headers to the message object as necessary.
The default encod-
ing is base64.
See the email.encoders module for a list of the built-in encoders.
Optional policy argument defaults to compat32.
Changed in version 3.6: Added policy keyword-only parameter.
Module: email.mime.audio
A subclass of MIMENonMultipart, the MIMEAudio class is used to create MIME message objects of major
type audio.
If this data can be decoded as au, wav, aiff, or
aifc, then the subtype will be automatically included in the Content-Type header.
Otherwise you can explicitly
specify the audio subtype via the _subtype argument.
If the minor type could not be guessed and _subtype was not
given, then TypeError is raised.
Optional _encoder is a callable (i.e.
function) which will perform the actual encoding of the audio data
for transport.
This callable takes one argument, which is the MIMEAudio instance.
It should use
get_payload() and set_payload() to change the payload to encoded form.
It should also add any
Content-Transfer-Encoding or other headers to the message object as necessary.
The default encod-
ing is base64.
See the email.encoders module for a list of the built-in encoders.
Optional policy argument defaults to compat32.
Changed in version 3.6: Added policy keyword-only parameter.
Module: email.mime.image
A subclass of MIMENonMultipart, the MIMEImage class is used to create MIME message objects of major
type image.
If this data type can be detected (jpeg, png,
gif, tiff, rgb, pbm, pgm, ppm, rast, xbm, bmp, webp, and exr attempted), then the subtype will be automatically
included in the Content-Type header.
Otherwise you can explicitly specify the image subtype via the _subtype
argument.
If the minor type could not be guessed and _subtype was not given, then TypeError is raised.
Optional _encoder is a callable (i.e.
function) which will perform the actual encoding of the image data
for transport.
This callable takes one argument, which is the MIMEImage instance.
It should use
get_payload() and set_payload() to change the payload to encoded form.
It should also add any
Content-Transfer-Encoding or other headers to the message object as necessary.
The default encod-
ing is base64.
See the email.encoders module for a list of the built-in encoders.
Optional policy argument defaults to compat32.
Changed in version 3.6: Added policy keyword-only parameter.
MIMEMessage(_msg, _subtype=’rfc822’, *, policy=compat32)
Module: email.mime.message
A subclass of MIMENonMultipart, the MIMEMessage class is used to create MIME objects of main type
message.
Optional _subtype sets the subtype of the message; it defaults to rfc822.
Optional policy argument defaults to compat32.
Changed in version 3.6: Added policy keyword-only parameter.
MIMEText(_text, _subtype=’plain’, _charset=None, *, policy=compat32)
Module: email.mime.text
A subclass of MIMENonMultipart, the MIMEText class is used to create MIME objects of major type text.
The _charset parameter accepts either a string
or a Charset instance.
Unless the _charset argument is explicitly set to None, the MIMEText object created will have both a
Content-Type header with a charset parameter, and a Content-Transfer-Encoding header.
This
means that a subsequent set_payload call will not result in an encoded payload, even if a charset is passed in the
set_payload command.
You can “reset” this behavior by deleting the Content-Transfer-Encoding
header, after which a set_payload call will automatically encode the new payload (and add a new
Content-Transfer-Encoding header).
Optional policy argument defaults to compat32.
Changed in version 3.5: _charset also accepts Charset instances.
Changed in version 3.6: Added policy keyword-only parameter.
In the current API encoding and decoding of headers is
handled transparently by the dictionary-like API of the EmailMessage class.
In addition to uses in legacy code, this
module can be useful in applications that need to completely control the character sets used when encoding headers.
The remaining text in this section is the original documentation of the module.
RFC 2822 is the base standard that describes the format of email messages.
It derives from the older RFC 822 standard
which came into widespread use at a time when most email was composed of ASCII characters only.
RFC 2822 is a
specification written assuming email contains only 7-bit ASCII characters.
Of course, as email has been deployed worldwide, it has become internationalized, such that language specific character
sets can now be used in email messages.
The base standard still requires email messages to be transferred using only 7-bit
ASCII characters, so a slew of RFCs have been written describing how to encode email containing non-ASCII characters
into RFC 2822-compliant format.
These RFCs include RFC 2045, RFC 2046, RFC 2047, and RFC 2231.
The email
package supports these standards in its email.header and email.charset modules.
If you want to include non-ASCII characters in your email headers, say in the Subject or To fields, you should use the
Header class and assign the field in the Message object to an instance of Header instead of using a string for the
header value.
Import the Header class from the email.header module.
For example:
'Subject: =?iso-8859-1?q?p=F6stal?=\n\n'
Notice here how we wanted the Subject field to contain a non-ASCII character?
We did this by creating a Header
instance and passing in the character set that the byte string was encoded in.
When the subsequent Message instance
19.1.
MIME-aware mail readers would show this header
using the embedded ISO-8859-1 character.
Here is the Header class description:
class email.header.
Header(s=None, charset=None, maxlinelen=None, header_name=None,
continuation_ws=’ ’, errors=’strict’)
Create a MIME-compliant header that can contain strings in different character sets.
Optional s is the initial header value.
If None (the default), the initial header value is not set.
You can later append
to the header with append() method calls.
Optional charset serves two purposes: it has the same meaning as the charset argument to the append() method.
It also sets the default character set for all subsequent append() calls that omit the charset argument.
If charset
is not provided in the constructor (the default), the us-ascii character set is used both as s’s initial charset and
as the default for subsequent append() calls.
The maximum line length can be specified explicitly via maxlinelen.
For splitting the first line to a shorter value (to
account for the field header which isn’t included in s, e.g. Subject) pass in the name of the field in header_name.
The default maxlinelen is 76, and the default value for header_name is None, meaning it is not taken into account
for the first line of a long, split header.
Optional continuation_ws must be RFC 2822-compliant folding whitespace, and is usually either a space or a hard
tab character.
This character will be prepended to continuation lines.
Optional errors is passed straight through to the append() method.
Optional charset, if given, should be a Charset instance (see email.charset) or the name of a character
set, which will be converted to a Charset instance.
A value of None (the default) means that the charset
given in the constructor is used.
If it is an instance of bytes, then charset is the encoding of that
byte string, and a UnicodeError will be raised if the string cannot be decoded with that character set.
If s is an instance of str, then charset is a hint specifying the character set of the characters in the string.
In either case, when producing an RFC 2822-compliant header using RFC 2047 rules, the string will be
encoded using the output codec of the charset.
If the string cannot be encoded using the output codec, a
UnicodeError will be raised.
Optional errors is passed as the errors argument to the decode call if s is a byte string.
Encode a message header into an RFC-compliant format, possibly wrapping long lines and encapsulating
non-ASCII parts in base64 or quoted-printable encodings.
Optional splitchars is a string containing characters which should be given extra weight by the splitting algo-
rithm during normal header wrapping.
Space and tab may be included in the string to indicate whether
preference should be given to one over the other as a split point when other split chars do not appear in the
line being split.
Splitchars does not affect RFC 2047 encoded lines.
It defaults to the most useful
value for Python application code (\n), but \r\n can be specified in order to produce headers with RFC-
compliant line separators.
Added the linesep argument.
The Header class also provides a number of methods to support standard operators and built-in functions.
__str__()
Returns an approximation of the Header as a string, using an unlimited line length.
All pieces are converted
to unicode using the specified encoding and joined together appropriately.
Any pieces with a charset of
'unknown-8bit' are decoded as ASCII using the 'replace' error handler.
Changed in version 3.2: Added handling for the 'unknown-8bit' charset.
This method allows you to compare two Header instances for equality.
This method allows you to compare two Header instances for inequality.
The email.header module also provides the following convenient functions.
The header value is in header.
This function returns a list of (decoded_string, charset) pairs containing each of the decoded parts of
the header.
Here’s an example:
[(b'p\xf6stal', 'iso-8859-1')]
email.header.make_header(decoded_seq, maxlinelen=None, header_name=None, continuation_ws=’ ’)
Create a Header instance from a sequence of pairs as returned by decode_header().
This function takes one of those sequence of pairs and returns a Header instance.
Optional maxlinelen,
header_name, and continuation_ws are as in the Header constructor.
In the new API only the aliases table is used.
The remaining text in this section is the original documentation of the module.
This module provides a class Charset for representing character sets and character set conversions in email messages,
as well as a character set registry and several convenience methods for manipulating this registry.
Instances of Charset
are used in several other modules within the email package.
Import this class from the email.charset module.
This class provides information about the requirements imposed on email for a specific character set.
It also provides
convenience routines for converting between character sets, given the availability of the applicable codecs.
Given
a character set, it will do its best to provide information on how to use that character set in an email message in an
RFC-compliant way.
Certain character sets must be encoded with quoted-printable or base64 when used in email headers or bodies.
Certain character sets must be converted outright, and are not allowed in email.
Optional input_charset is as described below; it is always coerced to lower case.
After being alias normalized it is
also used as a lookup into the registry of character sets to find out the header encoding, body encoding, and output
conversion codec to be used for the character set.
For example, if input_charset is iso-8859-1, then headers
and bodies will be encoded using quoted-printable and no output conversion codec is necessary.
If input_charset is
euc-jp, then headers will be encoded with base64, bodies will not be encoded, but output text will be converted
from the euc-jp character set to the iso-2022-jp character set.
Charset instances have the following data attributes:
input_charset
The initial character set specified.
Common aliases are converted to their official email names (e.g. latin_1
is converted to iso-8859-1).
Defaults to 7-bit us-ascii.
QP (for quoted-printable), charset.
Otherwise, it will be None.
SHORTEST is not allowed for body_encoding.
If the input_charset
is one of them, this attribute will contain the name of the character set output will be converted to.
Otherwise,
it will be None.
input_codec
The name of the Python codec used to convert the input_charset to Unicode.
If no conversion codec is
necessary, this attribute will be None.
If no conversion codec is
necessary, this attribute will have the same value as the input_codec.
Charset instances also have the following methods:
get_body_encoding()
Return the content transfer encoding used for body encoding.
This is either the string quoted-printable or base64 depending on the encoding used, or it is a func-
tion, in which case you should call the function with a single argument, the Message object being encoded.
The function should then set the Content-Transfer-Encoding header itself to whatever is appropri-
ate.
Returns the string quoted-printable if body_encoding is QP, returns the string base64 if
body_encoding is BASE64, and returns the string 7bit otherwise.
This is the output_charset attribute if that is not None, otherwise it is input_charset.
header_encode(string)
Header-encode the string string.
The type of encoding (base64 or quoted-printable) will be based on the header_encoding attribute.
This is similar to header_encode() except that the string is fit into maximum line lengths as given by
the argument maxlengths, which must be an iterator: each element returned from this iterator will provide the
next maximum line length.
The type of encoding (base64 or quoted-printable) will be based on the body_encoding attribute.
The Charset class also provides a number of methods to support standard operations and built-in functions.
__str__()
Returns input_charset as a string coerced to lower case.
This method allows you to compare two Charset instances for equality.
This method allows you to compare two Charset instances for inequality.
Add character properties to the global registry.
Optional header_enc and body_enc is either charset.
QP for quoted-printable, charset.
BASE64 for base64
encoding, charset.
SHORTEST for the shortest of quoted-printable or base64 encoding, or None for no en-
coding.
SHORTEST is only valid for header_enc.
The default is None for no encoding.
Optional output_charset is the character set that the output should be in.
Conversions will proceed from input
charset, to Unicode, to the output charset when the method Charset.convert() is called.
The default is to
output in the same character set as the input.
Both input_charset and output_charset must have Unicode codec entries in the module’s character set-to-codec
mapping; use add_codec() to add codecs the module does not know about.
See the codecs module’s docu-
mentation for more information.
The global character set registry is kept in the module global dictionary CHARSETS.
The global charset alias registry is kept in the module global dictionary ALIASES.
In the new API the functionality is provided by the cte
parameter of the set_content() method.
This module is deprecated in Python 3.
The functions provided here should not be called explicitly since the MIMEText
class sets the content type and CTE header using the _subtype and _charset values passed during the instantiation of that
class.
The remaining text in this section is the original documentation of the module.
When creating Message objects from scratch, you often need to encode the payloads for transport through compliant
mail servers.
This is especially true for image/* and text/* type messages containing binary data.
The email package provides some convenient encoders in its encoders module.
These encoders are actually used by
the MIMEAudio and MIMEImage class constructors to provide default encodings.
All encoder functions take exactly
one argument, the message object to encode.
They usually extract the payload, encode it, and reset the payload to this
newly encoded value.
They should also set the Content-Transfer-Encoding header as appropriate.
Note that these functions are not meaningful for a multipart message.
They must be applied to individual subparts instead,
and will raise a TypeError if passed a message whose type is multipart.
Here are the encoding functions provided:
email.encoders.encode_quopri(msg)
Encodes the payload into quoted-printable form and sets the Content-Transfer-Encoding header to
quoted-printable1.
This is a good encoding to use when most of your payload is normal printable data,
but contains a few unprintable characters.
Encodes the payload into base64 form and sets the Content-Transfer-Encoding header to base64.
This is a good encoding to use when most of your payload is unprintable data since it is a more compact form than
quoted-printable.
The drawback of base64 encoding is that it renders the text non-human readable.
This does nothing; it doesn’t even set the Content-Transfer-Encoding header.
If called without arguments, return current time.
Otherwise dt
argument should be a datetime instance, and it is converted to the local time zone according to the system time
zone database.
If dt is naive (that is, dt.tzinfo is None), it is assumed to be in local time.
The isdst parameter
is ignored.
New in version 3.3.
The isdst parameter.
Optional idstring if given, is a string
used to strengthen the uniqueness of the message id. Optional domain if given provides the portion of the msgid
after the ‘@’.
The default is the local hostname.
It is not normally necessary to override this default, but may be
useful certain cases, such as a constructing distributed system that uses a consistent domain name across multiple
hosts.
Changed in version 3.2: Added the domain keyword.
The remaining functions are part of the legacy (Compat32) email API.
There is no need to directly use these with the
new API, since the parsing and formatting they provide is done automatically by the header parsing machinery of the new
API.
email.utils.quote(str)
Return a new string with backslashes in str replaced by two backslashes, and double quotes replaced by backslash-
double quote.
email.utils.unquote(str)
Return a new string which is an unquoted version of str.
If str ends and begins with double quotes, they are stripped
off.
Likewise if str ends and begins with angle brackets, they are stripped off.
Returns a tuple of that information, unless the parse fails, in which case a 2-tuple
of ('', '') is returned.
The inverse of parseaddr(), this takes a 2-tuple of the form (realname, email_address) and returns
the string value suitable for a To or Cc header.
If the first element of pair is false, then the second element is returned
unmodified.
Optional charset is the character set that will be used in the RFC 2047 encoding of the realname if the
realname contains non-ASCII characters.
Can be an instance of str or a Charset.
This method returns a list of 2-tuples of the form returned by parseaddr().
Here’s a simple example that gets all the recipients of
a message:
19.1.
If it succeeds in parsing the date, parsedate()
returns a 9-tuple that can be passed directly to time.mktime(); otherwise None will be returned.
Note that
indexes 6, 7, and 8 of the result tuple are not usable.
email.utils.parsedate_tz(date)
Performs the same function as parsedate(), but returns either None or a 10-tuple; the first 9 elements make
up a tuple that can be passed directly to time.mktime(), and the tenth is the offset of the date’s timezone from
UTC (which is the official term for Greenwich Mean Time)1.
If the input string has no timezone, the last element
of the tuple returned is 0, which represents UTC.
The inverse of format_datetime().
Performs the same function as parsedate(), but on success returns
a datetime; otherwise ValueError is raised if date contains an invalid value such as an hour greater than 23
or a timezone offset not between -24 and 24 hours.
If the input date has a timezone of -0000, the datetime
will be a naive datetime, and if the date is conforming to the RFCs it will represent a time in UTC but with no
indication of the actual source timezone of the message the date comes from.
If the input date has any other valid
timezone offset, the datetime will be an aware datetime with the corresponding a timezone tzinfo.
New in version 3.3.
If the
timezone item in the tuple is None, assume local time.
Optional timeval if given is a floating point time value as accepted by time.gmtime() and time.
Optional localtime is a flag that when True, interprets timeval, and returns a date relative to the local timezone
instead of UTC, properly taking daylight savings time into account.
The default is False meaning UTC is used.
Optional usegmt is a flag that when True, outputs a date string with the timezone as an ascii string GMT, rather
than a numeric -0000.
This is needed for some protocols (such as HTTP).
This only applies when localtime is
False.
The default is False.
email.utils.format_datetime(dt, usegmt=False)
Like formatdate, but the input is a datetime instance.
If it is a naive datetime, it is assumed to be “UTC
with no information about the source timezone”, and the conventional -0000 is used for the timezone.
If it is
an aware datetime, then the numeric timezone offset is used.
This
provides a way to generate standards conformant HTTP date headers.
New in version 3.3.
Optional charset and language, if given is the character set name and
language name to use.
If neither is given, s is returned as-is.
If charset is given but language is not, the string is
encoded using the empty string for language.
When a header parameter is encoded in RFC 2231 format, Message.get_param may return a 3-tuple con-
taining the character set, language, and value.
Optional errors is passed to the errors argument of str’s encode() method; it defaults to 'replace'.
Op-
tional fallback_charset specifies the character set to use if the one in the RFC 2231 header is not known by Python;
it defaults to 'us-ascii'.
For convenience, if the value passed to collapse_rfc2231_value() is not a tuple, it should be a string and
it is returned unquoted.
The email.iterators module
provides some useful higher level iterations over message object trees.
This iterates over all the payloads in all the subparts of msg, returning the string payloads line-by-line.
It skips over
all the subpart headers, and it skips over any subpart with a payload that isn’t a Python string.
This is somewhat
equivalent to reading the flat text representation of the message from a file using readline(), skipping over all
the intervening headers.
Optional decode is passed through to Message.get_payload.
This iterates over all the subparts of msg, returning only those subparts that match the MIME type specified by
maintype and subtype.
Note that subtype is optional; if omitted, then subpart MIME type matching is done only with the main type.
Thus, by default typed_subpart_iterator() returns each subpart that has a MIME type of text/*.
The following function has been added as a useful debugging tool.
It should not be considered part of the supported public
interface for the package.
For example:
multipart/mixed
text/plain
text/plain
multipart/digest
message/rfc822
text/plain
message/rfc822
text/plain
message/rfc822
text/plain
message/rfc822
text/plain
message/rfc822
text/plain
text/plain
Optional fp is a file-like object to print the output to.
It must be suitable for Python’s print() function.
Warning:
Be cautious when parsing JSON data from untrusted sources.
A malicious JSON string may cause the
decoder to consume considerable CPU and memory resources.
Limiting the size of data to be parsed is recommended.
Note: JSON is a subset of YAML 1.2.
The JSON produced by this module’s default settings (in particular, the default
separators value) is also a subset of YAML 1.0 and 1.1.
This module can thus also be used as a YAML serializer.
Note: This module’s encoders and decoders preserve input and output order by default.
Order is only lost if the underlying
containers are unordered.
Basic Usage
json.dump(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None,
indent=None, separators=None, default=None, sort_keys=False, **kw)
Serialize obj as a JSON formatted stream to fp (a .write()-supporting file-like object) using this conversion
table.
If skipkeys is true (default: False), then dict keys that are not of a basic type (str, int, float, bool, None)
will be skipped instead of raising a TypeError.
The json module always produces str objects, not bytes objects.
Therefore, fp.write() must support
str input.
If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped.
If ensure_ascii is false, these characters will be output as-is.
If check_circular is false (default: True), then the circular reference check for container types will be skipped and
a circular reference will result in a RecursionError (or worse).
If allow_nan is false (default: True), then it will be a ValueError to serialize out of range float values (nan,
inf, -inf) in strict compliance of the JSON specification.
If allow_nan is true, their JavaScript equivalents (NaN,
Infinity, -Infinity) will be used.
If indent is a non-negative integer or string, then JSON array elements and object members will be pretty-printed
with that indent level.
An indent level of 0, negative, or "" will only insert newlines.
None (the default) selects
the most compact representation.
Using a positive integer indent indents that many spaces per level.
If indent is a
string (such as "\t"), that string is used to indent each level.
Changed in version 3.2: Allow strings for indent in addition to integers.
If specified, separators should be an (item_separator, key_separator) tuple.
The default is (', ',
': ') if indent is None and (',', ': ') otherwise.
To get the most compact JSON representation, you
should specify (',', ':') to eliminate whitespace.
Changed in version 3.4: Use (',', ': ') as default if indent is not None.
If specified, default should be a function that gets called for objects that can’t otherwise be serialized.
If not specified, TypeError is raised.
If sort_keys is true (default: False), then the output of dictionaries will be sorted by key.
To use a custom JSONEncoder subclass (e.g. one that overrides the default() method to serialize additional
types), specify it with the cls kwarg; otherwise JSONEncoder is used.
All optional parameters are now keyword-only.
Note: Unlike pickle and marshal, JSON is not a framed protocol, so trying to serialize multiple objects with
repeated calls to dump() using the same fp will result in an invalid JSON file.
The arguments have the same meaning as in
dump().
Note: Keys in key/value pairs of JSON are always of the type str.
When a dictionary is converted into JSON,
all the keys of the dictionary are coerced to strings.
As a result of this, if a dictionary is converted into JSON and
then back into a dictionary, the dictionary may not equal the original one.
That is, loads(dumps(x)) !
This feature can be used to implement custom
decoders (e.g. JSON-RPC class hinting).
The return value of object_pairs_hook will be used instead of the dict.
This feature can be
used to implement custom decoders.
If object_hook is also defined, the object_pairs_hook takes priority.
Changed in version 3.1: Added support for object_pairs_hook.
parse_float, if specified, will be called with the string of every JSON float to be decoded.
By default, this is
equivalent to float(num_str).
This can be used to use another datatype or parser for JSON floats (e.g.
19.2.
By default, this is equivalent
to int(num_str).
This can be used to use another datatype or parser for JSON integers (e.g. float).
The default parse_int of int() now limits the maximum length of the integer string via
the interpreter’s integer string conversion length limitation to help avoid denial of service attacks.
This can be used to raise an exception if invalid JSON numbers are encountered.
Changed in version 3.1: parse_constant doesn’t get called on ‘null’, ‘true’, ‘false’ anymore.
To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used.
Additional keyword arguments will be passed to the constructor of the class.
If the data being deserialized is not a valid JSON document, a JSONDecodeError will be raised.
All optional parameters are now keyword-only.
Changed in version 3.6: fp can now be a binary file.
The input encoding should be UTF-8, UTF-16 or UTF-32.
The other arguments have the same meaning as in load().
If the data being deserialized is not a valid JSON document, a JSONDecodeError will be raised.
Changed in version 3.6: s can now be of type bytes or bytearray.
The input encoding should be UTF-8,
UTF-16 or UTF-32.
The keyword argument encoding has been removed.
JSONDecoder(*, object_hook=None, parse_float=None, parse_int=None, parse_constant=None,
strict=True, object_pairs_hook=None)
Simple JSON decoder.
This can be used to provide custom deserializations (e.g. to support JSON-RPC
class hinting).
The return value of object_pairs_hook will be used instead of the dict.
This feature can be used to
implement custom decoders.
If object_hook is also defined, the object_pairs_hook takes priority.
Changed in version 3.1: Added support for object_pairs_hook.
parse_float, if specified, will be called with the string of every JSON float to be decoded.
By default, this is
equivalent to float(num_str).
This can be used to use another datatype or parser for JSON floats (e.g.
decimal.
Decimal).
By default, this is equivalent
to int(num_str).
This can be used to use another datatype or parser for JSON integers (e.g. float).
This can be used to raise an exception if invalid JSON numbers are encountered.
If strict is false (True is the default), then control characters will be allowed inside strings.
Control characters in
this context are those with character codes in the 0–31 range, including '\t' (tab), '\n', '\r' and '\0'.
If the data being deserialized is not a valid JSON document, a JSONDecodeError will be raised.
All parameters are now keyword-only.
JSONDecodeError will be raised if the given JSON document is not valid.
This can be used to decode a JSON document from a string that may have extraneous data at the end.
JSONEncoder(*, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True,
sort_keys=False, indent=None, separators=None, default=None)
Extensible JSON encoder for Python data structures.
Supports the following objects and types by default:
Python
JSON
dict
object
list, tuple
array
str
string
int, float, int- & float-derived Enums
number
True
true
False
false
None
null
Changed in version 3.4: Added support for int- and float-derived Enum classes.
To extend this to recognize other objects, subclass and implement a default() method with another method
that returns a serializable object for o if possible, otherwise it should call the superclass implementation (to raise
TypeError).
If skipkeys is false (the default), a TypeError will be raised when trying to encode keys that are not str, int,
float or None.
If skipkeys is true, such items are simply skipped.
If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped.
If ensure_ascii is false, these characters will be output as-is.
If check_circular is true (the default), then lists, dicts, and custom encoded objects will be checked for circular
references during encoding to prevent an infinite recursion (which would cause a RecursionError).
Otherwise,
no such check takes place.
If allow_nan is true (the default), then NaN, Infinity, and -Infinity will be encoded as such.
This be-
havior is not JSON specification compliant, but is consistent with most JavaScript based encoders and decoders.
Otherwise, it will be a ValueError to encode such floats.
If sort_keys is true (default: False), then the output of dictionaries will be sorted by key; this is useful for regression
tests to ensure that JSON serializations can be compared on a day-to-day basis.
If indent is a non-negative integer or string, then JSON array elements and object members will be pretty-printed
with that indent level.
An indent level of 0, negative, or "" will only insert newlines.
None (the default) selects
the most compact representation.
Using a positive integer indent indents that many spaces per level.
If indent is a
string (such as "\t"), that string is used to indent each level.
Changed in version 3.2: Allow strings for indent in addition to integers.
If specified, separators should be an (item_separator, key_separator) tuple.
The default is (', ',
': ') if indent is None and (',', ': ') otherwise.
To get the most compact JSON representation, you
should specify (',', ':') to eliminate whitespace.
Changed in version 3.4: Use (',', ': ') as default if indent is not None.
If specified, default should be a function that gets called for objects that can’t otherwise be serialized.
If not specified, TypeError is raised.
All parameters are now keyword-only.
JSONDecodeError(msg, doc, pos)
Subclass of ValueError with the following additional attributes:
msg
The unformatted error message.
New in version 3.5.
The JSON format is specified by RFC 7159 and by ECMA-404.
This section details this module’s level of compliance
with the RFC.
For simplicity, JSONEncoder and JSONDecoder subclasses, and parameters other than those explicitly
mentioned, are not considered.
This module does not comply with the RFC in a strict fashion, implementing some extensions that are valid JavaScript
but not valid JSON.
In particular:
• Infinite and NaN number values are accepted and output;
• Repeated names within an object are accepted, and only the value of the last name-value pair is used.
Since the RFC permits RFC-compliant parsers to accept input texts that are not RFC-compliant, this module’s deserializer
is technically RFC-compliant under default settings.
Character Encodings
The RFC requires that JSON be represented using either UTF-8, UTF-16, or UTF-32, with UTF-8 being the recom-
mended default for maximum interoperability.
As permitted, though not required, by the RFC, this module’s serializer sets ensure_ascii=True by default, thus escaping
the output so that the resulting strings only contain ASCII characters.
Other than the ensure_ascii parameter, this module is defined strictly in terms of conversion between Python objects and
Unicode strings, and thus does not otherwise directly address the issue of character encodings.
The RFC prohibits adding a byte order mark (BOM) to the start of a JSON text, and this module’s serializer does not add
a BOM to its output.
The RFC permits, but does not require, JSON deserializers to ignore an initial BOM in their input.
This module’s deserializer raises a ValueError when an initial BOM is present.
The RFC does not explicitly forbid JSON strings which contain byte sequences that don’t correspond to valid Unicode
characters (e.g. unpaired UTF-16 surrogates), but it does note that they may cause interoperability problems.
By default,
this module accepts and outputs (when present in the original str) code points for such sequences.
Despite that, by default, this module
accepts and outputs Infinity, -Infinity, and NaN as if they were valid JSON number literal values:
'-Infinity'
'NaN'
-inf
nan
In the serializer, the allow_nan parameter can be used to alter this behavior.
In the deserializer, the parse_constant
parameter can be used to alter this behavior.
Repeated Names Within an Object
The RFC specifies that the names within a JSON object should be unique, but does not mandate how repeated names in
JSON objects should be handled.
By default, this module does not raise an exception; instead, it ignores all but the last
name-value pair for a given name:
{'x': 3}
The object_pairs_hook parameter can be used to alter this behavior.
Top-level Non-Object, Non-Array Values
The old version of JSON specified by the obsolete RFC 4627 required that the top-level value of a JSON text must be
either a JSON object or array (Python dict or list), and could not be a JSON null, boolean, number, or string value.
RFC 7159 removed that restriction, and this module does not and has never implemented that restriction in either its
serializer or its deserializer.
Regardless, for maximum interoperability, you may wish to voluntarily adhere to the restriction yourself.
This module does not impose any such limits beyond those of the relevant Python datatypes themselves or the Python
interpreter itself.
When serializing to JSON, beware any such limitations in applications that may consume your JSON.
In particular, it is
common for JSON numbers to be deserialized into IEEE 754 double precision numbers and thus subject to that repre-
sentation’s range and precision limitations.
This is especially relevant when serializing Python int values of extremely
large magnitude, or when serializing instances of “exotic” numerical types such as decimal.
Decimal.
Use the --sort-keys option to sort the
output of dictionaries alphabetically by key.
Otherwise, write it to sys.stdout.
New in version 3.5.
--no-ensure-ascii
Disable escaping of non-ascii characters, see json.dumps() for more information.
New in version 3.9.
New in version 3.8.
New in version 3.9.
Mailbox offers a dictionary-like mapping from keys to messages.
Message extends the
email.message module’s Message class with format-specific state and behavior.
Supported mailbox formats are
Maildir, mbox, MH, Babyl, and MMDF.
See also:
Module email Represent and manipulate messages.
Mailbox
A mailbox, which may be inspected and modified.
The Mailbox class defines an interface and is not intended to be instantiated.
Instead, format-specific subclasses
should inherit from Mailbox and your code should instantiate a particular subclass.
The Mailbox interface is dictionary-like, with small keys corresponding to messages.
Keys are issued by the
Mailbox instance with which they will be used and are only meaningful to that Mailbox instance.
A key
continues to identify a message even if the corresponding message is modified, such as by replacing it with another
message.
Messages may be added to a Mailbox instance using the set-like method add() and removed using a del
statement or the set-like methods remove() and discard().
Mailbox interface semantics differ from dictionary semantics in some noteworthy ways.
Each time a message
is requested, a new representation (typically a Message instance) is generated based upon the current state of
the mailbox.
Similarly, when a message is added to a Mailbox instance, the provided message representation’s
contents are copied.
In neither case is a reference to the message representation kept by the Mailbox instance.
The default Mailbox iterator iterates over message representations, not keys as the default dictionary iterator does.
Moreover, modification of a mailbox during iteration is safe and well-defined.
Messages added to the mailbox after
an iterator is created will not be seen by the iterator.
Messages removed from the mailbox before the iterator yields
them will be silently skipped, though using a key from an iterator may result in a KeyError exception if the
corresponding message is subsequently removed.
The safest mailbox format to use for such tasks is Maildir; try to avoid using single-file formats such
as mbox for concurrent writing.
If you’re modifying a mailbox, you must lock it by calling the lock() and
unlock() methods before reading any messages in the file or making any changes by adding or deleting a
message.
Failing to lock the mailbox runs the risk of losing messages or corrupting the entire mailbox.
Mailbox instances have the following methods:
add(message)
Add message to the mailbox and return the key that has been assigned to it.
Parameter message may be a Message instance, an email.message.
Message instance, a string, a byte
string, or a file-like object (which should be open in binary mode).
If message is an instance of the appropriate
format-specific Message subclass (e.g., if it’s an mboxMessage instance and this is an mbox instance), its
format-specific information is used.
Otherwise, reasonable defaults for format-specific information are used.
Changed in version 3.2: Support for binary input was added.
Delete the message corresponding to key from the mailbox.
If no such message exists, a KeyError exception is raised if the method was called as remove() or
__delitem__() but no exception is raised if the method was called as discard().
The behavior of
discard() may be preferred if the underlying mailbox format supports concurrent modification by other
processes.
Raise a KeyError exception if no message already
corresponds to key.
As with add(), parameter message may be a Message instance, an email.message.
Message in-
stance, a string, a byte string, or a file-like object (which should be open in binary mode).
If message is an
instance of the appropriate format-specific Message subclass (e.g., if it’s an mboxMessage instance and
this is an mbox instance), its format-specific information is used.
Otherwise, the format-specific information
of the message that currently corresponds to key is left unchanged.
The messages are represented as instances of
the appropriate format-specific Message subclass unless a custom message factory was specified when the
Mailbox instance was initialized.
Note: The behavior of __iter__() is unlike that of dictionaries, which iterate over keys.
iteritems()
19.3.
The messages are represented as
instances of the appropriate format-specific Message subclass unless a custom message factory was specified
when the Mailbox instance was initialized.
If no such message exists, default is returned
if the method was called as get() and a KeyError exception is raised if the method was called as
__getitem__().
Return a byte representation of the message corresponding to key, or raise a KeyError exception if no such
message exists.
New in version 3.2.
get_string(key)
Return a string representation of the message corresponding to key, or raise a KeyError exception if no
such message exists.
The message is processed through email.message.
Message to convert it to a 7bit
clean representation.
The file-like object behaves as if open in binary mode.
This file should be closed once
it is no longer needed.
The file object really is a binary file; previously it was incorrectly returned in text
mode.
Also, the file-like object now supports the context management protocol: you can use a with statement
to automatically close it.
Note:
Unlike other representations of messages, file-like representations are not necessarily independent
of the Mailbox instance that created them or of the underlying mailbox.
Return True if key corresponds to a message, False otherwise.
If no such message
exists, return default.
The message is represented as an instance of the appropriate format-specific Message
subclass unless a custom message factory was specified when the Mailbox instance was initialized.
If the mailbox is empty, raise a KeyError exception.
The message is
represented as an instance of the appropriate format-specific Message subclass unless a custom message
factory was specified when the Mailbox instance was initialized.
Parameter arg should be a key-to-message mapping or an iterable of (key, message) pairs.
Updates the mailbox
so that, for each given key and message, the message corresponding to key is set to message as if by using
__setitem__().
As with __setitem__(), each key must already correspond to a message in the
mailbox or else a KeyError exception will be raised, so in general it is incorrect for arg to be a Mailbox
instance.
Note: Unlike with dictionaries, keyword arguments are not supported.
For some Mailbox subclasses, changes are always written
immediately and flush() does nothing, but you should still make a habit of calling this method.
An
ExternalClashError is raised if the lock is not available.
The particular locking mechanisms used
depend upon the mailbox format.
You should always lock the mailbox before making any modifications to
its contents.
Flush the mailbox, unlock it if necessary, and close any open files.
For some Mailbox subclasses, this
method does nothing.
Maildir
class mailbox.
A subclass of Mailbox for mailboxes in Maildir format.
Parameter factory is a callable object that accepts a
file-like message representation (which behaves as if opened in binary mode) and returns a custom representation.
If factory is None, MaildirMessage is used as the default message representation.
If create is True, the
mailbox is created if it does not exist.
If create is True and the dirname path exists, it will be treated as an existing maildir without attempting to verify
its directory layout.
It is for historical reasons that dirname is named as such rather than path.
Maildir is a directory-based mailbox format invented for the qmail mail transfer agent and now widely supported
by other programs.
Messages in a Maildir mailbox are stored in separate files within a common directory structure.
This design allows Maildir mailboxes to be accessed and modified by multiple unrelated programs without data
corruption, so file locking is unnecessary.
Maildir mailboxes contain three subdirectories, namely: tmp, new, and cur.
Messages are created momentarily
in the tmp subdirectory and then moved to the new subdirectory to finalize delivery.
A mail user agent may
subsequently move the message to the cur subdirectory and store information about the state of the message in a
special “info” section appended to its file name.
Folder names are represented by Maildir
without the leading '.'.
Each folder is itself a Maildir mailbox but should not contain other folders.
Note: The Maildir specification requires the use of a colon (':') in certain message file names.
However, some
operating systems do not permit this character in file names, If you wish to use a Maildir-like format on such an
operating system, you should specify another character to use instead.
The exclamation point ('!') is a popular
choice.
The colon attribute may also be set on a per-instance basis.
Maildir instances have all of the methods of Mailbox in addition to the following:
list_folders()
Return a list of the names of all folders.
A NoSuchMailboxError
exception is raised if the folder does not exist.
If the folder contains any messages, a NotEmptyError exception
will be raised and the folder will not be deleted.
The Maildir speci-
fication says that mail-reading programs should do this occasionally.
These methods generate unique file names based upon the current process ID.
When using
multiple threads, undetected name clashes may occur and cause corruption of the mailbox unless threads
are coordinated to avoid using these methods to manipulate the same mailbox simultaneously.
All changes to Maildir mailboxes are immediately applied, so this method does nothing.
Maildir mailboxes do not support (or require) locking, so these methods do nothing.
Depending upon the host platform, it may not be possible to modify or remove the underlying message while
the returned file remains open.
See also:
maildir man page from Courier A specification of the format.
Describes a common extension for supporting folders.
Using maildir format Notes on Maildir by its inventor.
A subclass of Mailbox for mailboxes in mbox format.
Parameter factory is a callable object that accepts a file-
like message representation (which behaves as if opened in binary mode) and returns a custom representation.
If
factory is None, mboxMessage is used as the default message representation.
If create is True, the mailbox is
created if it does not exist.
The mbox format is the classic format for storing mail on Unix systems.
All messages in an mbox mailbox are
stored in a single file with the beginning of each message indicated by a line whose first five characters are “From
“.
Several variations of the mbox format exist to address perceived shortcomings in the original.
In the interest of
compatibility, mbox implements the original format, which is sometimes referred to as mboxo.
This means that
the Content-Length header, if present, is ignored and that any occurrences of “From ” at the beginning of a
line in a message body are transformed to “>From ” when storing the message, although occurrences of “>From ”
are not transformed to “From ” when reading the message.
Some Mailbox methods implemented by mbox deserve special remarks:
get_file(key)
Using the file after calling flush() or close() on the mbox instance may yield unpredictable results or
raise an exception.
Why The Content-Length Format is Bad An argument for using the original
mbox format rather than a variation.
A subclass of Mailbox for mailboxes in MH format.
Parameter factory is a callable object that accepts a file-like
message representation (which behaves as if opened in binary mode) and returns a custom representation.
If factory
is None, MHMessage is used as the default message representation.
If create is True, the mailbox is created if
it does not exist.
MH is a directory-based mailbox format invented for the MH Message Handling System, a mail user agent.
Each
message in an MH mailbox resides in its own file.
An MH mailbox may contain other MH mailboxes (called
folders) in addition to messages.
Folders may be nested indefinitely.
MH mailboxes also support sequences, which
are named lists used to logically group messages without moving them to sub-folders.
Sequences are defined in a
file called .mh_sequences in each folder.
The MH class manipulates MH mailboxes, but it does not attempt to emulate all of mh’s behaviors.
In particular, it
does not modify and is not affected by the context or .mh_profile files that are used by mh to store its state
and configuration.
MH instances have all of the methods of Mailbox in addition to the following:
list_folders()
Return a list of the names of all folders.
A NoSuchMailboxError exception
is raised if the folder does not exist.
If the folder contains any messages, a NotEmptyError exception
will be raised and the folder will not be deleted.
If there are no sequences, the empty dictionary
is returned.
Re-define the sequences that exist in the mailbox based upon sequences, a dictionary of names mapped to key
lists, like returned by get_sequences().
Entries in the sequences list
are updated correspondingly.
Note: Already-issued keys are invalidated by this operation and should not be subsequently used.
These methods immediately delete the message.
The MH convention of marking a message for deletion by
prepending a comma to its name is not used.
Depending upon the host platform, it may not be possible to remove the underlying message while the returned
file remains open.
All changes to MH mailboxes are immediately applied, so this method does nothing.
See also:
nmh - Message Handling System Home page of nmh, an updated version of the original mh.
MH & nmh: Email for Users & Programmers A GPL-licensed book on mh and nmh, with some information on the
mailbox format.
Babyl
class mailbox.
A subclass of Mailbox for mailboxes in Babyl format.
Parameter factory is a callable object that accepts a file-
like message representation (which behaves as if opened in binary mode) and returns a custom representation.
If
factory is None, BabylMessage is used as the default message representation.
If create is True, the mailbox
is created if it does not exist.
Babyl is a single-file mailbox format used by the Rmail mail user agent included with Emacs.
The beginning of
a message is indicated by a line containing the two characters Control-Underscore ('\037') and Control-L ('\
014').
The end of a message is indicated by the start of the next message or, in the case of the last message, a
line containing a Control-Underscore ('\037') character.
Messages in a Babyl mailbox have two sets of headers, original headers and so-called visible headers.
Visible head-
ers are typically a subset of the original headers that have been reformatted or abridged to be more attractive.
Each
message in a Babyl mailbox also has an accompanying list of labels, or short strings that record extra information
about the message, and a list of all user-defined labels found in the mailbox is kept in the Babyl options section.
Babyl instances have all of the methods of Mailbox in addition to the following:
get_labels()
Return a list of the names of all user-defined labels used in the mailbox.
Note: The actual messages are inspected to determine which labels exist in the mailbox rather than consulting
the list of labels in the Babyl options section, but the Babyl section is updated whenever the mailbox is
modified.
In Babyl mailboxes, the headers of a message are not stored contiguously with the body of the message.
To
generate a file-like representation, the headers and body are copied together into an io.BytesIO instance,
19.3.
As a result, the file-like object is truly independent of the underlying
mailbox but does not save memory compared to a string representation.
Reading Mail with Rmail The Rmail manual, with some information on Babyl semantics.
MMDF
class mailbox.
A subclass of Mailbox for mailboxes in MMDF format.
Parameter factory is a callable object that accepts a
file-like message representation (which behaves as if opened in binary mode) and returns a custom representation.
If factory is None, MMDFMessage is used as the default message representation.
If create is True, the mailbox
is created if it does not exist.
MMDF is a single-file mailbox format invented for the Multichannel Memorandum Distribution Facility, a mail
transfer agent.
Each message is in the same form as an mbox message but is bracketed before and after by lines
containing four Control-A ('\001') characters.
As with the mbox format, the beginning of each message is
indicated by a line whose first five characters are “From “, but additional occurrences of “From ” are not transformed
to “>From ” when storing messages because the extra message separator lines prevent mistaking such occurrences
for the starts of subsequent messages.
Some Mailbox methods implemented by MMDF deserve special remarks:
get_file(key)
Using the file after calling flush() or close() on the MMDF instance may yield unpredictable results or
raise an exception.
MMDF A Wikipedia article describing the Multichannel Memorandum Distribution Facility.
Message objects
class mailbox.
Message(message=None)
A subclass of the email.message module’s Message.
Subclasses of mailbox.
Message add mailbox-
format-specific state and behavior.
If message is omitted, the new instance is created in a default, empty state.
If message is an email.message.
Message instance, its contents are copied; furthermore, any format-specific information is converted insofar as
possible if message is a Message instance.
If message is a string, a byte string, or a file, it should contain an RFC
2822-compliant message, which is read and parsed.
Files should be open in binary mode, but text mode files are
accepted for backward compatibility.
The format-specific state and behaviors offered by subclasses vary, but in general it is only the properties that
are not specific to a particular mailbox that are supported (although presumably the properties are specific to a
particular mailbox format).
For example, file offsets for single-file mailbox formats and file names for directory-
based mailbox formats are not retained, because they are only applicable to the original mailbox.
But state such as
whether a message has been read by the user or marked as important is retained, because it applies to the message
itself.
There is no requirement that Message instances be used to represent messages retrieved using Mailbox in-
stances.
In some situations, the time and memory required to generate Message representations might not be
acceptable.
For such situations, Mailbox instances also offer string and file-like representations, and a custom
message factory may be specified when a Mailbox instance is initialized.
MaildirMessage
class mailbox.
A message with Maildir-specific behaviors.
Parameter message has the same meaning as with the Message
constructor.
Typically, a mail user agent application moves all of the messages in the new subdirectory to the cur subdirectory
after the first time the user opens and closes the mailbox, recording that the messages are old whether or not they’ve
actually been read.
Each message in cur has an “info” section added to its file name to store information about its
state.
The “info” section may take one of
two forms: it may contain “2,” followed by a list of standardized flags (e.g., “2,FR”) or it may contain “1,” followed
by so-called experimental information.
MaildirMessage instances offer the following methods:
get_subdir()
Return either “new” (if the message should be stored in the new subdirectory) or “cur” (if the message should
be stored in the cur subdirectory).
Note: A message is typically moved from new to cur after its mailbox has been accessed, whether or not
the message is has been read.
A message msg has been read if "S" in msg.get_flags() is True.
set_subdir(subdir)
Set the subdirectory the message should be stored in.
Parameter subdir must be either “new” or “cur”.
If the message complies with the standard Maildir
format, the result is the concatenation in alphabetical order of zero or one occurrence of each of 'D', 'F',
'P', 'R', 'S', and 'T'.
The empty string is returned if no flags are set or if “info” contains experimental
semantics.
To add more than one flag at a time, flag may be
a string of more than one character.
Unset the flag(s) specified by flag without changing other flags.
To remove more than one flag at a time, flag
maybe a string of more than one character.
If “info” contains experimental information rather than flags, the
current “info” is not modified.
get_date()
Return the delivery date of the message as a floating-point number representing seconds since the epoch.
set_date(date)
Set the delivery date of the message to date, a floating-point number representing seconds since the epoch.
This is useful for accessing and modifying “info” that is
experimental (i.e., not a list of flags).
Set “info” to info, which should be a string.
Parameter message has the same meaning as with the Message con-
structor.
Messages in an mbox mailbox are stored together in a single file.
The sender’s envelope address and the time of
delivery are typically stored in a line beginning with “From ” that is used to indicate the start of a message, though
there is considerable variation in the exact format of this data among mbox implementations.
Flags that indicate
the state of the message, such as whether it has been read or marked as important, are typically stored in Status
and X-Status headers.
Conventional flags for mbox messages are as follows:
Flag
Meaning
Explanation
R
Read
Read
O
Old
Previously detected by MUA
D
Deleted
Marked for subsequent deletion
F
Flagged
Marked as important
A
Answered
Replied to
The “R” and “O” flags are stored in the Status header, and the “D”, “F”, and “A” flags are stored in the
X-Status header.
The flags and headers typically appear in the order mentioned.
The
leading “From ” and the trailing newline are excluded.
For convenience, time_ may be specified and will be formatted appropriately and appended to from_.
If
time_ is specified, it should be a time.struct_time instance, a tuple suitable for passing to time.
If the message complies with the conventional
format, the result is the concatenation in the following order of zero or one occurrence of each of 'R', 'O',
'D', 'F', and 'A'.
set_flags(flags)
Set the flags specified by flags and unset all others.
Parameter flags should be the concatenation in any order
of zero or more occurrences of each of 'R', 'O', 'D', 'F', and 'A'.
Unset the flag(s) specified by flag without changing other flags.
To remove more than one flag at a time, flag
maybe a string of more than one character.
When an mboxMessage instance is created based upon a MaildirMessage instance, a “From ” line is generated
based upon the MaildirMessage instance’s delivery date, and the following conversions take place:
19.3.
O flag
D flag
D flag
F flag
F flag
A flag
A flag
MHMessage
class mailbox.
A message with MH-specific behaviors.
Parameter message has the same meaning as with the Message construc-
tor.
MH messages do not support marks or flags in the traditional sense, but they do support sequences, which are
logical groupings of arbitrary messages.
Add sequence to the list of sequences that include this message.
Remove sequence from the list of sequences that include this message.
Resulting state
BabylMessage state
“unseen” sequence
“unseen” label
“replied” sequence
“answered” label
BabylMessage
class mailbox.
A message with Babyl-specific behaviors.
Parameter message has the same meaning as with the Message con-
structor.
Certain message labels, called attributes, are defined by convention to have special meanings.
The attributes are as
follows:
Label
Explanation
unseen
Not read, but previously detected by MUA
deleted
Marked for subsequent deletion
filed
Copied to another file or mailbox
answered
Replied to
forwarded
Forwarded
edited
Modified by the user
resent
Resent
19.3.
The BabylMessage class, though, uses the original headers
because they are more complete.
Visible headers may be accessed explicitly if desired.
BabylMessage instances offer the following methods:
get_labels()
Return a list of labels on the message.
Add label to the list of labels on the message.
Remove label from the list of labels on the message.
Parameter visible should be a
Message instance, an email.message.
Message instance, a string, or a file-like object (which should
be open in text mode).
When a BabylMessage instance’s original headers are modified, the visible headers are not automatically
modified to correspond.
This method updates the visible headers as follows: each visible header with a corre-
sponding original header is set to the value of the original header, each visible header without a corresponding
original header is removed, and any of Date, From, Reply-To, To, CC, and Subject that are present
in the original headers but not the visible headers are added to the visible headers.
MMDFMessage(message=None)
A message with MMDF-specific behaviors.
Parameter message has the same meaning as with the Message
constructor.
As with message in an mbox mailbox, MMDF messages are stored with the sender’s address and the delivery date
in an initial line beginning with “From “.
Likewise, flags that indicate the state of the message are typically stored
in Status and X-Status headers.
Conventional flags for MMDF messages are identical to those of mbox message and are as follows:
Flag
Meaning
Explanation
R
Read
Read
O
Old
Previously detected by MUA
D
Deleted
Marked for subsequent deletion
F
Flagged
Marked as important
A
Answered
Replied to
The “R” and “O” flags are stored in the Status header, and the “D”, “F”, and “A” flags are stored in the
X-Status header.
The flags and headers typically appear in the order mentioned.
MMDFMessage instances offer the following methods, which are identical to those offered by mboxMessage:
get_from()
Return a string representing the “From ” line that marks the start of the message in an mbox mailbox.
The
leading “From ” and the trailing newline are excluded.
For convenience, time_ may be specified and will be formatted appropriately and appended to from_.
If
time_ is specified, it should be a time.struct_time instance, a tuple suitable for passing to time.
If the message complies with the conventional
format, the result is the concatenation in the following order of zero or one occurrence of each of 'R', 'O',
'D', 'F', and 'A'.
set_flags(flags)
Set the flags specified by flags and unset all others.
Parameter flags should be the concatenation in any order
of zero or more occurrences of each of 'R', 'O', 'D', 'F', and 'A'.
Unset the flag(s) specified by flag without changing other flags.
To remove more than one flag at a time, flag
maybe a string of more than one character.
When an MMDFMessage instance is created based upon a MaildirMessage instance, a “From ” line is generated
based upon the MaildirMessage instance’s delivery date, and the following conversions take place:
19.3.
Resulting state
mboxMessage state
R flag
R flag
O flag
O flag
D flag
D flag
F flag
F flag
A flag
A flag
19.3.3 Exceptions
The following exception classes are defined in the mailbox module:
exception mailbox.
Error
The based class for all other module-specific exceptions.
NoSuchMailboxError
Raised when a mailbox is expected but is not found, such as when instantiating a Mailbox subclass with a path
that does not exist (and with the create parameter set to False), or when opening a folder that does not exist.
NotEmptyError
Raised when a mailbox is not empty but is expected to be, such as when deleting a folder that contains messages.
FormatError
Raised when the data in a file cannot be parsed, such as when an MH instance attempts to read a corrupted .
Examples
A simple example of printing the subjects of all messages in a mailbox that seem interesting:
import mailbox
for message in mailbox.mbox('~/mbox'):
subject = message['subject']
# Could possibly be None.
MH('~/Mail')
destination.lock()
for message in mailbox.
This example sorts mail from several mailing lists into different mailboxes, being careful to avoid mail corruption due to
concurrent modification by other programs, mail loss due to interruption of the program, or premature termination due
to malformed messages in the mailbox:
import mailbox
import email.errors
list_names = ('python-list', 'python-dev', 'python-bugs')
boxes = {name: mailbox.mbox('~/email/%s' % name) for name in list_names}
inbox = mailbox.
Maildir('~/Maildir', factory=None)
for key in inbox.iterkeys():
try:
message = inbox[key]
except email.errors.
The message is malformed.
Just leave it.
Conversions are provided from filename to MIME type and from MIME type to filename extension; encodings are
not supported for the latter conversion.
The module provides one class and a number of convenience functions.
The functions are the normal interface to this
module, but some applications may be interested in the class as well.
The functions described below provide the primary interface for this module.
If the module has not been initialized, they
will call init() if they rely on the information init() sets up.
URL can be a string or a path-like object.
The return value is a tuple (type, encoding) where type is None if the type can’t be guessed (missing or
unknown suffix) or a string of the form 'type/subtype', usable for a MIME content-type header.
The
encoding is suitable for use as a Content-Encoding header, not as a Content-Transfer-Encoding
header.
The mappings are table driven.
Encoding suffixes are case sensitive; type suffixes are first tried case
sensitively, then case insensitively.
The optional strict argument is a flag specifying whether the list of known MIME types is limited to only the official
types registered with IANA.
When strict is True (the default), only the IANA types are supported; when strict is
False, some additional non-standard but commonly used MIME types are also recognized.
Changed in version 3.8: Added support for url being a path-like object.
The return value is a list of strings giving
all possible filename extensions, including the leading dot ('.').
The extensions are not guaranteed to have been
associated with any particular data stream, but would be mapped to the MIME type type by guess_type().
The return value is a string giving a filename
extension, including the leading dot ('.').
The extension is not guaranteed to have been associated with any
particular data stream, but would be mapped to the MIME type type by guess_type().
If no extension can be
guessed for type, None is returned.
The optional strict argument has the same meaning as with the guess_type() function.
Some additional functions and data items are available for controlling the behavior of the module.
If given, files must be a sequence of file names which should be used to
augment the default type map.
If omitted, the file names to use are taken from knownfiles; on Windows, the
current registry settings are loaded.
Each file named in files or knownfiles takes precedence over those named
before it.
Calling init() repeatedly is allowed.
Specifying an empty list for files will prevent the system defaults from being applied: only the well-known values
will be present from a built-in list.
If files is None the internal data structure is completely rebuilt to its initial default value.
This is a stable operation
and will produce the same results when called multiple times.
Changed in version 3.2: Previously, Windows registry settings were ignored.
Load the type map given in the file filename, if it exists.
The type map is returned as a dictionary mapping filename
extensions, including the leading dot ('.'), to strings of the form 'type/subtype'.
If the file filename does
not exist or cannot be read, None is returned.
When the extension is already known, the new
type will replace the old one.
When the type is already known the extension will be added to the list of known
extensions.
When strict is True (the default), the mapping will be added to the official MIME types, otherwise to the non-
standard ones.
This is set to True by init().
mimetypes.knownfiles
List of type map file names commonly installed.
These files are typically named mime.types and are installed
in different locations by different packages.
This is used to allow recognition of encoded files for which the encoding
and the type are indicated by the same extension.
For example, the .tgz extension is mapped to .tar.gz to
allow the encoding and type to be recognized separately.
An example usage of the module:
19.4.
This class represents a MIME-types database.
By default, it provides access to the same database as the rest of this
module.
The initial database is a copy of that provided by the module, and may be extended by loading additional
mime.types-style files into the database using the read() or readfp() methods.
The mapping dictionaries
may also be cleared before loading additional data if the default data is not desired.
The optional filenames parameter can be used to cause additional files to be loaded “on top” of the default database.
This is used to allow recognition of encoded files for which the
encoding and the type are indicated by the same extension.
For example, the .tgz extension is mapped to
.tar.gz to allow the encoding and type to be recognized separately.
This is initially a copy of the global
suffix_map defined in the module.
This is initially a copy of the global
encodings_map defined in the module.
They are initialized by common_types
and types_map.
types_map_inv
Tuple containing two dictionaries, mapping MIME types to a list of filename extensions: the first dictio-
nary is for the non-standards types and the second one is for the standard types.
They are initialized by
common_types and types_map.
guess_extension(type, strict=True)
Similar to the guess_extension() function, using the tables stored as part of the object.
This uses readfp() to parse the file.
If strict is True, information will be added to list of standard types, else to the list of non-standard types.
The file must have the format of the standard mime.
If strict is True, information will be added to the list of standard types, else to the list of non-standard types.
Availability: Windows.
If strict is True, information will be added to the list of standard types, else to the list of non-standard types.
New in version 3.2.
19.5 base64 — Base16, Base32, Base64, Base85 Data Encodings
Source code: Lib/base64.py
This module provides functions for encoding binary data to printable ASCII characters and decoding such encodings back
to binary data.
It provides encoding and decoding functions for the encodings specified in RFC 4648, which defines the
Base16, Base32, and Base64 algorithms, and for the de-facto standard Ascii85 and Base85 encodings.
The RFC 4648 encodings are suitable for encoding binary data so that it can be safely sent by email, used as parts of URLs,
or included as part of an HTTP POST request.
The encoding algorithm is not the same as the uuencode program.
There are two interfaces provided by this module.
The modern interface supports encoding bytes-like objects to ASCII
bytes, and decoding bytes-like objects or strings containing ASCII to bytes.
Both base-64 alphabets defined in RFC
4648 (normal, and URL- and filesystem-safe) are supported.
The legacy interface does not support decoding from strings, but it does provide functions for encoding and decoding to
and from file objects.
It only supports the Base64 standard alphabet, and it adds newlines every 76 characters as per RFC
2045.
Note that if you are looking for RFC 2045 support you probably want to be looking at the email package instead.
Changed in version 3.3: ASCII-only Unicode strings are now accepted by the decoding functions of the modern interface.
Changed in version 3.4: Any bytes-like objects are now accepted by all encoding and decoding functions in this module.
Ascii85/Base85 support added.
The modern interface provides:
base64.b64encode(s, altchars=None)
Encode the bytes-like object s using Base64 and return the encoded bytes.
Optional altchars must be a bytes-like object of length 2 which specifies an alternative alphabet for the + and /
characters.
This allows an application to e.g. generate URL or filesystem safe Base64 strings.
The default is None,
for which the standard Base64 alphabet is used.
May assert or raise a ValueError if the length of altchars is not 2.
Raises a TypeError if altchars is not a
bytes-like object.
Decode the Base64 encoded bytes-like object or ASCII string s and return the decoded bytes.
Optional altchars must be a bytes-like object or ASCII string of length 2 which specifies the alternative alphabet
used instead of the + and / characters.
A binascii.
Error exception is raised if s is incorrectly padded.
If validate is False (the default), characters that are neither in the normal base-64 alphabet nor the alternative
alphabet are discarded prior to the padding check.
If validate is True, these non-alphabet characters in the input
result in a binascii.
Error.
For more information about the strict base64 check, see binascii.a2b_base64()
May assert or raise a ValueError if the length of altchars is not 2.
base64.standard_b64encode(s)
Encode bytes-like object s using the standard Base64 alphabet and return the encoded bytes.
Decode bytes-like object or ASCII string s using the URL- and filesystem-safe alphabet, which substitutes - instead
of + and _ instead of / in the standard Base64 alphabet, and return the decoded bytes.
Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input.
For security purposes,
the default is False.
O (oh), and for optional mapping of the
digit 1 (one) to either the letter I (eye) or letter L (el).
The optional argument map01 when not None, specifies
which letter the digit 1 should be mapped to (when map01 is not None, the digit 0 is always mapped to the letter
O).
For security purposes the default is None, so that 0 and 1 are not allowed in the input.
A binascii.
Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the
input.
Similar to b32encode() but uses the Extended Hex Alphabet, as defined in RFC 4648.
New in version 3.10.
This version does not allow the digit 0 (zero) to the letter O (oh) and digit 1 (one) to either the letter I (eye) or letter
L (el) mappings, all these characters are included in the Extended Hex Alphabet and are not interchangeable.
Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input.
For security purposes,
the default is False.
A binascii.
Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the
input.
This feature is not supported by the “standard” Ascii85 encoding.
If this is non-zero, each
output line will be at most this many characters long.
Note that the btoa implementation
always pads.
New in version 3.4.
Decode the Ascii85 encoded bytes-like object or ASCII string b and return the decoded bytes.
This feature is not supported by the “standard” Ascii85 encoding.
This should
only contain whitespace characters, and by default contains all whitespace characters in ASCII.
New in version 3.4.
If pad is true, the input is padded with b'\0' so its length is a multiple of 4 bytes before encoding.
New in version 3.4.
Decode the base85-encoded bytes-like object or ASCII string b and return the decoded bytes.
Padding is implicitly
removed, if necessary.
New in version 3.4.
The legacy interface:
base64.decode(input, output)
Decode the contents of the binary input file and write the resulting binary data to the output file.
New in version 3.1.
New in version 3.1.
Security Considerations
A new security considerations section was added to RFC 4648 (section 12); it’s recommended to review the security
section for any code deployed to production.
See also:
Module binascii Support module containing ASCII-to-binary and binary-to-ASCII conversions.
Part One: Mechanisms for Specifying and Describing the Format o
Section 5.2, “Base64 Content-Transfer-Encoding,” provides the definition of the base64 encoding.
The binascii module contains a number of methods to convert between binary and various ASCII-encoded binary
representations.
Normally, you will not use these functions directly but use wrapper modules like uu or base64 instead.
The binascii module contains low-level functions written in C for greater speed that are used by the higher-level
modules.
Note: a2b_* functions accept Unicode strings containing only ASCII characters.
Other functions only accept bytes-like
objects (such as bytes, bytearray and other objects that support the buffer protocol).
Changed in version 3.3: ASCII-only unicode strings are now accepted by the a2b_* functions.
The binascii module defines the following functions:
binascii.a2b_uu(string)
Convert a single line of uuencoded data back to binary and return the binary data.
Lines normally contain 45
(binary) bytes, except for the last line.
Line data may be followed by whitespace.
The length of data should be at most 45.
If backtick is true, zeros are represented by '`' instead of spaces.
Added the backtick parameter.
More than one line may be passed at a
time.
If strict_mode is true, only valid base64 data will be converted.
Invalid base64 data will raise binascii.
Error.
Valid base64:
• Conforms to RFC 3548.
Changed in version 3.11: Added the strict_mode parameter.
binascii.b2a_base64(data, *, newline=True)
Convert binary data to a line of ASCII characters in base64 coding.
The return value is the converted line, including
a newline char if newline is true.
The output of this function conforms to RFC 3548.
Added the newline parameter.
More than one line may be
passed at a time.
If the optional argument header is present and true, underscores will be decoded as spaces.
The return value is the converted
line(s).
If the optional argument quotetabs is present and true, all tabs and spaces will be encoded.
If the optional
argument istext is present and true, newlines are not encoded but trailing whitespace will be encoded.
If the optional
argument header is present and true, spaces will be encoded as underscores per RFC 1522.
If the optional argument
header is present and false, newline characters will be encoded as well; otherwise linefeed conversion might corrupt
the binary data stream.
This uses the
CRC-CCITT polynomial x16 + x12 + x5 + 1, often represented as 0x1021.
This CRC is used in the binhex4 format.
The default initial
CRC is zero.
The algorithm is consistent with the ZIP file checksum.
Since the algorithm is designed for use as a
checksum algorithm, it is not suitable for use as a general hash algorithm.
The result is always unsigned.
Every byte of data is converted into the corresponding
2-digit hex representation.
The returned bytes object is therefore twice as long as the length of data.
Similar functionality (but returning a text string) is also conveniently accessible using the bytes.hex() method.
If sep is specified, it must be a single character str or bytes object.
It will be inserted in the output after every
bytes_per_sep input bytes.
Separator placement is counted from the right end of the output by default, if you wish
to count from the left, supply a negative bytes_per_sep value.
The sep and bytes_per_sep parameters were added.
This function is the inverse of b2a_hex().
Similar functionality (accepting only text string arguments, but more liberal towards whitespace) is also accessible
using the bytes.fromhex() class method.
Error
Exception raised on errors.
These are usually programming errors.
Incomplete
Exception raised on incomplete data.
These are usually not programming errors, but may be handled by reading a
little more data and trying again.
See also:
Module base64 Support for RFC compliant base64-style encoding in base 16, 32, 64, and 85.
Module uu Support for UU encoding used on Unix.
Module quopri Support for quoted-printable encoding used in MIME email messages.
One: Mechanisms for Specifying and Describing the Format of Internet Message Bodies”.
The quoted-printable encoding is designed for data where there are relatively few nonprintable characters; the base64
encoding scheme available via the base64 module is more compact if there are many such characters, as when sending
a graphics file.
If the optional argument header is present and true, underscore will be decoded as
space.
Part Two: Message Header Extensions for Non-ASCII Text”.
Note that spaces
and tabs appearing at the end of lines are always encoded, as per RFC 1521.
By
default, it sends a False value to quotetabs parameter of the encode() function.
See also:
Module base64 Encode and decode MIME base64 data
19.7.
Internet Data Handling
CHAPTER
TWENTY
STRUCTURED MARKUP PROCESSING TOOLS
Python supports a variety of modules to work with various forms of structured data markup.
This includes modules
to work with the Standard Generalized Markup Language (SGML) and the Hypertext Markup Language (HTML), and
several interfaces for working with the Extensible Markup Language (XML).
This module defines utilities to manipulate HTML.
Use this if you need to display text that might
contain such characters in HTML.
If the optional flag quote is true, the characters (") and (') are also translated;
this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href="...">.
New in version 3.2.
Convert all named and numeric character references (e.g. &gt;, &#62;, &#x3e;) in the string s to the cor-
responding Unicode characters.
This function uses the rules defined by the HTML 5 standard for both valid and
invalid character references, and the list of HTML 5 named character references.
New in version 3.4.
Submodules in the html package are:
• html.parser – HTML/XHTML parser with lenient parsing mode
• html.entities – HTML entity definitions
20.2 html.parser — Simple HTML and XHTML parser
Source code: Lib/html/parser.py
This module defines a class HTMLParser which serves as the basis for parsing text files formatted in HTML (HyperText
Mark-up Language) and XHTML.
HTMLParser(*, convert_charrefs=True)
Create a parser instance able to parse invalid markup.
If convert_charrefs is True (the default), all character references (except the ones in script/style elements)
are automatically converted to the corresponding Unicode characters.
An HTMLParser instance is fed HTML data and calls handler methods when start tags, end tags, text, comments,
and other markup elements are encountered.
The user should subclass HTMLParser and override its methods to
implement the desired behavior.
This parser does not check that end tags match start tags or call the end-tag handler for elements which are closed
implicitly by closing an outer element.
Changed in version 3.4: convert_charrefs keyword argument added.
The default value for argument convert_charrefs is now True.
The output will then be:
Encountered a start tag: html
Encountered a start tag: head
Encountered a start tag: title
Encountered some data
: Test
Encountered an end tag : title
Encountered an end tag : head
Encountered a start tag: body
Encountered a start tag: h1
Encountered some data
: Parse me!
Methods
HTMLParser instances have the following methods:
HTMLParser.feed(data)
Feed some text to the parser.
It is processed insofar as it consists of complete elements; incomplete data is buffered
until more data is fed or close() is called.
This method may be redefined
by a derived class to define additional processing at the end of the input, but the redefined version should always
call the HTMLParser base class method close().
HTMLParser.reset()
Reset the instance.
Loses all unprocessed data.
This is called implicitly at instantiation time.
HTMLParser.getpos()
Return current line number and offset.
HTMLParser.get_starttag_text()
Return the text of the most recently opened start tag.
This should not normally be needed for structured processing,
but may be useful in dealing with HTML “as deployed” or for re-generating input with minimal changes (whitespace
between attributes can be preserved, etc.).
The following methods are called when data or markup elements are encountered and they are meant to be overridden in
a subclass.
This method is called to handle the start tag of an element (e.g. <div id="main">).
The tag argument is the name of the tag converted to lower case.
The attrs argument is a list of (name, value)
pairs containing the attributes found inside the tag’s <> brackets.
The name will be translated to lower case, and
quotes in the value have been removed, and character and entity references have been replaced.
A HREF="https://www.cwi.nl/">, this method would be called as
handle_starttag('a', [('href', 'https://www.cwi.nl/')]).
All entity references from html.entities are replaced in the attribute values.
This method is called to handle the end tag of an element (e.g. </div>).
The tag argument is the name of the tag converted to lower case.
HTMLParser.handle_startendtag(tag, attrs)
Similar to handle_starttag(), but called when the parser encounters an XHTML-style empty tag (<img
... />).
This method is called to process arbitrary data (e.g. text nodes and the content of <script>...</script>
and <style>...</style>).
This method is called to process a named character reference of the form &name; (e.g. &gt;), where name is a
general entity reference (e.g. 'gt').
This method is never called if convert_charrefs is True.
20.2.
This method is called to process decimal and hexadecimal numeric character references of the form &#NNN; and
&#xNNN;.
For example, the decimal equivalent for &gt; is &#62;, whereas the hexadecimal is &#x3E;; in this
case the method will receive '62' or 'x3E'.
This method is called when a comment is encountered (e.g. <!--comment-->).
For example, the comment <!-- comment --> will cause this method to be called with the argument '
comment '.
This method is called to handle an HTML doctype declaration (e.g. <!DOCTYPE html>).
The decl parameter will be the entire contents of the declaration inside the <!...
HTMLParser.handle_pi(data)
Method called when a processing instruction is encountered.
The data parameter will contain the entire processing
instruction.
For example, for the processing instruction <?proc color='red'>, this method would be called
as handle_pi("proc color='red'").
It is intended to be overridden by a derived class; the base class
implementation does nothing.
Note: The HTMLParser class uses the SGML syntactic rules for processing instructions.
This method is called when an unrecognized declaration is read by the parser.
The data parameter will be the entire contents of the declaration inside the <!
It is sometimes
useful to be overridden by a derived class.
The base class implementation does nothing.
IE 9]>IE-specific content<![endif]-->')
(continues on next page)
20.2.
Data
: tag soup
End tag
: p
End tag
: a
20.3 html.entities — Definitions of HTML general entities
Source code: Lib/html/entities.py
This module defines four dictionaries, html5, name2codepoint, codepoint2name, and entitydefs.
Note that the trailing semicolon is included in the name (e.g. 'gt;'), however
some of the names are accepted by the standard even without the semicolon: in this case the name is present with
and without the ';'.
See also html.unescape().
New in version 3.3.
Warning: The XML modules are not secure against erroneous or maliciously constructed data.
If you need to parse
untrusted or unauthenticated data see the XML vulnerabilities and The defusedxml Package sections.
It is important to note that modules in the xml package require that there be at least one SAX-compliant XML parser
available.
The Expat parser is included with Python, so the xml.parsers.expat module will always be available.
The documentation for the xml.dom and xml.sax packages are the definition of the Python bindings for the DOM
and SAX interfaces.
The XML handling submodules are:
• xml.etree.
ElementTree: the ElementTree API, a simple and lightweight XML processor
• xml.dom: the DOM API definition
• xml.dom.minidom: a minimal DOM implementation
• xml.dom.pulldom: support for building partial DOM trees
• xml.sax: SAX2 base classes and convenience functions
• xml.parsers.expat: the Expat parser binding
20.4.1 XML vulnerabilities
The XML processing modules are not secure against maliciously constructed data.
An attacker can abuse XML features
to carry out denial of service attacks, access local files, generate network connections to other machines, or circumvent
firewalls.
The following table gives an overview of the known attacks and whether the various modules are vulnerable to them.
Expat 2.4.1 and newer is not vulnerable to the “billion laughs” and “quadratic blowup” vulnerabilities.
Items still
listed as vulnerable due to potential reliance on system-provided libraries.
Check pyexpat.EXPAT_VERSION.
ElementTree doesn’t expand external entities and raises a ParseError when an entity occurs.
Since Python 3.7.1, external general entities are no longer processed by default.
Each entity refers to another entity several times, and the final entity
definition contains a small string.
The exponential expansion results in several gigabytes of text and consumes lots
of memory and CPU time.
Instead of nested entities it repeats one large entity with a couple of thousand chars over and over
again.
The attack isn’t as efficient as the exponential case but it avoids triggering parser countermeasures that forbid
deeply nested entities.
They can also point to
external resources or local files.
The XML parser accesses the resource and embeds the content into the XML
document.
DTD retrieval Some XML libraries like Python’s xml.dom.pulldom retrieve document type definitions from remote
or local locations.
The feature has similar implications as the external entity expansion issue.
For an attacker it can reduce the amount
of transmitted data by three magnitudes or more.
The documentation for defusedxml on PyPI has further information about all known attack vectors with examples and
references.
The defusedxml Package
defusedxml is a pure Python package with modified subclasses of all stdlib XML parsers that prevent any potentially
malicious operation.
Use of this package is recommended for any server code that parses untrusted XML data.
The
package also ships with example exploits and extended documentation on more XML exploits such as XPath injection.
ElementTree — The ElementTree XML API
Source code: Lib/xml/etree/ElementTree.py
The xml.etree.
ElementTree module implements a simple and efficient API for parsing and creating XML data.
This module will use a fast implementation whenever available.
Deprecated since version 3.3: The xml.etree.cElementTree module is deprecated.
Warning:
The xml.etree.
ElementTree module is not secure against maliciously constructed data.
If you
need to parse untrusted or unauthenticated data see XML vulnerabilities.
Tutorial
This is a short tutorial for using xml.etree.
ElementTree (ET in short).
The goal is to demonstrate some of the
building blocks and basic concepts of the module.
XML tree and elements
XML is an inherently hierarchical data format, and the most natural way to represent it is with a tree.
ET has two
classes for this purpose - ElementTree represents the whole XML document as a tree, and Element represents a
single node in this tree.
Interactions with the whole document (reading and writing to/from files) are usually done on the
ElementTree level.
Interactions with a single XML element and its sub-elements are done on the Element level.
We can import this data by reading from a file:
import xml.etree.
ElementTree as ET
tree = ET.parse('country_data.xml')
root = tree.getroot()
Or directly from a string:
root = ET.fromstring(country_data_as_string)
fromstring() parses XML from a string directly into an Element, which is the root element of the parsed tree.
Other parsing functions may create an ElementTree.
Check the documentation to be sure.
As an Element, root has a tag and a dictionary of attributes:
20.5. xml.etree.
Currently, this module skips
over any XML comments, processing instructions, and document type declarations in the input.
Nevertheless, trees built
using this module’s API rather than parsing from XML text can have comments and processing instructions in them;
they will be included when generating XML output.
A document type declaration may be accessed by passing a custom
TreeBuilder instance to the XMLParser constructor.
Pull API for non-blocking parsing
Most parsing functions provided by this module require the whole document to be read at once before returning any
result.
It is possible to use an XMLParser and feed data into it incrementally, but it is a push API that calls methods
on a callback target, which is too low-level and inconvenient for most needs.
Sometimes what the user really wants is to
be able to parse XML incrementally, without blocking operations, while enjoying the convenience of fully constructed
Element objects.
The most powerful tool for doing this is XMLPullParser.
It does not require a blocking read to obtain the XML data,
and is instead fed with data incrementally with XMLPullParser.feed() calls.
To get the parsed XML elements,
call XMLPullParser.read_events().
Here is an example:
[('start', <Element 'mytag' at 0x7fa66db2be58>)]

print(event)

print(elem.tag, 'text=', elem.text)

end
mytag text= sometext more text
The obvious use case is applications that operate in a non-blocking fashion where the XML data is being received from a
socket or read incrementally from some storage device.
In such cases, blocking reads are unacceptable.
Because it’s so flexible, XMLPullParser can be inconvenient to use for simpler use-cases.
It can be useful when you’re reading a large XML document and don’t want to hold it wholly in
memory.
Finding interesting elements
Element has some useful methods that help iterate recursively over all the sub-tree below it (its children, their children,
and so on).
For example, Element.iter():

print(neighbor.attrib)

{'name': 'Austria', 'direction': 'E'}
{'name': 'Switzerland', 'direction': 'W'}
{'name': 'Malaysia', 'direction': 'N'}
{'name': 'Costa Rica', 'direction': 'W'}
{'name': 'Colombia', 'direction': 'E'}
Element.findall() finds only elements with a tag which are direct children of the current element.
Element.
find() finds the first child with a particular tag, and Element.text accesses the element’s text content.
Liechtenstein 1
Singapore 4
Panama 68
More sophisticated specification of which elements to look for is possible by using XPath.
Modifying an XML File
ElementTree provides a simple way to build XML documents and write them to files.
The ElementTree.
write() method serves this purpose.
Once created, an Element object may be manipulated by directly changing its fields (such as Element.text), adding
and modifying attributes (Element.set() method), as well as adding new children (for example with Element.
Our XML now looks like this:
<?xml version="1.0"?>
<data>
<country name="Liechtenstein">
(continues on next page)
20.5.
We can remove elements using Element.remove().
Note that concurrent modification while iterating can lead to problems, just like when iterating and modifying Python
lists or dicts.
Therefore, the example first collects all matching elements with root.findall(), and only then iterates
over the list of matches.
Parsing XML with Namespaces
If the XML input has namespaces, tags and attributes with prefixes in the form prefix:sometag get expanded to
{uri}sometag where the prefix is replaced by the full URI.
Also, if there is a default namespace, that full URI gets
prepended to all of the non-prefixed tags.
One way to search and explore this XML example is to manually add the URI to every tag or attribute in the xpath of a
find() or findall():
root = fromstring(xml_text)
for actor in root.findall('{http://people.example.com}actor'):
name = actor.find('{http://people.example.com}name')
print(name.text)
for char in actor.findall('{http://characters.example.com}character'):
print(' |-->', char.text)
A better way to search the namespaced XML example is to create a dictionary with your own prefixes and use those in
the search functions:
ns = {'real_person': 'http://people.example.com',
'role': 'http://characters.example.com'}
for actor in root.findall('real_person:actor', ns):
name = actor.find('real_person:name', ns)
print(name.text)
for char in actor.findall('role:character', ns):
print(' |-->', char.text)
20.5. xml.etree.
Sir Robin
|--> Gunther
|--> Commander Clement
20.5.2 XPath support
This module provides limited support for XPath expressions for locating elements in a tree.
The goal is to support a small
subset of the abbreviated syntax; a full XPath engine is outside the scope of the module.
Example
Here’s an example that demonstrates some of the XPath capabilities of the module.
We’ll be using the countrydata
XML document from the Parsing XML section:
import xml.etree.
For example, spam selects all child
elements named spam, and spam/egg selects all grandchildren named egg in all
children named spam.
Changed in version 3.8: Support for star-wildcards was added.
For
example, */egg selects all grandchildren named egg.
.
Selects the current node.
This is mostly useful at the beginning of the path, to indicate
that it’s a relative path.
For example,
.//egg selects all egg elements in the entire tree.
..
Selects the parent element.
Returns None if the path attempts to reach the ancestors
of the start element (the element find was called on).
The value
cannot contain quotes.
The
value cannot contain quotes.
New in version 3.10.
Only immediate children are
supported.
New in version 3.7.
New in version 3.10.
New in version 3.10.
The position can be either
an integer (1 is the first position), the expression last() (for the last position), or a
position relative to the last position (e.g. last()-1).
Predicates (expressions within square brackets) must be preceded by a tag name, an asterisk, or another predicate.
ElementTree.canonicalize(xml_data=None, *, out=None, from_file=None, **options)
C14N 2.0 transformation function.
Canonicalization is a way to normalise XML output in a way that allows byte-by-byte comparisons and digital
signatures.
It reduced the freedom that XML serializers have and instead generates a more constrained XML
representation.
The main restrictions regard the placement of namespace declarations, the ordering of attributes,
and ignorable whitespace.
This function takes an XML data string (xml_data) or a file path or file-like object (from_file) as input, converts it
to the canonical form, and writes it out using the out file(-like) object, if provided, or returns it as a text string if
not.
The output file receives text, not bytes.
It should therefore be opened in text mode with utf-8 encoding.
In the option list above, “a set” refers to any collection or iterable of strings, no ordering is expected.
New in version 3.8.
ElementTree.
Comment(text=None)
Comment element factory.
This factory function creates a special element that will be serialized as an XML com-
ment by the standard serializer.
The comment string can be either a bytestring or a Unicode string.
Returns an element instance representing a comment.
Note that XMLParser skips over comments in the input instead of creating comment objects for them.
An
ElementTree will only contain comment nodes if they have been inserted into to the tree using one of the
Element methods.
ElementTree.dump(elem)
Writes an element tree or element structure to sys.stdout.
This function should be used for debugging only.
The exact output format is implementation dependent.
The dump() function now preserves the attribute order specified by the user.
xml.etree.
ElementTree.fromstring(text, parser=None)
Parses an XML section from a string constant.
Same as XML().
If not given, the standard XMLParser parser is used.
Returns an Element instance.
ElementTree.fromstringlist(sequence, parser=None)
Parses an XML document from a sequence of string fragments.
If not given, the standard XMLParser parser is used.
Returns an Element instance.
New in version 3.2.
Appends whitespace to the subtree to indent the tree visually.
This can be used to generate pretty-printed XML
output.
New in version 3.9.
xml.etree.
Check if an object appears to be a valid element object.
Return True if this is an
element object.
ElementTree.iterparse(source, events=None, parser=None)
Parses an XML section into an element tree incrementally, and reports what’s going on to the user.
The supported events
are the strings "start", "end", "comment", "pi", "start-ns" and "end-ns" (the “ns” events are used
to get detailed namespace information).
If events is omitted, only "end" events are reported.
If not given, the standard XMLParser parser is used.
Returns an iterator providing (event, elem) pairs;
it has a root attribute that references the root element of the resulting XML tree once source is fully read.
Note that while iterparse() builds the tree incrementally, it issues blocking reads on source (or the file it
names).
As such, it’s unsuitable for applications where blocking reads can’t be made.
For fully non-blocking
parsing, see XMLPullParser.
Note: iterparse() only guarantees that it has seen the “>” character of a starting tag when it emits a “start”
event, so the attributes are defined, but the contents of the text and tail attributes are undefined at that point.
The
same applies to the element children; they may or may not be present.
If you need a fully populated element, look for “end” events instead.
The parser argument.
The comment and pi events were added.
ElementTree.parse(source, parser=None)
Parses an XML section into an element tree.
If not given, the standard XMLParser parser is used.
Returns an ElementTree
instance.
ElementTree.
ProcessingInstruction(target, text=None)
PI element factory.
This factory function creates a special element that will be serialized as an XML processing
20.5. xml.etree.
Returns
an element instance, representing a processing instruction.
Note that XMLParser skips over processing instructions in the input instead of creating comment objects for
them.
An ElementTree will only contain processing instruction nodes if they have been inserted into to the tree
using one of the Element methods.
xml.etree.ElementTree.register_namespace(prefix, uri)
Registers a namespace prefix.
The registry is global, and any existing mapping for either the given prefix or the
namespace URI will be removed.
Tags and attributes in this
namespace will be serialized with the given prefix, if at all possible.
New in version 3.2.
ElementTree.
SubElement(parent, tag, attrib={}, **extra)
Subelement factory.
This function creates an element instance, and appends it to an existing element.
The element name, attribute names, and attribute values can be either bytestrings or Unicode strings.
Returns an element instance.
ElementTree.tostring(element, encoding=’us-ascii’, method=’xml’, *, xml_declaration=None,
default_namespace=None, short_empty_elements=True)
Generates a string representation of an XML element, including all subelements.
Use encoding="unicode" to generate a Unicode
string (otherwise, a bytestring is generated).
Returns an (optionally) encoded string containing the XML data.
The short_empty_elements parameter.
The xml_declaration and default_namespace parameters.
The tostring() function now preserves the attribute order specified by the user.
xml.etree.
ElementTree.tostringlist(element, encoding=’us-ascii’, method=’xml’, *,
xml_declaration=None, default_namespace=None,
short_empty_elements=True)
Generates a string representation of an XML element, including all subelements.
Use encoding="unicode" to generate a Unicode
string (otherwise, a bytestring is generated).
Returns a list of (optionally) encoded strings containing the XML data.
New in version 3.2.
The short_empty_elements parameter.
The xml_declaration and default_namespace parameters.
The tostringlist() function now preserves the attribute order specified by the user.
xml.etree.
ElementTree.
XML(text, parser=None)
Parses an XML section from a string constant.
This function can be used to embed “XML literals” in Python code.
If not given, the standard XMLParser
parser is used.
Returns an Element instance.
The encoding string included in XML output should conform to the appropriate standards.
For example, “UTF-8” is valid, but “UTF8” is not.
See https://www.w3.org/TR/2006/REC-xml11-20060816/#NT-EncodingDecl and https://www.iana.org/assignments/character-sets/character-sets.
xhtml.
ElementTree.
XMLID(text, parser=None)
Parses an XML section from a string constant, and also returns a dictionary which maps from element id:s to
elements.
If not given, the standard
XMLParser parser is used.
Returns a tuple containing an Element instance and a dictionary.
ElementInclude helper module.
This module can be used to insert subtrees and text strings into element trees, based on information in the tree.
Example
Here’s an example that demonstrates use of the XInclude module.
By default, the href attribute is treated as a file name.
You can use custom loaders to override this behaviour.
Also note
that the standard helper does not support XPointer syntax.
To process this file, load it as usual, and pass the root element to the xml.etree.
If the parse attribute is omitted, it defaults to “xml”.
The href attribute is required.
To include a text document, use the {http://www.w3.org/2001/XInclude}include element, and set the
parse attribute to “text”:
<?xml version="1.0"?>
<document xmlns:xi="http://www.w3.org/2001/XInclude">
Copyright (c) <xi:include href="year.txt" parse="text" />.
The result might look something like:
<document xmlns:xi="http://www.w3.org/2001/XInclude">
Copyright (c) 2003.
This default loader reads an included resource from disk.
If not given, encoding is utf-8.
Returns the
expanded resource.
If the parse mode is "xml", this is an ElementTree instance.
If the parse mode is “text”, this
is a Unicode string.
If the loader fails, it can return None or raise an exception.
This function expands XInclude directives.
If
omitted, it defaults to default_loader().
If given, it should be a callable that implements the same interface
as default_loader().
Limited to reduce the risk of malicious content ex-
plosion.
Pass a negative value to disable the limitation.
Returns the expanded resource.
If the parse mode is "xml", this is an ElementTree instance.
If the parse mode
is “text”, this is a Unicode string.
If the loader fails, it can return None or raise an exception.
The base_url and max_depth parameters.
Element Objects
class xml.etree.
ElementTree.
Element(tag, attrib={}, **extra)
Element class.
This class defines the Element interface, and provides a reference implementation of this interface.
The element name, attribute names, and attribute values can be either bytestrings or Unicode strings.
Their values are usually
strings but may be any application-specific object.
If the element is created from an XML file, the text attribute
holds either the text between the element’s start tag and its first child or end tag, or None, and the tail attribute
holds either the text between the element’s end tag and the next tag, or None.
For the XML data
<a><b>1<c>2<d/>3</c></b>4</a>
the a element has None for both text and tail attributes, the b element has text "1" and tail "4", the c element
has text "2" and tail None, and the d element has text None and tail "3".
To collect the inner text of an element, see itertext(), for example "".join(element.
Applications may store arbitrary objects in these attributes.
Note that while the attrib value is always a real mutable
Python dictionary, an ElementTree implementation may choose to use another internal representation, and
create the dictionary only if someone asks for it.
The following dictionary-like methods work on the element attributes.
This function removes all subelements, clears all attributes, and sets the text and tail
attributes to None.
get(key, default=None)
Gets the element attribute named key.
Returns the attribute value, or default if the attribute was not found.
The attributes are returned in an arbitrary
order.
The names are returned in an arbitrary order.
The following methods work on the element’s children (subelements).
Raises TypeError if
subelement is not an Element.
extend(subelements)
Appends subelements from a sequence object with zero or more elements.
Raises TypeError if a subele-
ment is not an Element.
New in version 3.2.
Returns an element instance
or None.
Pass '' as prefix to move
all unprefixed tag names in the expression into the given namespace.
Returns a list containing all matching elements in
document order.
Pass '' as prefix
to move all unprefixed tag names in the expression into the given namespace.
Returns the text
content of the first matching element, or default if no element was found.
Note that if the matching element
has no text content an empty string is returned.
Pass '' as prefix to move all unprefixed tag names in the expression into the given namespace.
Inserts subelement at the given position in this element.
The iterator iterates over this element and all
elements below it, in document (depth first) order.
If tag is not None or '*', only elements whose tag equals
tag are returned from the iterator.
If the tree structure is modified during iteration, the result is undefined.
New in version 3.2.
Returns an iterable yielding all matching elements in
document order.
New in version 3.2.
The iterator loops over this element and all subelements, in document order, and
returns all inner text.
New in version 3.2.
Removes subelement from the element.
Unlike the find* methods this method compares elements based on
the instance identity, not on tag value or contents.
Element objects also support the following sequence type methods for working with subelements:
__delitem__(), __getitem__(), __setitem__(), __len__().
Caution: Elements with no subelements will test as False.
Testing the truth value of an Element is deprecated
and will raise an exception in Python 3.14.
Use specific len(elem) or elem is None test instead.
Changed in version 3.12: Testing the truth value of an Element emits DeprecationWarning.
Prior to Python 3.8, the serialisation order of the XML attributes of elements was artificially made predictable by
sorting the attributes by their name.
Based on the now guaranteed ordering of dicts, this arbitrary reordering was
removed in Python 3.8 to preserve the order in which attributes were originally parsed or created by user code.
In general, user code should try not to depend on a specific ordering of attributes, given that the XML Information
Set explicitly excludes the attribute order from conveying information.
Code should be prepared to deal with any
ordering on input.
In cases where deterministic XML output is required, e.g. for cryptographic signing or test data
sets, canonical serialisation is available with the canonicalize() function.
In cases where canonical output is not applicable but a specific attribute order is still desirable on output, code
should aim for creating the attributes directly in the desired order, to avoid perceptual mismatches for readers of
the code.
ElementTree.
ElementTree(element=None, file=None)
ElementTree wrapper class.
This class represents an entire element hierarchy, and adds some extra support for
serialization to and from standard XML.
The tree is initialized with the contents of the XML file if given.
This discards the current contents of the tree, and replaces it with the
given element.
Use with care.
The iterator loops over all elements in this tree, in
section order.
New in version 3.2.
If not given, the standard XMLParser parser is used.
Returns the section root element.
Use False for never, True for always, None for only if not US-ASCII or
UTF-8 or Unicode (default is None).
The keyword-only short_empty_elements
parameter controls the formatting of elements that contain no content.
If True (the default), they are emitted
as a single self-closed tag, otherwise they are emitted as a pair of start/end tags.
The output is either a string (str) or binary (bytes).
This is controlled by the encoding argument.
If
encoding is "unicode", the output is a string; otherwise, it’s binary.
Note that this may conflict with the
type of file if it’s an open file object; make sure you do not try to write a string to a binary stream and vice
versa.
The short_empty_elements parameter.
The write() method now preserves the attribute order specified by the user.
This is the XML file that is going to be manipulated:
20.5. xml.etree.
QName Objects
class xml.etree.
ElementTree.QName(text_or_uri, tag=None)
QName wrapper.
This can be used to wrap a QName attribute value, in order to get proper namespace handling
on output.
If tag is given, the first argument is interpreted as a URI, and this argument is
interpreted as a local name.
QName instances are opaque.
TreeBuilder Objects
class xml.etree.
ElementTree.
TreeBuilder(element_factory=None, *, comment_factory=None,
pi_factory=None, insert_comments=False,
insert_pis=False)
Generic element structure builder.
This builder converts a sequence of start, data, end, comment and pi method
calls to a well-formed element structure.
You can use this class to build an element structure using a custom XML
parser, or a parser for some other XML-like format.
It is expected to return a new element instance.
The comment_factory and pi_factory functions, when given, should behave like the Comment() and
ProcessingInstruction() functions to create comments and processing instructions.
When not given,
the default factories will be used.
When insert_comments and/or insert_pis is true, comments/pis will be inserted
into the tree if they appear within the root element (but not outside of it).
Flushes the builder buffers, and returns the toplevel document element.
Closes the current element.
Returns the closed element.
Returns
the opened element.
If insert_comments is true, this will also add it to the tree.
New in version 3.8.
If insert_pis is true, this will also add it to the
tree.
New in version 3.8.
In addition, a custom TreeBuilder object can provide the following methods:
doctype(name, pubid, system)
Handles a doctype declaration.
This method does not exist on the default TreeBuilder class.
New in version 3.2.
start_ns(prefix, uri)
Is called whenever the parser encounters a new namespace declaration, before the start() callback for the
opening element that defines it.
New in version 3.8.
end_ns(prefix)
Is called after the end() callback of an element that declared a namespace prefix mapping, with the name
of the prefix that went out of scope.
New in version 3.8.
ElementTree.
C14NWriterTarget(write, *, with_comments=False, strip_text=False,
rewrite_prefixes=False, qname_aware_tags=None,
qname_aware_attrs=None, exclude_attrs=None,
exclude_tags=None)
A C14N 2.0 writer.
Arguments are the same as for the canonicalize() function.
This class does not build a
tree but translates the callback events directly into a serialised form using the write function.
New in version 3.8.
ElementTree.
This class is the low-level building block of the module.
It uses xml.parsers.expat for efficient, event-based
parsing of XML.
It can be fed XML data incrementally with the feed() method, and parsing events are translated
to a push API - by invoking callbacks on the target object.
If target is omitted, the standard TreeBuilder is
used.
If encodingPage 1326, 1 is given, the value overrides the encoding specified in the XML file.
Changed in version 3.8: Parameters are now keyword-only.
The html argument no longer supported.
Finishes feeding data to the parser.
Feeds data to the parser.
XMLParser.feed() calls target’s start(tag, attrs_dict) method for each opening tag, its
end(tag) method for each closing tag, and data is processed by method data(data).
For further supported
callback methods, see the TreeBuilder class.
XMLParser.close() calls target’s method close().
XMLParser can be used not only for building a tree structure.
ElementTree.
A pull parser suitable for non-blocking applications.
Its input-side API is similar to that of XMLParser, but
instead of pushing calls to a callback target, XMLPullParser collects an internal list of parsing events and lets
the user read from it.
The supported events are the strings "start",
"end", "comment", "pi", "start-ns" and "end-ns" (the “ns” events are used to get detailed namespace
information).
If events is omitted, only "end" events are reported.
Feed the given bytes data to the parser.
Signal the parser that the data stream is terminated.
Unlike XMLParser.close(), this method always re-
turns None.
Any events not yet retrieved when the parser is closed can still be read with read_events().
The iterator
yields (event, elem) pairs, where event is a string representing the type of event (e.g. "end") and elem
is the encountered Element object, or other context value as follows.
Events provided in a previous call to read_events() will not be yielded again.
Events are consumed
from the internal queue only when they are retrieved from the iterator, so multiple readers iterating in parallel
over iterators obtained from read_events() will have unpredictable results.
Note: XMLPullParser only guarantees that it has seen the “>” character of a starting tag when it emits a “start”
event, so the attributes are defined, but the contents of the text and tail attributes are undefined at that point.
The
same applies to the element children; they may or may not be present.
If you need a fully populated element, look for “end” events instead.
New in version 3.4.
The comment and pi events were added.
Exceptions
class xml.etree.
ElementTree.
ParseError
XML parse error, raised by the various parsing methods in this module when parsing fails.
The string representation
of an instance of this exception will contain a user-friendly error message.
In addition, it will have the following
attributes available:
code
A numeric error code from the expat parser.
See the documentation of xml.parsers.expat for the list
of error codes and their meanings.
A DOM implementation presents an XML document as a tree structure, or
allows client code to build such a structure from scratch.
It then gives access to the structure through a set of objects
which provided well-known interfaces.
The DOM is extremely useful for random-access applications.
SAX only allows you a view of one bit of the document at
a time.
If you are looking at one SAX element, you have no access to another.
If you are looking at a text node, you have
no access to a containing element.
When you write a SAX application, you need to keep track of your program’s position
in the document somewhere in your own code.
SAX does not do it for you.
Also, if you need to look ahead in the XML
document, you are just out of luck.
Some applications are simply impossible in an event driven model with no access to a tree.
Of course you could build
some sort of tree yourself in SAX events, but the DOM allows you to avoid writing that code.
The DOM is a standard
tree representation for XML data.
The Document Object Model is being defined by the W3C in stages, or “levels” in their terminology.
The Python mapping
of the API is substantially based on the DOM Level 2 recommendation.
DOM applications typically start by parsing some XML into a DOM.
How this is accomplished is not covered at all
by DOM Level 1, and Level 2 provides only limited improvements: There is a DOMImplementation object class
which provides access to Document creation methods, but no way to access an XML reader/parser/Document builder
in an implementation-independent way.
There is also no well-defined way to access these methods without an existing
Document object.
In Python, each DOM implementation will provide a function getDOMImplementation().
DOM Level 3 adds a Load/Store specification, which defines an interface to the reader, but this is not yet available in the
Python standard library.
Once you have a DOM document object, you can access the parts of your XML document through its properties and
methods.
These properties are defined in the DOM specification; this portion of the reference manual describes the
interpretation of the specification in Python.
The specification provided by the W3C defines the DOM API for Java, ECMAScript, and OMG IDL.
The Python mapping
defined here is based in large part on the IDL version of the specification, but strict compliance is not required (though
implementations are free to support the strict mapping from IDL).
See section Conformance for a detailed discussion of
mapping requirements.
The W3C recommendation upon which the Python DOM API
is based.
Document Object Model (DOM) Level 1 Specification The W3C recommendation for the DOM supported by xml.
Python Language Mapping Specification This specifies the mapping from OMG IDL to Python.
The factory function should return an object which implements
the DOMImplementation interface.
The factory function can return the same object every time, or a new one for
each call, as appropriate for the specific implementation (e.g. if that implementation supports some customization).
The name is either well-known, the module name of a DOM implementa-
tion, or None.
If it is not None, imports the corresponding module and returns a DOMImplementation object
if the import succeeds.
If no name is given, and if the environment variable PYTHON_DOM is set, this variable is
used to find the implementation.
If name is not given, this examines the available implementations to find one with the required feature set.
If no
implementation can be found, raise an ImportError.
The features list must be a sequence of (feature,
version) pairs which are passed to the hasFeature() method on available DOMImplementation ob-
jects.
The value used to indicate that no namespace is associated with a node in the DOM.
This is typically found as the
namespaceURI of a node, or used as the namespaceURI parameter to a namespaces-specific method.
The namespace URI for namespace declarations, as defined by Document Object Model (DOM) Level 2 Core
Specification (section 1.1.8).
xml.dom.XHTML_NAMESPACE
The URI of the XHTML namespace as defined by XHTML 1.0: The Extensible HyperText Markup Language
(section 3.1.1).
In addition, xml.dom contains a base Node class and the DOM exception classes.
The Node class provided by this
module does not implement any of the methods or attributes defined by the DOM specification; concrete DOM imple-
mentations must provide those.
The Node class provided as part of this module does provide the constants used for
the nodeType attribute on concrete Node objects; they are located within the class rather than at the module level to
conform with the DOM specifications.
It is fairly rare that you must do
this, however, so this usage is not yet documented.
Node
Node Objects
Base interface for most objects in a document.
NodeList
NodeList Objects
Interface for a sequence of nodes.
DocumentType
DocumentType Objects
Information about the declarations needed to process a
document.
Document
Document Objects
Object which represents an entire document.
Element
Element Objects
Element nodes in the document hierarchy.
Attr
Attr Objects
Attribute value nodes on element nodes.
Comment
Comment Objects
Representation of comments in the source document.
Text
Text
and
CDATASection
Objects
Nodes containing textual content from the document.
ProcessingInstructionProcessingInstruction
Objects
Processing instruction representation.
An additional section describes the exceptions defined for working with the DOM in Python.
DOMImplementation Objects
The DOMImplementation interface provides a way for applications to determine the availability of particular features
in the DOM they are using.
DOM Level 2 added the ability to create new Document and DocumentType objects
using the DOMImplementation as well.
DOMImplementation.hasFeature(feature, version)
Return True if the feature identified by the pair of strings feature and version is implemented.
DOMImplementation.createDocument(namespaceUri, qualifiedName, doctype)
Return a new Document object (the root of the DOM), with a child Element object having the
given namespaceUri and qualifiedName.
The doctype must be a DocumentType object created by
createDocumentType(), or None.
In the Python DOM API, the first two arguments can also be None
in order to indicate that no Element child is to be created.
DOMImplementation.createDocumentType(qualifiedName, publicId, systemId)
Return a new DocumentType object that encapsulates the given qualifiedName, publicId, and systemId strings,
representing the information contained in an XML document type declaration.
Node Objects
All of the components of an XML document are subclasses of Node.
Node.nodeType
An integer representing the node type.
Symbolic constants for the types are on the Node ob-
ject:
ELEMENT_NODE, ATTRIBUTE_NODE, TEXT_NODE, CDATA_SECTION_NODE, ENTITY_NODE,
PROCESSING_INSTRUCTION_NODE, COMMENT_NODE, DOCUMENT_NODE, DOCUMENT_TYPE_NODE,
NOTATION_NODE.
This is a read-only attribute.
Node.parentNode
The parent of the current node, or None for the document node.
The value is always a Node object or None.
For Element nodes, this will be the parent element, except for the root element, in which case it will be the
Document object.
For Attr nodes, this is always None.
A NamedNodeMap of attribute objects.
Only elements have actual values for this; others provide None for this
attribute.
This is a read-only attribute.
Node.previousSibling
The node that immediately precedes this one with the same parent.
For instance the element with an end-tag that
comes just before the self element’s start-tag.
Of course, XML documents are made up of more than just elements
so the previous sibling could be text, a comment, or something else.
If this node is the first child of the parent, this
attribute will be None.
This is a read-only attribute.
Node.nextSibling
The node that immediately follows this one with the same parent.
See also previousSibling.
If this is the
last child of the parent, this attribute will be None.
This is a read-only attribute.
Node.childNodes
A list of nodes contained within this node.
This is a read-only attribute.
Node.firstChild
The first child of the node, if there are any, or None.
This is a read-only attribute.
Node.lastChild
The last child of the node, if there are any, or None.
This is a read-only attribute.
Node.localName
The part of the tagName following the colon if there is one, else the entire tagName.
The value is a string.
Node.prefix
The part of the tagName preceding the colon if there is one, else the empty string.
The value is a string, or None.
Node.namespaceURI
The namespace associated with the element name.
This will be a string or None.
This is a read-only attribute.
Node.nodeName
This has a different meaning for each node type; see the DOM specification for details.
You can always get the
information you would get here from another property such as the tagName property for elements or the name
property for attributes.
For all node types, the value of this attribute will be either a string or None.
This is a
read-only attribute.
This has a different meaning for each node type; see the DOM specification for details.
The situation is similar to
that with nodeName.
The value is a string or None.
Node.hasAttributes()
Return True if the node has any attributes.
Node.hasChildNodes()
Return True if the node has any child nodes.
Node.isSameNode(other)
Return True if other refers to the same node as this node.
This is especially useful for DOM implementations
which use any sort of proxy architecture (because more than one object can refer to the same node).
Note: This is based on a proposed DOM Level 3 API which is still in the “working draft” stage, but this particular
interface appears uncontroversial.
Changes from the W3C will not necessarily affect this method in the Python
DOM interface (though any new W3C API for this would also be supported).
If the node was already in
the tree, it is removed first.
Node.insertBefore(newChild, refChild)
Insert a new child node before an existing child.
It must be the case that refChild is a child of this node; if not,
ValueError is raised.
If refChild is None, it inserts newChild at the end of the children’s
list.
Remove a child node.
If oldChild will not be used further, its unlink() method should be called.
Node.replaceChild(newChild, oldChild)
Replace an existing node with a new node.
It must be the case that oldChild is a child of this node; if not,
ValueError is raised.
Node.normalize()
Join adjacent text nodes so that all stretches of text are stored as single Text instances.
This simplifies processing
text from a DOM tree for many applications.
Node.cloneNode(deep)
Clone this node.
Setting deep means to clone all child nodes as well.
This returns the clone.
NodeList Objects
A NodeList represents a sequence of nodes.
These objects are used in two ways in the DOM Core recommen-
dation: an Element object provides one as its list of child nodes, and the getElementsByTagName() and
getElementsByTagNameNS() methods of Node return objects with this interface to represent query results.
Return the i’th item from the sequence, if there is one, or None.
The index i is not allowed to be less than zero or
greater than or equal to the length of the sequence.
The number of nodes in the sequence.
In addition, the Python DOM interface requires that some additional support is provided to allow NodeList ob-
jects to be used as Python sequences.
All NodeList implementations must include support for __len__() and
__getitem__(); this allows iteration over the NodeList in for statements and proper support for the len()
built-in function.
The DocumentType for a document is
available from the Document object’s doctype attribute; if there is no DOCTYPE declaration for the document, the
document’s doctype attribute will be set to None instead of an instance of this interface.
DocumentType is a specialization of Node, and adds the following attributes:
DocumentType.publicId
The public identifier for the external subset of the document type definition.
This will be a string or None.
DocumentType.systemId
The system identifier for the external subset of the document type definition.
This will be a URI as a string, or
None.
DocumentType.internalSubset
A string giving the complete internal subset from the document.
This does not include the brackets which enclose
the subset.
If the document has no internal subset, this should be None.
The name of the root element as given in the DOCTYPE declaration, if present.
DocumentType.entities
This is a NamedNodeMap giving the definitions of external entities.
For entity names defined more than once,
only the first definition is provided (others are ignored as required by the XML recommendation).
This may be
None if the information is not provided by the parser, or if no entities are defined.
This is a NamedNodeMap giving the definitions of notations.
For notation names defined more than once, only
the first definition is provided (others are ignored as required by the XML recommendation).
This may be None
if the information is not provided by the parser, or if no notations are defined.
Document Objects
A Document represents an entire XML document, including its constituent elements, attributes, processing instructions,
comments etc.
Remember that it inherits properties from Node.
Document.documentElement
The one and only root element of the document.
Document.createElement(tagName)
Create and return a new element node.
The element is not inserted into the document when it is created.
You need
to explicitly insert it with one of the other methods such as insertBefore() or appendChild().
Document.createElementNS(namespaceURI, tagName)
Create and return a new element with a namespace.
The tagName may have a prefix.
The element is not inserted
into the document when it is created.
You need to explicitly insert it with one of the other methods such as
insertBefore() or appendChild().
Document.createTextNode(data)
Create and return a text node containing the data passed as a parameter.
As with the other creation methods, this
one does not insert the node into the tree.
Document.createComment(data)
Create and return a comment node containing the data passed as a parameter.
As with the other creation methods,
this one does not insert the node into the tree.
20.6.
As with the
other creation methods, this one does not insert the node into the tree.
Document.createAttribute(name)
Create and return an attribute node.
This method does not associate the attribute node with any particular element.
You must use setAttributeNode() on the appropriate Element object to use the newly created attribute
instance.
Document.createAttributeNS(namespaceURI, qualifiedName)
Create and return an attribute node with a namespace.
The tagName may have a prefix.
This method does not asso-
ciate the attribute node with any particular element.
You must use setAttributeNode() on the appropriate
Element object to use the newly created attribute instance.
Document.getElementsByTagName(tagName)
Search for all descendants (direct children, children’s children, etc.) with a particular element type name.
Document.getElementsByTagNameNS(namespaceURI, localName)
Search for all descendants (direct children, children’s children, etc.) with a particular namespace URI and local-
name.
The localname is the part of the namespace after the prefix.
Element Objects
Element is a subclass of Node, so inherits all the attributes of that class.
Element.tagName
The element type name.
In a namespace-using document it may have colons in it.
The value is a string.
Element.getElementsByTagName(tagName)
Same as equivalent method in the Document class.
Element.getElementsByTagNameNS(namespaceURI, localName)
Same as equivalent method in the Document class.
Element.hasAttribute(name)
Return True if the element has an attribute named by name.
Element.hasAttributeNS(namespaceURI, localName)
Return True if the element has an attribute named by namespaceURI and localName.
Element.getAttribute(name)
Return the value of the attribute named by name as a string.
If no such attribute exists, an empty string is returned,
as if the attribute had no value.
Element.getAttributeNode(attrname)
Return the Attr node for the attribute named by attrname.
Element.getAttributeNS(namespaceURI, localName)
Return the value of the attribute named by namespaceURI and localName as a string.
If no such attribute exists,
an empty string is returned, as if the attribute had no value.
Remove an attribute by name.
If oldAttr is not present, NotFoundErr is raised.
Element.removeAttributeNS(namespaceURI, localName)
Remove an attribute by name.
Note that it uses a localName, not a qname.
No exception is raised if there is no
matching attribute.
Element.setAttribute(name, value)
Set an attribute value from a string.
Element.setAttributeNode(newAttr)
Add a new attribute node to the element, replacing an existing attribute if necessary if the name attribute
matches.
If a replacement occurs, the old attribute node will be returned.
If newAttr is already in use,
InuseAttributeErr will be raised.
Add a new attribute node to the element, replacing an existing attribute if necessary if the namespaceURI and
localName attributes match.
If a replacement occurs, the old attribute node will be returned.
If newAttr is
already in use, InuseAttributeErr will be raised.
Element.setAttributeNS(namespaceURI, qname, value)
Set an attribute value from a string, given a namespaceURI and a qname.
Note that a qname is the whole attribute
name.
This is different than above.
Attr Objects
Attr inherits from Node, so inherits all its attributes.
Attr.name
The attribute name.
In a namespace-using document it may include a colon.
Attr.localName
The part of the name following the colon if there is one, else the entire name.
This is a read-only attribute.
Attr.prefix
The part of the name preceding the colon if there is one, else the empty string.
The text value of the attribute.
This is a synonym for the nodeValue attribute.
NamedNodeMap Objects
NamedNodeMap does not inherit from Node.
NamedNodeMap.length
The length of the attribute list.
Return an attribute with a particular index.
The order you get the attributes in is arbitrary but will be consistent for
the life of a DOM.
Each item is an attribute node.
Get its value with the value attribute.
There are also experimental methods that give this class more mapping behavior.
You can use them or you can use the
standardized getAttribute*() family of methods on the Element objects.
20.6.
It is a subclass of Node, but cannot have child nodes.
Comment.data
The content of the comment as a string.
The attribute contains all characters between the leading <!-- and trailing
-->, but does not include them.
Text and CDATASection Objects
The Text interface represents text in the XML document.
If the parser and DOM implementation support the DOM’s
XML extension, portions of the text enclosed in CDATA marked sections are stored in CDATASection objects.
These
two interfaces are identical, but provide different values for the nodeType attribute.
These interfaces extend the Node interface.
They cannot have child nodes.
The content of the text node as a string.
Note:
The use of a CDATASection node does not indicate that the node represents a complete CDATA marked
section, only that the content of the node was part of a CDATA section.
A single CDATA section may be represented by
more than one node in the document tree.
There is no way to determine whether two adjacent CDATASection nodes
represent different CDATA marked sections.
ProcessingInstruction Objects
Represents a processing instruction in the XML document; this inherits from the Node interface and cannot have child
nodes.
ProcessingInstruction.target
The content of the processing instruction up to the first whitespace character.
This is a read-only attribute.
ProcessingInstruction.data
The content of the processing instruction following the first whitespace character.
Exceptions
The DOM Level 2 recommendation defines a single exception, DOMException, and a number of constants that allow
applications to determine what sort of error occurred.
DOMException instances carry a code attribute that provides
the appropriate value for the specific exception.
The Python DOM interface provides the constants, but also expands the set of exceptions so that a specific exception exists
for each of the exception codes defined by the DOM.
The implementations must raise the appropriate specific exception,
each of which carries the appropriate value for the code attribute.
DOMException
Base exception class used for all specific DOM exceptions.
This exception class cannot be directly instantiated.
DomstringSizeErr
Raised when a specified range of text does not fit into a string.
IndexSizeErr
Raised when an index or size parameter to a method is negative or exceeds the allowed values.
InuseAttributeErr
Raised when an attempt is made to insert an Attr node that is already present elsewhere in the document.
Raised if a parameter or an operation is not supported on the underlying object.
This exception is raised when a string parameter contains a character that is not permitted in the context it’s being
used in by the XML 1.0 recommendation.
For example, attempting to create an Element node with a space in
the element type name will cause this error to be raised.
InvalidModificationErr
Raised when an attempt is made to modify the type of a node.
NamespaceErr
If an attempt is made to change any object in a way that is not permitted with regard to the Namespaces in XML
recommendation, this exception is raised.
NotFoundErr
Exception when a node does not exist in the referenced context.
For example, NamedNodeMap.
removeNamedItem() will raise this if the node passed in does not exist in the map.
NotSupportedErr
Raised when the implementation does not support the requested type of object or operation.
This is raised if data is specified for a node which does not support data.
NoModificationAllowedErr
Raised on attempts to modify an object where modifications are not allowed (such as for read-only nodes).
SyntaxErr
Raised when an invalid or illegal string is specified.
Type Mapping
The IDL types used in the DOM specification are mapped to Python types according to the following table.
IDL Type
Python Type
boolean
bool or int
int
int
long int
int
unsigned int
int
DOMString
str or bytes
null
None
Accessor Methods
The mapping from OMG IDL to Python defines accessor functions for IDL attribute declarations in much the way
the Java mapping does.
Mapping the IDL declarations
readonly attribute string someValue;
attribute string anotherValue;
yields three accessor functions: a “get” method for someValue (_get_someValue()), and “get” and “set” methods
for anotherValue (_get_anotherValue() and _set_anotherValue()).
The mapping, in particular, does
not require that the IDL attributes are accessible as normal Python attributes: object.someValue is not required to
work, and may raise an AttributeError.
The Python DOM API, however, does require that normal attribute access work.
This means that the typical surrogates
generated by Python IDL compilers are not likely to work, and wrapper objects may be needed on the client if the DOM
objects are accessed via CORBA.
Attributes that are
declared readonly may not restrict write access in all DOM implementations.
In the Python DOM API, accessor functions are not required.
If provided, they should take the form defined by the
Python IDL mapping, but these methods are considered unnecessary since the attributes are accessible directly from
Python.
The IDL definitions do not fully embody the requirements of the W3C DOM API, such as the notion of certain objects,
such as the return value of getElementsByTagName(), being “live”.
Lib/xml/dom/minidom.py
xml.dom.minidom is a minimal implementation of the Document Object Model interface, with an API similar to
that in other languages.
It is intended to be simpler than the full DOM and also significantly smaller.
Users who are
not already proficient with the DOM should consider using the xml.etree.
ElementTree module for their XML
processing instead.
Warning:
The xml.dom.minidom module is not secure against maliciously constructed data.
If you need to
parse untrusted or unauthenticated data see XML vulnerabilities.
DOM applications typically start by parsing some XML into a DOM.
The parse() function can take either a filename or an open file object.
This function will change the document handler of the parser and activate
namespace support; other parser configuration (like setting an entity resolver) must have been done in advance.
If you have XML in a string, you can use the parseString() function instead:
xml.dom.minidom.parseString(string, parser=None)
Return a Document that represents the string.
This method creates an io.
StringIO object for the string and
passes that on to parse().
Both functions return a Document object representing the content of the document.
What the parse() and parseString() functions do is connect an XML parser with a “DOM builder” that can
accept parse events from any SAX parser and convert them into a DOM tree.
The name of the functions are perhaps
misleading, but are easy to grasp when learning the interfaces.
The parsing of the document will be completed before
these functions return; it’s simply that these functions do not provide a parser implementation themselves.
You can also create a Document by calling a method on a “DOM Implementation” object.
You can get this object
either by calling the getDOMImplementation() function in the xml.dom package or the xml.dom.minidom
module.
Once you have a Document, you can add child nodes to it to populate the DOM:
from xml.dom.minidom import getDOMImplementation
impl = getDOMImplementation()
newdoc = impl.createDocument(None, "some_tag", None)
top_element = newdoc.documentElement
text = newdoc.createTextNode('Some textual content.')
top_element.appendChild(text)
Once you have a DOM document object, you can access the parts of your XML document through its properties and
methods.
These properties are defined in the DOM specification.
The main property of the document object is the
documentElement property.
It gives you the main element in the XML document: the one that holds all others.
Here
is an example program:
dom3 = parseString("<myxml>Some data</myxml>")
assert dom3.documentElement.tagName == "myxml"
When you are finished with a DOM tree, you may optionally call the unlink() method to encourage early cleanup of
the now-unneeded objects.
Otherwise, Python’s garbage collector will eventually take care of the objects
in the tree.
The W3C recommendation for the DOM supported by xml.
dom.minidom.
The definition of the DOM API for Python is given as part of the xml.dom module documentation.
This section lists
the differences between the API and xml.dom.minidom.
Node.unlink()
Break internal references within the DOM so that it will be garbage collected on versions of Python without cyclic
GC.
Even when cyclic GC is available, using this can make large amounts of memory available sooner, so calling
this on DOM objects as soon as they are no longer needed is good practice.
This only needs to be called on the
Document object, but may be called on child nodes to discard children of that node.
You can avoid calling this method explicitly by using the with statement.
The writer receives texts but not bytes as input, it should have a write() method
which matches that of the file object interface.
The indent parameter is the indentation of the current node.
The
addindent parameter is the incremental indentation to use for subnodes of the current one.
The newl parameter
specifies the string to use to terminate newlines.
For the Document node, an additional keyword argument encoding can be used to specify the encoding field of
the XML header.
If the value is set to True, standalone="yes" is added, otherwise it is
set to "no".
Not stating the argument will omit the declaration from the document.
The writexml() method now preserves the attribute order specified by the user.
The standalone parameter was added.
Node.toxml(encoding=None, standalone=None)
Return a string or byte string containing the XML represented by the DOM node.
With an explicit encoding1 argument, the result is a byte string in the specified encoding.
With no encoding argu-
ment, the result is a Unicode string, and the XML declaration in the resulting string does not specify an encoding.
Encoding this string in an encoding other than UTF-8 is likely incorrect, since UTF-8 is the default encoding of
XML.
The standalone argument behaves exactly as in writexml().
The toxml() method now preserves the attribute order specified by the user.
The standalone parameter was added.
Node.toprettyxml(indent=’\t’, newl=’\n’, encoding=None, standalone=None)
Return a pretty-printed version of the document.
The encoding argument behaves like the corresponding argument of toxml().
The standalone argument behaves exactly as in writexml().
The toprettyxml() method now preserves the attribute order specified by the user.
The standalone parameter was added.
This example program is a fairly realistic example of a simple program.
In this particular case, we do not take much
advantage of the flexibility of the DOM.
The encoding name included in the XML output should conform to the appropriate standards.
For example, “UTF-8” is valid, but
“UTF8” is not valid in an XML document’s declaration, even though Python accepts it as an encoding name.
See https://www.w3.org/TR/2006/
REC-xml11-20060816/#NT-EncodingDecl and https://www.iana.org/assignments/character-sets/character-sets.xhtml.
20.7.
Usage of the DOM interface in Python is straight-forward.
Interfaces are accessed through instance objects.
Applications should not instantiate the classes themselves; they
should use the creator functions available on the Document object.
Derived interfaces support all operations (and
attributes) from the base interfaces, plus any new operations.
Since the DOM uses only in parameters, the arguments are passed in normal
order (from left to right).
There are no optional arguments.
IDL attributes map to instance attributes.
For compatibility with the OMG IDL language mapping for Python, an
attribute foo can also be accessed through accessor methods _get_foo() and _set_foo().
The types short int, unsigned int, unsigned long long, and boolean all map to Python integer
objects.
The type DOMString maps to Python strings.
Values of type DOMString may also be None where allowed to have the IDL null
value by the DOM specification from the W3C.
• const declarations map to variables in their respective scope (e.g.
xml.dom.minidom.
Node.
PROCESSING_INSTRUCTION_NODE); they must not be changed.
Instead, xml.dom.minidom uses stan-
dard Python exceptions such as TypeError and AttributeError.
NodeList objects are implemented using Python’s built-in list type.
These objects provide the interface defined in
the DOM specification, but with earlier versions of Python they do not support the official API.
They are, however,
much more “Pythonic” than the interface defined in the W3C recommendations.
The following interfaces have no implementation in xml.dom.minidom:
• DOMTimeStamp
• EntityReference
Most of these reflect information in the XML document that is not of general utility to most DOM users.
The basic concept involves pulling “events” from a stream of incoming XML
and processing them.
In contrast to SAX which also employs an event-driven processing model together with callbacks,
the user of a pull parser is responsible for explicitly pulling events from the stream, looping over those events until either
processing is finished or an error condition occurs.
Warning:
The xml.dom.pulldom module is not secure against maliciously constructed data.
If you need to
parse untrusted or unauthenticated data see XML vulnerabilities.
Document, xml.dom.minidom.
Element or xml.dom.
minidom.
Text.
Since the document is treated as a “flat” stream of events, the document “tree” is implicitly traversed and the desired
elements are found regardless of their depth in the tree.
In other words, one does not need to consider hierarchical issues
such as recursive searching of the document nodes, although if the context of elements were important, one would either
need to maintain some context-related state (i.e. remembering where one is in the document at any given point) or to
make use of the DOMEventStream.expandNode() method and switch to DOM-related processing.
PullDom(documentFactory=None)
Subclass of xml.sax.handler.
ContentHandler.
SAX2DOM(documentFactory=None)
Subclass of xml.sax.handler.
ContentHandler.
This function will change the document handler of the
parser and activate namespace support; other parser configuration (like setting an entity resolver) must have been
done in advance.
The value of this variable can be changed before calling parse() and the new value will take effect.
DOMEventStream(stream, parser, bufsize)
Changed in version 3.11: Support for __getitem__() method has been removed.
Document if event
equals START_DOCUMENT, xml.dom.minidom.
Element if event equals START_ELEMENT or
END_ELEMENT or xml.dom.minidom.
Text if event equals CHARACTERS.
Expands all children of node into node.
The package itself provides the SAX exceptions and the convenience functions which will be most used by users
of the SAX API.
Warning: The xml.sax module is not secure against maliciously constructed data.
If you need to parse untrusted
or unauthenticated data see XML vulnerabilities.
Changed in version 3.7.1: The SAX parser no longer processes general external entities by default to increase secu-
rity.
Before, the parser created network connections to fetch remote files or loaded local files from the file system for
20.9.
The feature can be enabled again with method setFeature() on the parser object and argument
feature_external_ges.
The first parser found will be used.
If parser_list is provided, it
must be an iterable of strings which name modules that have a function named create_parser().
Modules
listed in parser_list will be used before modules in the default list of parsers.
The parser_list argument can be any iterable, not just a list.
ErrorHandler())
Create a SAX parser and use it to parse a document.
The document, passed in as filename_or_stream, can be a
filename or a file object.
The handler parameter needs to be a SAX ContentHandler instance.
If error_handler
is given, it must be a SAX ErrorHandler instance; if omitted, SAXParseException will be raised on all
errors.
There is no return value; all work must be done by the handler passed in.
Similar to parse(), but parses from a buffer string received as a parameter.
Changed in version 3.5: Added support of str instances.
A typical SAX application uses three kinds of objects: readers, handlers and input sources.
The events then get distributed to the handler objects, i.e. the reader invokes a method on the
handler.
A SAX application must therefore obtain a reader object, create or open the input sources, create the handlers,
and connect these objects all together.
As the final step of preparation, the reader is called to parse the input.
During
parsing, methods on the handler objects are called based on structural and syntactic events from the input data.
For these objects, only the interfaces are relevant; they are normally not instantiated by the application itself.
Since
Python does not have an explicit notion of interface, they are formally introduced as classes, but applications may use
implementations which do not inherit from the provided classes.
The InputSource, Locator, Attributes,
AttributesNS, and XMLReader interfaces are defined in the module xml.sax.xmlreader.
The handler inter-
faces are defined in xml.sax.handler.
For convenience, InputSource (which is often instantiated directly) and
the handler classes are also available from xml.sax.
These interfaces are described below.
In addition to these classes, xml.sax provides the following exception classes.
SAXException(msg, exception=None)
Encapsulate an XML error or warning.
This class can contain basic error or warning information from either the
XML parser or the application: it can be subclassed to provide additional functionality or to add localization.
Note
that although the handlers defined in the ErrorHandler interface receive instances of this exception, it is not
required to actually raise the exception — it is also useful as a container for information.
When instantiated, msg should be a human-readable description of the error.
The optional exception parameter, if
given, should be None or an exception that was caught by the parsing code and is being passed along as information.
This is the base class for the other SAX exception classes.
SAXParseException(msg, exception, locator)
Subclass of SAXException raised on parse errors.
Instances of this class are passed to the methods of the SAX
ErrorHandler interface to provide information about the parse error.
This class supports the SAX Locator
interface as well as the SAXException interface.
SAX applications and extensions may use
this class for similar purposes.
The Simple API for XML This site is the focal point for the definition of the SAX API.
It provides a Java im-
plementation and online documentation.
Links to implementations and historical information are also available.
Definitions of the interfaces for application-provided objects.
Module xml.sax.saxutils Convenience functions for use in SAX applications.
Definitions of the interfaces for parser-provided objects.
SAXException Objects
The SAXException exception class supports the following methods:
SAXException.getMessage()
Return a human-readable message describing the error condition.
Base classes for SAX handlers
Source code: Lib/xml/sax/handler.py
The SAX API defines five kinds of handlers: content handlers, DTD handlers, error handlers, entity resolvers and lexical
handlers.
Applications normally only need to implement those interfaces whose events they are interested in; they can
implement the interfaces in a single object or in multiple objects.
Handler implementations should inherit from the base
classes provided in the module xml.sax.handler, so that all methods get default implementations.
ContentHandler
This is the main callback interface in SAX, and the one most important to applications.
The order of events in this
interface mirrors the order of the information in the document.
DTDHandler
Handle DTD events.
This interface specifies only those DTD events required for basic parsing (unparsed entities and attributes).
EntityResolver
Basic interface for resolving entities.
If you create an object implementing this interface, then register the object
with your Parser, the parser will call the method in your object to resolve all external entities.
ErrorHandler
Interface used by the parser to present error and warning messages to the application.
The methods of this object
control whether errors are immediately converted to exceptions or are handled in some other way.
20.10.
LexicalHandler
Interface used by the parser to represent low frequency events which may not be of interest to many applications.
In addition to these classes, xml.sax.handler provides symbolic constants for the feature and property names.
LexicalHandler (not supported in Python 2)
description: An optional extension handler for lexical events like comments.
DeclHandler (not supported in Python 2)
description: An optional extension handler for DTD-related events other than notations and unparsed entities.
Node (not supported in Python 2)
description: When parsing, the current DOM node being visited if this is a DOM iterator; when not parsing, the
root DOM node for iteration.
The following methods are called by
the parser on the appropriate events in the input document:
ContentHandler.setDocumentLocator(locator)
Called by the parser to give the application a locator for locating the origin of document events.
SAX parsers are strongly encouraged (though not absolutely required) to supply a locator: if it does so, it must
supply the locator to the application by invoking this method before invoking any of the other methods in the
DocumentHandler interface.
The locator allows the application to determine the end position of any document-related event, even if the parser
is not reporting an error.
Typically, the application will use this information for reporting its own errors (such as
character content that does not match an application’s business rules).
The information returned by the locator is
probably not sufficient for use with a search engine.
20.10.
The
application should not attempt to use it at any other time.
ContentHandler.startDocument()
Receive notification of the beginning of a document.
The SAX parser will invoke this method only once, before any other methods in this interface or in DTDHandler
(except for setDocumentLocator()).
ContentHandler.endDocument()
Receive notification of the end of a document.
The SAX parser will invoke this method only once, and it will be the last method invoked during the parse.
The
parser shall not invoke this method until it has either abandoned parsing (because of an unrecoverable error) or
reached the end of input.
ContentHandler.startPrefixMapping(prefix, uri)
Begin the scope of a prefix-URI Namespace mapping.
The information from this event is not necessary for normal Namespace processing: the SAX XML reader will
automatically replace prefixes for element and attribute names when the feature_namespaces feature is en-
abled (the default).
There are cases, however, when applications need to use prefixes in character data or in attribute values, where
they cannot safely be expanded automatically; the startPrefixMapping() and endPrefixMapping()
events supply the information to the application to expand prefixes in those contexts itself, if necessary.
Note that startPrefixMapping() and endPrefixMapping() events are not guaranteed to be prop-
erly nested relative to each-other: all startPrefixMapping() events will occur before the correspond-
ing startElement() event, and all endPrefixMapping() events will occur after the corresponding
endElement() event, but their order is not guaranteed.
ContentHandler.endPrefixMapping(prefix)
End the scope of a prefix-URI mapping.
See startPrefixMapping() for details.
This event will always occur after the corresponding
endElement() event, but the order of endPrefixMapping() events is not otherwise guaranteed.
ContentHandler.startElement(name, attrs)
Signals the start of an element in non-namespace mode.
The name parameter contains the raw XML 1.0 name of the element type as a string and the attrs parameter holds
an object of the Attributes interface (see The Attributes Interface) containing the attributes of the element.
The object passed as attrs may be re-used by the parser; holding on to a reference to it is not a reliable way to keep
a copy of the attributes.
To keep a copy of the attributes, use the copy() method of the attrs object.
ContentHandler.endElement(name)
Signals the end of an element in non-namespace mode.
The name parameter contains the name of the element type, just as with the startElement() event.
ContentHandler.startElementNS(name, qname, attrs)
Signals the start of an element in namespace mode.
The name parameter contains the name of the element type as a (uri, localname) tuple, the qname pa-
rameter contains the raw XML 1.0 name used in the source document, and the attrs parameter holds an instance
of the AttributesNS interface (see The AttributesNS Interface) containing the attributes of the element.
If no
namespace is associated with the element, the uri component of name will be None.
The object passed as attrs
may be re-used by the parser; holding on to a reference to it is not a reliable way to keep a copy of the attributes.
The name parameter contains the name of the element type, just as with the startElementNS() method,
likewise the qname parameter.
ContentHandler.characters(content)
Receive notification of character data.
The Parser will call this method to report each chunk of character data.
SAX parsers may return all contiguous
character data in a single chunk, or they may split it into several chunks; however, all of the characters in any single
event must come from the same external entity so that the Locator provides useful information.
Note:
The earlier SAX 1 interface provided by the Python XML Special Interest Group used a more Java-like
interface for this method.
Since most parsers used from Python did not take advantage of the older interface, the
simpler signature was chosen to replace it.
To convert old code to the new interface, use content instead of slicing
content with the old offset and length parameters.
ContentHandler.ignorableWhitespace(whitespace)
Receive notification of ignorable whitespace in element content.
Validating Parsers must use this method to report each chunk of ignorable whitespace (see the W3C XML 1.0
recommendation, section 2.10): non-validating parsers may also use this method if they are capable of parsing and
using content models.
SAX parsers may return all contiguous whitespace in a single chunk, or they may split it into several chunks;
however, all of the characters in any single event must come from the same external entity, so that the Locator
provides useful information.
ContentHandler.processingInstruction(target, data)
Receive notification of a processing instruction.
The Parser will invoke this method once for each processing instruction found: note that processing instructions
may occur before or after the main document element.
A SAX parser should never report an XML declaration (XML 1.0, section 2.8) or a text declaration (XML 1.0,
section 4.3.1) using this method.
Receive notification of a skipped entity.
The Parser will invoke this method once for each entity skipped.
Non-validating processors may skip entities if
they have not seen the declarations (because, for example, the entity was declared in an external DTD subset).
All processors may skip external entities, depending on the values of the feature_external_ges and the
feature_external_pes properties.
20.10.
DTDHandler.unparsedEntityDecl(name, publicId, systemId, ndata)
Handle an unparsed entity declaration event.
The default implementation returns systemId.
If you create an
object that implements this interface, then register the object with your XMLReader, the parser will call the methods in
your object to report all warnings and errors.
There are three levels of errors available: warnings, (possibly) recoverable
errors, and unrecoverable errors.
All methods take a SAXParseException as the only parameter.
Errors and warnings
may be converted to an exception by raising the passed-in exception object.
ErrorHandler.error(exception)
Called when the parser encounters a recoverable error.
If this method does not raise an exception, parsing may
continue, but further document information should not be expected by the application.
Allowing the parser to
continue may allow additional errors to be discovered in the input document.
ErrorHandler.fatalError(exception)
Called when the parser encounters an error it cannot recover from; parsing is expected to terminate when this
method returns.
ErrorHandler.warning(exception)
Called when the parser presents minor warning information to the application.
Parsing is expected to continue
when this method returns, and document information will continue to be passed to the application.
Raising an
exception in this method will cause parsing to end.
This handler is used to obtain lexical information about an XML document.
Lexical information includes information
describing the document encoding used and XML comments embedded in the document, as well as section boundaries
for the DTD and for any CDATA sections.
The lexical handlers are used in the same manner as content handlers.
Set the LexicalHandler of an XMLReader by using the setProperty method with the property identifier 'http://xml.
org/sax/properties/lexical-handler'.
LexicalHandler.comment(content)
Reports a comment anywhere in the document (including the DTD and outside the document element).
LexicalHandler.endDTD()
Reports the end of DTD declaration.
LexicalHandler.startCDATA()
Reports the start of a CDATA marked section.
The contents of the CDATA marked section will be reported through the characters handler.
LexicalHandler.endCDATA()
Reports the end of a CDATA marked section.
You can escape other strings of data by passing a dictionary as the optional entities parameter.
The keys and values
must all be strings; each key will be replaced with its corresponding value.
The characters '&', '<' and '>' are
always escaped, even if entities is provided.
Note: This function should only be used to escape characters that can’t be used directly in XML.
Unescape '&amp;', '&lt;', and '&gt;' in a string of data.
You can unescape other strings of data by passing a dictionary as the optional entities parameter.
The keys and values
must all be strings; each key will be replaced with its corresponding value.
The return value is a quoted version
of data with any additional required replacements.
If both single- and double-quote
characters are already in data, the double-quote characters will be encoded and data will be wrapped in double-
quotes.
This function is useful when generating attribute values for HTML or any SGML using the reference concrete
syntax.
This class implements the ContentHandler interface by writing SAX events back into an XML document.
In
other words, using an XMLGenerator as the content handler will reproduce the original document being parsed.
out should be a file-like object which will default to sys.stdout.
The short_empty_elements parameter.
This class is designed to sit between an XMLReader and the client application’s event handlers.
This function takes an input source and an optional base URL and returns a fully resolved InputSource object
ready for reading.
The input source can be given as a string, a file-like object, or an InputSource object; parsers
will use this function to implement the polymorphic source argument to their parse() method.
20.12 xml.sax.xmlreader — Interface for XML parsers
Source code: Lib/xml/sax/xmlreader.py
SAX parsers implement the XMLReader interface.
They are implemented in a Python module, which must provide a
function create_parser().
This function is invoked by xml.sax.make_parser() with no arguments to create
a new parser object.
XMLReader
Base class which can be inherited by SAX parsers.
In some cases, it is desirable not to parse an input source at once, but to feed chunks of the document as they get
available.
Note that the reader will normally not read the entire file, but read it in chunks as well; still parse()
won’t return until the entire document is processed.
So these interfaces should be used if the blocking behaviour
of parse() is not desirable.
When the parser is instantiated it is ready to begin accepting data from the feed method immediately.
After parsing
has been finished with a call to close the reset method must be called to make the parser ready to accept new data,
either from feed or using the parse method.
Note that these methods must not be called during parsing, that is, after parse has been called and before it returns.
By default, the class also implements the parse method of the XMLReader interface using the feed, close and reset
methods of the IncrementalParser interface as a convenience to SAX 2.0 driver writers.
Locator
Interface for associating a SAX event with a document location.
A locator object will return valid results only
during calls to DocumentHandler methods; at any other time, the results are unpredictable.
InputSource(system_id=None)
Encapsulation of the information needed by the XMLReader to read entities.
This class may include information about the public identifier, system identifier, byte stream (possibly with character
encoding information) and/or the character stream of an entity.
Applications will create objects of this class for use in the XMLReader.parse() method and for returning from
EntityResolver.resolveEntity.
This is an implementation of the Attributes interface (see section The Attributes Interface).
This is a dictionary-
like object which represents the element attributes in a startElement() call.
In addition to the most useful
dictionary operations, it supports a number of other methods as described by the interface.
Objects of this class
should be instantiated by readers; attrs must be a dictionary-like object containing a mapping from attribute names
to attribute values.
AttributesNSImpl(attrs, qnames)
Namespace-aware variant of AttributesImpl, which will be passed to startElementNS().
It is derived
from AttributesImpl, but understands attribute names as two-tuples of namespaceURI and localname.
In
addition, it provides a number of methods expecting qualified names as they appear in the original document.
This
class implements the AttributesNS interface (see section The AttributesNS Interface).
Process an input source, producing SAX events.
The source object can be a system identifier (a string identifying
the input source – typically a file name or a URL), a pathlib.
Path or path-like object, or an InputSource
object.
When parse() returns, the input is completely processed, and the parser object can be discarded or reset.
Changed in version 3.5: Added support of character streams.
Changed in version 3.8: Added support of path-like objects.
XMLReader.getContentHandler()
Return the current ContentHandler.
XMLReader.setContentHandler(handler)
Set the current ContentHandler.
If no ContentHandler is set, content events will be discarded.
XMLReader.getDTDHandler()
Return the current DTDHandler.
XMLReader.setDTDHandler(handler)
Set the current DTDHandler.
If no DTDHandler is set, DTD events will be discarded.
XMLReader.getEntityResolver()
Return the current EntityResolver.
XMLReader.setEntityResolver(handler)
Set the current EntityResolver.
XMLReader.setErrorHandler(handler)
Set the current error handler.
If no ErrorHandler is set, errors will be raised as exceptions, and warnings will
be printed.
XMLReader.setLocale(locale)
Allow an application to set the locale for errors and warnings.
SAX parsers are not required to provide localization for errors and warnings; if they cannot support the requested
locale, however, they must raise a SAX exception.
Return
the
current
setting
for
feature
featurename.
If
the
feature
is
not
recognized,
SAXNotRecognizedException is raised.
The well-known featurenames are listed in the module
xml.sax.handler.
XMLReader.setFeature(featurename, value)
Set the featurename to value.
If the feature is not recognized, SAXNotRecognizedException is raised.
If
the feature or its setting is not supported by the parser, SAXNotSupportedException is raised.
Return
the
current
setting
for
property
propertyname.
If
the
property
is
not
recognized,
a
SAXNotRecognizedException is raised.
The well-known propertynames are listed in the module
xml.sax.handler.
XMLReader.setProperty(propertyname, value)
Set the propertyname to value.
If the property is not recognized, SAXNotRecognizedException is raised.
If the property or its setting is not supported by the parser, SAXNotSupportedException is raised.
IncrementalParser.close()
Assume the end of the document.
That will check well-formedness conditions that can be checked only at the end,
invoke handlers, and may clean up resources allocated during parsing.
This method is called after close has been called to reset the parser so that it is ready to parse new documents.
Locator.getLineNumber()
Return the line number where the current event begins.
Locator.getPublicId()
Return the public identifier for the current event.
Locator.getSystemId()
Return the system identifier for the current event.
Returns the public identifier of this InputSource.
InputSource.setSystemId(id)
Sets the system identifier of this InputSource.
InputSource.getSystemId()
Returns the system identifier of this InputSource.
InputSource.setEncoding(encoding)
Sets the character encoding of this InputSource.
The encoding must be a string acceptable for an XML encoding declaration (see section 4.3.3 of the XML recom-
mendation).
The encoding attribute of the InputSource is ignored if the InputSource also contains a character stream.
InputSource.getEncoding()
Get the character encoding of this InputSource.
InputSource.setByteStream(bytefile)
Set the byte stream (a binary file) for this input source.
The SAX parser will ignore this if there is also a character stream specified, but it will use a byte stream in preference
to opening a URI connection itself.
If the application knows the character encoding of the byte stream, it should set it with the setEncoding method.
InputSource.getByteStream()
Get the byte stream for this input source.
The getEncoding method will return the character encoding for this byte stream, or None if unknown.
InputSource.setCharacterStream(charfile)
Set the character stream (a text file) for this input source.
The following methods are also provided:
Attributes.getLength()
Return the number of attributes.
Attributes.getNames()
Return the names of the attributes.
Return the value of attribute name.
Interface
This interface is a subtype of the Attributes interface (see section The Attributes Interface).
All methods supported
by that interface are also available on AttributesNS objects.
The following methods are also available:
AttributesNS.getValueByQName(name)
Return the value for a qualified name.
AttributesNS.getNameByQName(name)
Return the (namespace, localname) pair for a qualified name.
AttributesNS.getQNameByName(name)
Return the qualified name for a (namespace, localname) pair.
AttributesNS.getQNames()
Return the qualified names of all attributes.
If you need to parse untrusted
or unauthenticated data see XML vulnerabilities.
The xml.parsers.expat module is a Python interface to the Expat non-validating XML parser.
The module pro-
vides a single extension type, xmlparser, that represents the current state of an XML parser.
After an xmlparser
object has been created, various attributes of the object can be set to handler functions.
When an XML document is then
fed to the parser, the handler functions are called for the character data and markup in the XML document.
This module uses the pyexpat module to provide access to the Expat parser.
Direct use of the pyexpat module is
deprecated.
This module provides one exception and one type object:
exception xml.parsers.expat.
ExpatError
The exception raised when Expat reports an error.
See section ExpatError Exceptions for more information on
interpreting Expat errors.
XMLParserType
The type of the return values from the ParserCreate() function.
The xml.parsers.expat module contains two functions:
xml.parsers.expat.ErrorString(errno)
Returns an explanatory string for a given error number errno.
ParserCreate(encoding=None, namespace_separator=None)
Creates and returns a new xmlparser object.
Expat doesn’t support as many encodings as Python does, and its repertoire of encodings can’t
be extended; it supports UTF-8, UTF-16, ISO-8859-1 (Latin1), and ASCII.
If encoding1 is given it will override
the implicit or explicit encoding of the document.
Expat can optionally do XML namespace processing for you, enabled by providing a value for names-
pace_separator.
The value must be a one-character string; a ValueError will be raised if the string has an
illegal length (None is considered the same as omission).
When namespace processing is enabled, element type
names and attribute names that belong to a namespace will be expanded.
The element name passed to the element
handlers StartElementHandler and EndElementHandler will be the concatenation of the namespace
URI, the namespace separator character, and the local part of the name.
If the namespace separator is a zero byte
(chr(0)) then the namespace URI and the local part will be concatenated without any separator.
Call ParserCreate for each document to provide unique parser instances.
See also:
The Expat XML Parser Home page of the Expat project.
The encoding string included in XML output should conform to the appropriate standards.
For example, “UTF-8” is valid, but “UTF8” is not.
See https://www.w3.org/TR/2006/REC-xml11-20060816/#NT-EncodingDecl and https://www.iana.org/assignments/character-sets/character-sets.
xhtml.
20.13.
Parse XML data reading from the object file.
SetBase(base)
Sets the base to be used for resolving relative URIs in system identifiers in declarations.
Re-
solving
relative
identifiers
is
left
to
the
application:
this
value
will
be
passed
through
as
the
base
argument
to
the
ExternalEntityRefHandler(),
NotationDeclHandler(),
and
UnparsedEntityDeclHandler() functions.
GetBase()
Returns a string containing the base set by a previous call to SetBase(), or None if SetBase() hasn’t been
called.
GetInputContext()
Returns the input data that generated the current event as a string.
The data is in the encoding of the entity which
contains the text.
When called while an event handler is not active, the return value is None.
xmlparser.
The context parameter should be the string passed to the ExternalEntityRefHandler()
handler function, described below.
The child parser is created with the ordered_attributes and
specified_attributes set to the values of this parser.
Control parsing of parameter entities (including the external DTD subset).
Possible flag values are
XML_PARAM_ENTITY_PARSING_NEVER,
XML_PARAM_ENTITY_PARSING_UNLESS_STANDALONE
and XML_PARAM_ENTITY_PARSING_ALWAYS.
Return true if setting the flag was successful.
UseForeignDTD([flag])
Calling this with a true value for flag (the default) will cause Expat to call the ExternalEntityRefHandler
with None for all arguments to allow an alternate DTD to be loaded.
If the document does not
contain a document type declaration, the ExternalEntityRefHandler will still be called, but the
StartDoctypeDeclHandler and EndDoctypeDeclHandler will not be called.
Passing a false value for flag will cancel a previous call that passed a true value, but otherwise has no effect.
This method can only be called before the Parse() or ParseFile() methods are called; calling it after
either of those have been called causes ExpatError to be raised with the code attribute set to errors.
XML_ERROR_CANT_CHANGE_FEATURE_ONCE_PARSING].
A new buffer size can be set by assigning a new integer
value to this attribute.
When the size is changed, the buffer will be flushed.
This can improve performance substantially
since Expat normally breaks character data into chunks at every line ending.
This attribute is false by default, and
may be changed at any time.
If buffer_text is enabled, the number of bytes stored in the buffer.
These bytes represent UTF-8 encoded
text.
This attribute has no meaningful interpretation when buffer_text is false.
The
attributes are presented in the order found in the document text.
For each attribute, two list entries are presented:
the attribute name and the attribute value.
By default, this
attribute is false; it may be changed at any time.
Applications which set this need to be
especially careful to use what additional information is available from the declarations as needed to comply with
the standards for the behavior of XML processors.
By default, this attribute is false; it may be changed at any time.
The following attributes contain values relating to the most recent error encountered by an xmlparser object, and
will only have correct values once a call to Parse() or ParseFile() has raised an xml.parsers.expat.
ExpatError exception.
ErrorByteIndex
Byte index at which an error occurred.
ErrorCode
Numeric code specifying the problem.
This value can be passed to the ErrorString() function, or compared
to one of the constants defined in the errors object.
ErrorColumnNumber
Column number at which an error occurred.
ErrorLineNumber
Line number at which an error occurred.
The following attributes contain values relating to the current parse location in an xmlparser object.
During a callback
reporting a parse event they indicate the location of the first of the sequence of characters that generated the event.
When
called outside of a callback, the position indicated will be just past the last parse event (regardless of whether there was
an associated callback).
CurrentByteIndex
Current byte index in the parser input.
CurrentColumnNumber
Current column number in the parser input.
CurrentLineNumber
Current line number in the parser input.
Here is the list of handlers that can be set.
To set a handler on an xmlparser object o, use o.handlername =
func.
The arguments are all strings, unless otherwise stated.
20.13.
The XML declaration is the (optional) declaration of the applicable
version of the XML recommendation, the encoding of the document text, and an optional “standalone” declaration.
version and encoding will be strings, and standalone will be 1 if the document is declared standalone, 0 if it is
declared not to be standalone, or -1 if the standalone clause was omitted.
This is only available with Expat version
1.95.0 or newer.
xmlparser.
StartDoctypeDeclHandler(doctypeName, systemId, publicId, has_internal_subset)
Called when Expat begins parsing the document type declaration (<!DOCTYPE ...).
The doctypeName is pro-
vided exactly as presented.
The systemId and publicId parameters give the system and public identifiers if specified,
or None if omitted.
This requires Expat version 1.2 or newer.
xmlparser.EndDoctypeDeclHandler()
Called when Expat is done parsing the document type declaration.
This requires Expat version 1.2 or newer.
xmlparser.
ElementDeclHandler(name, model)
Called once for each element type declaration.
AttlistDeclHandler(elname, attname, type, default, required)
Called for each declared attribute for an element type.
If an attribute list declaration declares three attributes, this
handler is called three times, once for each attribute.
The attribute type is a string passed as type; the possible
values are 'CDATA', 'ID', 'IDREF', … default gives the default value for the attribute used when the attribute
is not specified by the document instance, or None if there is no default value (#IMPLIED values).
If the attribute
is required to be given in the document instance, required will be true.
This requires Expat version 1.95.0 or newer.
xmlparser.
StartElementHandler(name, attributes)
Called for the start of every element.
If ordered_attributes is true, this is a list (see ordered_attributes for a full description).
Otherwise it’s a dictionary mapping names to values.
EndElementHandler(name)
Called for the end of every element.
xmlparser.
ProcessingInstructionHandler(target, data)
Called for every processing instruction.
xmlparser.
CharacterDataHandler(data)
Called for character data.
This will be called for normal character data, CDATA marked content, and ignorable
whitespace.
Applications which must distinguish these cases can use the StartCdataSectionHandler,
EndCdataSectionHandler, and ElementDeclHandler callbacks to collect the required information.
UnparsedEntityDeclHandler(entityName, base, systemId, publicId, notationName)
Called for unparsed (NDATA) entity declarations.
This is only present for version 1.2 of the Expat library; for
more recent versions, use EntityDeclHandler instead.
EntityDeclHandler(entityName, is_parameter_entity, value, base, systemId, publicId,
notationName)
Called for all entity declarations.
For parameter and internal entities, value will be a string giving the declared
contents of the entity; this will be None for external entities.
The notationName parameter will be None for
parsed entities, and the name of the notation for unparsed entities.
NotationDeclHandler(notationName, base, systemId, publicId)
Called for notation declarations.
If the public
identifier is omitted, publicId will be None.
xmlparser.
StartNamespaceDeclHandler(prefix, uri)
Called when an element contains a namespace declaration.
Namespace declarations are processed before the
StartElementHandler is called for the element on which declarations are placed.
EndNamespaceDeclHandler(prefix)
Called when the closing tag is reached for an element that contained a namespace declaration.
This is
called once for each namespace declaration on the element in the reverse of the order for which the
StartNamespaceDeclHandler was called to indicate the start of each namespace declaration’s scope.
Calls
to this handler are made after the corresponding EndElementHandler for the end of the element.
CommentHandler(data)
Called for comments.
This and EndCdataSectionHandler are needed to be able to identify
the syntactical start and end for CDATA sections.
EndCdataSectionHandler()
Called at the end of a CDATA section.
This means
characters that are part of a construct which could be reported, but for which no handler has been supplied.
This is the same as the DefaultHandler(), but doesn’t inhibit expansion of internal entities.
The entity
reference will not be passed to the default handler.
NotStandaloneHandler()
Called if the XML document hasn’t been declared as being a standalone document.
This happens when there is
an external subset or a reference to a parameter entity, but the XML declaration does not set standalone to yes in
an XML declaration.
If this handler returns 0, then the parser will raise an XML_ERROR_NOT_STANDALONE
error.
If this handler is not set, no exception is raised by the parser for this condition.
ExternalEntityRefHandler(context, base, systemId, publicId)
Called for references to external entities.
The
public and system identifiers, systemId and publicId, are strings if given; if the public identifier is not given, publicId
will be None.
The context value is opaque and should only be used as described below.
For external entities to be parsed, this handler must be implemented.
It is responsible for creating the
sub-parser using ExternalEntityParserCreate(context), initializing it with the appropriate call-
backs, and parsing the entity.
This handler should return an integer; if it returns 0, the parser will raise an
XML_ERROR_EXTERNAL_ENTITY_HANDLING error, otherwise parsing will continue.
If this handler is not provided, external entities are reported by the DefaultHandler callback, if provided.
20.13.
The errors.messages dictionary maps these error num-
bers to Expat’s error messages.
The errors module also provides error message constants and a dictionary codes mapping these messages back
to the error codes, see below.
ExpatError.lineno
Line number on which the error was detected.
The first line is numbered 1.
ExpatError.offset
Character offset into the line where the error occurred.
The first column is numbered 0.
The following program defines three handlers that just print out their arguments.
Each tuple contains four values: the type, the quantifier, the name, and
a tuple of children.
Children are simply additional content model descriptions.
The values of the first two fields are constants defined in the xml.parsers.expat.model module.
These constants
can be collected in two groups: the model type group and the quantifier group.
The constants in the model type group are:
xml.parsers.expat.model.
XML_CTYPE_ANY
The element named by the model name was declared to have a content model of ANY.
xml.parsers.expat.model.
XML_CTYPE_CHOICE
The named element allows a choice from a number of options; this is used for content models such as (A | B |
C).
XML_CTYPE_MIXED
xml.parsers.expat.model.
XML_CTYPE_NAME
xml.parsers.expat.model.
XML_CTYPE_SEQ
Models which represent a series of models which follow one after the other are indicated with this model type.
This
is used for models such as (A, B, C).
The constants in the quantifier group are:
xml.parsers.expat.model.
No modifier is given, so it can appear exactly once, as for A.
xml.parsers.expat.model.
The model is optional: it can appear once or not at all, as for A?.
The model must occur one or more times (like A+).
The model must occur zero or more times, as for A*.
20.13.
These constants are useful in
interpreting some of the attributes of the ExpatError exception objects raised when an error has occurred.
Since for
backwards compatibility reasons, the constants’ value is the error message and not the numeric error code, you do this by
comparing its code attribute with errors.codes[errors.
XML_ERROR_CONSTANT_NAME].
The errors module has the following attributes:
xml.parsers.expat.errors.codes
A dictionary mapping string descriptions to their error codes.
New in version 3.2.
New in version 3.2.
XML_ERROR_ASYNC_ENTITY
xml.parsers.expat.errors.
An entity reference in an attribute value referred to an external entity instead of an internal entity.
A character reference referred to a character which is illegal in XML (for example, character 0, or ‘&#0;’).
XML_ERROR_BINARY_ENTITY_REF
An entity reference referred to an entity which was declared with a notation, so cannot be parsed.
XML_ERROR_DUPLICATE_ATTRIBUTE
An attribute was used more than once in a start tag.
XML_ERROR_INCORRECT_ENCODING
xml.parsers.expat.errors.
XML_ERROR_INVALID_TOKEN
Raised when an input byte could not properly be assigned to a character; for example, a NUL byte (value 0) in a
UTF-8 input stream.
XML_ERROR_JUNK_AFTER_DOC_ELEMENT
Something other than whitespace occurred after the document element.
XML_ERROR_MISPLACED_XML_PI
An XML declaration was found somewhere other than the start of the input data.
The document contains no elements (XML requires all documents to contain exactly one top-level element)..
XML_ERROR_NO_MEMORY
Expat was not able to allocate memory internally.
A parameter entity reference was found where it was not allowed.
XML_ERROR_RECURSIVE_ENTITY_REF
An entity reference contained another reference to the same entity; possibly via a different name, and possibly
indirectly.
XML_ERROR_SYNTAX
Some unspecified syntax error was encountered.
XML_ERROR_TAG_MISMATCH
An end tag did not match the innermost open start tag.
xml.parsers.expat.errors.
XML_ERROR_UNCLOSED_TOKEN
Some token (such as a start tag) was not closed before the end of the stream or the next token was encountered.
A reference was made to an entity which was not defined.
XML_ERROR_UNKNOWN_ENCODING
The document encoding is not supported by Expat.
xml.parsers.expat.errors.
A CDATA marked section was not closed.
XML_ERROR_EXTERNAL_ENTITY_HANDLING
xml.parsers.expat.errors.XML_ERROR_NOT_STANDALONE
The parser determined that the document was not “standalone” though it declared itself to be in the XML declara-
tion, and the NotStandaloneHandler was set and returned 0.
XML_ERROR_ENTITY_DECLARED_IN_PE
xml.parsers.expat.errors.
XML_ERROR_FEATURE_REQUIRES_XML_DTD
An operation was requested that requires DTD support to be compiled in, but Expat was configured without DTD
support.
This should never be reported by a standard build of the xml.parsers.expat module.
XML_ERROR_CANT_CHANGE_FEATURE_ONCE_PARSING
A behavioral change was requested after parsing started that can only be changed before parsing has started.
This
is (currently) only raised by UseForeignDTD().
XML_ERROR_UNBOUND_PREFIX
An undeclared prefix was found when namespace processing was enabled.
The document attempted to remove the namespace declaration associated with a prefix.
A parameter entity contained incomplete markup.
The document contained no document element at all.
xml.parsers.expat.errors.
XML_ERROR_TEXT_DECL
There was an error parsing a text declaration in an external entity.
XML_ERROR_PUBLICID
Characters were found in the public id that are not allowed.
20.13.
The requested operation was made on a suspended parser, but isn’t allowed.
This includes attempts to provide
additional input or to stop the parser.
XML_ERROR_NOT_SUSPENDED
An attempt to resume the parser was made when the parser had not been suspended.
The requested operation was made on a parser which was finished parsing input, but isn’t allowed.
This includes
attempts to provide additional input or to stop the parser.
xml.parsers.expat.errors.
XML_ERROR_SUSPEND_PE
xml.parsers.expat.errors.
An attempt was made to undeclare reserved namespace prefix xml or to bind it to another namespace URI.
An attempt was made to declare or undeclare reserved namespace prefix xmlns.
An attempt was made to bind the URI of one the reserved namespace prefixes xml and xmlns to another names-
pace prefix.
XML_ERROR_INVALID_ARGUMENT
This should not be reported to Python applications.
This should not be reported to Python applications.
XML_ERROR_AMPLIFICATION_LIMIT_BREACH
The limit on input amplification factor (from DTD and entities) has been breached.
The modules described in this chapter implement internet protocols and support for related technology.
They are all
implemented in Python.
Most of these modules require the presence of the system-dependent module socket, which
is currently supported on most popular platforms.
Here is an overview:
21.1 webbrowser — Convenient web-browser controller
Source code: Lib/webbrowser.py
The webbrowser module provides a high-level interface to allow displaying web-based documents to users.
Under
most circumstances, simply calling the open() function from this module will do the right thing.
Under Unix, graphical browsers are preferred under X11, but text-mode browsers will be used if graphical browsers are
not available or an X11 display isn’t available.
If text-mode browsers are used, the calling process will block until the user
exits the browser.
If the environment variable BROWSER exists, it is interpreted as the os.pathsep-separated list of browsers to try
ahead of the platform defaults.
When the value of a list part contains the string %s, then it is interpreted as a literal
browser command line to be used with the argument URL substituted for %s; if the part does not contain %s, it is simply
interpreted as the name of the browser to launch.1
For non-Unix platforms, or when a remote browser is available on Unix, the controlling process will not wait for the user
to finish with the browser, but allow the remote browser to maintain its own windows on the display.
If remote browsers
are not available on Unix, the controlling process will launch a new browser and wait.
The script webbrowser can be used as a command-line interface for the module.
It accepts a URL as the argument.
It
accepts the following optional parameters: -n opens the URL in a new browser window, if possible; -t opens the URL
in a new browser page (“tab”).
The options are, naturally, mutually exclusive.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The following exception is defined:
exception webbrowser.
Error
Exception raised when a browser control error occurs.
The following functions are defined:
1 Executables named here without a full path will be searched in the directories given in the PATH environment variable.
If new is 0, the url is opened in the same browser window if possible.
If new
is 1, a new browser window is opened if possible.
If new is 2, a new browser page (“tab”) is opened if possible.
If autoraise is True, the window is raised if possible (note that under many window managers this will occur
regardless of the setting of this variable).
Note that on some platforms, trying to open a filename using this function, may work and start the operating system’s
associated program.
However, this is neither supported nor portable.
Raises an auditing event webbrowser.open with argument url.
If using is None, return a controller for a default browser
appropriate to the caller’s environment.
webbrowser.register(name, constructor, instance=None, *, preferred=False)
Register the browser type name.
Once a browser type is registered, the get() function can return a controller for
that browser type.
If instance is not provided, or is None, constructor will be called without parameters to create
an instance when needed.
If instance is provided, constructor will never be called, and may be None.
Setting preferred to True makes this browser a preferred result for a get() call with no argument.
Otherwise,
this entry point is only useful if you plan to either set the BROWSER variable or call get() with a nonempty
argument matching the name of a handler you declare.
Changed in version 3.7: preferred keyword-only parameter was added.
A number of browser types are predefined.
This table gives the type names that may be passed to the get() function
and the corresponding instantiations for the controller classes, all defined in this module.
Some way of reliably detecting KDE would be nice; the KDEDIR variable is not sufficient.
Note also that
the name “kfm” is used even when using the konqueror command with KDE 2 — the implementation selects
the best strategy for running Konqueror.
New in version 3.3: Support for Chrome/Chromium has been added.
Changed in version 3.12: Support for several obsolete browsers has been removed.
Removed browsers include Grail,
Mosaic, Netscape, Galeon, Skipstone, Iceape, and Firefox versions 35 and below.
MacOSX is deprecated, use MacOSXOSAScript
instead.
If new is 1, a new browser window is opened if possible.
If new is 2, a new browser page (“tab”) is opened if possible.
Alias open_new().
controller.open_new_tab(url)
Open url in a new page (“tab”) of the browser handled by this controller, if possible, otherwise equivalent to
open_new().
Having a standard interface makes it easy to use an application that supports WSGI with a number of
different web servers.
You don’t need to understand every detail of WSGI just to install a WSGI application or to write a web application using
an existing framework.
It provides utilities for manipulating WSGI environment variables and response headers, base classes for
implementing WSGI servers, a demo HTTP server that serves WSGI applications, types for static type checking, and a
validation tool that checks WSGI servers and applications for conformance to the WSGI specification (PEP 3333).
See wsgi.readthedocs.io for more information about WSGI, and links to tutorials and other resources.
A WSGI environment is
a dictionary containing HTTP request variables as described in PEP 3333.
All of the functions taking an environ
parameter expect a WSGI-compliant dictionary to be supplied; please see PEP 3333 for a detailed specification and
WSGIEnvironment for a type alias that can be used in type annotations.
The return value is a string.
This function is useful when creating a gateway that wraps CGI or a CGI-like protocol such as FastCGI.
Typically,
servers providing such protocols will include a HTTPS variable with a value of “1”, “yes”, or “on” when a request
is received via SSL.
So, this function returns “https” if such a value is found, and “http” otherwise.
If include_query is false, the query string is not included in the resulting URI.
wsgiref.util.application_uri(environ)
Similar to request_uri(), except that the PATH_INFO and QUERY_STRING variables are ignored.
The
result is the base URI of the application object addressed by the request.
The environ dictionary is modified
in-place; use a copy if you need to keep the original PATH_INFO or SCRIPT_NAME intact.
If there are no remaining path segments in PATH_INFO, None is returned.
Typically, this routine is used to process each portion of a request URI path, for example to treat the path as a
series of dictionary keys.
This routine modifies the passed-in environment to make it suitable for invoking another
WSGI application that is located at the target URI.
For example, if there is a WSGI application at /foo, and the
request URI path is /foo/bar/baz, and the WSGI application at /foo calls shift_path_info(), it will
receive the string “bar”, and the environment will be updated to be suitable for passing to a WSGI application at
/foo/bar.
That is, SCRIPT_NAME will change from /foo to /foo/bar, and PATH_INFO will change from
/bar/baz to /baz.
When PATH_INFO is just a “/”, this routine returns an empty string and appends a trailing slash to
SCRIPT_NAME, even though empty path segments are normally ignored, and SCRIPT_NAME doesn’t normally
end in a slash.
This is intentional behavior, to ensure that an application can tell the difference between URIs
ending in /x from ones ending in /x/ when using this routine to do object traversal.
This routine adds various parameters required for WSGI, including HTTP_HOST, SERVER_NAME,
SERVER_PORT, REQUEST_METHOD, SCRIPT_NAME, PATH_INFO, and all of the PEP 3333-defined wsgi.
It only supplies default values, and does not replace any existing settings for these variables.
This routine is intended to make it easier for unit tests of WSGI servers and applications to set up dummy environ-
ments.
It should NOT be used by actual WSGI servers or applications, since the data is fake!
Example usage:
from wsgiref.util import setup_testing_defaults
from wsgiref.simple_server import make_server
# A relatively simple WSGI application.
In addition to the environment functions above, the wsgiref.util module also provides these miscellaneous utilities:
wsgiref.util.is_hop_by_hop(header_name)
Return True if ‘header_name’ is an HTTP/1.1 “Hop-by-Hop” header, as defined by RFC 2616.
A concrete implementation of the wsgiref.types.
FileWrapper protocol used to convert a file-like object
to an iterator.
The resulting objects are iterables.
As the object is iterated over, the optional blksize parameter will
be repeatedly passed to the filelike object’s read() method to obtain bytestrings to yield.
When read() returns
an empty bytestring, iteration is ended and is not resumable.
If filelike has a close() method, the returned object will also have a close() method, and it will invoke the
filelike object’s close() method when called.
StringIO("This is an example file-like object"*10)
wrapper = FileWrapper(filelike, blksize=5)
for chunk in wrapper:
print(chunk)
Changed in version 3.11: Support for __getitem__() method has been removed.
Headers([headers])
Create a mapping-like object wrapping headers, which must be a list of header name/value tuples as described in
PEP 3333.
The default value of headers is an empty list.
Headers objects support typical mapping operations including __getitem__(), get(), __setitem__(),
setdefault(), __delitem__() and __contains__().
For each of these methods, the key is the header
name (treated case-insensitively), and the value is the first value associated with that header name.
Setting a header
deletes any existing values for that header, then adds a new value at the end of the wrapped header list.
Headers’
existing order is generally maintained, with new headers added to the end of the wrapped list.
Unlike a dictionary, Headers objects do not raise an error when you try to get or delete a key that isn’t in the
wrapped header list.
Getting a nonexistent header just returns None, and deleting a nonexistent header does
nothing.
Headers objects also support keys(), values(), and items() methods.
The lists returned by keys() and
items() can include the same key more than once if there is a multi-valued header.
The len() of a Headers
object is the same as the length of its items(), which is the same as the length of the wrapped header list.
In
fact, the items() method just returns a copy of the wrapped header list.
Calling bytes() on a Headers object returns a formatted bytestring suitable for transmission as HTTP response
headers.
Each header is placed on a line with its value, separated by a colon and a space.
Each line is terminated
by a carriage return and line feed, and the bytestring is terminated with a blank line.
In addition to their mapping interface and formatting features, Headers objects also have the following methods
for querying and adding multi-valued headers, and for adding headers with MIME parameters:
get_all(name)
Return a list of all the values for the named header.
The returned list will be sorted in the order they appeared in the original header list or were added to this
instance, and may contain duplicates.
Any fields deleted and re-inserted are always appended to the header
list.
If no fields exist with the given name, returns an empty list.
Add a (possibly multi-valued) header, with optional MIME parameters specified via keyword arguments.
Keyword arguments can be used to set MIME parameters for the header field.
Each parameter must be a string or None.
Underscores in parameter names are converted to dashes, since
dashes are illegal in Python identifiers, but many MIME parameter names include dashes.
If the parameter
value is a string, it is added to the header value parameters in the form name="value".
If it is None, only
the parameter name is added.
Each server
instance serves a single WSGI application on a given host and port.
If you want to serve multiple applications on a single
host and port, you should create a WSGI application that parses PATH_INFO to select which application to invoke for
each request.
The return value is an instance
of the supplied server_class, and will process requests using the specified handler_class.
It’s useful for verifying that a WSGI
server (such as wsgiref.simple_server) is able to run a simple WSGI application correctly.
You do not normally need to call this constructor, as the make_server() function can handle all the details for
you.
WSGIServer
is
a
subclass
of
http.server.HTTPServer,
so
all
of
its
methods
(such
as
serve_forever() and handle_request()) are available.
WSGIServer also provides these WSGI-
specific methods:
set_app(application)
Sets the callable application as the WSGI application that will receive requests.
get_app()
Returns the currently set application callable.
Normally, however, you do not need to use these additional methods, as set_app() is normally called by
make_server(), and the get_app() exists mainly for the benefit of request handler instances.
WSGIRequestHandler(request, client_address, server)
Create an HTTP handler for the given request (i.e. a socket), client_address (a (host,port) tuple), and server
(WSGIServer instance).
You do not need to create instances of this class directly; they are automatically created as needed by WSGIServer
objects.
You can, however, subclass this class and supply it as a handler_class to the make_server() function.
Some possibly relevant methods for overriding in subclasses:
21.2.
The default implementation copies the contents
of the WSGIServer object’s base_environ dictionary attribute and then adds various headers derived
from the HTTP request.
Each call to this method should return a new dictionary containing all of the relevant
CGI environment variables as specified in PEP 3333.
The default implementation just returns
sys.stderr.
The default implementation creates a handler instance using a wsgiref.
When creating new WSGI application objects, frameworks, servers, or middleware, it can be useful to validate the new
code’s conformance using wsgiref.validate.
This module provides a function that creates WSGI application ob-
jects that validate communications between a WSGI server or gateway and a WSGI application object, to check both
sides for protocol conformance.
Note that this utility does not guarantee complete PEP 3333 compliance; an absence of errors from this module does not
necessarily mean that errors do not exist.
However, if this module does produce an error, then it is virtually certain that
either the server or application is not 100% compliant.
This module is based on the paste.lint module from Ian Bicking’s “Python Paste” library.
The returned application will forward all requests to
the original application, and will check that both the application and the server invoking it are conforming to the
WSGI specification and to RFC 2616.
Any detected nonconformance results in an AssertionError being raised; note, however, that how these er-
rors are handled is server-dependent.
For example, wsgiref.simple_server and other servers based on
wsgiref.handlers (that don’t override the error handling methods to do something else) will simply output
a message that an error has occurred, and dump the traceback to sys.stderr or some other error stream.
This wrapper may also generate output using the warnings module to indicate behaviors that are questionable
but which may not actually be prohibited by PEP 3333.
Unless they are suppressed using Python command-line
options or the warnings API, any such warnings will be written to sys.stderr (not wsgi.errors, unless
they happen to be the same object).
This module provides base handler classes for implementing WSGI servers and gateways.
These base classes handle most
of the work of communicating with a WSGI application, as long as they are given a CGI-like environment, along with
input, output, and error streams.
CGIHandler
CGI-based invocation via sys.stdin, sys.stdout, sys.stderr and os.environ.
This is useful when
you have a WSGI application and want to run it as a CGI script.
Simply invoke CGIHandler().run(app),
where app is the WSGI application object you wish to invoke.
This class is a subclass of BaseCGIHandler that sets wsgi.run_once to true, wsgi.multithread to
false, and wsgi.multiprocess to true, and always uses sys and os to obtain the necessary CGI streams and
environment.
A specialized alternative to CGIHandler, for use when deploying on Microsoft’s IIS web server, without having
set the config allowPathInfo option (IIS>=7) or metabase allowPathInfoForScriptMappings (IIS<7).
By default, IIS gives a PATH_INFO that duplicates the SCRIPT_NAME at the front, causing problems for WSGI
applications that wish to implement routing.
This handler strips any such duplicated path.
IIS can be configured to pass the correct PATH_INFO, but this causes another bug where PATH_TRANSLATED
is wrong.
Luckily this variable is rarely used and is not guaranteed by WSGI.
On IIS<7, though, the setting can
only be made on a vhost level, affecting all other script mappings, many of which break when exposed to the
PATH_TRANSLATED bug.
For this reason IIS<7 is almost never deployed with the fix (Even IIS7 rarely uses it
because there is still no UI for it.).
There is no way for CGI code to tell whether the option was set, so a separate handler class is provided.
It is used
in the same way as CGIHandler, i.e., by calling IISCGIHandler().run(app), where app is the WSGI
application object you wish to invoke.
New in version 3.2.
BaseCGIHandler(stdin, stdout, stderr, environ, multithread=True,
multiprocess=False)
Similar to CGIHandler, but instead of using the sys and os modules, the CGI environment and I/O streams
are specified explicitly.
The multithread and multiprocess values are used to set the wsgi.multithread and
wsgi.multiprocess flags for any applications run by the handler instance.
This class is a subclass of SimpleHandler intended for use with software other than HTTP “origin servers”.
If you are writing a gateway protocol implementation (such as CGI, FastCGI, SCGI, etc.) that uses a Status:
header to send an HTTP status, you probably want to subclass this instead of SimpleHandler.
SimpleHandler(stdin, stdout, stderr, environ, multithread=True,
multiprocess=False)
21.2.
If you are writing an HTTP server
implementation, you will probably want to subclass this instead of BaseCGIHandler.
This class is a subclass of BaseHandler.
It overrides the __init__(), get_stdin(), get_stderr(),
add_cgi_vars(), _write(), and _flush() methods to support explicitly setting the environment and
streams via the constructor.
The supplied environment and streams are stored in the stdin, stdout, stderr,
and environ attributes.
The write() method of stdout should write each chunk in full, like io.BufferedIOBase.
class wsgiref.handlers.
BaseHandler
This is an abstract base class for running WSGI applications.
Each instance will handle a single HTTP request,
although in principle you could create a subclass that was reusable for multiple requests.
Run the specified WSGI application, app.
All of the other BaseHandler methods are invoked by this method in the process of running the application,
and thus exist primarily to allow customizing the process.
Buffer the bytes data for transmission to the client.
It’s okay if this method actually transmits the data;
BaseHandler just separates write and flush operations for greater efficiency when the underlying system
actually has such a distinction.
It’s okay if this method is a no-op (i.e., if _write()
actually sends the data).
Here are some other methods and attributes you may wish to override.
This list is only a summary, however, and
does not include every method that can be overridden.
You should consult the docstrings and source code for
additional information before attempting to create a customized BaseHandler subclass.
Attributes and methods for customizing the WSGI environment:
wsgi_multithread
The value to be used for the wsgi.multithread environment variable.
It defaults to true in
BaseHandler, but may have a different default (or be set by the constructor) in the other subclasses.
It defaults to false in BaseHandler,
but CGIHandler sets it to true by default.
By default, this is
a copy of os.environ at the time that wsgiref.handlers was imported, but subclasses can either
create their own at the class or instance level.
Note that the dictionary should be considered read-only, since
the default value is shared between multiple classes and instances.
It is ignored for
handlers (such as BaseCGIHandler and CGIHandler) that are not HTTP origin servers.
Changed in version 3.3: The term “Python” is replaced with implementation specific term like “CPython”,
“Jython” etc.
get_scheme()
Return the URL scheme being used for the current request.
The default implementation uses the
guess_scheme() function from wsgiref.util to guess whether the scheme should be “http” or
“https”, based on the current request’s environ variables.
setup_environ()
Set the environ attribute to a fully populated WSGI environment.
The default implementation uses all of
the above methods and attributes, plus the get_stdin(), get_stderr(), and add_cgi_vars()
methods and the wsgi_file_wrapper attribute.
It also inserts a SERVER_SOFTWARE key if not
present, as long as the origin_server attribute is a true value and the server_software attribute
is set.
Methods and attributes for customizing exception handling:
log_exception(exc_info)
Log the exc_info tuple in the server log.
The default
implementation simply writes the traceback to the request’s wsgi.errors stream and flushes it.
Subclasses
can override this method to change the format or retarget the output, mail the traceback to an administrator,
or whatever other action may be deemed suitable.
If None, all frames are included.
This method is a WSGI application to generate an error page for the user.
It is only invoked if an error occurs
before headers are sent to the client.
This method can access the current error using sys.exception(), and should pass that information to
start_response when calling it (as described in the “Error Handling” section of PEP 3333).
The default implementation just uses the error_status, error_headers, and error_body at-
tributes to generate an output page.
Subclasses can override this to produce more dynamic error output.
Note, however, that it’s not recommended from a security perspective to spit out diagnostics to any old user;
ideally, you should have to do something special to enable diagnostic output, which is why the default imple-
mentation doesn’t include any.
This should be a status string as defined in PEP 3333; it defaults
to a 500 code and message.
This should be a list of WSGI response headers ((name,
value) tuples), as described in PEP 3333.
The default list just sets the content type to text/plain.
This should be an HTTP response body bytestring.
It defaults to the plain text, “A
server error occurred.
FileWrapper, or None.
The
default value of this attribute is the wsgiref.util.
FileWrapper class.
It should return
a true value if it was able to successfully transmit the file, so that the default transmission code will not be
executed.
The default implementation of this method just returns a false value.
This attribute’s default value is true in BaseHandler, but false in BaseCGIHandler and CGIHandler.
It defaults to "1.0".
This function is used by CGIHandler and IISCGIHandler in place of directly using os.environ, which is
not necessarily WSGI-compliant on all platforms and web servers using Python 3 – specifically, ones where the OS’s
actual environment is Unicode (i.e. Windows), or ones where the environment is bytes, but the system encoding
used by Python to decode it is anything other than ISO-8859-1 (e.g. Unix systems using UTF-8).
If you are implementing a CGI-based handler of your own, you probably want to use this routine instead of just
copying values out of os.environ directly.
New in version 3.2.
New in version 3.11.
StartResponse
A typing.
Protocol describing start_response() callables (PEP 3333).
wsgiref.types.
WSGIEnvironment
A type alias describing a WSGI environment dictionary.
WSGIApplication
A type alias describing a WSGI application callable.
InputStream
A typing.
Protocol describing a WSGI Input Stream.
ErrorStream
A typing.
Protocol describing a WSGI Error Stream.
FileWrapper
A typing.
Protocol describing a file wrapper.
See wsgiref.util.
FileWrapper for a concrete im-
plementation of this protocol.
Examples
This is a working “Hello World” WSGI application:
"""
Every WSGI application must have an application object - a callable
object that accepts two arguments.
For that purpose, we're going to
use a function (note that you're not limited to a function, you can
use a class for example).
The first argument passed to the function
is a dictionary containing CGI-style environment variables and the
second variable is the callable object.
Takes a path to serve from and an
optional port number (defaults to 8000), then tries to serve files.
MIME types are guessed from the file names, 404 errors are raised
if the file is not found.
Lib/urllib/request.py
The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP) in a complex
world — basic and digest authentication, redirections, cookies and more.
See also:
The Requests package is recommended for a higher-level HTTP client interface.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The urllib.request module defines the following functions:
urllib.request.urlopen(url, data=None[, timeout], *, cafile=None, capath=None, cadefault=False,
context=None)
Open url, which can be either a string containing a valid, properly encoded URL, or a Request object.
See
Request for details.
The optional timeout parameter specifies a timeout in seconds for blocking operations like the connection attempt
(if not specified, the global default timeout setting will be used).
This actually only works for HTTP, HTTPS and
FTP connections.
If context is specified, it must be a ssl.
SSLContext instance describing the various SSL options.
See
HTTPSConnection for more details.
The optional cafile and capath parameters specify a set of trusted CA certificates for HTTPS requests.
More information can be found in ssl.SSLContext.load_verify_locations().
The cadefault parameter is ignored.
This function always returns an object which can work as a context manager and has the properties url, headers,
and status.
See urllib.response.addinfourl for more detail on these properties.
For HTTP and HTTPS URLs, this function returns a http.client.
HTTPResponse object slightly modi-
fied.
In addition to the three new methods above, the msg attribute contains the same information as the reason
attribute — the reason phrase returned by server — instead of the response headers as it is specified in the docu-
mentation for HTTPResponse.
For FTP, file, and data URLs and requests explicitly handled by legacy URLopener and FancyURLopener
classes, this function returns a urllib.response.addinfourl object.
Raises URLError on protocol errors.
Note that None may be returned if no handler handles the request (though the default installed global
OpenerDirector uses UnknownHandler to ensure this never happens).
In addition, if proxy settings are detected (for example, when a *_proxy environment variable like http_proxy
is set), ProxyHandler is default installed and makes sure the requests are handled through the proxy.
The legacy urllib.urlopen function from Python 2.6 and earlier has been discontinued; urllib.
request.urlopen() corresponds to the old urllib2.urlopen.
Proxy handling, which was done by
passing a dictionary parameter to urllib.urlopen, can be obtained by using ProxyHandler objects.
The default opener raises an auditing event urllib.
Request with arguments fullurl, data, headers,
method taken from the request object.
Changed in version 3.2: cafile and capath were added.
Changed in version 3.2: HTTPS virtual hosts are now supported if possible (that is, if ssl.HAS_SNI is true).
New in version 3.2: data can be an iterable object.
Changed in version 3.3: cadefault was added.
Changed in version 3.4.3: context was added.
HTTPS connection now send an ALPN extension with protocol indicator http/1.1
when no context is given.
Custom context should set ALPN protocols with set_alpn_protocol().
Deprecated since version 3.6: cafile, capath and cadefault are deprecated in favor of context.
Please use ssl.
SSLContext.load_cert_chain() instead, or let ssl.create_default_context() select the
system’s trusted CA certificates for you.
urllib.request.install_opener(opener)
Install an OpenerDirector instance as the default global opener.
Installing an opener is only necessary if you
want urlopen to use that opener; otherwise, simply call OpenerDirector.open() instead of urlopen().
The code does not check for a real OpenerDirector, and any class with the appropriate interface will work.
Instances of the following classes will be in front of the
handlers, unless the handlers contain them, instances of them or subclasses of them:
ProxyHandler
(if proxy settings are detected), UnknownHandler, HTTPHandler, HTTPDefaultErrorHandler,
HTTPRedirectHandler, FTPHandler, FileHandler, HTTPErrorProcessor.
If the Python installation has SSL support (i.e., if the ssl module can be imported), HTTPSHandler will also
be added.
A BaseHandler subclass may also change its handler_order attribute to modify its position in the handlers
list.
urllib.request.pathname2url(path)
Convert the pathname path from the local syntax for a path to the form used in the path component of a URL.
This
does not produce a complete URL.
The return value will already be quoted using the quote() function.
urllib.request.url2pathname(path)
Convert the path component path from a percent-encoded URL to the local syntax for a path.
This does not accept
a complete URL.
This helper function returns a dictionary of scheme to proxy server URL mappings.
It scans the environment for
variables named <scheme>_proxy, in a case insensitive approach, for all operating systems first, and when it
cannot find it, looks for proxy information from System Configuration for macOS and Windows Systems Registry
for Windows.
If both lowercase and uppercase environment variables exist (and disagree), lowercase is preferred.
Note: If the environment variable REQUEST_METHOD is set, which usually indicates your script is running in a
CGI environment, the environment variable HTTP_PROXY (uppercase _PROXY) will be ignored.
This is because
that variable can be injected by a client using the “Proxy:” HTTP header.
The following classes are provided:
class urllib.request.
This class is an abstraction of a URL request.
Currently
HTTP requests are the only ones that use data.
The supported object types include bytes, file-like objects, and
iterables of bytes-like objects.
If no Content-Length nor Transfer-Encoding header field has been
provided, HTTPHandler will set these headers according to the type of data.
Content-Length will be used
to send bytes objects, while Transfer-Encoding: chunked as specified in RFC 7230, Section 3.3.1 will
be used to send files and other iterables.
For an HTTP POST request method,
data should be a buffer in the standard application/
x-www-form-urlencoded format.
The urllib.parse.urlencode() function takes a mapping or
sequence of 2-tuples and returns an ASCII string in this format.
It should be encoded to bytes before being used
as the data parameter.
This is often used to “spoof” the User-Agent header value, which is used by a browser to
identify itself – some HTTP servers only allow requests coming from common browsers as opposed to scripts.
Firefox/2.0.0.11", while urllib’s default user agent string is "Python-urllib/2.6"
(on Python 2.6).
All header keys are sent in camel case.
An appropriate Content-Type header should be included if the data argument is present.
If this header has not
been provided and data is not None, Content-Type: application/x-www-form-urlencoded will
be added as a default.
The next two arguments are only of interest for correct handling of third-party HTTP cookies:
origin_req_host should be the request-host of the origin transaction, as defined by RFC 2965.
It defaults to http.
cookiejar.request_host(self).
This is the host name or IP address of the original request that was
initiated by the user.
For example, if the request is for an image in an HTML document, this should be the request-
host of the request for the page containing the image.
unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.
It defaults to False.
An
unverifiable request is one whose URL the user did not have the option to approve.
For example, if the request is
for an image in an HTML document, and the user had no option to approve the automatic fetching of the image,
this should be true.
If provided, its
value is stored in the method attribute and is used by get_method().
The default is 'GET' if data is None
or 'POST' otherwise.
Subclasses may indicate a different default method by setting the method attribute in the
class itself.
Note:
The request will not work as expected if the data object is unable to deliver its content more than once
(e.g. a file or an iterable that can produce the content only once) and the request is retried for HTTP redirects
or authentication.
The data is sent to the HTTP server right away after the headers.
There is no support for a
100-continue expectation in the library.
Changed in version 3.3: Request.method argument is added to the Request class.
Default Request.method may be indicated at the class level.
Changed in version 3.6: Do not raise an error if the Content-Length has not been provided and data is neither
None nor a bytes object.
Fall back to use chunked transfer encoding instead.
OpenerDirector
The OpenerDirector class opens URLs via BaseHandlers chained together.
It manages the chaining of
handlers, and recovery from errors.
BaseHandler
This is the base class for all registered handlers — and handles only the simple mechanics of registration.
A class which defines a default handler for HTTP error responses; all responses are turned into HTTPError
exceptions.
HTTPRedirectHandler
A class to handle redirections.
HTTPCookieProcessor(cookiejar=None)
A class to handle HTTP Cookies.
ProxyHandler(proxies=None)
Cause requests to go through a proxy.
If proxies is given, it must be a dictionary mapping protocol names to
URLs of proxies.
The default is to read the list of proxies from the environment variables <protocol>_proxy.
If no proxy environment variables are set, then in a Windows environment proxy settings are obtained from the
registry’s Internet Settings section, and in a macOS environment proxy information is retrieved from the System
Configuration Framework.
To disable autodetected proxy pass an empty dictionary.
The no_proxy environment variable can be used to specify hosts which shouldn’t be reached via proxy; if set,
it should be a comma-separated list of hostname suffixes, optionally with :port appended, for example cern.
Note:
HTTP_PROXY will be ignored if a variable REQUEST_METHOD is set; see the documentation on
getproxies().
HTTPPasswordMgr
Keep a database of (realm, uri) -> (user, password) mappings.
HTTPPasswordMgrWithDefaultRealm
Keep a database of (realm, uri) -> (user, password) mappings.
A realm of None is considered a
catch-all realm, which is searched if no other realm fits.
HTTPPasswordMgrWithPriorAuth
A
variant
of
HTTPPasswordMgrWithDefaultRealm
that
also
has
a
database
of
uri ->
is_authenticated mappings.
Can be used by a BasicAuth handler to determine when to send authentication
credentials immediately instead of waiting for a 401 response first.
New in version 3.5.
This is a mixin class that helps with HTTP authentication, both to the remote host and to a proxy.
If is_authenticated returns True for the URI, credentials are sent.
If
is_authenticated is False, credentials are not sent, and then if a 401 response is received the request
is re-sent with the authentication credentials.
If authentication succeeds, update_authenticated is called
to set is_authenticated True for the URI, so that subsequent requests to the URI or any of its super-URIs
will automatically include the authentication credentials.
New in version 3.5: Added is_authenticated support.
HTTPBasicAuthHandler(password_mgr=None)
Handle authentication with the remote host.
HTTPBasicAuthHandler will raise a ValueError when presented with a wrong Authentication
scheme.
ProxyBasicAuthHandler(password_mgr=None)
Handle authentication with the proxy.
This is a mixin class that helps with HTTP authentication, both to the remote host and to a proxy.
HTTPDigestAuthHandler(password_mgr=None)
Handle authentication with the remote host.
When both Digest Authentication Handler and Basic Authentication Handler are both added, Digest
Authentication is always tried first.
If the Digest Authentication returns a 40x response again, it is sent to Ba-
sic Authentication handler to Handle.
This Handler method will raise a ValueError when presented with an
authentication scheme other than Digest or Basic.
Raise ValueError on unsupported Authentication Scheme.
ProxyDigestAuthHandler(password_mgr=None)
Handle authentication with the proxy.
HTTPHandler
A class to handle opening of HTTP URLs.
A class to handle opening of HTTPS URLs.
HTTPSConnection.
Changed in version 3.2: context and check_hostname were added.
FileHandler
Open local files.
DataHandler
Open data URLs.
New in version 3.4.
21.4.
FTPHandler
Open FTP URLs.
CacheFTPHandler
Open FTP URLs, keeping a cache of open FTP connections to minimize delays.
UnknownHandler
A catch-all class to handle unknown URLs.
HTTPErrorProcessor
Process HTTP error responses.
The following methods describe Request’s public interface, and so all may be overridden in subclasses.
It also defines
several public attributes that can be used by clients to inspect the parsed request.
Request.full_url
The original URL passed to the constructor.
Changed in version 3.4.
Request.full_url is a property with setter, getter and a deleter.
Getting full_url returns the original request
URL with the fragment, if it was present.
Request.type
The URI scheme.
Request.host
The URI authority, typically a host, but may also contain a port separated by a colon.
Request.origin_req_host
The original host for the request, without port.
Request.selector
The URI path.
If the Request uses a proxy, then selector will be the full URL that is passed to the proxy.
The entity body for the request, or None if not specified.
Changing value of Request.data now deletes “Content-Length” header if it was
previously set or calculated.
Request.unverifiable
boolean, indicates whether the request is unverifiable as defined by RFC 2965.
Request.method
The HTTP request method to use.
By default its value is None, which means that get_method() will do its
normal computation of the method to be used.
New in version 3.3.
Changed in version 3.4: A default value can now be set in subclasses; previously it could only be set via the
constructor argument.
If Request.method is not None, return its value, oth-
erwise return 'GET' if Request.data is None, or 'POST' if it’s not.
This is only meaningful for HTTP
requests.
Changed in version 3.3: get_method now looks at the value of Request.method.
Request.add_header(key, val)
Add another header to the request.
Headers are currently ignored by all handlers except HTTP handlers, where
they are added to the list of headers sent to the server.
Note that there cannot be more than one header with the
same name, and later calls will overwrite previous calls in case the key collides.
Currently, this is no loss of HTTP
functionality, since all headers which have meaning when used more than once have a (header-specific) way of
gaining the same functionality using only one header.
Note that headers added using this method are also added to
redirected requests.
Request.add_unredirected_header(key, header)
Add a header that will not be added to a redirected request.
Request.has_header(header)
Return whether the instance has the named header (checks both regular and unredirected).
Remove named header from the request instance (both from regular and unredirected headers).
New in version 3.4.
Request.get_full_url()
Return the URL given in the constructor.
Changed in version 3.4.
Returns Request.full_url
Request.set_proxy(host, type)
Prepare the request by connecting to a proxy server.
The host and type will replace those of the instance, and the
instance’s selector will be the original URL given in the constructor.
Request.get_header(header_name, default=None)
Return the value of the given header.
If the header is not present, return the default value.
Request.header_items()
Return a list of tuples (header_name, header_value) of the Request headers.
The request methods add_data, has_data, get_data, get_type, get_host, get_selector,
get_origin_req_host and is_unverifiable that were deprecated since 3.3 have been removed.
OpenerDirector Objects
OpenerDirector instances have the following methods:
OpenerDirector.add_handler(handler)
handler should be an instance of BaseHandler.
The following methods are searched, and added to the possible
chains (note that HTTP errors are a special case).
Note that, in the following, protocol should be replaced with the
actual protocol to handle, for example http_response() would be the HTTP protocol response handler.
Also
type should be replaced with the actual HTTP code, for example http_error_404() would handle HTTP 404
errors.
See BaseHandler.<protocol>_open() for more information.
See BaseHandler.http_error_<nnn>() for more information.
See BaseHandler.<protocol>_request() for more information.
See BaseHandler.<protocol>_response() for more information.
OpenerDirector.open(url, data=None[, timeout])
Open the given url (which can be a request object or a string), optionally passing the given data.
Arguments, return
values and exceptions raised are the same as those of urlopen() (which simply calls the open() method on the
currently installed global OpenerDirector).
The optional timeout parameter specifies a timeout in seconds for
blocking operations like the connection attempt (if not specified, the global default timeout setting will be used).
The timeout feature actually works only for HTTP, HTTPS and FTP connections.
OpenerDirector.error(proto, *args)
Handle an error of the given protocol.
This will call the registered error handlers for the given protocol with the
given arguments (which are protocol specific).
Return values and exceptions raised are the same as those of urlopen().
OpenerDirector objects open URLs in three stages:
The order in which these methods are called within each stage is determined by sorting the handler instances.
Every handler with a method named like <protocol>_request() has that method called to pre-process the
request.
Handlers with a method named like <protocol>_open() are called to handle the request.
This stage ends
when a handler either returns a non-None value (ie.
Exceptions are allowed to propagate.
In fact, the above algorithm is first tried for methods named default_open().
If all such methods return None,
the algorithm is repeated for methods named like <protocol>_open().
If all such methods return None, the
algorithm is repeated for methods named unknown_open().
Note that the implementation of these methods may involve calls of the parent OpenerDirector instance’s
open() and error() methods.
These are intended for direct use:
BaseHandler.add_parent(director)
Add a director as parent.
BaseHandler.close()
Remove any parents.
The following attribute and methods should only be used by classes derived from BaseHandler.
Note:
The
convention
has
been
adopted
that
subclasses
defining
<protocol>_request()
or
<protocol>_response() methods are named *Processor; all others are named *Handler.
BaseHandler.parent
A valid OpenerDirector, which can be used to open using a different protocol, or handle errors.
This method is not defined in BaseHandler, but subclasses should define it if they want to catch all URLs.
This method, if implemented, will be called by the parent OpenerDirector.
It should return a file-like ob-
ject as described in the return value of the open() method of OpenerDirector, or None.
It should raise
URLError, unless a truly exceptional thing happens (for example, MemoryError should not be mapped to
URLError).
This method will be called before any protocol-specific open method.
This method is not defined in BaseHandler, but subclasses should define it if they want to handle URLs with
the given protocol.
This method, if defined, will be called by the parent OpenerDirector.
This method is not defined in BaseHandler, but subclasses should define it if they want to catch all URLs with
no specific registered handler to open it.
This method, if implemented, will be called by the parent OpenerDirector.
Return values should be the
same as for default_open().
This method is not defined in BaseHandler, but subclasses should override it if they intend to provide a catch-all
for otherwise unhandled HTTP errors.
It will be called automatically by the OpenerDirector getting the error,
and should not normally be called in other circumstances.
Return values and exceptions raised should be the same as those of urlopen().
BaseHandler.http_error_<nnn>(req, fp, code, msg, hdrs)
nnn should be a three-digit HTTP error code.
This method is also not defined in BaseHandler, but will be
called, if it exists, on an instance of a subclass, when an HTTP error with code nnn occurs.
Subclasses should override this method to handle specific HTTP errors.
21.4.
This method is not defined in BaseHandler, but subclasses should define it if they want to pre-process requests
of the given protocol.
This method, if defined, will be called by the parent OpenerDirector.
This method is not defined in BaseHandler, but subclasses should define it if they want to post-process responses
of the given protocol.
This method, if defined, will be called by the parent OpenerDirector.
The return value should
implement the same interface as the return value of urlopen().
If this is the case, HTTPError is raised.
See RFC 2616 for details of the precise meanings of the various redirection codes.
An HTTPError exception raised as a security consideration if the HTTPRedirectHandler is presented with a redirected
URL which is not an HTTP, HTTPS or FTP URL.
HTTPRedirectHandler.redirect_request(req, fp, code, msg, hdrs, newurl)
Return a Request or None in response to a redirect.
This is called by the default implementations of the
http_error_30*() methods when a redirection is received from the server.
If a redirection should take
place, return a new Request to allow http_error_30*() to perform the redirect to newurl.
Otherwise,
raise HTTPError if no other handler should try to handle this URL, or return None if you can’t but another
handler might.
Note:
The default implementation of this method does not strictly follow RFC 2616, which says that 301 and
302 responses to POST requests must not be automatically redirected without confirmation by the user.
In re-
ality, browsers do allow automatic redirection of these responses, changing the POST to a GET, and the default
implementation reproduces this behavior.
HTTPRedirectHandler.http_error_301(req, fp, code, msg, hdrs)
Redirect to the Location: or URI: URL.
This method is called by the parent OpenerDirector when
getting an HTTP ‘moved permanently’ response.
The same as http_error_301(), but called for the ‘found’ response.
The same as http_error_301(), but called for the ‘temporary redirect’ response.
The same as http_error_301(), but called for the ‘permanent redirect’ response.
It does not allow changing
the request method from POST to GET.
New in version 3.11.
CookieJar in which cookies are stored.
The ProxyHandler will have a method <protocol>_open() for every protocol which has a proxy in the
proxies dictionary given in the constructor.
The method will modify requests to go through the proxy, by calling
request.set_proxy(), and call the next handler in the chain to actually execute the protocol.
HTTPPasswordMgr.add_password(realm, uri, user, passwd)
uri can be either a single URI, or a sequence of URIs.
This causes (user,
passwd) to be used as authentication tokens when authentication for realm and a super-URI of any of the given
URIs is given.
HTTPPasswordMgr.find_user_password(realm, authuri)
Get user/password for given realm and URI, if any.
This method will return (None, None) if there is no
matching user/password.
For HTTPPasswordMgrWithDefaultRealm objects, the realm None will be searched if the given realm
has no matching user/password.
If is_authenticated is specified as True,
realm is ignored.
HTTPPasswordMgrWithPriorAuth.find_user_password(realm, authuri)
Same as for HTTPPasswordMgrWithDefaultRealm objects
HTTPPasswordMgrWithPriorAuth.update_authenticated(self, uri, is_authenticated=False)
Update the is_authenticated flag for the given uri or list of URIs.
AbstractBasicAuthHandler Objects
AbstractBasicAuthHandler.http_error_auth_reqed(authreq, host, req, headers)
Handle an authentication request by getting a user/password pair, and re-trying the request.
In either case, the authority must not contain a userinfo component (so, "python.org"
and "python.org:80" are fine, "joe:password@python.org" is not).
Retry the request with authentication information, if available.
This method is applicable only for local hostnames.
When a remote hostname is given, an
URLError is raised.
This kind of URL contains the content encoded in the URL itself.
The data URL syntax is
specified in RFC 2397.
This implementation ignores white spaces in base64 encoded data URLs so the URL may
be wrapped in whatever source file it comes from.
But even though some browsers don’t mind about a missing
padding at the end of a base64 encoded data URL, this implementation will raise an ValueError in that case.
The login is always done with empty username and password.
CacheFTPHandler.setMaxConns(m)
Set maximum number of cached connections to m.
21.4.
Process HTTP error responses.
For 200 error codes, the response object is returned immediately.
For non-200 error codes, this simply passes the job on to the http_error_<type>() handler methods, via
OpenerDirector.error().
Eventually, HTTPDefaultErrorHandler will raise an HTTPError if
no other handler handles the error.
HTTPErrorProcessor.https_response(request, response)
Process HTTPS error responses.
The behavior is same as http_response().
In addition to the examples below, more examples are given in urllib-howto.
This example gets the python.org main page and displays the first 300 bytes of it.

print(f.read(300))

b'<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">\n\n\n<html
xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">\n\n<head>\n
<meta http-equiv="content-type" content="text/html; charset=utf-8" />\n
<title>Python Programming '
Note that urlopen returns a bytes object.
This is because there is no way for urlopen to automatically determine the
encoding of the byte stream it receives from the HTTP server.
In general, a program will decode the returned bytes
object to string once it determines or guesses the appropriate encoding.
The following W3C document, https://www.w3.org/International/O-charset, lists the various ways in which an (X)HTML
or an XML document could have specified its encoding information.
As the python.org website uses utf-8 encoding as specified in its meta tag, we will use the same for decoding the bytes
object.
Here is an example of doing a PUT request using Request:
import urllib.request
DATA = b'some data'
req = urllib.request.
By default, ProxyHandler
uses the environment variables named <scheme>_proxy, where <scheme> is the URL scheme involved.
For ex-
ample, the http_proxy environment variable is read to obtain the HTTP proxy’s URL.
This example replaces the default ProxyHandler with one that uses programmatically supplied proxy URLs, and adds
proxy authorization support with ProxyBasicAuthHandler.
ProxyBasicAuthHandler()
(continues on next page)
21.4.
OpenerDirector automatically adds a User-Agent header to every Request.
Also, remember that a few standard headers (Content-Length, Content-Type and Host) are added when the
Request is passed to urlopen() (or OpenerDirector.open()).
The following example uses the POST method instead.
They
might become deprecated at some point in the future.
If the URL points to a local file, the object will not be
copied unless filename is supplied.
Return a tuple (filename, headers) where filename is the local file
name under which the object can be found, and headers is whatever the info() method of the object returned by
urlopen() returned (for a remote object).
Exceptions are the same as for urlopen().
The second argument, if present, specifies the file location to copy to (if absent, the location will be a tempfile with
a generated name).
The third argument, if present, is a callable that will be called once on establishment of the
network connection and once after each block read thereafter.
The callable will be passed three arguments; a count
of blocks transferred so far, a block size in bytes, and the total size of the file.
The third argument may be -1 on
older FTP servers which do not return a file size in response to a retrieval request.
The following example illustrates the most common usage scenario:
If the url uses the http: scheme identifier, the optional data argument may be given to specify a POST request
(normally the request type is GET).
The data argument must be a bytes object in standard application/
x-www-form-urlencoded format; see the urllib.parse.urlencode() function.
This can occur, for example,
when the download is interrupted.
The Content-Length is treated as a lower bound: if there’s more data to read, urlretrieve reads more data, but if less
data is available, it raises the exception.
You can still retrieve the downloaded data in this case, it is stored in the content attribute of the exception
instance.
If no Content-Length header was supplied, urlretrieve can not check the size of the data it has downloaded, and just
returns it.
In this case you just have to assume that the download was successful.
URLopener(proxies=None, **x509)
Deprecated since version 3.3.
Base class for opening and reading URLs.
Unless you need to support opening objects using schemes other than
http:, ftp:, or file:, you probably want to use FancyURLopener.
By default, the URLopener class sends a User-Agent header of urllib/VVV, where VVV is the urllib
version number.
Applications can define their own User-Agent header by subclassing URLopener or
21.4.
The optional proxies parameter should be a dictionary mapping scheme names to proxy URLs, where an empty
dictionary turns proxies off completely.
Its default value is None, in which case environmental proxy settings will
be used if present, as discussed in the definition of urlopen(), above.
Additional keyword parameters, collected in x509, may be used for authentication of the client when using the
https: scheme.
The keywords key_file and cert_file are supported to provide an SSL key and certificate; both
are needed to support client authentication.
URLopener objects will raise an OSError exception if the server returns an error code.
open(fullurl, data=None)
Open fullurl using the appropriate protocol.
This method sets up cache and proxy information, then calls the
appropriate open method with its input arguments.
If the scheme is not recognized, open_unknown() is
called.
The data argument has the same meaning as the data argument of urlopen().
This method always quotes fullurl using quote().
The return value is a tuple consisting of a local filename
and either an email.message.
Message object containing the response headers (for remote URLs) or
None (for local URLs).
The caller must then open and read the contents of filename.
If filename is not given
and the URL refers to a local file, the input filename is returned.
If the URL is non-local and filename is
not given, the filename is the output of tempfile.mktemp() with a suffix that matches the suffix of the
last path component of the input URL.
If reporthook is given, it must be a function accepting three numeric
parameters: A chunk number, the maximum size chunks are read in and the total size of the download (-1 if
unknown).
It will be called once at the start and after each chunk of data is read from the network.
If the url uses the http: scheme identifier, the optional data argument may be given to specify a
POST request (normally the request type is GET).
The data argument must in standard application/
x-www-form-urlencoded format; see the urllib.parse.urlencode() function.
To get urllib to tell servers that it is a particular
user agent, set this in a subclass as a class variable or in the constructor before calling the base constructor.
FancyURLopener subclasses URLopener providing default handling for the following HTTP response codes:
301, 302, 303, 307 and 401.
For the 30x response codes listed above, the Location header is used to fetch the
actual URL.
For 401 response codes (authentication required), basic HTTP authentication is performed.
For the
30x response codes, recursion is bounded by the value of the maxtries attribute, which defaults to 10.
For all other response codes, the method http_error_default() is called which you can override in sub-
classes to handle the error appropriately.
Note: According to the letter of RFC 2616, 301 and 302 responses to POST requests must not be automatically
redirected without confirmation by the user.
In reality, browsers do allow automatic redirection of these responses,
changing the POST to a GET, and urllib reproduces this behaviour.
When
performing
basic
authentication,
a
FancyURLopener
instance
calls
its
prompt_user_passwd() method.
The default implementation asks the users for the required infor-
mation on the controlling terminal.
A subclass may override this method to support more appropriate behavior if
needed.
The FancyURLopener class offers one additional method that should be overloaded to provide the appropriate
behavior:
prompt_user_passwd(host, realm)
Return information needed to authenticate the user at the given host in the specified security realm.
The implementation prompts for this information on the terminal; an application should override this method
to use an appropriate interaction model in the local environment.
Changed in version 3.4: Added support for data URLs.
The caching feature of urlretrieve() has been disabled until someone finds the time to hack proper processing
of Expiration time headers.
This can sometimes cause confusing error messages.
The urlopen() and urlretrieve() functions can cause arbitrarily long delays while waiting for a network
connection to be set up.
This means that it is difficult to build an interactive web client using these functions without
using threads.
The data returned by urlopen() or urlretrieve() is the raw data returned by the server.
This may be
binary data (such as an image), plain text or (for example) HTML.
The HTTP protocol provides type information
in the reply header, which can be inspected by looking at the Content-Type header.
If the returned data is
HTML, you can use the module html.parser to parse it.
The code handling the FTP protocol cannot differentiate between a file and a directory.
This can lead to unexpected
behavior when attempting to read a URL that points to a file that is not accessible.
If the URL ends in a /, it is
assumed to refer to a directory and will be handled accordingly.
But if an attempt to read a file leads to a 550 error
(meaning the URL cannot be found or is not accessible, often for permission reasons), then the path is treated as a
directory in order to handle the case when a directory is specified by a URL but the trailing / has been left off.
This
can cause misleading results when you try to fetch a file whose read permissions make it inaccessible; the FTP code
will try to read it, fail with a 550 error, and then perform a directory listing for the unreadable file.
If fine-grained
control is needed, consider using the ftplib module, subclassing FancyURLopener, or changing _urlopener
to meet your needs.
Functions defined by this module are used internally by the urllib.request module.
The typical response object is a urllib.response.addinfourl instance:
class urllib.response.addinfourl
url
URL of the resource retrieved, commonly used to determine if a redirect was followed.
Status code returned by server.
The module has been designed to match the internet RFC on Relative Uniform Resource Locators.
It supports the fol-
lowing URL schemes: file, ftp, gopher, hdl, http, https, imap, mailto, mms, news, nntp, prospero,
rsync, rtsp, rtsps, rtspu, sftp, shttp, sip, sips, snews, svn, svn+ssh, telnet, wais, ws, wss.
The urllib.parse module defines functions that fall into two broad categories: URL parsing and URL quoting.
These are covered in detail in the following sections.
This corresponds to the general structure of a
URL: scheme://netloc/path;parameters?query#fragment.
Each tuple item is a string, possibly
empty.
The components are not broken up into smaller parts (for example, the network location is a single string),
and % escapes are not expanded.
The delimiters as shown above are not part of the result, except for a leading
slash in the path component, which is retained if present.
For example:
ParseResult(scheme='scheme', netloc='netloc', path='/path;parameters', params='',
query='query', fragment='fragment')

"highlight=params#url-parsing")
ParseResult(scheme='http', netloc='docs.python.org:80',
path='/3/library/urllib.parse.html', params='',
query='highlight=params', fragment='url-parsing')
'http'
'docs.python.org:80'
'docs.python.org'

'http://docs.python.org:80/3/library/urllib.parse.html?highlight=params'
Following the syntax specifications in RFC 1808, urlparse recognizes a netloc only if it is properly introduced by
‘//’.
Otherwise the input is presumed to be a relative URL and thus to start with a path component.
The scheme argument gives the default addressing scheme, to be used only if the URL does not specify one.
It
should be the same type (text or bytes) as urlstring, except that the default value '' is always allowed, and is
automatically converted to b'' if appropriate.
If the allow_fragments argument is false, fragment identifiers are not recognized.
Instead, they are parsed as part
of the path, parameters or query component, and fragment is set to the empty string in the return value.
See section
Structured Parse Results for more information on the result object.
Unmatched square brackets in the netloc attribute will raise a ValueError.
Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding)
into any of /, ?, #, @, or : will raise a ValueError.
If the URL is decomposed before parsing, no error will
be raised.
As is the case with all named tuples, the subclass has a few additional methods and attributes that are particularly
useful.
One such method is _replace().
Warning: urlparse() does not perform validation.
See URL parsing security for details.
Changed in version 3.2: Added IPv6 URL parsing capabilities.
Changed in version 3.3: The fragment is now parsed for all URL schemes (unless allow_fragment is false), in
accordance with RFC 3986.
Previously, an allowlist of schemes that support fragments existed.
Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.
Changed in version 3.8:
Characters that affect netloc parsing under NFKC normalization will now raise
ValueError.
Data are returned as a dictionary.
The dictionary keys are the unique query variable names and the values are lists
of values for each name.
The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries
should be treated as blank strings.
A true value indicates that blanks should be retained as blank strings.
The optional argument strict_parsing is a flag indicating what to do with parsing errors.
If false (the default), errors
are silently ignored.
If true, errors raise a ValueError exception.
The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode char-
acters, as accepted by the bytes.decode() method.
The optional argument max_num_fields is the maximum number of fields to read.
If set, then throws a
ValueError if there are more than max_num_fields fields read.
The optional argument separator is the symbol to use for separating the query arguments.
It defaults to &.
Use the urllib.parse.urlencode() function (with the doseq parameter set to True) to convert such
dictionaries into query strings.
Add encoding and errors parameters.
Changed in version 3.8: Added max_num_fields parameter.
Changed in version 3.10: Added separator parameter with the default value of &.
Python versions earlier than
Python 3.10 allowed using both ; and & as query parameter separator.
This has been changed to allow only a
single separator key, with & as the default separator.
Data are returned as a list of name, value pairs.
The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries
should be treated as blank strings.
A true value indicates that blanks should be retained as blank strings.
The
default false value indicates that blank values are to be ignored and treated as if they were not included.
The optional argument strict_parsing is a flag indicating what to do with parsing errors.
If false (the default), errors
are silently ignored.
If true, errors raise a ValueError exception.
The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode char-
acters, as accepted by the bytes.decode() method.
The optional argument max_num_fields is the maximum number of fields to read.
If set, then throws a
ValueError if there are more than max_num_fields fields read.
The optional argument separator is the symbol to use for separating the query arguments.
It defaults to &.
Use the urllib.parse.urlencode() function to convert such lists of pairs into query strings.
Add encoding and errors parameters.
Changed in version 3.8: Added max_num_fields parameter.
Changed in version 3.10: Added separator parameter with the default value of &.
Python versions earlier than
Python 3.10 allowed using both ; and & as query parameter separator.
This has been changed to allow only a
single separator key, with & as the default separator.
The parts argument can be any six-item iterable.
This may result in a slightly different, but equivalent URL, if the URL that was parsed originally had unnecessary
delimiters (for example, a ?
This is similar to urlparse(), but does not split the params from the URL.
This should generally be used instead
of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path
portion of the URL (see RFC 2396) is wanted.
A separate function is needed to separate the path segments and
parameters.
Network location part
empty string
path

Hierarchical path
empty string
query

Query component
empty string
fragment

Fragment identifier
empty string
username
User name
None
password
Password
None
hostname
Host name (lower case)
None
port
Port number as integer, if present
None
Reading the port attribute will raise a ValueError if an invalid port is specified in the URL.
See section
Structured Parse Results for more information on the result object.
Unmatched square brackets in the netloc attribute will raise a ValueError.
Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding)
into any of /, ?, #, @, or : will raise a ValueError.
If the URL is decomposed before parsing, no error will
be raised.
Following some of the WHATWG spec that updates RFC 3986, leading C0 control and space characters are
stripped from the URL.
Warning: urlsplit() does not perform validation.
See URL parsing security for details.
Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.
Changed in version 3.8:
Characters that affect netloc parsing under NFKC normalization will now raise
ValueError.
ASCII newline and tab characters are stripped from the URL.
Changed in version 3.12: Leading WHATWG C0 control and space characters are stripped from the URL.
The parts argument
can be any five-item iterable.
This may result in a slightly different, but equivalent URL, if the URL that was
parsed originally had unnecessary delimiters (for example, a ?
Informally, this uses
components of the base URL, in particular the addressing scheme, the network location and (part of) the path, to
provide missing components in the relative URL.
The allow_fragments argument has the same meaning and default as for urlparse().
For example:

'//www.python.org/%7Eguido')
'http://www.python.org/%7Eguido'
If you do not want that behavior, preprocess the url with urlsplit() and urlunsplit(), removing possible
scheme and netloc parts.
If url contains a fragment identifier, return a modified version of url with no fragment identifier, and the fragment
identifier as a separate string.
If there is no fragment identifier in url, return url unmodified and an empty string.
The return value is a named tuple, its items can be accessed by index or as named attributes:
Attribute
Index
Value
Value if not present
url

URL with no fragment
empty string
fragment

Fragment identifier
empty string
See section Structured Parse Results for more information on the result object.
Changed in version 3.2: Result is a structured object rather than a simple 2-tuple.
If url is not a
wrapped URL, it is returned without changes.
They may not raise errors on inputs
that other applications consider invalid.
They may also succeed on some inputs that might not be considered URLs
elsewhere.
Their purpose is for practical functionality rather than purity.
Instead of raising an exception on unusual input, they may instead return some component parts as empty strings.
Or
components may contain more than perhaps they should.
We recommend that users of these APIs where the values may be used anywhere with security implications code de-
fensively.
Do some verification within your code before trusting a returned component part.
Does that scheme make
sense?
Is that a sensible path?
Is there anything strange about that hostname?
What constitutes a URL is not universally well defined.
Different applications have different needs and desired constraints.
For instance the living WHATWG spec describes what user facing web clients such as a web browser require.
While RFC
3986 is more general.
These functions incorporate some aspects of both, but cannot be claimed compliant with either.
The APIs and existing user code with expectations on specific behaviors predate both standards leading us to be very
cautious about making API behavior changes.
Parsing ASCII Encoded Bytes
The URL parsing functions were originally designed to operate on character strings only.
In practice, it is useful to be able
to manipulate properly quoted and encoded URLs as sequences of ASCII bytes.
Accordingly, the URL parsing functions
in this module all operate on bytes and bytearray objects in addition to str objects.
If str data is passed in, the result will also contain only str data.
If bytes or bytearray data is passed in, the
result will contain only bytes data.
Attempting to mix str data with bytes or bytearray in a single function call will result in a TypeError being
raised, while attempting to pass in non-ASCII byte values will trigger UnicodeDecodeError.
To support easier conversion of result objects between str and bytes, all return values from URL parsing functions
provide either an encode() method (when the result contains str data) or a decode() method (when the result
contains bytes data).
The signatures of these methods match those of the corresponding str and bytes methods
(except that the default encoding is 'ascii' rather than 'utf-8').
Each produces a value of a corresponding type
that contains either bytes data (for encode() methods) or str data (for decode() methods).
Applications that need to operate on potentially improperly quoted URLs that may contain non-ASCII data will need to
do their own decoding from bytes to characters before invoking the URL parsing methods.
The behaviour described in this section applies only to the URL parsing functions.
The URL quoting functions use their
own rules when producing or consuming byte sequences as detailed in the documentation of the individual URL quoting
functions.
Structured Parse Results
The result objects from the urlparse(), urlsplit() and urldefrag() functions are subclasses of the tuple
type.
These subclasses add the attributes listed in the documentation for those functions, the encoding and decoding
support described in the previous section, as well as an additional method:
urllib.parse.SplitResult.geturl()
Return the re-combined version of the original URL as a string.
This may differ from the original URL in that the
scheme may be normalized to lower case and empty components may be dropped.
Specifically, empty parameters,
queries, and fragment identifiers will be removed.
For urldefrag() results, only empty fragment identifiers will be removed.
For urlsplit() and
urlparse() results, all noted changes will be made to the URL returned by this method.
The result of this method remains unchanged if passed back through the original parsing function:
'http://www.Python.org/doc/'
'http://www.Python.org/doc/'
The following classes provide the implementations of the structured parse results when operating on str objects:
class urllib.parse.
DefragResult(url, fragment)
Concrete class for urldefrag() results containing str data.
The encode() method returns a
DefragResultBytes instance.
ParseResult(scheme, netloc, path, params, query, fragment)
Concrete class for urlparse() results containing str data.
The encode() method returns a
ParseResultBytes instance.
SplitResult(scheme, netloc, path, query, fragment)
Concrete class for urlsplit() results containing str data.
The encode() method returns a
SplitResultBytes instance.
The following classes provide the implementations of the parse results when operating on bytes or bytearray objects:
class urllib.parse.
DefragResultBytes(url, fragment)
Concrete class for urldefrag() results containing bytes data.
The decode() method returns a
DefragResult instance.
New in version 3.2.
ParseResultBytes(scheme, netloc, path, params, query, fragment)
Concrete class for urlparse() results containing bytes data.
The decode() method returns a
ParseResult instance.
New in version 3.2.
SplitResultBytes(scheme, netloc, path, query, fragment)
Concrete class for urlsplit() results containing bytes data.
The decode() method returns a
SplitResult instance.
New in version 3.2.
They also support reversing these operations to recreate
the original data from the contents of a URL component if that task isn’t already covered by the URL parsing functions
above.
Letters, digits, and the characters '_.-~' are never
quoted.
By default, this function is intended for quoting the path section of a URL.
The optional safe parameter
specifies additional ASCII characters that should not be quoted — its default value is '/'.
Moved from RFC 2396 to RFC 3986 for quoting URL strings.
The optional encoding and errors parameters specify how to deal with non-ASCII characters, as accepted by the
str.encode() method.
Note
that
quote(string, safe, encoding, errors)
is
equivalent
to
quote_from_bytes(string.encode(encoding, errors), safe).
Example: quote('/El Niño/') yields '/El%20Ni%C3%B1o/'.
Plus signs in the original string are escaped unless they are included in safe.
It
also does not have safe default to '/'.
Example: quote_plus('/El Niño/') yields '%2FEl+Ni%C3%B1o%2F'.
Like quote(), but accepts a bytes object rather than a str, and does not perform string-to-bytes encoding.
Example: quote_from_bytes(b'a&\xef') yields 'a%26%EF'.
Replace %xx escapes with their single-character equivalent.
The optional encoding and errors parameters spec-
ify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode()
method.
Example: unquote('/El%20Ni%C3%B1o/') yields '/El Niño/'.
Changed in version 3.9: string parameter supports bytes and str objects (previously only str).
Like unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values.
string must be a str.
Example: unquote_plus('/El+Ni%C3%B1o/') yields '/El Niño/'.
Replace %xx escapes with their single-octet equivalent, and return a bytes object.
If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes.
Example: unquote_to_bytes('a%26%EF') yields b'a&\xef'.
urllib.parse.urlencode(query, doseq=False, safe=”, encoding=None, errors=None, quote_via=quote_plus)
Convert a mapping object or a sequence of two-element tuples, which may contain str or bytes objects, to
a percent-encoded ASCII text string.
If the resultant string is to be used as a data for POST operation with the
urlopen() function, then it should be encoded to bytes, otherwise it would result in a TypeError.
The resulting string is a series of key=value pairs separated by '&' characters, where both key and value
are quoted using the quote_via function.
By default, quote_plus() is used to quote the values, which means
spaces are quoted as a '+' character and ‘/’ characters are encoded as %2F, which follows the standard for GET re-
quests (application/x-www-form-urlencoded).
An alternate function that can be passed as quote_via
is quote(), which will encode spaces as %20 and not encode ‘/’ characters.
For maximum control of what is
quoted, use quote and specify a value for safe.
When a sequence of two-element tuples is used as the query argument, the first element of each tuple is a key and
the second is a value.
The value element in itself can be a sequence and in that case, if the optional parameter doseq
evaluates to True, individual key=value pairs separated by '&' are generated for each element of the value
sequence for the key.
The order of parameters in the encoded string will match the order of parameter tuples in
the sequence.
The safe, encoding, and errors parameters are passed down to quote_via (the encoding and errors parameters are
only passed when a query element is a str).
To reverse this encoding process, parse_qs() and parse_qsl() are provided in this module to parse query
strings into Python data structures.
Refer to urllib examples to find out how the urllib.parse.urlencode() method can be used for generating
the query string of a URL or data for a POST request.
Changed in version 3.2: query supports bytes and string objects.
This is the current standard (STD66).
Any changes to urllib.parse module
should conform to this.
Certain deviations could be observed, which are mostly for backward compatibility purposes
and for certain de-facto parsing requirements as commonly observed in major browsers.
Addresses in URL’s.
This specifies the parsing requirements of IPv6 URLs.
RFC 2396 - Uniform Resource Identifiers (URI): Generic Syntax Document describing the generic syntactic re-
quirements for both Uniform Resource Names (URNs) and Uniform Resource Locators (URLs).
The mailto URL scheme.
Parsing requirements for mailto URL schemes.
RFC 1808 - Relative Uniform Resource Locators This Request For Comments includes the rules for joining an ab-
solute and a relative URL, including a fair number of “Abnormal Examples” which govern the treatment of border
cases.
This specifies the formal syntax and semantics of absolute URLs.
21.7 urllib.error — Exception classes raised by urllib.request
Source code: Lib/urllib/error.py
The urllib.error module defines the exception classes for exceptions raised by urllib.request.
The base
exception class is URLError.
The following exceptions are raised by urllib.error as appropriate:
exception urllib.error.
The handlers raise this exception (or derived exceptions) when they run into a problem.
It is a subclass of OSError.
It can be a message string or another exception instance.
Changed in version 3.3: URLError used to be a subtype of IOError, which is now an alias of OSError.
Though being an exception (a subclass of URLError), an HTTPError can also function as a non-exceptional
file-like return value (the same thing that urlopen() returns).
This is useful when handling exotic HTTP errors,
such as requests for authentication.
An alias for filename attribute.
This numeric value corresponds to a value found in the
dictionary of codes as found in http.server.BaseHTTPRequestHandler.responses.
An alias for msg attribute.
An alias for hdrs attribute.
New in version 3.4.
fp
A file-like object where the HTTP error body can be read from.
This exception is raised when the urlretrieve() function detects that the amount of the downloaded data is
less than the expected amount (given by the Content-Length header).
Lib/urllib/robotparser.py
This module provides a single class, RobotFileParser, which answers questions about whether or not a particular
user agent can fetch a URL on the web site that published the robots.txt file.
For more details on the structure of
robots.txt files, see http://www.robotstxt.org/orig.html.
Sets the URL referring to a robots.txt file.
Returns True if the useragent is allowed to fetch the url according to the rules contained in the parsed
robots.txt file.
This is useful for long-running web spiders that
need to check for new robots.txt files periodically.
Sets the time the robots.txt file was last fetched to the current time.
If
there is no such parameter or it doesn’t apply to the useragent specified or the robots.txt entry for this
parameter has invalid syntax, return None.
New in version 3.6.
request_rate(useragent)
Returns the contents of the Request-rate parameter from robots.txt as a named tuple
RequestRate(requests, seconds).
If there is no such parameter or it doesn’t apply to the usera-
gent specified or the robots.txt entry for this parameter has invalid syntax, return None.
New in version 3.6.
site_maps()
Returns the contents of the Sitemap parameter from robots.txt in the form of a list().
If there is
no such parameter or the robots.txt entry for this parameter has invalid syntax, return None.
New in version 3.8.
HTTPStatus
New in version 3.5.
A subclass of enum.
IntEnum that defines a set of HTTP status codes, reason phrases and long descriptions
written in English.
Usage:
HTTPStatus.
CONTINUE, HTTPStatus.SWITCHING_PROTOCOLS, ...]
21.9.1 HTTP status codes
Supported, IANA-registered status codes available in http.
In order to preserve backwards compatibility, enum values are also present in the http.client module in the form
of constants.
The enum name is equal to the constant name (i.e. http.
HTTPStatus.
OK is also available as http.
client.
OK).
Changed in version 3.7: Added 421 MISDIRECTED_REQUEST status code.
New in version 3.8: Added 451 UNAVAILABLE_FOR_LEGAL_REASONS status code.
New in version 3.9: Added 103 EARLY_HINTS, 418 IM_A_TEAPOT and 425 TOO_EARLY status codes.
The enum values have several properties to indicate the HTTP status category:
Property
Indicates that
Details
is_informational
100 <= status <= 199
HTTP/1.1 RFC 7231, Section 6
is_success
200 <= status <= 299
HTTP/1.1 RFC 7231, Section 6
is_redirection
300 <= status <= 399
HTTP/1.1 RFC 7231, Section 6
is_client_error
400 <= status <= 499
HTTP/1.1 RFC 7231, Section 6
is_server_error
500 <= status <= 599
HTTP/1.1 RFC 7231, Section 6
Usage:
True
False
class http.
HTTPMethod
New in version 3.11.
A subclass of enum.
StrEnum that defines a set of HTTP methods and descriptions written in English.
Usage:
<HTTPMethod.
CONNECT>,
<HTTPMethod.
DELETE>,
<HTTPMethod.
GET>,
<HTTPMethod.
HEAD>,
<HTTPMethod.
OPTIONS>,
<HTTPMethod.
PATCH>,
<HTTPMethod.
POST>,
<HTTPMethod.
PUT>,
<HTTPMethod.
HTTPMethod are:
Method
Enum Name
Details
GET
GET
HTTP/1.1 RFC 7231, Section 4.3.1
HEAD
HEAD
HTTP/1.1 RFC 7231, Section 4.3.2
POST
POST
HTTP/1.1 RFC 7231, Section 4.3.3
PUT
PUT
HTTP/1.1 RFC 7231, Section 4.3.4
DELETE
DELETE
HTTP/1.1 RFC 7231, Section 4.3.5
CONNECT
CONNECT
HTTP/1.1 RFC 7231, Section 4.3.6
OPTIONS
OPTIONS
HTTP/1.1 RFC 7231, Section 4.3.7
TRACE
TRACE
HTTP/1.1 RFC 7231, Section 4.3.8
PATCH
PATCH
HTTP/1.1 RFC 5789
21.10 http.client — HTTP protocol client
Source code: Lib/http/client.py
This module defines classes that implement the client side of the HTTP and HTTPS protocols.
It is normally not used
directly — the module urllib.request uses it to handle URLs that use HTTP and HTTPS.
See also:
The Requests package is recommended for a higher-level HTTP client interface.
Note: HTTPS support is only available if Python was compiled with SSL support (through the ssl module).
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The module provides the following classes:
class http.client.
An HTTPConnection instance represents one transaction with an HTTP server.
It should be instantiated by
passing it a host and optional port number.
If no port number is passed, the port is extracted from the host string if
it has the form host:port, else the default HTTP port (80) is used.
If the optional timeout parameter is given,
blocking operations (like connection attempts) will timeout after that many seconds (if it is not given, the global
default timeout setting is used).
The optional source_address parameter may be a tuple of a (host, port) to use as
the source address the HTTP connection is made from.
The optional blocksize parameter sets the buffer size in
bytes for sending a file-like message body.
For example, the following calls all create instances that connect to the server at the same host and port:
21.10.
Changed in version 3.4: The strict parameter was removed.
HTTP 0.9-style “Simple Responses” are no longer
supported.
Changed in version 3.7: blocksize parameter was added.
A subclass of HTTPConnection that uses SSL for communication with secure servers.
Default port is 443.
If
context is specified, it must be a ssl.
SSLContext instance describing the various SSL options.
Please read Security considerations for more information on best practices.
Changed in version 3.2: source_address, context and check_hostname were added.
Changed in version 3.2: This class now supports HTTPS virtual hosts if possible (that is, if ssl.HAS_SNI is
true).
Changed in version 3.4: The strict parameter was removed.
HTTP 0.9-style “Simple Responses” are no longer
supported.
This class now performs all the necessary certificate and hostname checks by default.
To
revert to the previous, unverified, behavior ssl._create_unverified_context() can be passed to the
context parameter.
This class now enables TLS 1.3 ssl.SSLContext.post_handshake_auth for
the default context or when cert_file is passed with a custom context.
This class now sends an ALPN extension with protocol indicator http/1.1 when no
context is given.
Custom context should set ALPN protocols with set_alpn_protocol().
The deprecated key_file, cert_file and check_hostname parameters have been removed.
HTTPResponse(sock, debuglevel=0, method=None, url=None)
Class whose instances are returned upon successful connection.
Not instantiated directly by user.
Changed in version 3.4: The strict parameter was removed.
HTTP 0.9 style “Simple Responses” are no longer
supported.
Parse the headers from a file pointer fp representing a HTTP request/response.
The file has to be a
BufferedIOBase reader (i.e. not text) and must provide a valid RFC 2822 style header.
This function returns an instance of http.client.
HTTPMessage that holds the header fields, but no payload
(the same as HTTPResponse.msg and http.server.BaseHTTPRequestHandler.headers).
After
returning, the file pointer fp is ready to read the HTTP body.
Note: parse_headers() does not parse the start-line of a HTTP message; it only parses the Name: value
lines.
The file has to be ready to read these field lines, so the first line should already be consumed before calling
the function.
The following exceptions are raised as appropriate:
exception http.client.
HTTPException
The base class of the other exceptions in this module.
NotConnected
A subclass of HTTPException.
InvalidURL
A subclass of HTTPException, raised if a port is given and is either non-numeric or empty.
UnknownProtocol
A subclass of HTTPException.
UnknownTransferEncoding
A subclass of HTTPException.
UnimplementedFileMode
A subclass of HTTPException.
IncompleteRead
A subclass of HTTPException.
ImproperConnectionState
A subclass of HTTPException.
CannotSendRequest
A subclass of ImproperConnectionState.
CannotSendHeader
A subclass of ImproperConnectionState.
ResponseNotReady
A subclass of ImproperConnectionState.
A subclass of HTTPException.
Raised if a server responds with a HTTP status code that we don’t understand.
LineTooLong
A subclass of HTTPException.
Raised if an excessively long line is received in the HTTP protocol from the
server.
RemoteDisconnected
A subclass of ConnectionResetError and BadStatusLine.
Raised by HTTPConnection.
getresponse() when the attempt to read the response results in no data read from the connection, indicating
that the remote end has closed the connection.
New in version 3.5: Previously, BadStatusLine('') was raised.
The constants defined in this module are:
http.client.
The default port for the HTTP protocol (always 80).
HTTPS_PORT
The default port for the HTTPS protocol (always 443).
Example: http.client.responses[http.client.NOT_FOUND] is 'Not Found'.
See HTTP status codes for a list of HTTP status codes that are available in this module as constants.
21.10.
This will send a request to the server using the HTTP request method method and the request URI url.
The provided
url must be an absolute path to conform with RFC 2616 §5.1.2 (unless connecting to an HTTP proxy server or
using the OPTIONS or CONNECT methods).
If body is specified, the specified data is sent after the headers are finished.
It may be a str, a bytes-like object, an
open file object, or an iterable of bytes.
If body is a string, it is encoded as ISO-8859-1, the default for HTTP.
If it
is a bytes-like object, the bytes are sent as is.
If it is a file object, the contents of the file is sent; this file object should
support at least the read() method.
If the file object is an instance of io.TextIOBase, the data returned by
the read() method will be encoded as ISO-8859-1, otherwise the data returned by read() is sent as is.
If body
is an iterable, the elements of the iterable are sent as is until the iterable is exhausted.
The headers argument should be a mapping of extra HTTP headers to send with the request.
A Host header must
be provided to conform with RFC 2616 §5.1.2 (unless connecting to an HTTP proxy server or using the OPTIONS
or CONNECT methods).
If headers contains neither Content-Length nor Transfer-Encoding, but there is a request body, one of those header
fields will be added automatically.
If body is None, the Content-Length header is set to 0 for methods that expect a
body (PUT, POST, and PATCH).
If body is a string or a bytes-like object that is not also a file, the Content-Length
header is set to its length.
Any other type of body (files and iterables in general) will be chunk-encoded, and the
Transfer-Encoding header will automatically be set instead of Content-Length.
The encode_chunked argument is only relevant if Transfer-Encoding is specified in headers.
If encode_chunked is
False, the HTTPConnection object assumes that all encoding is handled by the calling code.
If it is True, the
body will be chunk-encoded.
For example, to perform a GET request to https://docs.python.org/3/:
200 OK
Note: Chunked transfer encoding has been added to the HTTP protocol version 1.1.
Unless the HTTP server is
known to handle HTTP 1.1, the caller must either specify the Content-Length, or must pass a str or bytes-like
object that is not also a file as the body representation.
New in version 3.2: body can now be an iterable.
If neither Content-Length nor Transfer-Encoding are set in headers, file and iterable body
objects are now chunk-encoded.
The encode_chunked argument was added.
No attempt is made to determine the
Content-Length for file objects.
HTTPConnection.getresponse()
Should be called after a request is sent to get the response from the server.
Returns an HTTPResponse instance.
The default debug level is 0, meaning no debugging output is printed.
Any value greater
than 0 will cause all currently defined debug output to be printed to stdout.
The debuglevel is passed to any
new HTTPResponse objects that are created.
New in version 3.1.
HTTPConnection.set_tunnel(host, port=None, headers=None)
Set the host and the port for HTTP Connect Tunnelling.
This allows running the connection through a proxy server.
The host and port arguments specify the endpoint of the tunneled connection (i.e. the address included in the
CONNECT request, not the address of the proxy server).
The headers argument should be a mapping of extra HTTP headers to send with the CONNECT request.
As HTTP/1.1 is used for HTTP CONNECT tunnelling request, as per the RFC, a HTTP Host: header must be
provided, matching the authority-form of the request target provided as the destination for the CONNECT request.
If a HTTP Host: header is not provided via the headers argument, one is generated and transmitted automatically.
For example, to tunnel through a HTTPS proxy server running locally on port 8080, we would pass the address of
the proxy to the HTTPSConnection constructor, and the address of the host that we eventually want to reach
to the set_tunnel() method:
New in version 3.2.
Changed in version 3.12: HTTP CONNECT tunnelling requests use protocol HTTP/1.1, upgraded from protocol
HTTP/1.0.
Host: HTTP headers are mandatory for HTTP/1.1, so one will be automatically generated and
transmitted if not provided in the headers argument.
HTTPConnection.get_proxy_response_headers()
Returns a dictionary with the headers of the response received from the proxy server to the CONNECT request.
If the CONNECT request was not sent, the method returns None.
New in version 3.12.
HTTPConnection.connect()
Connect to the server specified when the object was created.
By default, this is called automatically when making
a request if the client does not already have a connection.
Raises an auditing event http.client.connect with arguments self, host, port.
HTTPConnection.close()
Close the connection to the server.
HTTPConnection.blocksize
Buffer size in bytes for sending a file-like message body.
New in version 3.7.
As an alternative to using the request() method described above, you can also send your request step by step, by using
the four functions below.
21.10.
This should be the first call after the connection to the server has been made.
It sends a line to the server consisting
of the method string, the url string, and the HTTP version (HTTP/1.1).
Send an RFC 822-style header to the server.
It sends a line to the server consisting of the header, a colon and a
space, and the first argument.
If more arguments are given, continuation lines are sent, each consisting of a tab and
an argument.
HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)
Send a blank line to the server, signalling the end of the headers.
The optional message_body argument can be used
to pass a message body associated with the request.
If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in
RFC 7230, Section 3.3.1.
How the data is encoded is dependent on the type of message_body.
If message_body
implements the buffer interface the encoding will result in a single chunk.
If message_body is a collections.
Iterable, each iteration of message_body will result in a chunk.
If message_body is a file object, each
call to .read() will result in a chunk.
The method automatically signals the end of the chunk-encoded data
immediately after message_body.
Note:
Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by
the chunk-encoder.
This is to avoid premature termination of the read of the request by the target server due to
malformed encoding.
Chunked encoding support.
The encode_chunked parameter was added.
Send data to the server.
This should be used directly only after the endheaders() method has been called and
before getresponse() is called.
Raises an auditing event http.client.send with arguments self, data.
It provides access to the request headers and
the entity body.
The response is an iterable object and can be used in a with statement.
The io.BufferedIOBase interface is now implemented and all of its reader operations are
supported.
Reads and returns the response body, or up to the next amt bytes.
HTTPResponse.readinto(b)
Reads up to the next len(b) bytes of the response body into the buffer b.
Returns the number of bytes read.
New in version 3.3.
HTTPResponse.getheader(name, default=None)
Return the value of the header name, or default if there is no header matching name.
If there is more than one
header with the name name, return all of the values joined by ‘, ‘.
HTTPResponse.fileno()
Return the fileno of the underlying socket.
HTTPResponse.msg
A
http.client.
HTTPMessage
instance
containing
the
response
headers.
http.client.
HTTPMessage is a subclass of email.message.
Message.
HTTPResponse.version
HTTP protocol version used by server. 10 for HTTP/1.0, 11 for HTTP/1.1.
HTTPResponse.url
URL of the resource retrieved, commonly used to determine if a redirect was followed.
HTTPResponse.headers
Headers of the response in the form of an email.message.
EmailMessage instance.
HTTPResponse.status
Status code returned by server.
HTTPResponse.reason
Reason phrase returned by server.
HTTPResponse.debuglevel
A debugging hook.
If debuglevel is greater than zero, messages will be printed to stdout as the response is
read and parsed.
HTTPResponse.closed
Is True if the stream is closed.
HTTPResponse.geturl()
Deprecated since version 3.9: Deprecated in favor of url.
HTTPResponse.info()
Deprecated since version 3.9: Deprecated in favor of headers.
HTTPResponse.getcode()
Deprecated since version 3.9: Deprecated in favor of status.
This will return entire content.
Note that the HEAD method never returns any data.
The difference lies only on the server side where
HTTP servers will allow resources to be created via PUT requests.
It should be noted that custom HTTP methods are
also handled in urllib.request.
Request by setting the appropriate method attribute.
HTTPMessage instance holds the headers from an HTTP response.
It is implemented using the
email.message.
Message class.
The FTP class implements the client side of the FTP protocol.
You can use this to write Python programs that perform a variety of automated FTP jobs, such as mirroring other FTP
servers.
It is also used by the module urllib.request to handle URLs that use FTP.
For more information on FTP
(File Transfer Protocol), see internet RFC 959.
The default encoding is UTF-8, following RFC 2640.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Here’s a sample session using the ftplib module:
# connect to host, default port
# user anonymous, passwd anonymous@
'230 Login successful.
The module defines the following items:
class ftplib.
FTP(host=”, user=”, passwd=”, acct=”, timeout=None, source_address=None, *, encoding=’utf-8’)
Return a new instance of the FTP class.
When host is given, the method call connect(host) is made.
When
user is given, additionally the method call login(user, passwd, acct) is made (where passwd and acct
default to the empty string when not given).
The optional timeout parameter specifies a timeout in seconds for
21.11.
The
encoding parameter specifies the encoding for directories and filenames.
Changed in version 3.3: source_address parameter was added.
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
The encoding parameter was added, and the default was changed from Latin-1
to UTF-8 to follow RFC 2640.
FTP_TLS(host='', user='', passwd='', acct='', *, context=None,
timeout=None, source_address=None, encoding='utf-8')
A FTP subclass which adds TLS support to FTP as described in RFC 4217.
Connect as usual to port 21 implicitly
securing the FTP control connection before authenticating.
Securing the data connection requires the user to ex-
plicitly ask for it by calling the prot_p() method.
SSLContext object which allows bundling
SSL configuration options, certificates and private keys into a single (potentially long-lived) structure.
Please read
Security considerations for best practices.
New in version 3.2.
Changed in version 3.3: source_address parameter was added.
Changed in version 3.4: The class now supports hostname check with ssl.
SSLContext.check_hostname
and Server Name Indication (see ssl.HAS_SNI).
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
The encoding parameter was added, and the default was changed from Latin-1
to UTF-8 to follow RFC 2640.
The deprecated keyfile and certfile parameters have been removed.
This set includes the four exceptions listed
above as well as OSError and EOFError.
See also:
Module netrc Parser for the .netrc file format.
The file .netrc is typically used by FTP clients to load user
authentication information before prompting the user.
These are named for
the command which is used followed by lines for the text version or binary for the binary version.
FTP instances have the following methods:
FTP.set_debuglevel(level)
Set the instance’s debugging level.
This controls the amount of debugging output printed.
The default, 0, produces
no debugging output.
A value of 1 produces a moderate amount of debugging output, generally a single line per
request.
A value of 2 or higher produces the maximum amount of debugging output, logging each line sent and
received on the control connection.
FTP.connect(host=”, port=0, timeout=None, source_address=None)
Connect to the given host and port.
The default port number is 21, as specified by the FTP protocol specification.
It is rarely needed to specify a different port number.
This function should be called only once for each instance;
it should not be called at all if a host was given when the instance was created.
All other methods can only be
used after a connection has been made.
The optional timeout parameter specifies a timeout in seconds for the
connection attempt.
If no timeout is passed, the global default timeout setting will be used.
Raises an auditing event ftplib.connect with arguments self, host, port.
Changed in version 3.3: source_address parameter was added.
FTP.getwelcome()
Return the welcome message sent by the server in reply to the initial connection.
The passwd and acct parameters are optional and default to the empty string.
If no user
is specified, it defaults to 'anonymous'.
If user is 'anonymous', the default passwd is 'anonymous@'.
This function should be called only once for each instance, after a connection has been established; it should not be
called at all if a host and user were given when the instance was created.
Most FTP commands are only allowed
after the client has logged in.
The acct parameter supplies “accounting information”; few systems implement this.
Abort a file transfer that is in progress.
Using this does not always work, but it’s worth a try.
FTP.sendcmd(cmd)
Send a simple command string to the server and return the response string.
Raises an auditing event ftplib.sendcmd with arguments self, cmd.
FTP.voidcmd(cmd)
Send a simple command string to the server and handle the response.
Return nothing if a response code corre-
sponding to success (codes in the range 200–299) is received.
Raise error_reply otherwise.
Raises an auditing event ftplib.sendcmd with arguments self, cmd.
FTP.retrbinary(cmd, callback, blocksize=8192, rest=None)
Retrieve a file in binary transfer mode.
The callback function is called for each block of data received, with a single bytes argument giving the data block.
The optional blocksize argument specifies the maximum chunk size to read on the low-level socket object created
to do the actual transfer (which will also be the largest size of the data blocks passed to callback).
A reasonable
default is chosen.
FTP.retrlines(cmd, callback=None)
Retrieve a file or directory listing in the encoding specified by the encoding parameter at initialization.
LIST retrieves a list of files and information about those files.
NLST retrieves a list of file
names.
The callback function is called for each line with a string argument containing the line with the trailing
CRLF stripped.
The default callback prints the line to sys.stdout.
FTP.set_pasv(val)
Enable “passive” mode if val is true, otherwise disable passive mode.
Passive mode is on by default.
FTP.storbinary(cmd, fp, blocksize=8192, callback=None, rest=None)
Store a file in binary transfer mode.
The blocksize argument defaults to 8192.
Changed in version 3.2: rest parameter added.
FTP.storlines(cmd, fp, callback=None)
Store a file in line mode.
Lines are read
until EOF from the file object fp (opened in binary mode) using its readline() method to provide the data to
be stored.
FTP.transfercmd(cmd, rest=None)
Initiate a transfer over the data connection.
If the transfer is active, send an EPRT or PORT command and the
transfer command specified by cmd, and accept the connection.
If the server is passive, send an EPSV or PASV
command, connect to it, and start the transfer command.
Either way, return the socket for the connection.
If optional rest is given, a REST command is sent to the server, passing rest as an argument.
Note however that the transfercmd() method converts rest to a string with the
encoding parameter specified at initialization, but no check is performed on the string’s contents.
If the server
does not recognize the REST command, an error_reply exception will be raised.
If this happens, simply call
transfercmd() without a rest argument.
FTP.ntransfercmd(cmd, rest=None)
Like transfercmd(), but returns a tuple of the data connection and the expected size of the data.
If the
expected size could not be computed, None will be returned as the expected size.
If path is omitted the current
directory is assumed.
Return a generator object yielding a tuple of two elements for every file found in path.
First element is the file name, the second one is a dictionary containing facts about the file name.
Content of this
dictionary might be limited by the facts argument but server is not guaranteed to return all requested facts.
New in version 3.3.
The optional argument is a directory to list (default is
the current server directory).
Multiple arguments can be used to pass non-standard options to the NLST command.
Note: If your server supports the command, mlsd() offers a better API.
The optional ar-
gument is a directory to list (default is the current server directory).
Multiple arguments can be used to pass
non-standard options to the LIST command.
If the last argument is a function, it is used as a callback function as
for retrlines(); the default prints to sys.stdout.
This method returns None.
Note: If your server supports the command, mlsd() offers a better API.
FTP.rename(fromname, toname)
Rename file fromname on the server to toname.
Remove the file named filename from the server.
If successful, returns the text of the response, otherwise raises
error_perm on permission errors or error_reply on other errors.
FTP.cwd(pathname)
Set the current directory on the server.
FTP.mkd(pathname)
Create a new directory on the server.
Remove the directory named dirname on the server.
21.11.
On success, the size of the file is returned as an integer,
otherwise None is returned.
Note that the SIZE command is not standardized, but is supported by many common
server implementations.
FTP.quit()
Send a QUIT command to the server and close the connection.
This is the “polite” way to close a connection, but
it may raise an exception if the server responds with an error to the QUIT command.
This implies a call to the
close() method which renders the FTP instance useless for subsequent calls (see below).
FTP.close()
Close the connection unilaterally.
This should not be applied to an already closed connection such as after a
successful call to quit().
After this call the FTP instance should not be used any more (after a call to close()
or quit() you cannot reopen the connection by issuing another login() method).
FTP_TLS.ssl_version
The SSL version to use (defaults to ssl.PROTOCOL_SSLv23).
FTP_TLS.auth()
Set up a secure control connection by using TLS or SSL, depending on what is specified in the ssl_version
attribute.
Changed
in
version
3.4:
The
method
now
supports
hostname
check
with
ssl.
SSLContext.
FTP_TLS.ccc()
Revert control channel back to plaintext.
This can be useful to take advantage of firewalls that know how to handle
NAT with non-secure FTP without opening fixed ports.
New in version 3.3.
FTP_TLS.prot_p()
Set up secure data connection.
FTP_TLS.prot_c()
Set up clear text data connection.
Lib/poplib.py
This module defines a class, POP3, which encapsulates a connection to a POP3 server and implements the protocol
as defined in RFC 1939.
The POP3 class supports both the minimal and optional command sets from RFC 1939.
The
POP3 class also supports the STLS command introduced in RFC 2595 to enable encrypted communication on an already
established connection.
The implementation quality of POP3 servers varies widely,
and too many are quite poor.
If your mailserver supports IMAP, you would be better off using the imaplib.
IMAP4
class, as IMAP servers tend to be better implemented.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The poplib module provides two classes:
class poplib.
This class implements the actual POP3 protocol.
The connection is created when the instance is initialized.
If port
is omitted, the standard POP3 port (110) is used.
The optional timeout parameter specifies a timeout in seconds
for the connection attempt (if not specified, the global default timeout setting will be used).
Raises an auditing event poplib.connect with arguments self, host, port.
All commands will raise an auditing event poplib.putline with arguments self and line, where line is
the bytes about to be sent to the remote host.
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
This is a subclass of POP3 that connects to the server over an SSL encrypted socket.
If port is not specified, 995,
the standard POP3-over-SSL port is used.
SSLContext object which allows bundling SSL configuration options, certificates and private keys into a single
(potentially long-lived) structure.
Please read Security considerations for best practices.
Raises an auditing event poplib.connect with arguments self, host, port.
All commands will raise an auditing event poplib.putline with arguments self and line, where line is
the bytes about to be sent to the remote host.
Changed in version 3.2: context parameter added.
Changed in version 3.4: The class now supports hostname check with ssl.
SSLContext.check_hostname
and Server Name Indication (see ssl.HAS_SNI).
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
The deprecated keyfile and certfile parameters have been removed.
One exception is defined as an attribute of the poplib module:
exception poplib.error_proto
Exception raised on any errors from this module (errors from socket module are not caught).
The reason for the
exception is passed to the constructor as a string.
The standard Python IMAP module.
Frequently Asked Questions About Fetchmail The FAQ for the fetchmail POP/IMAP client collects information
on POP3 server variations and RFC noncompliance that may be useful if you need to write an application based
on the POP protocol.
21.12.
A POP3 instance has the following methods:
POP3.set_debuglevel(level)
Set the instance’s debugging level.
This controls the amount of debugging output printed.
The default, 0, produces
no debugging output.
A value of 1 produces a moderate amount of debugging output, generally a single line per
request.
A value of 2 or higher produces the maximum amount of debugging output, logging each line sent and
received on the control connection.
POP3.getwelcome()
Returns the greeting string sent by the POP3 server.
POP3.capa()
Query the server’s capabilities as specified in RFC 2449.
New in version 3.4.
Send user command, response should indicate that a password is required.
Send password, response includes message count and mailbox size.
Note: the mailbox on the server is locked until
quit() is called.
Get mailbox status.
The result is a tuple of 2 integers: (message count, mailbox size).
POP3.list([which])
Request message list, result is in the form (response, ['mesg_num octets', ...], octets).
If
which is set, it is the message to list.
POP3.retr(which)
Retrieve whole message number which, and set its seen flag.
Result is in form (response, ['line', ...
], octets).
POP3.dele(which)
Flag message number which for deletion.
On most servers deletions are not actually performed until QUIT (the
major exception is Eudora QPOP, which deliberately violates the RFCs by doing pending deletes on any disconnect).
POP3.rset()
Remove any deletion marks for the mailbox.
POP3.noop()
Do nothing.
Might be used as a keep-alive.
Signoff: commit changes, unlock mailbox, drop connection.
Result
is in form (response, ['line', ...], octets).
The POP3 TOP command this method uses, unlike the RETR command, doesn’t set the message’s seen flag;
unfortunately, TOP is poorly specified in the RFCs and is frequently broken in off-brand servers.
Test this method
by hand against the POP3 servers you will use before trusting it.
POP3.uidl(which=None)
Return message digest (unique id) list.
If which is specified, result contains the unique id for that message in
the form 'response mesgnum uid, otherwise result is list (response, ['mesgnum uid', ...],
octets).
POP3.utf8()
Try to switch to UTF-8 mode.
Returns the server response if successful, raises error_proto if not.
Specified
in RFC 6856.
New in version 3.5.
POP3.stls(context=None)
Start a TLS session on the active connection as specified in RFC 2595.
This is only allowed before user authenti-
cation
context parameter is a ssl.
SSLContext object which allows bundling SSL configuration options, certificates
and private keys into a single (potentially long-lived) structure.
Please read Security considerations for best practices.
This method supports hostname checking via ssl.
SSLContext.check_hostname and Server Name Indi-
cation (see ssl.HAS_SNI).
New in version 3.4.
Instances of POP3_SSL have no additional methods.
The interface of this subclass is identical to its parent.
At the end of the module, there is a test section that contains a more extensive example of usage.
It is backward
compatible with IMAP4 (RFC 1730) servers, but note that the STATUS command is not supported in IMAP4.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
This class implements the actual IMAP4 protocol.
The connection is created and protocol version (IMAP4 or
IMAP4rev1) is determined when the instance is initialized.
If host is not specified, '' (the local host) is used.
If port is omitted, the standard IMAP4 port (143) is used.
The optional timeout parameter specifies a timeout in
seconds for the connection attempt.
If timeout is not given or is None, the global default socket timeout is used.
The IMAP4 class supports the with statement.
When used like this, the IMAP4 LOGOUT command is issued
automatically when the with statement exits.
E.g.:

M.noop()

('OK', [b'Nothing Accomplished.
Changed in version 3.5: Support for the with statement was added.
The optional timeout parameter was added.
Three exceptions are defined as attributes of the IMAP4 class:
exception IMAP4.error
Exception raised on any errors.
The reason for the exception is passed to the constructor as a string.
This is a sub-class of IMAP4.error.
Note that closing
the instance and instantiating a new one will usually allow recovery from this exception.
This exception is raised when a writable mailbox has its status changed by the server.
This is a sub-class of IMAP4.
error.
Some other client now has write permission, and the mailbox will need to be re-opened to re-obtain write
permission.
There’s also a subclass for secure connections:
class imaplib.
This is a subclass derived from IMAP4 that connects over an SSL encrypted socket (to use this class you need
a socket module that was compiled with SSL support).
If host is not specified, '' (the local host) is used.
If
port is omitted, the standard IMAP4-over-SSL port (993) is used.
SSLContext object
which allows bundling SSL configuration options, certificates and private keys into a single (potentially long-lived)
structure.
The optional timeout parameter specifies a timeout in seconds for the connection attempt.
If timeout is not given
or is None, the global default socket timeout is used.
Changed in version 3.3: ssl_context parameter was added.
Changed in version 3.4: The class now supports hostname check with ssl.
SSLContext.check_hostname
and Server Name Indication (see ssl.HAS_SNI).
The optional timeout parameter was added.
The deprecated keyfile and certfile parameters have been removed.
The second subclass allows for connections created by a child process:
class imaplib.
This is a subclass derived from IMAP4 that connects to the stdin/stdout file descriptors created by passing
command to subprocess.
Popen().
The following utility functions are defined:
imaplib.
Internaldate2tuple(datestr)
Parse an IMAP4 INTERNALDATE string and return corresponding local time.
The return value is a time.
Int2AP(num)
Converts an integer into a bytes representation using characters from the set [A .. P].
imaplib.
ParseFlags(flagstr)
Converts an IMAP4 FLAGS response to a tuple of individual flags.
Time2Internaldate(date_time)
Convert date_time to an IMAP4 INTERNALDATE representation.
The return value is a string in the form:
"DD-Mmm-YYYY HH:MM:SS +HHMM" (including double-quotes).
The date_time argument can be a number
(int or float) representing seconds since epoch (as returned by time.time()), a 9-tuple representing local time an
instance of time.struct_time (as returned by time.localtime()), an aware instance of datetime.
In the last case, it is assumed to already be in the correct format.
Note that IMAP4 message numbers change as the mailbox changes; in particular, after an EXPUNGE command performs
deletions the remaining messages are renumbered.
So it is highly advisable to use UIDs instead, with the UID command.
At the end of the module, there is a test section that contains a more extensive example of usage.
See also:
Documents describing the protocol, sources for servers implementing it, by the University of Washington’s IMAP Infor-
mation Center can all be found at (Source Code) https://github.com/uw-imap/imap (Not Maintained).
All arguments to commands are converted to strings, except for AUTHENTICATE, and the last argument to APPEND
which is passed as an IMAP4 literal.
If necessary (the string contains IMAP4 protocol-sensitive characters and isn’t
enclosed with either parentheses or double quotes) each string is quoted.
However, the password argument to the LOGIN
command is always quoted.
If you want to avoid having an argument string quoted (eg: the flags argument to STORE)
then enclose the string in parentheses (eg: r'(\Deleted)').
Each command returns a tuple: (type, [data, ...]) where type is usually 'OK' or 'NO', and data is either the
text from the command response, or mandated results from the command.
Each data is either a bytes, or a tuple.
If a
tuple, then the first part is the header of the response, and the second part contains the data (ie: ‘literal’ value).
The message_set options to commands below is a string specifying one or more messages to be acted upon.
It may be a
simple message number ('1'), a range of message numbers ('2:4'), or a group of non-contiguous ranges separated by
commas ('1:3,6:9').
A range can contain an asterisk to indicate an infinite upper bound ('3:*').
Append message to named mailbox.
IMAP4.authenticate(mechanism, authobject)
Authenticate command — requires response processing.
mechanism specifies which authentication mechanism is to be used - it should appear in the instance variable
capabilities in the form AUTH=mechanism.
It
should return bytes data that will be base64 encoded and sent to the server.
It should return None if the client
abort response * should be sent instead.
Changed in version 3.5: string usernames and passwords are now encoded to utf-8 instead of being limited to
ASCII.
IMAP4.check()
Checkpoint mailbox on server.
IMAP4.close()
Close currently selected mailbox.
Deleted messages are removed from writable mailbox.
This is the recommended
command before LOGOUT.
IMAP4.copy(message_set, new_mailbox)
Copy message_set messages onto end of new_mailbox.
Delete old mailbox named mailbox.
IMAP4.deleteacl(mailbox, who)
Delete the ACLs (remove any rights) set for who on mailbox.
IMAP4.enable(capability)
Enable capability (see RFC 5161).
Most capabilities do not need to be enabled.
Currently only the UTF8=ACCEPT
capability is supported (see RFC 6855).
The enable() method itself, and RFC 6855 support.
IMAP4.expunge()
Permanently remove deleted items from selected mailbox.
Generates an EXPUNGE response for each deleted
message.
Returned data contains a list of EXPUNGE message numbers in order received.
IMAP4.fetch(message_set, message_parts)
Fetch (parts of) messages.
The method is non-standard, but is supported by the Cyrus server.
IMAP4.getannotation(mailbox, entry, attribute)
Retrieve the specified ANNOTATIONs for mailbox.
The method is non-standard, but is supported by the Cyrus
server.
Get the quota root’s resource usage and limits.
Get the list of quota roots for the named mailbox.
This method is part of the IMAP4 QUOTA extension
defined in rfc2087.
IMAP4.list([directory[, pattern]])
List mailbox names in directory matching pattern.
Returned data contains a list of LIST responses.
IMAP4.login(user, password)
Identify the client using a plaintext password.
The password will be quoted.
IMAP4.login_cram_md5(user, password)
Force use of CRAM-MD5 authentication when identifying the client to protect the password.
Will only work if the
server CAPABILITY response includes the phrase AUTH=CRAM-MD5.
IMAP4.logout()
Shutdown connection to server.
Returns server BYE response.
The method no longer ignores silently arbitrary exceptions.
List subscribed mailbox names in directory matching pattern.
Returned data are tuples of message part envelope and data.
IMAP4.myrights(mailbox)
Show my ACLs for a mailbox (i.e. the rights that I have on mailbox).
IMAP4.namespace()
Returns IMAP namespaces as defined in RFC 2342.
IMAP4.noop()
Send NOOP to server.
IMAP4.open(host, port, timeout=None)
Opens socket to port at host.
The optional timeout parameter specifies a timeout in seconds for the connection
attempt.
If timeout is not given or is None, the global default socket timeout is used.
Also note that if the timeout
parameter is set to be zero, it will raise a ValueError to reject creating a non-blocking socket.
This method is
implicitly called by the IMAP4 constructor.
The connection objects established by this method will be used in the
IMAP4.read(), IMAP4.readline(), IMAP4.send(), and IMAP4.shutdown() methods.
You may
override this method.
Raises an auditing event imaplib.open with arguments self, host, port.
The timeout parameter was added.
IMAP4.partial(message_num, message_part, start, length)
Fetch truncated part of a message.
Returned data is a tuple of message part envelope and data.
Allows an authorised administrator to proxy into any user’s mailbox.
IMAP4.read(size)
Reads size bytes from the remote server.
You may override this method.
IMAP4.readline()
Reads one line from the remote server.
You may override this method.
IMAP4.recent()
Prompt server for an update.
Returned data is None if no new messages, else value of RECENT response.
IMAP4.rename(oldmailbox, newmailbox)
Rename mailbox named oldmailbox to newmailbox.
IMAP4.response(code)
Return data for response code if received, or None.
Returns the given code, instead of the usual type.
IMAP4.search(charset, criterion[, ...])
Search mailbox for matching messages.
The IMAP protocol requires that at least one criterion be specified; an exception will be
raised when the server returns an error.
Returned data is the count of messages in mailbox (EXISTS response).
The default mailbox is
'INBOX'.
If the readonly flag is set, modifications to the mailbox are not allowed.
Sends data to the remote server.
You may override this method.
Raises an auditing event imaplib.send with arguments self, data.
IMAP4.setacl(mailbox, who, what)
Set an ACL for mailbox.
The method is non-standard, but is supported by the Cyrus server.
The method is non-standard, but is supported by the Cyrus server.
IMAP4.setquota(root, limits)
Set the quota root’s resource limits.
This method is part of the IMAP4 QUOTA extension defined in rfc2087.
IMAP4.shutdown()
Close connection established in open.
This method is implicitly called by IMAP4.logout().
You may override
this method.
The sort command is a variant of search with sorting semantics for the results.
Returned data contains a space
separated list of matching message numbers.
Sort has two arguments before the search_criterion argument(s); a parenthesized list of sort_criteria, and the search-
ing charset.
Note that unlike search, the searching charset argument is mandatory.
There is also a uid sort
command which corresponds to sort the way that uid search corresponds to search.
The sort command
first searches the mailbox for messages that match the given searching criteria using the charset argument for the
interpretation of strings in the searching criteria.
It then returns the numbers of matching messages.
This is an IMAP4rev1 extension command.
IMAP4.starttls(ssl_context=None)
Send a STARTTLS command.
The ssl_context argument is optional and should be a ssl.
SSLContext object.
This will enable encryption on the IMAP connection.
Please read Security considerations for best practices.
New in version 3.2.
Changed
in
version
3.4:
The
method
now
supports
hostname
check
with
ssl.
SSLContext.
Request named status conditions for mailbox.
IMAP4.store(message_set, command, flag_list)
Alters flag dispositions for messages in mailbox.
For example, to set the delete flag on all messages:
typ, data = M.search(None, 'ALL')
for num in data[0].split():
M.store(num, '+FLAGS', '\\Deleted')
M.expunge()
Note:
Creating flags containing ‘]’ (for example: “[test]”) violates RFC 3501 (the IMAP protocol).
However,
imaplib has historically allowed creation of such tags, and popular IMAP servers, such as Gmail, accept and produce
such flags.
There are non-Python programs which also create such tags.
Although it is an RFC violation and IMAP
clients and servers are supposed to be strict, imaplib nonetheless continues to allow such tags to be created for
backward compatibility reasons, and as of Python 3.6, handles them if they are sent from the server, since this
improves real-world compatibility.
IMAP4.subscribe(mailbox)
Subscribe to new mailbox.
The thread command is a variant of search with threading semantics for the results.
Returned data contains
a space separated list of thread members.
Thread members consist of zero or more messages numbers, delimited by spaces, indicating successive parent and
child.
Thread has two arguments before the search_criterion argument(s); a threading_algorithm, and the searching
charset.
Note that unlike search, the searching charset argument is mandatory.
There is also a uid thread
command which corresponds to thread the way that uid search corresponds to search.
The thread
21.13.
It then returns the matching messages threaded
according to the specified threading algorithm.
Execute command args with messages identified by UID, rather than message number.
Returns response appro-
priate to command.
At least one argument must be supplied; if none are provided, the server will return an error
and an exception will be raised.
Unsubscribe from old mailbox.
IMAP4.unselect()
imaplib.IMAP4.unselect() frees server’s resources associated with the selected mailbox and returns the
server to the authenticated state.
This command performs the same actions as imaplib.IMAP4.close(),
except that no messages are permanently removed from the currently selected mailbox.
New in version 3.9.
IMAP4.xatom(name[, ...])
Allow simple extension commands notified by server in CAPABILITY response.
The following attributes are defined on instances of IMAP4:
IMAP4.PROTOCOL_VERSION
The most recent supported protocol in the CAPABILITY response from the server.
IMAP4.debug
Integer value to control debugging output.
The initialize value is taken from the module variable Debug.
Values
greater than three trace each command.
IMAP4.utf8_enabled
Boolean value that is normally False, but is set to True if an enable() command is successfully issued for
the UTF8=ACCEPT capability.
New in version 3.5.
Lib/smtplib.py
The smtplib module defines an SMTP client session object that can be used to send mail to any internet machine
with an SMTP or ESMTP listener daemon.
For details of SMTP and ESMTP operation, consult RFC 821 (Simple Mail
Transfer Protocol) and RFC 1869 (SMTP Service Extensions).
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
An SMTP instance encapsulates an SMTP connection.
It has methods that support a full repertoire of SMTP
and ESMTP operations.
If the optional host and port parameters are given, the SMTP connect() method
is called with those parameters during initialization.
If specified, local_hostname is used as the FQDN of the
local host in the HELO/EHLO command.
Otherwise, the local hostname is found using socket.getfqdn().
If the connect() call returns anything other than a success code, an SMTPConnectError is raised.
The
optional timeout parameter specifies a timeout in seconds for blocking operations like the connection attempt (if
not specified, the global default timeout setting will be used).
If the timeout expires, TimeoutError is raised.
The optional source_address parameter allows binding to some specific source address in a machine with multiple
network interfaces, and/or to some specific source TCP port.
It takes a 2-tuple (host, port), for the socket
to bind to as its source address before connecting.
For normal use, you should only require the initialization/connect, sendmail(), and SMTP.quit() methods.
An example is included below.
The SMTP class supports the with statement.
When used like this, the SMTP QUIT command is issued automat-
ically when the with statement exits.
All commands will raise an auditing event smtplib.SMTP.send with arguments self and data, where
data is the bytes about to be sent to the remote host.
Changed in version 3.3: Support for the with statement was added.
Changed in version 3.3: source_address argument was added.
The SMTPUTF8 extension (RFC 6531) is now supported.
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
An SMTP_SSL instance behaves exactly the same as instances of SMTP.
SMTP_SSL should be used for situations
where SSL is required from the beginning of the connection and using starttls() is not appropriate.
If host
is not specified, the local host is used.
If port is zero, the standard SMTP-over-SSL port (465) is used.
The
21.14.
Please read Security considerations for best practices.
Changed in version 3.3: context was added.
The source_address argument was added.
Changed in version 3.4: The class now supports hostname check with ssl.
SSLContext.check_hostname
and Server Name Indication (see ssl.HAS_SNI).
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket
Changed in version 3.12: The deprecated keyfile and certfile parameters have been removed.
The LMTP protocol, which is very similar to ESMTP, is heavily based on the standard SMTP client.
It’s common
to use Unix sockets for LMTP, so our connect() method must support that as well as a regular host:port server.
To specify a Unix socket, you must use an absolute path for host, starting with a ‘/’.
Authentication is supported, using the regular SMTP mechanism.
When using a Unix socket, LMTP generally
don’t support or require any authentication, but your mileage might vary.
The optional timeout parameter was added.
A nice selection of exceptions is defined as well:
exception smtplib.
SMTPException
Subclass of OSError that is the base exception class for all the other exceptions provided by this module.
Changed in version 3.4: SMTPException became subclass of OSError
exception smtplib.
SMTPServerDisconnected
This exception is raised when the server unexpectedly disconnects, or when an attempt is made to use the SMTP
instance before connecting it to a server.
SMTPResponseException
Base class for all exceptions that include an SMTP error code.
These exceptions are generated in some instances
when the SMTP server returns an error code.
The error code is stored in the smtp_code attribute of the error,
and the smtp_error attribute is set to the error message.
SMTPSenderRefused
Sender address refused.
In addition to the attributes set by on all SMTPResponseException exceptions, this
sets ‘sender’ to the string that the SMTP server refused.
SMTPRecipientsRefused
All recipient addresses refused.
The errors for each recipient are accessible through the attribute recipients,
which is a dictionary of exactly the same sort as SMTP.sendmail() returns.
SMTPDataError
The SMTP server refused to accept the message data.
Error occurred during establishment of a connection with the server.
SMTPHeloError
The server refused our HELO message.
SMTPNotSupportedError
The command or option attempted is not supported by the server.
New in version 3.5.
SMTPAuthenticationError
SMTP authentication went wrong.
Most probably the server didn’t accept the username/password combination
provided.
See also:
RFC 821 - Simple Mail Transfer Protocol Protocol definition for SMTP.
This document covers the model, operating
procedure, and protocol details for SMTP.
RFC 1869 - SMTP Service Extensions Definition of the ESMTP extensions for SMTP.
This describes a framework
for extending SMTP with new commands, supporting dynamic discovery of the commands provided by the server,
and defines a few additional commands.
A value of 1 or True for level results in debug messages for connection and for all
messages sent to and received from the server.
A value of 2 for level results in these messages being timestamped.
Send a command cmd to the server.
The optional argument args is simply concatenated to the command, separated
by a space.
In normal operation it should not be necessary to call this method explicitly.
It is used to implement other methods
and may be useful for testing private extensions.
If the connection to the server is lost while waiting for the reply, SMTPServerDisconnected will be raised.
SMTP.connect(host=’localhost’, port=0)
Connect to a host on a given port.
The defaults are to connect to the local host at the standard SMTP port (25).
If the
hostname ends with a colon (':') followed by a number, that suffix will be stripped off and the number interpreted
as the port number to use.
This method is automatically invoked by the constructor if a host is specified during
instantiation.
Returns a 2-tuple of the response code and message sent by the server in its connection response.
Raises an auditing event smtplib.connect with arguments self, host, port.
SMTP.helo(name=”)
Identify yourself to the SMTP server using HELO.
The hostname argument defaults to the fully qualified domain
name of the local host.
The message returned by the server is stored as the helo_resp attribute of the object.
In normal operation it should not be necessary to call this method explicitly.
It will be implicitly called by the
sendmail() when necessary.
SMTP.ehlo(name=”)
Identify yourself to an ESMTP server using EHLO.
The hostname argument defaults to the fully qualified domain
name of the local host.
Examine the response for ESMTP option and store them for use by has_extn().
Also sets several informational attributes: the message returned by the server is stored as the ehlo_resp
21.14.
Unless you wish to use has_extn() before sending mail, it should not be necessary to call this method explicitly.
It will be implicitly called by sendmail() when necessary.
This method calls ehlo() and/or helo() if there has been no previous EHLO or HELO command this session.
It tries ESMTP EHLO first.
The server didn’t reply properly to the HELO greeting.
SMTP.has_extn(name)
Return True if name is in the set of SMTP service extensions returned by the server, False otherwise.
Case is
ignored.
SMTP.verify(address)
Check the validity of an address on this server using SMTP VRFY.
Returns a tuple consisting of code 250 and a
full RFC 822 address (including human name) if the user address is valid.
Otherwise returns an SMTP error code
of 400 or greater and an error string.
Note: Many sites disable SMTP VRFY in order to foil spammers.
SMTP.login(user, password, *, initial_response_ok=True)
Log in on an SMTP server that requires authentication.
The arguments are the username and the password to
authenticate with.
If there has been no previous EHLO or HELO command this session, this method tries ESMTP
EHLO first.
SMTPHeloError The server didn’t reply properly to the HELO greeting.
The server didn’t accept the username/password combination.
The AUTH command is not supported by the server.
No suitable authentication method was found.
Each of the authentication methods supported by smtplib are tried in turn if they are advertised as supported by
the server.
See auth() for a list of supported authentication methods.
Optional keyword argument initial_response_ok specifies whether, for authentication methods that support it, an
“initial response” as specified in RFC 4954 can be sent along with the AUTH command, rather than requiring a
challenge/response.
Changed in version 3.5: SMTPNotSupportedError may be raised, and the initial_response_ok parameter was
added.
SMTP.auth(mechanism, authobject, *, initial_response_ok=True)
Issue an SMTP AUTH command for the specified authentication mechanism, and handle the challenge response via
authobject.
mechanism specifies which authentication mechanism is to be used as argument to the AUTH command; the valid
values are those listed in the auth element of esmtp_features.
If optional keyword argument initial_response_ok is true, authobject() will be called first with no argument.
It can return the RFC 4954 “initial response” ASCII str which will be encoded and sent with the AUTH command
as below.
If the authobject() does not support an initial response (e.g. because it requires a challenge), it
should return None when called with challenge=None.
If initial_response_ok is false, then authobject()
will not be called first with None.
If the initial response check returns None, or if initial_response_ok is false, authobject() will be called to
process the server’s challenge response; the challenge argument it is passed will be a bytes.
It should return ASCII
str data that will be base64 encoded and sent to the server.
The SMTP class provides authobjects for the CRAM-MD5, PLAIN, and LOGIN mechanisms; they are named
SMTP.auth_cram_md5, SMTP.auth_plain, and SMTP.auth_login respectively.
They all require that
the user and password properties of the SMTP instance are set to appropriate values.
User code does not normally need to call auth directly, but can instead call the login() method, which will
try each of the above mechanisms in turn, in the order listed.
New in version 3.5.
SMTP.starttls(*, context=None)
Put the SMTP connection in TLS (Transport Layer Security) mode.
All SMTP commands that follow will be
encrypted.
You should then call ehlo() again.
If keyfile and certfile are provided, they are used to create an ssl.SSLContext.
Optional context parameter is an ssl.
SSLContext object; This is an alternative to using a keyfile and a certfile
and if specified both keyfile and certfile should be None.
If there has been no previous EHLO or HELO command this session, this method tries ESMTP EHLO first.
The deprecated keyfile and certfile parameters have been removed.
The server didn’t reply properly to the HELO greeting.
The server does not support the STARTTLS extension.
RuntimeError SSL/TLS support is not available to your Python interpreter.
Changed in version 3.3: context was added.
The method now supports hostname check with SSLContext.check_hostname
and Server Name Indicator (see HAS_SNI).
Send mail.
The required arguments are an RFC 822 from-address string, a list of RFC 822 to-address strings (a
bare string will be treated as a list with 1 address), and a message string.
The caller may pass a list of ESMTP
options (such as 8bitmime) to be used in MAIL FROM commands as mail_options.
ESMTP options (such as
DSN commands) that should be used with all RCPT commands can be passed as rcpt_options.
Note: The from_addr and to_addrs parameters are used to construct the message envelope used by the transport
agents.
A string is encoded to bytes using
the ascii codec, and lone \r and \n characters are converted to \r\n characters.
A byte string is not modified.
If there has been no previous EHLO or HELO command this session, this method tries ESMTP EHLO first.
If the
server does ESMTP, message size and each of the specified options will be passed to it (if the option is in the
feature set the server advertises).
If EHLO fails, HELO will be tried and ESMTP options suppressed.
This method will return normally if the mail is accepted for at least one recipient.
Otherwise it will raise an
exception.
That is, if this method does not raise an exception, then someone should get your mail.
If this method
does not raise an exception, it returns a dictionary, with one entry for each recipient that was refused.
Each entry
contains a tuple of the SMTP error code and the accompanying error message sent by the server.
If SMTPUTF8 is included in mail_options, and the server supports it, from_addr and to_addrs may contain non-
ASCII characters.
This method may raise the following exceptions:
SMTPRecipientsRefused All recipients were refused.
Nobody got the mail.
The recipients attribute
of the exception object is a dictionary with information about the refused recipients (like the one returned
when at least one recipient was accepted).
The server didn’t reply properly to the HELO greeting.
SMTPSenderRefused The server didn’t accept the from_addr.
SMTPDataError The server replied with an unexpected error code (other than a refusal of a recipient).
SMTPNotSupportedError SMTPUTF8 was given in the mail_options but is not supported by the server.
Unless otherwise noted, the connection will be open even after an exception is raised.
Changed in version 3.2: msg may be a byte string.
Changed in version 3.5: SMTPUTF8 support added, and SMTPNotSupportedError may be raised if
SMTPUTF8 is specified but the server does not support it.
This is a convenience method for calling sendmail() with the message represented by an email.message.
Message object.
The arguments have the same meaning as for sendmail(), except that msg is a Message
object.
If from_addr is None or to_addrs is None, send_message fills those arguments with addresses extracted from
the headers of msg as specified in RFC 5322: from_addr is set to the Sender field if it is present, and otherwise
to the From field.
If exactly one
set of Resent-* headers appear in the message, the regular headers are ignored and the Resent-* headers are
used instead.
If the message contains more than one set of Resent-* headers, a ValueError is raised, since
there is no way to unambiguously detect the most recent set of Resent- headers.
Regardless of the values of from_addr and to_addrs, send_message
does not transmit any Bcc or Resent-Bcc headers that may appear in msg.
If any of the addresses in
from_addr and to_addrs contain non-ASCII characters and the server does not advertise SMTPUTF8 support,
an SMTPNotSupported error is raised.
Otherwise the Message is serialized with a clone of its policy with
the utf8 attribute set to True, and SMTPUTF8 and BODY=8BITMIME are added to mail_options.
New in version 3.2.
New in version 3.5: Support for internationalized addresses (SMTPUTF8).
SMTP.quit()
Terminate the SMTP session and close the connection.
Normally these do not need to be called directly, so they are not documented here.
For details,
consult the module code.
This example prompts the user for addresses needed in the message envelope (‘To’ and ‘From’ addresses), and the message
to be delivered.
Note that the headers to be included with the message must be included in the message as entered; this
example doesn’t do any processing of the RFC 822 headers.
In particular, the ‘To’ and ‘From’ addresses must be included
in the message headers explicitly.
If all you want is a unique ID, you should probably call uuid1() or uuid4().
Note that uuid1() may compromise
privacy since it creates a UUID containing the computer’s network address.
A safe UUID is one
which is generated using synchronization methods that ensure no two processes can obtain the same UUID.
All instances
of UUID have an is_safe attribute which relays any information about the UUID’s safety, using this enumeration:
class uuid.
SafeUUID
New in version 3.7.
safe
The UUID was generated by the platform in a multiprocessing-safe way.
The UUID was not generated in a multiprocessing-safe way.
UUID(hex=None, bytes=None, bytes_le=None, fields=None, int=None, version=None, *,
is_safe=SafeUUID.unknown)
Create a UUID from either a string of 32 hexadecimal digits, a string of 16 bytes in big-endian order as the bytes
argument, a string of 16 bytes in little-endian order as the bytes_le argument, a tuple of six integers (32-bit time_low,
16-bit time_mid, 16-bit time_hi_version, 8-bit clock_seq_hi_variant, 8-bit clock_seq_low, 48-bit node) as the fields
argument, or a single 128-bit integer as the int argument.
When a string of hex digits is given, curly braces, hyphens,
and a URN prefix are all optional.
UUID(int=0x12345678123456781234567812345678)
Exactly one of hex, bytes, bytes_le, fields, or int must be given.
The version argument is optional; if given, the
resulting UUID will have its variant and version number set according to RFC 4122, overriding bits in the given
hex, bytes, bytes_le, fields, or int.
Comparison of UUID objects are made by way of comparing their UUID.int attributes.
Comparison with a
non-UUID object raises a TypeError.
UUID instances have these read-only attributes:
UUID.bytes
The UUID as a 16-byte string (containing the six integer fields in big-endian byte order).
UUID.bytes_le
The UUID as a 16-byte string (with time_low, time_mid, and time_hi_version in little-endian byte order).
The first 32 bits of the UUID.
UUID.time_mid
The next 16 bits of the UUID.
UUID.time_hi_version
The next 16 bits of the UUID.
UUID.clock_seq_hi_variant
The next 8 bits of the UUID.
UUID.clock_seq_low
The next 8 bits of the UUID.
UUID.node
The last 48 bits of the UUID.
UUID.time
The 60-bit timestamp.
UUID.clock_seq
The 14-bit sequence number.
UUID.hex
The UUID as a 32-character lowercase hexadecimal string.
UUID.int
The UUID as a 128-bit integer.
UUID.urn
The UUID as a URN as specified in RFC 4122.
UUID.variant
The UUID variant, which determines the internal layout of the UUID.
This will be one of the constants
RESERVED_NCS, RFC_4122, RESERVED_MICROSOFT, or RESERVED_FUTURE.
UUID.version
The UUID version number (1 through 5, meaningful only when the variant is RFC_4122).
UUID.is_safe
An enumeration of SafeUUID which indicates whether the platform generated the UUID in a multiprocessing-
safe way.
New in version 3.7.
The uuid module defines the following functions:
uuid.getnode()
Get the hardware address as a 48-bit positive integer.
The first time this runs, it may launch a separate program,
which could be quite slow.
If all attempts to obtain the hardware address fail, we choose a random 48-bit number
with the multicast bit (least significant bit of the first octet) set to 1 as recommended in RFC 4122.
On a machine with multiple network interfaces, universally
21.15.
Changed in version 3.7: Universally administered MAC addresses are preferred over locally administered MAC
addresses, since the former are guaranteed to be globally unique, while the latter are not.
If node is not given, getnode() is
used to obtain the hardware address.
If clock_seq is given, it is used as the sequence number; otherwise a random
14-bit sequence number is chosen.
The uuid module defines the following namespace identifiers for use with uuid3() or uuid5().
uuid.
NAMESPACE_DNS
When this namespace is specified, the name string is a fully qualified domain name.
NAMESPACE_URL
When this namespace is specified, the name string is a URL.
uuid.
NAMESPACE_OID
When this namespace is specified, the name string is an ISO OID.
uuid.
NAMESPACE_X500
When this namespace is specified, the name string is an X.500 DN in DER or a text output format.
The uuid module defines the following constants for the possible values of the variant attribute:
uuid.
RESERVED_NCS
Reserved for NCS compatibility.
RFC_4122
Specifies the UUID layout given in RFC 4122.
uuid.
Reserved for Microsoft compatibility.
RESERVED_FUTURE
Reserved for future definition.
This specification defines a Uniform Re-
source Name namespace for UUIDs, the internal format of UUIDs, and methods of generating UUIDs.
The uuid module can be executed as a script from the command line.
By default uuid4() is used.
The namespace is a UUID, or @ns where ns is a well-known predefined UUID addressed by namespace name.
Such as @dns, @url, @oid, and @x500.
Only required for uuid3() / uuid5() functions.
The name used as part of generating the uuid.
Only required for uuid3() / uuid5() functions.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
There are four basic concrete server classes:
class socketserver.
This uses the internet TCP protocol, which provides for continuous streams of data between the client and
server.
If bind_and_activate is true, the constructor automatically attempts to invoke server_bind() and
server_activate().
The other parameters are passed to the BaseServer base class.
This uses datagrams, which are discrete packets of information that may arrive out of order or be lost while in
transit.
The parameters are the same as for TCPServer.
UnixStreamServer(server_address, RequestHandlerClass, bind_and_activate=True)
class socketserver.
UnixDatagramServer(server_address, RequestHandlerClass,
bind_and_activate=True)
These more infrequently used classes are similar to the TCP and UDP classes, but use Unix domain sockets; they’re
not available on non-Unix platforms.
The parameters are the same as for TCPServer.
These four classes process requests synchronously; each request must be completed before the next request can be started.
This isn’t suitable if each request takes a long time to complete, because it requires a lot of computation, or because it
returns a lot of data which the client is slow to process.
First, you must create a request handler class by subclassing the
BaseRequestHandler class and overriding its handle() method; this method will process incoming requests.
Second, you must instantiate one of the server classes, passing it the server’s address and the request handler class.
It is
recommended to use the server in a with statement.
Then call the handle_request() or serve_forever()
method of the server object to process one or many requests.
Finally, call server_close() to close the socket (unless
you used a with statement).
When inheriting from ThreadingMixIn for threaded connection behavior, you should explicitly declare how you want
your threads to behave on an abrupt shutdown.
The ThreadingMixIn class defines an attribute daemon_threads, which
indicates whether or not the server should wait for thread termination.
You should set the flag explicitly if you would like
threads to behave autonomously; the default is False, meaning that Python will not exit until all threads created by
ThreadingMixIn have exited.
Server classes have the same external methods and attributes, no matter what network protocol they use.
ForkingMixIn
class socketserver.
ThreadingMixIn
Forking and threading versions of each type of server can be created using these mix-in classes.
Setting the various attributes
also changes the behavior of the underlying server mechanism.
ForkingMixIn and the Forking classes mentioned below are only available on POSIX platforms that support
fork().
ThreadingMixIn.server_close
waits
until
all
non-daemon
threads
complete,
except
if
block_on_close attribute is False.
Changed in version 3.7:
ForkingMixIn.server_close and ThreadingMixIn.server_close
now waits until all child processes and non-daemonic threads complete.
Add a new ForkingMixIn.
ForkingTCPServer
class socketserver.
ForkingUDPServer
class socketserver.
ThreadingTCPServer
class socketserver.
ThreadingUDPServer
class socketserver.
ForkingUnixStreamServer
class socketserver.
ForkingUnixDatagramServer
class socketserver.
ThreadingUnixStreamServer
class socketserver.
These classes are pre-defined using the mix-in classes.
New in version 3.12: The ForkingUnixStreamServer and ForkingUnixDatagramServer classes were
added.
To implement a service, you must derive a class from BaseRequestHandler and redefine its handle() method.
You can then run various versions of the service by combining one of the server classes with your request handler class.
The request handler class must be different for datagram or stream services.
This can be hidden by using the handler
subclasses StreamRequestHandler or DatagramRequestHandler.
Of course, you still have to use your head!
For instance, it makes no sense to use a forking server if the service contains
state in memory that can be modified by different requests, since the modifications in the child process would never reach
the initial state kept in the parent process and passed to each child.
In this case, you can use a threading server, but you
will probably have to use locks to protect the integrity of the shared data.
On the other hand, if you are building an HTTP server where all data is stored externally (for instance, in the file system),
a synchronous class will essentially render the service “deaf” while one request is being handled – which may be for a
very long time if a client is slow to receive all the data it has requested.
Here a threading or forking server is appropriate.
In some cases, it may be appropriate to process part of a request synchronously, but to finish processing in a forked child
depending on the request data.
This can be implemented by using a synchronous server and doing an explicit fork in the
request handler class handle() method.
Another approach to handling multiple simultaneous requests in an environment that supports neither threads nor fork()
(or where these are too expensive or inappropriate for the service) is to maintain an explicit table of partially finished
requests and to use selectors to decide which request to work on next (or whether to handle a new incoming request).
This is particularly important for stream services where each client can potentially be connected for a long time (if threads
or subprocesses cannot be used).
This is the superclass of all Server objects in the module.
It defines the interface, given below, but does not
implement most of the methods, which is done in subclasses.
The two parameters are stored in the respective
server_address and RequestHandlerClass attributes.
This function is most commonly
passed to selectors, to allow monitoring multiple servers in the same process.
This function calls the following methods in order:
get_request(),
verify_request(), and process_request().
If the user-provided handle() method of the
handler class raises an exception, the server’s handle_error() method will be called.
Handle requests until an explicit shutdown() request.
Poll for shutdown every poll_interval seconds.
Ignores the timeout attribute.
It also calls service_actions(), which may be used by a subclass
or mixin to provide actions specific to a given service.
For example, the ForkingMixIn class uses
service_actions() to clean up zombie child processes.
Added service_actions call to the serve_forever method.
This is called in the serve_forever() loop.
This method can be overridden by subclasses or mixin
classes to perform actions specific to a given service, such as cleanup actions.
New in version 3.3.
May be overridden.
Common examples are socket.
AF_INET
and socket.
AF_UNIX.
RequestHandlerClass
The user-provided request handler class; an instance of this class is created for each request.
The format of addresses varies depending on the protocol family;
see the documentation for the socket module for details.
For internet protocols, this is a tuple containing
a string giving the address, and an integer port number: ('127.0.0.1', 80), for example.
The server classes support the following class variables:
allow_reuse_address
Whether the server will allow the reuse of an address.
This defaults to False, and can be set in subclasses
to change the policy.
If it takes a long time to process a single request, any requests that arrive while
the server is busy are placed into a queue, up to request_queue_size requests.
Once the queue is full,
further requests from clients will get a “Connection denied” error.
The default value is usually 5, but this can
be overridden by subclasses.
SOCK_DGRAM are two
common values.
If handle_request() receives
no incoming requests within the timeout period, the handle_timeout() method is called.
There are various server methods that can be overridden by subclasses of base server classes like TCPServer;
these methods aren’t useful to external users of the server object.
This function is called if the handle() method of a RequestHandlerClass instance raises an excep-
tion.
The default action is to print the traceback to standard error and continue handling further requests.
Changed in version 3.6: Now only called for exceptions derived from the Exception class.
This function is called when the timeout attribute has been set to a value other than None and the timeout
period has passed with no requests being received.
The default action for forking servers is to collect the
status of any child processes that have exited, while in threading servers this method does nothing.
If desired, this func-
tion can create a new process or thread to handle the request; the ForkingMixIn and ThreadingMixIn
classes do this.
The default behavior for a TCP server just invokes
listen() on the server’s socket.
May be overridden.
May be overridden.
This function can be overridden to implement access controls for a server.
The default
implementation always returns True.
Support for the context manager protocol was added.
Exiting the context manager is
equivalent to calling server_close().
This is the superclass of all request handler objects.
It defines the interface, given below.
A concrete request handler
subclass must define a new handle() method, and can override any of the other methods.
A new instance of the
subclass is created for each request.
This function must do all the work required to service a request.
The default implementation does nothing.
Several instance attributes are available to it; the request is available as request; the client address as
client_address; and the server instance as server, in case it needs access to per-server information.
The type of request is different for datagram or stream services.
For stream services, request is a socket
object; for datagram services, request is a pair of string and socket.
The default implementation
does nothing.
If setup() raises an exception, this function will not be called.
StreamRequestHandler
class socketserver.
These BaseRequestHandler subclasses override the setup() and finish() methods, and provide
rfile and wfile attributes.
Support the io.
BufferedIOBase readable inter-
face.
Support the io.
BufferedIOBase writable interface
Changed in version 3.6: wfile also supports the io.BufferedIOBase writable interface.
This is the server side:
import socketserver
class MyTCPHandler(socketserver.BaseRequestHandler):
"""
The request handler class for our server.
It is instantiated once per connection to the server, and must
override the handle() method to implement communication to the
client.
The difference is that the readline() call in the second handler will call recv() multiple times until it encounters
a newline character, while the single recv() call in the first handler will just return what has been sent from the client
in one sendall() call.
This is the server side:
import socketserver
class MyUDPHandler(socketserver.BaseRequestHandler):
"""
This class works similar to the TCP handler class, except that
self.request consists of a pair of data and client socket, and since
there is no connection the client address must be given explicitly
when sending data back via sendto().
This is the client side:
import socket
import sys
HOST, PORT = "localhost", 9999
data = " ".join(sys.argv[1:])
# SOCK_DGRAM is the socket type to use for UDP sockets
sock = socket.socket(socket.
The output of the example should look exactly like for the TCP server example.
Asynchronous Mixins
To build asynchronous handlers, use the ThreadingMixIn and ForkingMixIn classes.
An example for the ThreadingMixIn class:
import socket
import threading
import socketserver
class ThreadedTCPRequestHandler(socketserver.
ThreadingMixIn, socketserver.
TCPServer):

with socket.socket(socket.
The ForkingMixIn class is used in the same way, except that the server will spawn a new process for each request.
Available only on POSIX platforms that support fork().
This module defines classes for implementing HTTP servers.
Warning: http.server is not recommended for production.
It only implements basic security checks.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
One class, HTTPServer, is a socketserver.
TCPServer subclass.
It creates and listens at the HTTP socket,
dispatching the requests to a handler.
This class builds on the TCPServer class by storing the server address as instance variables named
server_name and server_port.
The server is accessible by the handler, typically through the handler’s
server instance variable.
This class is identical to HTTPServer but uses threads to handle requests by using the ThreadingMixIn.
This
is useful to handle web browsers pre-opening sockets, on which HTTPServer would wait indefinitely.
New in version 3.7.
This class is used to handle the HTTP requests that arrive at the server.
By itself, it cannot respond to
any actual HTTP requests; it must be subclassed to handle each request method (e.g.
GET or POST).
BaseHTTPRequestHandler provides a number of class and instance variables, and methods for use by sub-
classes.
The handler will parse the request and the headers, then call a method specific to the request type.
The method
name is constructed from the request.
For example, for the request method SPAM, the do_SPAM() method will be
21.17.
All of the relevant information is stored in instance variables of the handler.
Subclasses
should not need to override or extend the __init__() method.
BaseHTTPRequestHandler has the following instance variables:
client_address
Contains a tuple of the form (host, port) referring to the client’s address.
The terminating CRLF is stripped.
This attribute
should be set by handle_one_request().
If no valid request line was processed, it should be set to the
empty string.
For example, 'GET'.
If query component of the URL is present, then path includes the query.
Using
the terminology of RFC 3986, path here includes hier-part and the query.
For example, 'HTTP/1.0'.
headers
Holds an instance of the class specified by the MessageClass class variable.
This instance parses and
manages the headers in the HTTP request.
The parse_headers() function from http.client is
used to parse the headers and it requires that the HTTP request provide a valid RFC 2822 style header.
Proper adherence to the HTTP protocol
must be used when writing to this stream in order to achieve successful interoperation with HTTP clients.
This is an io.BufferedIOBase stream.
BaseHTTPRequestHandler has the following attributes:
server_version
Specifies the server software version.
You may want to override this.
The format is multiple whitespace-
separated strings, where each string is of the form name[/version].
For example, 'BaseHTTP/0.2'.
sys_version
Contains the Python system version, in a form usable by the version_string method and the
server_version class variable.
For example, 'Python/1.4'.
error_message_format
Specifies a format string that should be used by send_error() method for building an error response to
the client.
The default value is 'text/
html'.
It is sent in responses to let the client know the
server’s communication capabilities for future requests.
If set to 'HTTP/1.1', the server will permit HTTP
persistent connections; however, your server must then include an accurate Content-Length header (us-
ing send_header()) in all of its responses to clients.
For backwards compatibility, the setting defaults to
'HTTP/1.0'.
MessageClass
Specifies an email.message.
Message-like class to parse HTTP headers.
Typically, this is not over-
ridden, and it defaults to http.client.
HTTPMessage.
For example, {code: (shortmessage, longmessage)}.
The shortmessage is usu-
ally used as the message key in an error response, and longmessage as the explain key.
It is used by
send_response_only() and send_error() methods.
A BaseHTTPRequestHandler instance has the following methods:
handle()
Calls handle_one_request() once (or, if persistent connections are enabled, multiple times) to handle
incoming HTTP requests.
You should never need to override it; instead, implement appropriate do_*()
methods.
This method will parse and dispatch the request to the appropriate do_*() method.
You should never need
to override it.
When an HTTP/1.1 conformant server receives an Expect: 100-continue request header it responds
back with a 100 Continue followed by 200 OK headers.
This method can be overridden to raise an error
if the server does not want the client to continue.
For e.g. server can choose to send 417 Expectation
Failed as a response header and return False.
New in version 3.2.
The numeric code specifies the HTTP error code, with mes-
sage as an optional, short, human readable description of the error.
The explain argument can be used to pro-
vide more detailed information about the error; it will be formatted using the error_message_format
attribute and emitted, after a complete set of headers, as the response body.
The responses attribute holds
the default values for message and explain that will be used if no value is provided; for unknown codes the
default value for both is the string ???.
The body will be empty if the method is HEAD or the response code
is one of the following: 1xx, 204 No Content, 205 Reset Content, 304 Not Modified.
The error response includes a Content-Length header.
Added the explain argument.
The HTTP response line is written
to the internal buffer, followed by Server and Date headers.
The values for these two headers are picked
up from the version_string() and date_time_string() methods, respectively.
If the server
does not intend to send any other headers using the send_header() method, then send_response()
should be followed by an end_headers() call.
Note that, after the send_header calls are done, end_headers() MUST BE
called in order to complete the operation.
Changed in version 3.2: Headers are stored in an internal buffer.
The headers not buffered and sent directly the output stream.
If the message is not specified, the
HTTP message corresponding the response code is sent.
New in version 3.2.
Changed in version 3.2: The buffered headers are written to the output stream.
New in version 3.3.
Logs an accepted (successful) request.
If a size of the response is available, then it should be passed as the size parameter.
Logs an error when a request cannot be fulfilled.
Logs an arbitrary message to sys.stderr.
This is typically overridden to create custom error logging
mechanisms.
The format argument is a standard printf-style format string, where the additional arguments to
log_message() are applied as inputs to the formatting.
The client ip address and current date and time
are prefixed to every message logged.
This is a combination of the server_version and
sys_version attributes.
If timestamp is omitted, it uses the current date and time.
The result looks like 'Sun, 06 Nov 1994 08:49:37 GMT'.
To avoid name resolution delays, it now
always returns the IP address.
This class serves files from the directory directory and below, or the current directory if directory is not provided,
directly mapping the directory structure to HTTP requests.
The directory parameter.
The directory parameter accepts a path-like object.
A lot of the work, such as parsing the request, is done by the base class BaseHTTPRequestHandler.
This
class implements the do_GET() and do_HEAD() functions.
The mapping is used case-insensitively, and so should contain only lower-cased keys.
This dictionary is no longer filled with the default system mappings, but only contains
overrides.
This method serves the 'HEAD' request type: it sends the headers it would send for the equivalent GET
request.
See the do_GET() method for a more complete explanation of the possible headers.
If the request was mapped to a directory, the directory is checked for a file named index.html or index.
If found, the file’s contents are returned; otherwise a directory listing is generated by
calling the list_directory() method.
This method uses os.listdir() to scan the directory, and
returns a 404 error response if the listdir() fails.
If the request was mapped to a file, it is opened.
Any OSError exception in opening the requested file is
mapped to a 404, 'File not found' error.
If there was a 'If-Modified-Since' header in the
request, and the file was not modified after this time, a 304, 'Not Modified' response is sent.
Otherwise,
the content type is guessed by calling the guess_type() method, which in turn uses the extensions_map
variable, and the file contents are returned.
A
'Content-type:'
header
with
the
guessed
content
type
is
output,
followed
by
a
'Content-Length:'
header with the file’s size and a 'Last-Modified:'
header with the
file’s modification time.
Then follows a blank line signifying the end of the headers, and then the contents of the file are output.
If the
file’s MIME type starts with text/ the file is opened in text mode; otherwise binary mode is used.
Support of the 'If-Modified-Since' header.
The SimpleHTTPRequestHandler class can be used in the following manner in order to create a very basic web-
server serving files relative to the current directory:
21.17.
TCPServer(("", PORT), Handler) as httpd:
print("serving at port", PORT)
httpd.serve_forever()
SimpleHTTPRequestHandler can also be subclassed to enhance behavior, such as using different index file names
by overriding the class attribute index_pages.
The default can be overridden by passing the desired port number as an
argument:
python -m http.server 9000
By default, the server binds itself to all interfaces.
The option -b/--bind specifies a specific address to which it should
bind.
Both IPv4 and IPv6 addresses are supported.
For example, the following command causes the server to bind to
localhost only:
python -m http.server --bind 127.0.0.1
New in version 3.4: --bind argument was introduced.
New in version 3.8: --bind argument enhanced to support IPv6
By default, the server uses the current directory.
The option -d/--directory specifies a directory to which it should
serve the files.
By default, the server is conformant to HTTP/1.0.
The option -p/--protocol specifies the HTTP version to which
the server is conformant.
This class is used to serve either files or output of CGI scripts from the current directory and below.
Note that map-
ping HTTP hierarchic structure to local directory structure is exactly as in SimpleHTTPRequestHandler.
Note: CGI scripts run by the CGIHTTPRequestHandler class cannot execute redirects (HTTP code 302),
because code 200 (script output follows) is sent prior to execution of the CGI script.
The class will however, run the CGI script, instead of serving it as a file, if it guesses it to be a CGI script.
Only
directory-based CGI are used — the other common server configuration is to treat special extensions as denoting
CGI scripts.
The do_GET() and do_HEAD() functions are modified to run CGI scripts and serve the output, instead of
serving files, if the request leads to somewhere below the cgi_directories path.
The CGIHTTPRequestHandler defines the following data member:
cgi_directories
This defaults to ['/cgi-bin', '/htbin'] and describes directories to treat as containing CGI scripts.
This method serves the 'POST' request type, only allowed for CGI scripts.
Error 501, “Can only POST to
CGI scripts”, is output when trying to POST to a non-CGI url.
Note that CGI scripts will be run with UID of user nobody, for security reasons.
Earlier versions of Python did not scrub control characters from the log messages emitted to stderr from python -m
http.server or the default BaseHTTPRequestHandler .log_message implementation.
This could allow
remote clients connecting to your server to send nefarious control codes to your terminal.
New in version 3.12: Control characters are scrubbed in stderr logs.
It supports both simple string-only cookies, and provides an abstraction for having any serializable data-type as
cookie value.
The module formerly strictly applied the parsing rules described in the RFC 2109 and RFC 2068 specifications.
It has
since been discovered that MSIE 3.0x doesn’t follow the character rules outlined in those specs and also many current day
browsers and servers have relaxed parsing rules when comes to Cookie handling.
As a result, the parsing rules used are a
bit less strict.
The character set, string.ascii_letters, string.digits and !#$%&'*+-.^_`|~: denote the set of
valid characters allowed by this module in Cookie name (as key).
Changed in version 3.3: Allowed ‘:’ as a valid Cookie name character.
CookieError
Exception failing because of RFC 2109 invalidity: incorrect attributes, incorrect Set-Cookie header, etc.
This class is a dictionary-like object whose keys are strings and whose values are Morsel instances.
Note that
upon setting a key to a value, the value is first converted to a Morsel containing the key and the value.
If input is given, it is passed to the load() method.
This class derives from BaseCookie and overrides value_decode() and value_encode().
Simple-
Cookie supports strings as cookie values.
When setting the value, SimpleCookie calls the builtin str() to convert
the value to a string.
Values received from HTTP are kept as strings.
See also:
Module http.cookiejar HTTP cookie handling for web clients.
The http.cookiejar and http.cookies
modules do not depend on each other.
This is the state management specification implemented by this
module.
This method does no decoding in BaseCookie — it exists so it can be overridden.
BaseCookie.value_encode(val)
Return a tuple (real_value, coded_value).
This method does no encoding in BaseCookie — it exists so it can be overridden.
In general, it should be the case that value_encode() and value_decode() are inverses on the range of
value_decode.
BaseCookie.output(attrs=None, header=’Set-Cookie:’, sep=’\r\n’)
Return a string representation suitable to be sent as HTTP headers.
BaseCookie.js_output(attrs=None)
Return an embeddable JavaScript snippet, which, if run on a browser which supports JavaScript, will act the same
as if the HTTP headers was sent.
If rawdata is a string, parse it as an HTTP_COOKIE and add the values found there as Morsels.
Morsel
Abstract a key/value pair, which has some RFC 2109 attributes.
Morsels are dictionary-like objects, whose set of keys is constant — the valid RFC 2109 attributes, which are
• expires
• path
• comment
• domain
• max-age
• secure
• version
• httponly
• samesite
The attribute httponly specifies that the cookie is only transferred in HTTP requests, and is not accessible
through JavaScript.
This is intended to mitigate some forms of cross-site scripting.
The attribute samesite specifies that the browser is not allowed to send the cookie along with cross-site requests.
This helps to mitigate CSRF attacks.
Valid values for this attribute are “Strict” and “Lax”.
The keys are case-insensitive and their default value is ''.
Changed in version 3.5: __eq__() now takes key and value into account.
Attributes key, value and coded_value are read-only.
Use set() for setting them.
Changed in version 3.8: Added support for the samesite attribute.
Morsel.value
The value of the cookie.
Morsel.coded_value
The encoded value of the cookie — this is what should be sent.
Whether K is a member of the set of keys of a Morsel.
Morsel.output(attrs=None, header=’Set-Cookie:’)
Return a string representation of the Morsel, suitable to be sent as an HTTP header.
By default, all the attributes
are included, unless attrs is given, in which case it should be a list of attributes to use.
Morsel.js_output(attrs=None)
Return an embeddable JavaScript snippet, which, if run on a browser which supports JavaScript, will act the same
as if the HTTP header was sent.
The meaning for attrs is the same as in output().
21.18.
OutputString(attrs=None)
Return a string representing the Morsel, without any surrounding HTTP or JavaScript.
The meaning for attrs is the same as in output().
Morsel.update(values)
Update the values in the Morsel dictionary with the values in the dictionary values.
Raise an error if any of the
keys in the values dict is not a valid RFC 2109 attribute.
Changed in version 3.5: an error is raised for invalid keys.
Morsel.copy(value)
Return a shallow copy of the Morsel object.
Changed in version 3.5: return a Morsel object instead of a dict.
Morsel.setdefault(key, value=None)
Raise an error if key is not a valid RFC 2109 attribute, otherwise behave the same as dict.setdefault().
It is useful for accessing web
sites that require small pieces of data – cookies – to be set on the client machine by an HTTP response from a web server,
and then returned to the server in later HTTP requests.
Both the regular Netscape cookie protocol and the protocol defined by RFC 2965 are handled.
RFC 2965 handling is
switched off by default.
RFC 2109 cookies are parsed as Netscape cookies and subsequently treated either as Netscape or
RFC 2965 cookies according to the ‘policy’ in effect.
Note that the great majority of cookies on the internet are Netscape
cookies.
Note:
The various named parameters found in Set-Cookie and Set-Cookie2 headers (eg.
domain and
expires) are conventionally referred to as attributes.
To distinguish them from Python attributes, the documentation
for this module uses the term cookie-attribute instead.
The module defines the following exception:
exception http.cookiejar.
LoadError
Instances of FileCookieJar raise this exception on failure to load cookies from a file.
LoadError is a
subclass of OSError.
Changed in version 3.3: LoadError used to be a subtype of IOError, which is now an alias of OSError.
The following classes are provided:
class http.cookiejar.
CookieJar(policy=None)
policy is an object implementing the CookiePolicy interface.
The CookieJar class stores HTTP cookies.
It extracts cookies from HTTP requests, and returns them in HTTP
responses.
CookieJar instances automatically expire contained cookies when necessary.
Subclasses are also
responsible for storing and retrieving cookies from a file or database.
FileCookieJar(filename=None, delayload=None, policy=None)
policy is an object implementing the CookiePolicy interface.
For the other arguments, see the documentation
for the corresponding attributes.
21.19.
Cookies are NOT
loaded from the named file until either the load() or revert() method is called.
Subclasses of this class are
documented in section FileCookieJar subclasses and co-operation with web browsers.
This should not be initialized directly – use its subclasses below instead.
The filename parameter supports a path-like object.
CookiePolicy
This class is responsible for deciding whether each cookie should be accepted from / returned to the server.
DefaultCookiePolicy(blocked_domains=None, allowed_domains=None,
netscape=True, rfc2965=False,
rfc2109_as_netscape=None, hide_cookie2=False,
strict_domain=False, strict_rfc2965_unverifiable=True,
strict_ns_unverifiable=False,
strict_ns_domain=DefaultCookiePolicy.
Constructor arguments should be passed as keyword arguments only.
By default https and wss (secure websocket) are considered secure protocols.
For
all other arguments, see the documentation for CookiePolicy and DefaultCookiePolicy objects.
DefaultCookiePolicy implements the standard accept / reject rules for Netscape and RFC 2965 cook-
ies.
By default, RFC 2109 cookies (ie.
However, if RFC 2965 handling is turned off
or rfc2109_as_netscape is True, RFC 2109 cookies are ‘downgraded’ by the CookieJar instance to
Netscape cookies, by setting the version attribute of the Cookie instance to 0.
DefaultCookiePolicy
also provides some parameters to allow some fine-tuning of policy.
Cookie
This class represents Netscape, RFC 2109 and RFC 2965 cookies.
It is not expected that users of http.
Instead, if necessary, call make_cookies() on a
CookieJar instance.
See also:
Module urllib.request URL opening with automatic cookie handling.
Module http.cookies HTTP cookie classes, principally useful for server-side code.
The http.cookiejar and
http.cookies modules do not depend on each other.
The specification of the original Netscape cookie protocol.
Though this is still
the dominant protocol, the ‘Netscape cookie protocol’ implemented by all the major browsers (and http.
RFC 2109 - HTTP State Management Mechanism Obsoleted by RFC 2965.
The Netscape protocol with the bugs fixed.
Uses Set-Cookie2
in place of Set-Cookie.
Not widely used.
CookieJar has the following methods:
CookieJar.add_cookie_header(request)
Add correct Cookie header to request.
If policy allows (ie.
The
request
object
(usually
a
urllib.request.
Request
instance)
must
support
the
methods
get_full_url(),
has_header(),
get_header(),
header_items(),
add_unredirected_header()
and
the
attributes
host,
type,
unverifiable
and
origin_req_host as documented by urllib.request.
Changed in version 3.3: request object needs origin_req_host attribute.
Dependency on a deprecated method
get_origin_req_host() has been removed.
Extract cookies from HTTP response and store them in the CookieJar, where allowed by policy.
The CookieJar will look for allowable Set-Cookie and Set-Cookie2 headers in the response argument,
and store cookies as appropriate (subject to the CookiePolicy.set_ok() method’s approval).
The response object (usually the result of a call to urllib.request.urlopen(), or similar) should support
an info() method, which returns an email.message.
Message instance.
The
request
object
(usually
a
urllib.request.
Request
instance)
must
support
the
method
get_full_url() and the attributes host, unverifiable and origin_req_host, as documented by
urllib.request.
The request is used to set default values for cookie-attributes as well as for checking that
the cookie is allowed to be set.
Changed in version 3.3: request object needs origin_req_host attribute.
Dependency on a deprecated method
get_origin_req_host() has been removed.
CookieJar.set_policy(policy)
Set the CookiePolicy instance to be used.
CookieJar.make_cookies(response, request)
Return sequence of Cookie objects extracted from response object.
See the documentation for extract_cookies() for the interfaces required of the response and request argu-
ments.
CookieJar.set_cookie_if_ok(cookie, request)
Set a Cookie if policy says it’s OK to do so.
CookieJar.set_cookie(cookie)
Set a Cookie, without checking with policy to see whether or not it should be set.
Clear some cookies.
If invoked without arguments, clear all cookies.
If given a single argument, only cookies belonging to that domain
will be removed.
If given two arguments, cookies belonging to the specified domain and URL path are removed.
If given three arguments, then the cookie with the specified domain, path and name is removed.
Raises KeyError if no matching cookie exists.
21.19.
Discard all session cookies.
Discards all contained cookies that have a true discard attribute (usually because they had either no max-age
or expires cookie-attribute, or an explicit discard cookie-attribute).
For interactive browsers, the end of a
session usually corresponds to closing the browser window.
Note that the save() method won’t save session cookies anyway, unless you ask otherwise by passing a true
ignore_discard argument.
FileCookieJar implements the following additional methods:
FileCookieJar.save(filename=None, ignore_discard=False, ignore_expires=False)
Save cookies to a file.
This base class raises NotImplementedError.
Subclasses may leave this method unimplemented.
If filename is not specified, self.filename is used
(whose default is the value passed to the constructor, if any); if self.filename is None, ValueError is
raised.
Saved cookies can be restored
later using the load() or revert() methods.
Load cookies from a file.
Old cookies are kept unless overwritten by newly loaded ones.
Arguments are as for save().
The named file must be in the format understood by the class, or LoadError will be raised.
Also, OSError
may be raised, for example if the file does not exist.
Changed in version 3.3: IOError used to be raised, it is now an alias of OSError.
FileCookieJar.revert(filename=None, ignore_discard=False, ignore_expires=False)
Clear all cookies and reload cookies from a saved file.
If there is a failure, the object’s state will not be altered.
FileCookieJar instances have the following public attributes:
FileCookieJar.filename
Filename of default file in which to keep cookies.
This attribute may be assigned to.
FileCookieJar.delayload
If true, load cookies lazily from disk.
This attribute should not be assigned to.
This is only a hint, since this only
affects performance, not behaviour (unless the cookies on disk are changing).
A CookieJar object may ignore
it.
The following CookieJar subclasses are provided for reading and writing.
MozillaCookieJar(filename=None, delayload=None, policy=None)
A FileCookieJar that can load from and save cookies to disk in the Mozilla cookies.txt file format
(which is also used by curl and the Lynx and Netscape browsers).
Note: This loses information about RFC 2965 cookies, and also about newer or non-standard cookie-attributes
such as port.
Warning: Back up your cookies before saving if you have cookies whose loss / corruption would be inconve-
nient (there are some subtleties which may lead to slight changes in the file over a load / save round-trip).
Also note that cookies saved while Mozilla is running will get clobbered by Mozilla.
A FileCookieJar that can load from and save cookies to disk in format compatible with the libwww-perl
library’s Set-Cookie3 file format.
This is convenient if you want to store cookies in a human-readable file.
The filename parameter supports a path-like object.
CookiePolicy.return_ok(cookie, request)
Return boolean value indicating whether cookie should be returned to server.
CookiePolicy.domain_return_ok(domain, request)
Return False if cookies should not be returned, given cookie domain.
This method is an optimization.
It removes the need for checking every cookie with a particular domain (which
might involve reading many files).
Returning true from domain_return_ok() and path_return_ok()
leaves all the work to return_ok().
If domain_return_ok() returns true for the cookie domain, path_return_ok() is called for the cookie
path.
Otherwise, path_return_ok() and return_ok() are never called for that cookie domain.
If
path_return_ok() returns true, return_ok() is called with the Cookie object itself for a full check.
Otherwise, return_ok() is never called for that cookie path.
Note that domain_return_ok() is called for every cookie domain, not just for the request domain.
For ex-
ample, the function might be called with both ".example.com" and "www.example.com" if the request
domain is "www.example.com".
The same goes for path_return_ok().
The request argument is as documented for return_ok().
21.19.
See the documentation for domain_return_ok().
In addition to implementing the methods above, implementations of the CookiePolicy interface must also supply the
following attributes, indicating which protocols should be used, and how.
All of these attributes may be assigned to.
CookiePolicy.netscape
Implement Netscape protocol.
CookiePolicy.rfc2965
Implement RFC 2965 protocol.
CookiePolicy.hide_cookie2
Don’t add Cookie2 header to requests (the presence of this header indicates to the server that we understand
RFC 2965 cookies).
The most useful way to define a CookiePolicy class is by subclassing from DefaultCookiePolicy and overrid-
ing some or all of the methods above.
CookiePolicy itself may be used as a ‘null policy’ to allow setting and receiving
any and all cookies (this is unlikely to be useful).
Both RFC 2965 and Netscape cookies are covered.
RFC 2965 handling is switched off by default.
The easiest way to provide your own policy is to override this class and call its methods in your overridden implementations
before adding your own additional checks:
import http.cookiejar
class MyCookiePolicy(http.cookiejar.
In addition to the features required to implement the CookiePolicy interface, this class allows you to block and allow
domains from setting and receiving cookies.
There are also some strictness switches that allow you to tighten up the rather
loose Netscape protocol rules a little bit (at the cost of blocking some benign cookies).
A domain blocklist and allowlist is provided (both off by default).
Only domains not in the blocklist and present in
the allowlist (if the allowlist is active) participate in cookie setting and returning.
Use the blocked_domains constructor
argument, and blocked_domains() and set_blocked_domains() methods (and the corresponding argument
and methods for allowed_domains).
If you set an allowlist, you can turn it off again by setting it to None.
Domains in block or allow lists that do not start with a dot must equal the cookie domain to be matched.
For example,
"example.com" matches a blocklist entry of "example.com", but "www.example.com" does not.
Domains
that do start with a dot are matched by more specific domains too.
IP addresses
are an exception, and must match exactly.
For example, if blocked_domains contains "192.168.1.2" and ".168.
1.2", 192.168.1.2 is blocked, but 193.168.1.2 is not.
DefaultCookiePolicy.set_blocked_domains(blocked_domains)
Set the sequence of blocked domains.
DefaultCookiePolicy.is_blocked(domain)
Return True if domain is on the blocklist for setting or receiving cookies.
DefaultCookiePolicy.allowed_domains()
Return None, or the sequence of allowed domains (as a tuple).
DefaultCookiePolicy.set_allowed_domains(allowed_domains)
Set the sequence of allowed domains, or None.
DefaultCookiePolicy.is_not_allowed(domain)
Return True if domain is not on the allowlist for setting or receiving cookies.
DefaultCookiePolicy instances have the following attributes, which are all initialised from the constructor argu-
ments of the same name, and which may all be assigned to.
RFC 2109 cookies (ie.
cookies received in a
Set-Cookie header with a version cookie-attribute of 1) to Netscape cookies by setting the version attribute
of the Cookie instance to 0.
The default value is None, in which case RFC 2109 cookies are downgraded if and
only if RFC 2965 handling is turned off.
Therefore, RFC 2109 cookies are downgraded by default.
General strictness switches:
DefaultCookiePolicy.strict_domain
Don’t allow sites to set two-component domains with country-code top-level domains like .co.uk, .gov.uk,
.co.nz.etc.
This is far from perfect and isn’t guaranteed to work!
RFC 2965 protocol strictness switches:
DefaultCookiePolicy.strict_rfc2965_unverifiable
Follow RFC 2965 rules on unverifiable transactions (usually, an unverifiable transaction is one resulting from a
redirect or a request for an image hosted on another site).
If this is false, cookies are never blocked on the basis of
verifiability
Netscape protocol strictness switches:
DefaultCookiePolicy.strict_ns_unverifiable
Apply RFC 2965 rules on unverifiable transactions even to Netscape cookies.
DefaultCookiePolicy.strict_ns_domain
Flags indicating how strict to be with domain-matching rules for Netscape cookies.
See below for acceptable values.
DefaultCookiePolicy.strict_ns_set_initial_dollar
Ignore cookies in Set-Cookie: headers that have names starting with '$'.
DefaultCookiePolicy.strict_ns_set_path
Don’t allow setting cookies whose path doesn’t path-match request URI.
Its value is constructed by or-ing together (for example,
DomainStrictNoDots|DomainStrictNonDomain means both flags are set).
DefaultCookiePolicy.
When setting cookies, the ‘host prefix’ must not contain a dot (eg. www.foo.bar.com can’t set a cookie for
.bar.com, because www.foo contains a dot).
DefaultCookiePolicy.
When setting cookies, require a full RFC 2965 domain-match.
The following attributes are provided for convenience, and are the most useful combinations of the above flags:
DefaultCookiePolicy.
DomainLiberal
Equivalent to 0 (ie.
DefaultCookiePolicy.
DomainStrict
Equivalent to DomainStrictNoDots|DomainStrictNonDomain.
The correspondence is not one-to-one, because there are complicated rules for assigning default values,
because the max-age and expires cookie-attributes contain equivalent information, and because RFC 2109 cookies
may be ‘downgraded’ by http.cookiejar from version 1 to version 0 (Netscape) cookies.
Assignment to these attributes should not be necessary other than in rare circumstances in a CookiePolicy method.
The class does not enforce internal consistency, so you should know what you’re doing if you do that.
Cookie.version
Integer or None.
Netscape cookies have version 0.
RFC 2965 and RFC 2109 cookies have a version
cookie-attribute of 1.
However, note that http.cookiejar may ‘downgrade’ RFC 2109 cookies to Netscape
cookies, in which case version is 0.
Cookie.name
Cookie name (a string).
Cookie.value
Cookie value (a string), or None.
Cookie.port
String representing a port or a set of ports (eg.
Cookie.secure
True if cookie should only be returned over a secure connection.
Cookie.expires
Integer expiry date in seconds since epoch, or None.
See also the is_expired() method.
Cookie.discard
True if this is a session cookie.
This attribute is provided because http.cookiejar
may ‘downgrade’ RFC 2109 cookies to Netscape cookies, in which case version is 0.
Cookie.port_specified
True if a port or set of ports was explicitly specified by the server (in the Set-Cookie / Set-Cookie2
header).
Cookie.domain_specified
True if a domain was explicitly specified by the server.
Cookie.domain_initial_dot
True if the domain explicitly specified by the server began with a dot ('.').
Cookies may have additional non-standard cookie-attributes.
These may be accessed using the following methods:
Cookie.has_nonstandard_attr(name)
Return True if cookie has the named cookie-attribute.
If cookie has the named cookie-attribute, return its value.
Otherwise, return default.
Cookie.set_nonstandard_attr(name, value)
Set the value of the named cookie-attribute.
The Cookie class also defines the following method:
Cookie.is_expired(now=None)
True if cookie has passed the time at which the server requested it should expire.
If now is given (in seconds since
the epoch), return whether the cookie has expired at the specified time.
The next example illustrates the use of DefaultCookiePolicy.
Turn on RFC 2965 cookies, be more strict about
domains when setting and returning Netscape cookies, and block some domains from setting cookies or having them
returned:
21.19.
DomainStrict,
blocked_domains=["ads.net", ".ads.net"])
cj = CookieJar(policy)
opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))
r = opener.open("http://example.com/")
21.20 xmlrpc — XMLRPC server and client modules
XML-RPC is a Remote Procedure Call method that uses XML passed via HTTP as a transport.
With it, a client can call
methods with parameters on a remote server (the server is named by a URI) and get back structured data.
The modules are:
• xmlrpc.client
• xmlrpc.server
21.21 xmlrpc.client — XML-RPC client access
Source code: Lib/xmlrpc/client.py
XML-RPC is a Remote Procedure Call method that uses XML passed via HTTP(S) as a transport.
With it, a client can
call methods with parameters on a remote server (the server is named by a URI) and get back structured data.
This module
supports writing XML-RPC client code; it handles all the details of translating between conformable Python objects and
XML on the wire.
Warning: The xmlrpc.client module is not secure against maliciously constructed data.
If you need to parse
untrusted or unauthenticated data see XML vulnerabilities.
For HTTPS URIs, xmlrpc.client now performs all the necessary certificate and hostname
checks by default.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
A ServerProxy instance is an object that manages communication with a remote XML-RPC server.
The
required first argument is a URI (Uniform Resource Indicator), and will normally be the URL of the server.
The
optional second argument is a transport factory instance; by default it is an internal SafeTransport instance for
https: URLs and an internal HTTP Transport instance otherwise.
The optional third argument is an encoding,
by default UTF-8.
The optional fourth argument is a debugging flag.
The following parameters govern the use of the returned proxy instance.
If allow_none is true, the Python con-
stant None will be translated into XML; the default behaviour is for None to raise a TypeError.
The use_builtin_types flag can be used to cause
date/time values to be presented as datetime.datetime objects and binary data to be presented as bytes
objects; this flag is false by default.
The headers parameter is an optional sequence of HTTP headers to send with each request, expressed as a
sequence of 2-tuples representing the header name and value.
The use_builtin_types flag was added.
The headers parameter was added.
Both the HTTP and HTTPS transports support the URL syntax extension for HTTP Basic Authentication: http:/
/user:pass@host:port/path.
The user:pass portion will be base64-encoded as an HTTP ‘Authorization’
header, and sent to the remote server as part of the connection process when invoking an XML-RPC method.
You only
need to use this if the remote server requires a Basic Authentication user and password.
If an HTTPS URL is provided,
context may be ssl.
SSLContext and configures the SSL settings of the underlying HTTPS connection.
The returned instance is a proxy object with methods that can be used to invoke corresponding RPC calls on the remote
server.
If the remote server supports the introspection API, the proxy can also be used to query the remote server for the
methods it supports (service discovery) and fetch other server-associated metadata.
Types that are conformable (e.g. that can be marshalled through XML), include the following (and except where noted,
they are unmarshalled as the same Python type):
XML-RPC type
Python type
boolean
bool
int, i1, i2, i4, i8 or biginteger
int in range from -2147483648 to 2147483647.
Values get the <int>
tag.
Values get the <double> tag.
string
str
array
list or tuple containing conformable elements.
Arrays are returned
as lists.
Keys must be strings, values may be any conformable type.
Objects of user-defined classes can be passed in; only their __dict__
attribute is transmitted.
Returned type depends on
values of use_builtin_types and use_datetime flags.
Returned type depends on the value
of the use_builtin_types flag.
Passing is allowed only if allow_none is true.
Decimal.
Returned type only.
This is the full set of data types supported by XML-RPC.
Method calls may also raise a special Fault instance, used to
signal XML-RPC server errors, or ProtocolError used to signal an error in the HTTP/HTTPS transport layer.
Both
Fault and ProtocolError derive from a base class called Error.
Note that the xmlrpc client module currently
does not marshal instances of subclasses of built-in types.
When passing strings, characters special to XML such as <, >, and & will be automatically escaped.
However, it’s the
caller’s responsibility to ensure that the string is free of characters that aren’t allowed in XML, such as the control characters
with ASCII values between 0 and 31 (except, of course, tab, newline and carriage return); failing to do this will result
in an XML-RPC request that isn’t well-formed XML.
If you have to pass arbitrary bytes via XML-RPC, use bytes or
bytearray classes or the Binary wrapper class described below.
Server is retained as an alias for ServerProxy for backwards compatibility.
New code should use ServerProxy.
Added the context argument.
Added support of unmarshalling
additional types used by Apache XML-RPC implementation for numerics: i1, i2, i8, biginteger, float and
bigdecimal.
See https://ws.apache.org/xmlrpc/types.html for a description.
A good description of XML-RPC operation and client software in several languages.
Contains
pretty much everything an XML-RPC client developer needs to know.
XML-RPC Introspection Describes the XML-RPC protocol extension for introspection.
XML-RPC Specification The official specification.
Calling the method performs an RPC, dispatched by both name and argument signature (e.g. the same method name can
be overloaded with multiple argument signatures).
The RPC finishes by returning a value, which may be either returned
data in a conformant type or a Fault or ProtocolError object indicating an error.
This method takes one parameter, the name of a method implemented by the XML-RPC server.
It returns an array
of possible signatures for this method.
A signature is an array of types.
The first of these types is the return type
of the method, the rest are parameters.
Because multiple signatures (ie. overloading) is permitted, this method returns a list of signatures rather than a
singleton.
Signatures themselves are restricted to the top level parameters expected by a method.
For instance if a method
expects one array of structs as a parameter, and it returns a string, its signature is simply “string, array”.
If it expects
three integers and returns a string, its signature is “string, int, int, int”.
If no signature is defined for the method, a non-array value is returned.
This method takes one parameter, the name of a method implemented by the XML-RPC server.
It returns a
documentation string describing the use of that method.
If no such string is available, an empty string is returned.
The documentation string may contain HTML markup.
Changed in version 3.5: Instances of ServerProxy support the context manager protocol for closing the underlying
transport.
A working example follows.
The client code for the preceding server:
import xmlrpc.client
with xmlrpc.client.
This class may be initialized with seconds since the epoch, a time tuple, an ISO 8601 time/date string, or a
datetime.datetime instance.
It has the following methods, supported mainly for internal use by the mar-
shalling/unmarshalling code:
decode(string)
Accept a string as the instance’s new time value.
encode(out)
Write the XML-RPC encoding of this DateTime item to the out stream object.
It also supports certain of Python’s built-in operators through rich comparison and __repr__() methods.
A working example follows.
The client code for the preceding server:
import xmlrpc.client
import datetime
proxy = xmlrpc.client.
ServerProxy("http://localhost:8000/")
today = proxy.today()
# convert the ISO8601 string to a datetime object
converted = datetime.datetime.strptime(today.value, "%Y%m%dT%H:%M:%S")
print("Today: %s" % converted.strftime("%d.%m.%Y, %H:%M"))
21.21.
Binary
This class may be initialized from bytes data (which may include NULs).
The primary access to the content of a
Binary object is provided by an attribute:
data
The binary data encapsulated by the Binary instance.
The data is provided as a bytes object.
Binary objects have the following methods, supported mainly for internal use by the marshalling/unmarshalling
code:
decode(bytes)
Accept a base64 bytes object and decode it as the instance’s new data.
encode(out)
Write the XML-RPC base 64 encoding of this binary item to the out stream object.
The encoded data will have newlines every 76 characters as per RFC 2045 section 6.8, which was the de
facto standard base64 specification when the XML-RPC spec was written.
It also supports certain of Python’s built-in operators through __eq__() and __ne__() methods.
Example usage of the binary objects.
ServerProxy("http://localhost:8000/")
with open("fetched_python_logo.jpg", "wb") as handle:
handle.write(proxy.python_logo().data)
21.21.4 Fault Objects
class xmlrpc.client.
Fault
A Fault object encapsulates the content of an XML-RPC fault tag.
Fault objects have the following attributes:
faultCode
An int indicating the fault type.
In the following example we’re going to intentionally cause a Fault by returning a complex type object.
The client code for the preceding server:
import xmlrpc.client
proxy = xmlrpc.client.
Fault as err:
print("A fault occurred")
print("Fault code: %d" % err.faultCode)
print("Fault string: %s" % err.faultString)
21.21.5 ProtocolError Objects
class xmlrpc.client.
ProtocolError
A ProtocolError object describes a protocol error in the underlying transport layer (such as a 404 ‘not found’
error if the server named by the URI does not exist).
It has the following attributes:
url
The URI or URL that triggered the error.
In the following example we’re going to intentionally cause a ProtocolError by providing an invalid URI:
import xmlrpc.client
# create a ServerProxy with a URI that doesn't respond to XMLRPC requests
proxy = xmlrpc.client.
ServerProxy("http://google.com/")
try:
proxy.some_method()
except xmlrpc.client.
ProtocolError as err:
(continues on next page)
21.21.
MultiCall(server)
Create an object used to boxcar method calls.
Calls can be made to the result
object, but they will immediately return None, and only store the call name and parameters in the MultiCall
object.
Calling the object itself causes all stored calls to be transmitted as a single system.multicall request.
The result of this call is a generator; iterating over this generator yields the individual results.
A usage example of this class follows.
The client code for the preceding server:
import xmlrpc.client
proxy = xmlrpc.client.
ServerProxy("http://localhost:8000/")
multicall = xmlrpc.client.
If methodresponse is true, only a single value can
be returned, meaning that params must be of length 1.
None value cannot be used in standard XML-RPC; to allow using it via an
extension, provide a true value for allow_none.
If the XML-RPC
packet represents a fault condition, this function will raise a Fault exception.
The use_builtin_types flag can be
used to cause date/time values to be presented as datetime.datetime objects and binary data to be presented
as bytes objects; this flag is false by default.
The use_builtin_types flag was added.
The following example
shows how:
import http.client
import xmlrpc.client
class ProxiedTransport(xmlrpc.client.Transport):

self.proxy_headers = headers

21.21.
Servers can either be free standing, using SimpleXMLRPCServer, or embedded in a CGI environment, using
CGIXMLRPCRequestHandler.
Warning: The xmlrpc.server module is not secure against maliciously constructed data.
If you need to parse
untrusted or unauthenticated data see XML vulnerabilities.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
SimpleXMLRPCServer(addr, requestHandler=SimpleXMLRPCRequestHandler,
logRequests=True, allow_none=False, encoding=None,
bind_and_activate=True, use_builtin_types=False)
Create a new server instance.
This class provides methods for registration of functions that can be called by
the XML-RPC protocol.
The requestHandler parameter should be a factory for request handler instances; it
defaults to SimpleXMLRPCRequestHandler.
The addr and requestHandler parameters are passed to the
socketserver.
TCPServer constructor.
If logRequests is true (the default), requests will be logged; setting
this parameter to false will turn off logging.
The allow_none and encoding parameters are passed on to xmlrpc.
The bind_and_activate param-
eter controls whether server_bind() and server_activate() are called immediately by the constructor;
it defaults to true.
Setting it to false allows code to manipulate the allow_reuse_address class variable before the
address is bound.
The use_builtin_types parameter is passed to the loads() function and controls which types
are processed when date/times values or binary data are received; it defaults to false.
The use_builtin_types flag was added.
The allow_none and encoding pa-
rameters are passed on to xmlrpc.client and control the XML-RPC responses that will be returned from the
server.
The use_builtin_types parameter is passed to the loads() function and controls which types are processed
when date/times values or binary data are received; it defaults to false.
The use_builtin_types flag was added.
SimpleXMLRPCRequestHandler
Create a new request handler instance.
This request handler supports POST requests and modifies logging so that
the logRequests parameter to the SimpleXMLRPCServer constructor parameter is honored.
TCPServer and provides a means of creating
simple, stand alone XML-RPC servers.
SimpleXMLRPCServer.register_function(function=None, name=None)
Register a function that can respond to XML-RPC requests.
If name is given, it will be the method name associated
with function, otherwise function.__name__ will be used.
This method can also be used as a decorator.
When used as a decorator, name can only be given as a keyword
argument to register function under name.
If no name is given, function.__name__ will be used.
Changed in version 3.7: register_function() can be used as a decorator.
SimpleXMLRPCServer.register_instance(instance, allow_dotted_names=False)
Register an object which is used to expose method names which have not been registered using
register_function().
If instance contains a _dispatch() method, it is called with the requested method
name and the parameters from the request.
Its API is def _dispatch(self, method, params) (note
that params does not represent a variable argument list).
If it calls an underlying function to perform its task, that
function is called as func(*params), expanding the parameter list.
The return value from _dispatch()
is returned to the client as the result.
If instance does not have a _dispatch() method, it is searched for an
attribute matching the name of the requested method.
If the optional allow_dotted_names argument is true and the instance does not have a _dispatch() method, then
if the requested method name contains periods, each component of the method name is searched for individually,
with the effect that a simple hierarchical search is performed.
The value found from this search is then called with
the parameters from the request, and the return value is passed back to the client.
Warning: Enabling the allow_dotted_names option allows intruders to access your module’s global variables
and may allow intruders to execute arbitrary code on your machine.
Only use this option on a secure, closed
network.
SimpleXMLRPCServer.register_introspection_functions()
Registers the XML-RPC introspection functions system.listMethods, system.methodHelp and
system.methodSignature.
SimpleXMLRPCServer.register_multicall_functions()
Registers the XML-RPC multicall function system.multicall.
An attribute value that must be a tuple listing valid path portions of the URL for receiving XML-RPC requests.
21.22.
If this tuple is empty, all paths will
be considered valid.
The default value is ('/', '/RPC2').
SimpleXMLRPCServer Example
Server code:
from xmlrpc.server import SimpleXMLRPCServer
from xmlrpc.server import SimpleXMLRPCRequestHandler
# Restrict to a particular path.
The following client code will call the methods made available by the preceding server:
import xmlrpc.client
s = xmlrpc.client.
The following example included in the Lib/xmlrpc/server.py module shows a server allowing dotted names and
registering a multicall function.
Warning:
Enabling the allow_dotted_names option allows intruders to access your module’s global variables and
may allow intruders to execute arbitrary code on your machine.
Only use this example only within a secure, closed
network.
The CGIXMLRPCRequestHandler class can be used to handle XML-RPC requests sent to Python CGI scripts.
CGIXMLRPCRequestHandler.register_function(function=None, name=None)
Register a function that can respond to XML-RPC requests.
If name is given, it will be the method name associated
with function, otherwise function.__name__ will be used.
This method can also be used as a decorator.
When used as a decorator, name can only be given as a keyword
argument to register function under name.
If no name is given, function.__name__ will be used.
Changed in version 3.7: register_function() can be used as a decorator.
CGIXMLRPCRequestHandler.register_instance(instance)
Register an object which is used to expose method names which have not been registered using
register_function().
If instance contains a _dispatch() method, it is called with the requested
method name and the parameters from the request; the return value is returned to the client as the result.
If
instance does not have a _dispatch() method, it is searched for an attribute matching the name of the re-
quested method; if the requested method name contains periods, each component of the method name is searched
for individually, with the effect that a simple hierarchical search is performed.
The value found from this search is
then called with the parameters from the request, and the return value is passed back to the client.
CGIXMLRPCRequestHandler.register_introspection_functions()
Register the XML-RPC introspection functions system.listMethods, system.methodHelp and
system.methodSignature.
CGIXMLRPCRequestHandler.register_multicall_functions()
Register the XML-RPC multicall function system.multicall.
If request_text is given, it should be the POST data provided by the HTTP server,
otherwise the contents of stdin will be used.
Servers can either be free standing, using DocXMLRPCServer, or embedded in a CGI environment, using
DocCGIXMLRPCRequestHandler.
class xmlrpc.server.
DocXMLRPCServer(addr, requestHandler=DocXMLRPCRequestHandler,
logRequests=True, allow_none=False, encoding=None,
bind_and_activate=True, use_builtin_types=True)
Create a new server instance.
All parameters have the same meaning as for SimpleXMLRPCServer; re-
questHandler defaults to DocXMLRPCRequestHandler.
The use_builtin_types flag was added.
DocCGIXMLRPCRequestHandler
Create a new instance to handle XML-RPC requests in a CGI environment.
DocXMLRPCRequestHandler
Create a new request handler instance.
This request handler supports XML-RPC POST requests, documentation
GET requests, and modifies logging so that the logRequests parameter to the DocXMLRPCServer constructor
parameter is honored.
HTTP POST requests are handled as XML-RPC method calls.
HTTP
GET requests are handled by generating pydoc-style HTML documentation.
This allows a server to provide its own
web-based documentation.
DocXMLRPCServer.set_server_title(server_title)
Set the title used in the generated HTML documentation.
Set the name used in the generated HTML documentation.
This name will appear at the top of the generated
documentation inside a “h1” element.
21.22.
This description will appear as a paragraph, below
the server name, in the documentation.
HTTP POST requests are handled as XML-RPC method
calls.
HTTP GET requests are handled by generating pydoc-style HTML documentation.
This allows a server to provide
its own web-based documentation.
DocCGIXMLRPCRequestHandler.set_server_title(server_title)
Set the title used in the generated HTML documentation.
Set the name used in the generated HTML documentation.
Set the description used in the generated HTML documentation.
This description will appear as a paragraph, below
the server name, in the documentation.
The functions and classes in this module make it straightforward to handle various tasks related to IP addresses, including
checking whether or not two hosts are on the same subnet, iterating over all hosts in a particular subnet, checking whether
or not a string represents a valid IP address or network definition, and so on.
This is the full module API reference—for an overview and introduction, see ipaddress-howto.
New in version 3.3.
Either
IPv4 or IPv6 addresses may be supplied; integers less than 2**32 will be considered to be IPv4 by default.
A
ValueError is raised if address does not represent a valid IPv4 or IPv6 address.
Either IPv4 or IPv6 networks may be supplied; integers less than
2**32 will be considered to be IPv4 by default.
A ValueError is raised if address does not represent a valid IPv4 or IPv6 address, or if the network has host
bits set.
IPv4Network('192.168.0.0/28')
ipaddress.ip_interface(address)
Return an IPv4Interface or IPv6Interface object depending on the IP address passed as argument.
Either IPv4 or IPv6 addresses may be supplied; integers
less than 2**32 will be considered to be IPv4 by default.
A ValueError is raised if address does not represent
a valid IPv4 or IPv6 address.
One downside of these convenience functions is that the need to handle both IPv4 and IPv6 formats means that error
messages provide minimal information on the precise error, as the functions don’t know whether the IPv4 or IPv6 format
was intended.
More detailed error reporting can be obtained by calling the appropriate version specific class constructors
directly.
Some attributes that are only mean-
ingful for IPv6 addresses are also implemented by IPv4Address objects, in order to make it easier to write code that
handles both IP versions correctly.
Address objects are hashable, so they can be used as keys in dictionaries.
IPv4Address(address)
Construct an IPv4 address.
An AddressValueError is raised if address is not a valid IPv4 address.
The following constitutes a valid IPv4 address:
1.
A string in decimal-dot notation, consisting of four decimal integers in the inclusive range 0–255, separated
by dots (e.g. 192.168.0.1).
Each integer represents an octet (byte) in the address.
Leading zeroes are
not tolerated to prevent confusion with octal notation.
An integer that fits into 32 bits.
An integer packed into a bytes object of length 4 (most significant octet first).
Leading zeros are tolerated, even in ambiguous cases that look like octal notation.
Leading zeros are no longer tolerated and are treated as an error.
IPv4 address strings
are now parsed as strict as glibc inet_pton().
Changed in version 3.9.5: The above change was also included in Python 3.9 starting with version 3.9.5.
Changed in version 3.8.12: The above change was also included in Python 3.8 starting with version 3.8.12.
21.23.
The prefix defines the number of leading bits in an address that are compared to determine whether or not an
address is part of a network.
Leading zeroes are never included in the representation.
As IPv4 does not define a shorthand notation for addresses with octets set to zero, these two attributes are
always the same as str(addr) for IPv4 addresses.
Exposing these attributes makes it easier to write display
code that can handle both IPv4 and IPv6 addresses.
This is 4 bytes for IPv4 and 16 bytes for IPv6.
New in version 3.5.
See RFC 3171 (for IPv4) or RFC 2373 (for IPv6).
See iana-ipv4-special-registry (for IPv4) or iana-ipv6-
special-registry (for IPv6).
See iana-ipv4-special-registry (for IPv4) or iana-ipv6-
special-registry (for IPv6).
New in version 3.4.
is_unspecified
True if the address is unspecified.
See RFC 5735 (for IPv4) or RFC 2373 (for IPv6).
See RFC 3330 (for IPv4) or RFC 2373 (for IPv6).
For binary and hexadecimal representations, the form specifier '#' and the grouping option '_'
are available.
IPv6Address(address)
Construct an IPv6 address.
An AddressValueError is raised if address is not a valid IPv6 address.
The following constitutes a valid IPv6 address:
1.
A string consisting of eight groups of four hexadecimal digits, each group representing 16 bits.
The
groups are separated by colons.
This describes an exploded (longhand) notation.
The string can also
be compressed (shorthand notation) by various means.
See RFC 4291 for details.
For example,
"0000:0000:0000:0000:0000:0abc:0007:0def" can be compressed to "::abc:7:def".
Optionally, the string may also have a scope zone ID, expressed with a suffix %scope_id.
If present,
the scope ID must be non-empty, and may not contain %.
See RFC 4007 for details.
For example,
fe80::1234%1 might identify address fe80::1234 on the first link of the node.
An integer that fits into 128 bits.
An integer packed into a bytes object of length 16, big-endian.
IPv6Address('2001:db8::1000')
IPv6Address('ff02::5678%1')
compressed
The short form of the address representation, with leading zeroes in groups omitted and the longest sequence of
groups consisting entirely of zeroes collapsed to a single empty group.
This is also the value returned by str(addr) for IPv6 addresses.
For the following attributes and methods, see the corresponding documentation of the IPv4Address class:
packed
reverse_pointer
21.23.
Note that the site-local address space has been deprecated
by RFC 3879.
Use is_private to test if this address is in the space of unique local addresses as defined
by RFC 4193.
For any other address, this property will be None.
scope_id
For scoped addresses as defined by RFC 4007, this property identifies the particular zone of the address’s
scope that the address belongs to, as a string.
When no scope zone is specified, this property will be None.
sixtofour
For addresses that appear to be 6to4 addresses (starting with 2002::/16) as defined by RFC 3056, this
property will report the embedded IPv4 address.
For any other address, this property will be None.
teredo
For addresses that appear to be Teredo addresses (starting with 2001::/32) as defined by RFC 4380,
this property will report the embedded (server, client) IP address pair.
Refer to the corresponding method documentation in IPv4Address.
New in version 3.9.
Conversion to Strings and Integers
To interoperate with networking interfaces such as the socket module, addresses must be converted to strings or integers.
Operators
Address objects support some operators.
Unless stated otherwise, operators can only be applied between compatible
objects (i.e. IPv4 with IPv4, IPv6 with IPv6).
Comparison operators
Address objects can be compared with the usual set of comparison operators.
Same IPv6 addresses with different scope
zone IDs are not equal.
Some examples:
True
False
True
False
True
Arithmetic operators
Integers can be added to or subtracted from address objects.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ipaddress.
AddressValueError: 4294967296 (>= 2**32) is not permitted as an IPv4 address
21.23.3 IP Network definitions
The IPv4Network and IPv6Network objects provide a mechanism for defining and inspecting IP network defini-
tions.
A network definition consists of a mask and a network address, and as such defines a range of IP addresses that equal
the network address when masked (binary AND) with the mask.
For example, a network definition with the mask 255.
255.255.0 and the network address 192.168.1.0 consists of IP addresses in the inclusive range 192.168.1.0
to 192.168.1.255.
A prefix /<nbits> is a notation that denotes how many
high-order bits are set in the network mask.
A net mask is an IP address with some number of high-order bits set.
Thus
the prefix /24 is equivalent to the net mask 255.255.255.0 in IPv4, or ffff:ff00:: in IPv6.
In addition, a host
mask is the logical inverse of a net mask, and is sometimes used (for example in Cisco access control lists) to denote a
network mask.
The host mask equivalent to /24 in IPv4 is 0.0.0.255.
Network objects
All attributes implemented by address objects are implemented by network objects as well.
In addition, network objects
implement additional attributes.
All of these are common between IPv4Network and IPv6Network, so to avoid
duplication they are only documented for IPv4Network.
Network objects are hashable, so they can be used as keys in
dictionaries.
IPv4Network(address, strict=True)
Construct an IPv4 network definition.
A string consisting of an IP address and an optional mask, separated by a slash (/).
The IP address is the net-
work address, and the mask can be either a single number, which means it’s a prefix, or a string representation
of an IPv4 address.
If it’s the latter, the mask is interpreted as a net mask if it starts with a non-zero field, or
as a host mask if it starts with a zero field, with the single exception of an all-zero mask which is treated as a
net mask.
If no mask is provided, it’s considered to be /32.
For example, the following address specifications are equivalent: 192.168.1.0/24, 192.168.1.0/
255.255.255.0 and 192.168.1.0/0.0.0.255.
An integer that fits into 32 bits.
This is equivalent to a single-address network, with the network address being
address and the mask being /32.
An integer packed into a bytes object of length 4, big-endian.
The interpretation is similar to an integer
address.
A two-tuple of an address description and a netmask, where the address description is either a string, a 32-
bits integer, a 4-bytes packed integer, or an existing IPv4Address object; and the netmask is either an integer
representing the prefix length (e.g. 24) or a string representing the prefix mask (e.g. 255.255.255.0).
An AddressValueError is raised if address is not a valid IPv4 address.
A NetmaskValueError is raised
if the mask is not valid for an IPv4 address.
If strict is True and host bits are set in the supplied address, then ValueError is raised.
Otherwise, the host
bits are masked out to determine the appropriate network address.
Unless stated otherwise, all network methods accepting other network/address objects will raise TypeError if
the argument’s IP version is incompatible to self.
Added the two-tuple form for the address constructor parameter.
The network address and the prefix length together uniquely define a
network.
Packets sent to the broadcast address should be received by every host
on the network.
The usable hosts are all the IP addresses that belong to
the network, except the network address itself and the network broadcast address.
For networks with a mask
length of 31, the network address and network broadcast address are also included in the result.
Networks
with a mask of 32 will return a list containing the single host address.
Computes the network definitions resulting from removing the given network from this one.
Returns an
iterator of network objects.
Raises ValueError if network is not completely contained in this network.
The subnets that join to make the current network definition, depending on the argument values.
One and only one of prefixlen_diff and new_prefix must be set.
Returns an
iterator of network objects.
The supernet containing this network definition, depending on the argument values.
One and only one of prefixlen_diff and new_prefix must be set.
Returns a single
network object.
IPv4Network('192.0.0.0/22')
IPv4Network('192.0.0.0/20')
subnet_of(other)
Return True if this network is a subnet of other.
True
New in version 3.7.
compare_networks(other)
Compare this network to other.
In this comparison only the network addresses are considered; host bits aren’t.
Returns either -1, 0 or 1.
-1


Deprecated since version 3.7: It uses the same ordering and comparison algorithm as “<”, “==”, and “>”
class ipaddress.
IPv6Network(address, strict=True)
Construct an IPv6 network definition.
A string consisting of an IP address and an optional prefix length, separated by a slash (/).
The IP address is
the network address, and the prefix length must be a single number, the prefix.
If no prefix length is provided,
it’s considered to be /128.
Note that currently expanded netmasks are not supported.
That means 2001:db00::0/24 is a valid
argument while 2001:db00::0/ffff:ff00:: is not.
An integer that fits into 128 bits.
This is equivalent to a single-address network, with the network address
being address and the mask being /128.
An integer packed into a bytes object of length 16, big-endian.
The interpretation is similar to an integer
address.
A two-tuple of an address description and a netmask, where the address description is either a string, a 128-
bits integer, a 16-bytes packed integer, or an existing IPv6Address object; and the netmask is an integer
representing the prefix length.
An AddressValueError is raised if address is not a valid IPv6 address.
A NetmaskValueError is raised
if the mask is not valid for an IPv6 address.
If strict is True and host bits are set in the supplied address, then ValueError is raised.
Otherwise, the host
bits are masked out to determine the appropriate network address.
Added the two-tuple form for the address constructor parameter.
The usable hosts are all the IP addresses that belong
to the network, except the Subnet-Router anycast address.
For networks with a mask length of 127, the
Subnet-Router anycast address is also included in the result.
Networks with a mask of 128 will return a list
containing the single host address.
overlaps(other)
address_exclude(network)
subnets(prefixlen_diff=1, new_prefix=None)
supernet(prefixlen_diff=1, new_prefix=None)
subnet_of(other)
supernet_of(other)
compare_networks(other)
Refer to the corresponding attribute documentation in IPv4Network.
Unless stated otherwise, operators can only be applied between compatible
objects (i.e. IPv4 with IPv4, IPv6 with IPv6).
Logical operators
Network objects can be compared with the usual set of logical operators.
Network objects are ordered first by network
address, then by net mask.
Iteration
Network objects can be iterated to list all the addresses belonging to the network.
For iteration, all hosts are returned,
including unusable hosts (for usable hosts, use the hosts() method).
IPv4Address('192.0.2.15')
Networks as containers of addresses
Network objects can act as containers of addresses.
True
False
21.23.
IPv4Interface(address)
Construct an IPv4 interface.
The meaning of address is as in the constructor of IPv4Network, except that
arbitrary host addresses are always accepted.
IPv4Interface is a subclass of IPv4Address, so it inherits all the attributes from that class.
In addition,
the following attributes are available:
ip
The address (IPv4Address) without network information.
IPv4Address('192.0.2.5')
network
The network (IPv4Network) this interface belongs to.
IPv4Network('192.0.2.0/24')
with_prefixlen
A string representation of the interface with the mask in prefix notation.
IPv6Interface(address)
Construct an IPv6 interface.
The meaning of address is as in the constructor of IPv6Network, except that
arbitrary host addresses are always accepted.
IPv6Interface is a subclass of IPv6Address, so it inherits all the attributes from that class.
Operators
Interface objects support some operators.
Unless stated otherwise, operators can only be applied between compatible
objects (i.e. IPv4 with IPv4, IPv6 with IPv6).
Logical operators
Interface objects can be compared with the usual set of logical operators.
For equality comparison (== and !
An
interface will not compare equal to any address or network object.
For ordering (<, >, etc) the rules are different.
Interface and address objects with the same IP version can be compared,
and the address objects will always sort before the interface objects.
Two interface objects are first compared by their
networks and, if those are the same, then by their IP addresses.
The module also provides the following module level functions:
ipaddress.v4_int_to_packed(address)
Represent an address as 4 packed bytes in network (big-endian) order.
A ValueError is raised if the integer is negative or too large to be an IPv4 IP address.
IPv4Address('192.0.2.1')
b'\xc0\x00\x02\x01'
ipaddress.v6_int_to_packed(address)
Represent an address as 16 packed bytes in network (big-endian) order.
A ValueError is raised if the integer is negative or too large to be an IPv6 IP address.
A TypeError is raised if first or last are not IP addresses or are not of the same version.
A ValueError
is raised if last is not greater than first or if first address version is not 4 or 6.

ipaddress.
IPv4Address('192.0.2.0'),

ipaddress.
A TypeError is raised if addresses contains mixed version ob-
jects.
... ipaddress.collapse_addresses([ipaddress.
IPv4Network('192.0.2.0/25'),
... ipaddress.
Address and Network objects are not sortable
by default; they’re fundamentally different, so the expression:
IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')
doesn’t make sense.
There are some times however, where you may wish to have ipaddress sort these anyway.
If you need to do this, you can use this function as the key argument to sorted().
Any value error related to the address.
The modules described in this chapter implement various algorithms or interfaces that are mainly useful for multimedia
applications.
They are available at the discretion of the installation.
Here’s an overview:
22.1 wave — Read and write WAV files
Source code: Lib/wave.py
The wave module provides a convenient interface to the Waveform Audio “WAVE” (or “WAV”) file format.
Only
uncompressed PCM encoded wave files are supported.
Changed in version 3.12: Support for WAVE_FORMAT_EXTENSIBLE headers was added, provided that the extended
format is KSDATAFORMAT_SUBTYPE_PCM.
If file is a string, open the file by that name, otherwise treat it as a file-like object.
Note that it does not allow read/write WAV files.
A mode of 'rb' returns a Wave_read object, while a mode of 'wb' returns a Wave_write object.
If mode
is omitted and a file-like object is passed as file, file.mode is used as the default value for mode.
If you pass in a file-like object, the wave object will not close it when its close() method is called; it is the caller’s
responsibility to close the file object.
The open() function may be used in a with statement.
When the with block completes, the Wave_read.
Changed in version 3.4: Added support for unseekable files.
Error
An error raised when something is impossible because it violates the WAV specification or hits an implementation
deficiency.
Wave_read
Read a WAV file.
Wave_read objects, as returned by open(), have the following methods:
close()
Close the stream if it was opened by wave, and make the instance unusable.
This is called automatically on
object collection.
Usually 'not compressed' parallels 'NONE'.
getparams()
Returns
a
namedtuple()
(nchannels, sampwidth, framerate, nframes,
comptype, compname), equivalent to output of the get*() methods.
Reads and returns at most n frames of audio, as a bytes object.
The following two methods are defined for compatibility with the aifc module, and don’t do anything interesting.
Set the file pointer to the specified position.
Wave_write
Write a WAV file.
Wave_write objects, as returned by open().
For seekable output streams, the wave header will automatically be updated to reflect the number of frames actually
written.
For unseekable streams, the nframes value must be accurate when the first frame data is written.
An
accurate nframes value can be achieved either by calling setnframes() or setparams() with the number
of frames that will be written before close() is called and then using writeframesraw() to write the frame
data, or by calling writeframes() with all of the frame data to be written.
In the latter case writeframes()
will calculate the number of frames in the data and set nframes accordingly before writing the frame data.
Changed in version 3.4: Added support for unseekable files.
Wave_write objects have the following methods:
close()
Make sure nframes is correct, and close the file if it was opened by wave.
This method is called upon object
collection.
It will raise an exception if the output stream is not seekable and nframes does not match the
number of frames actually written.
Set the frame rate to n.
Changed in version 3.2: A non-integral input to this method is rounded to the nearest integer.
This will be changed later if the number of frames actually written is different
(this update attempt will raise an error if the output stream is not seekable).
The
tuple
should
be
(nchannels, sampwidth, framerate, nframes, comptype,
compname), with values valid for the set*() methods.
Sets all parameters.
Changed in version 3.4: Any bytes-like object is now accepted.
It will raise an error if the output stream is not seekable
and the total number of frames that have been written after data has been written does not match the previously
set value for nframes.
Note that it is invalid to set any parameters after calling writeframes() or writeframesraw(), and
any attempt to do so will raise wave.
Error.
Coordinates in all of these color spaces are floating point values.
In the
YIQ space, the Y coordinate is between 0 and 1, but the I and Q coordinates can be positive or negative.
In all other
spaces, the coordinates are all between 0 and 1.
See also:
More information about color spaces can be found at https://poynton.ca/ColorFAQ.html and https://www.
The colorsys module defines the following functions:
colorsys.rgb_to_yiq(r, g, b)
Convert the color from RGB coordinates to YIQ coordinates.
The modules described in this chapter help you write software that is independent of language and locale by providing
mechanisms for selecting a language to be used in program messages or by tailoring output to match local conventions.
The list of modules described in this chapter is:
23.1 gettext — Multilingual internationalization services
Source code: Lib/gettext.py
The gettext module provides internationalization (I18N) and localization (L10N) services for your Python modules and
applications.
It supports both the GNU gettext message catalog API and a higher level, class-based API that may be
more appropriate for Python files.
The interface described below allows you to write your module and application messages
in one natural language, and provide a catalog of translated messages for running under different natural languages.
Some hints on localizing your Python modules and applications are also given.
If you use this
API you will affect the translation of your entire application globally.
Often this is what you want if your application is
monolingual, with the choice of language dependent on the locale of your user.
If you are localizing a Python module, or
if your application needs to switch languages on the fly, you probably want to use the class-based API instead.
More concretely, gettext will look for binary .mo files for
the given domain using the path (on Unix): localedir/language/LC_MESSAGES/domain.mo, where
language is searched for in the environment variables LANGUAGE, LC_ALL, LC_MESSAGES, and LANG respec-
tively.
If localedir is omitted or None, then the current binding for domain is returned.1
gettext.textdomain(domain=None)
Change or query the current global domain.
If domain is None, then the current global domain is returned, other-
wise the global domain is set to domain, which is returned.
The default locale directory is system dependent; for example, on RedHat Linux it is /usr/share/locale, but on Solaris it is /usr/lib/
locale.
The gettext module does not try to support these system dependent defaults; instead its default is sys.base_prefix/share/
locale (see sys.base_prefix).
For this reason, it is always best to call bindtextdomain() with an explicit absolute path at the start of
your application.
This function is usually aliased as _() in the local namespace (see examples below).
If a translation is found, apply the plural formula to n, and return
the resulting message (some languages have more than two plural forms).
If no translation is found, return singular
if n is 1; return plural otherwise.
The Plural formula is taken from the catalog header.
It is a C or Python expression that has a free variable n; the
expression evaluates to the index of the plural in the catalog.
See the GNU gettext documentation for the precise
syntax to be used in .po files and the formulas for a variety of languages.
Similar to the corresponding functions without the p in the prefix (that is, gettext(), dgettext(),
ngettext(), dngettext()), but the translation is restricted to the given message context.
New in version 3.8.
Note that GNU gettext also defines a dcgettext() method, but this was deemed not useful and so it is currently
unimplemented.
It is the recommended way of localizing your Python applications and modules.
This function implements the standard .mo file search algorithm.
It takes a domain, identical to what
textdomain() takes.
Optional localedir is as in bindtextdomain().
Optional languages is a list of strings,
where each string is a language code.
If languages is not given, then the following
environment variables are searched: LANGUAGE, LC_ALL, LC_MESSAGES, and LANG.
The environment variables should contain a colon separated
list of languages, which will be split on the colon to produce the expected list of language code strings.
If no such file is found, then None is returned.
If all
is given, it returns a list of all file names, in the order in which they appear in the languages list or the environment
variables.
gettext.translation(domain, localedir=None, languages=None, class_=None, fallback=False)
Return a *Translations instance based on the domain, localedir, and languages, which are first passed to
find() to get a list of the associated .mo file paths.
Instances with identical .mo file names are cached.
The
actual class instantiated is class_ if provided, otherwise GNUTranslations.
The class’s constructor must take
a single file object argument.
If multiple files are found, later files are used as fallbacks for earlier ones.
To allow setting the fallback, copy.
If no .mo file is found, this function raises OSError if fallback is false (which is the default), and returns a
NullTranslations instance if fallback is true.
Changed in version 3.3: IOError used to be raised, it is now an alias of OSError.
Changed in version 3.11: codeset parameter is removed.
This installs the function _() in Python’s builtins namespace, based on domain and localedir which are passed to
the function translation().
For the names parameter, please see the description of the translation object’s install() method.
For convenience, you want the _() function to be installed in Python’s builtins namespace, so it is easily accessible
in all modules of your application.
Changed in version 3.11: names is now a keyword-only parameter.
The NullTranslations class
Translation classes are what actually implement the translation of original source file message strings to translated message
strings.
The base class used by all translation classes is NullTranslations; this provides the basic interface you can
use to write your own specialized translation classes.
Here are the methods of NullTranslations:
class gettext.
NullTranslations(fp=None)
Takes an optional file object fp, which is ignored by the base class.
Initializes “protected” instance variables _info
and _charset which are set by derived classes, as well as _fallback, which is set through add_fallback().
It
then calls self._parse(fp) if fp is not None.
_parse(fp)
No-op in the base class, this method takes file object fp, and reads the data from the file, initializing its message
catalog.
If you have an unsupported message catalog file format, you should override this method to parse
your format.
23.1.
A translation object should consult the
fallback if it cannot provide a translation for a given message.
gettext(message)
If a fallback has been set, forward gettext() to the fallback.
Otherwise, return message.
Overridden in
derived classes.
If a fallback has been set, forward ngettext() to the fallback.
Otherwise, return singular if n is 1; return
plural otherwise.
Overridden in derived classes.
If a fallback has been set, forward pgettext() to the fallback.
Otherwise, return the translated message.
Overridden in derived classes.
New in version 3.8.
If a fallback has been set, forward npgettext() to the fallback.
Otherwise, return the translated message.
Overridden in derived classes.
New in version 3.8.
This method installs gettext() into the built-in namespace, binding it to _.
If the names parameter is given, it must be a sequence containing the names of functions you want to in-
stall in the builtins namespace in addition to _().
Supported names are 'gettext', 'ngettext',
'pgettext', and 'npgettext'.
Note that this is only one way, albeit the most convenient way, to make the _() function available to your
application.
Because it affects the entire application globally, and specifically the built-in namespace, localized
modules should never install _().
This puts _() only in the module’s global namespace and so only affects calls within this module.
Changed in version 3.8: Added 'pgettext' and 'npgettext'.
This
class overrides _parse() to enable reading GNU gettext format .mo files in both big-endian and little-endian
format.
GNUTranslations parses optional metadata out of the translation catalog.
It is convention with GNU gettext to
include metadata as the translation for the empty string.
This metadata is in RFC 822-style key: value pairs, and
should contain the Project-Id-Version key.
If the key Content-Type is found, then the charset property is
used to initialize the “protected” _charset instance variable, defaulting to None if not found.
If the charset encoding
is specified, then all message ids and message strings read from the catalog are converted to Unicode using this encoding,
else ASCII is assumed.
Since message ids are read as Unicode strings too, all *gettext() methods will assume message ids as Unicode strings,
not byte strings.
The entire set of key/value pairs are placed into a dictionary and set as the “protected” _info instance variable.
If the .mo file’s magic number is invalid, the major version number is unexpected, or if other problems occur while
reading the file, instantiating a GNUTranslations class can raise OSError.
GNUTranslations
The following methods are overridden from the base class implementation:
gettext(message)
Look up the message id in the catalog and return the corresponding message string, as a Unicode string.
If
there is no entry in the catalog for the message id, and a fallback has been set, the look up is forwarded to the
fallback’s gettext() method.
Otherwise, the message id is returned.
The returned message string is a Unicode
string.
If the message id is not found in the catalog, and a fallback is specified, the request is forwarded to the
fallback’s ngettext() method.
Otherwise, when n is 1 singular is returned, and plural is returned in all
other cases.
If there is no entry in the catalog for the message id and context, and a fallback has been set, the look
up is forwarded to the fallback’s pgettext() method.
Otherwise, the message id is returned.
New in version 3.8.
If the message id for context is not found in the catalog, and a fallback is specified, the request is forwarded
to the fallback’s npgettext() method.
Otherwise, when n is 1 singular is returned, and plural is returned
in all other cases.
New in version 3.8.
Solaris message catalog support
The Solaris operating system defines its own binary .mo file format, but since no documentation can be found on this
format, it is not supported at this time.
The Catalog constructor
GNOME uses a version of the gettext module by James Henstridge, but this version has a slightly different API.
Its
documented usage was:
import gettext
cat = gettext.
For compatibility with this older module, the function Catalog() is an alias for the translation() function
described above.
One difference between this module and Henstridge’s: his catalog objects supported access through a mapping API, but
this appears to be unused and so is not currently supported.
Localization
(L10N) refers to the adaptation of your program, once internationalized, to the local language and cultural habits.
In
order to provide multilingual messages for your Python programs, you need to take the following steps:
1.
In order to prepare your code for I18N, you need to look at all the strings in your files.
Any string that needs to be
translated should be marked by wrapping it in _('...') — that is, a call to the function _.
In this example, the string 'writing a log message' is marked as a candidate for translation, while the strings
'mylog.txt' and 'w' are not.
There are a few tools to extract the strings meant for translation.
The original GNU gettext only supported C or C++
source code but its extended version xgettext scans code written in a number of languages, including Python, to find
strings marked as translatable.
Babel is a Python internationalization library that includes a pybabel script to extract
and compile message catalogs.
They are structured human-
readable files that contain every marked string in the source code, along with a placeholder for the translated versions of
these strings.
Copies of these .po files are then handed over to the individual human translators who write translations for every
supported natural language.
The .mo files are used
by the gettext module for the actual translation processing at run-time.
How you use the gettext module in your code depends on whether you are internationalizing a single module or your
entire application.
The next two sections will discuss each case.
Localizing your module
If you are localizing your module, you must take care not to make global changes, e.g. to the built-in namespace.
You
should not use the GNU gettext API but instead the class-based API.
This will let all your application-specific files just use _('...') without having to
explicitly install it in each file.
If you need to set the locale directory, you can pass it into the install() function:
import gettext
gettext.install('myapplication', '/usr/share/locale')
23.1.
Occasionally however, you need to mark strings
for translation, but defer actual translation until later.
Here, you want to mark the strings in the animals list as being translatable, but you don’t actually want to translate
them until they are printed.
This works because the dummy definition of _() simply returns the string unchanged.
And this dummy definition will
temporarily override any definition of _() in the built-in namespace (until the del command).
In this case, you are marking translatable strings with the function N_(), which won’t conflict with any defini-
tion of _().
However, you will need to teach your message extraction program to look for translatable strings
marked with N_().
The choice of N_() here is totally arbitrary; it could have just as easily been
MarkThisStringForTranslation().
Acknowledgements
The following people contributed code, feedback, design suggestions, previous implementations, and valuable experience
to the creation of this module:
• Peter Funk
• James Henstridge
• Juan David Ibáñez Palomar
• Marc-André Lemburg
• Martin von Löwis
• François Pinard
• Barry Warsaw
• Gustavo Niemeyer
23.2 locale — Internationalization services
Source code: Lib/locale.py
The locale module opens access to the POSIX locale database and functionality.
The POSIX locale mechanism allows
programmers to deal with certain cultural issues in an application, without requiring the programmer to know all the
specifics of each country where the software is executed.
The locale module is implemented on top of the _locale module, which in turn uses an ANSI C locale implemen-
tation if available.
The locale module defines the following exception and functions:
23.2.
If locale is given and not None, setlocale() modifies the locale setting for the category.
The available cat-
egories are listed in the data description below.
If it’s an iterable, it’s converted to a locale name using the locale aliasing engine.
An empty
string specifies the user’s default settings.
If the modification of the locale fails, the exception Error is raised.
If
successful, the new locale setting is returned.
If locale is omitted or None, the current setting for category is returned.
Applications typically start with a call of
import locale
locale.setlocale(locale.
This sets the locale for all categories to the user’s default setting (typically specified in the LANG environment
variable).
If the locale is not changed thereafter, using multithreading should not cause problems.
If
the sequence is terminated with
CHAR_MAX, no further grouping is
performed.
If the sequence
terminates with a 0, the last group
size is repeatedly used.
All numeric values can be set to CHAR_MAX to indicate that there is no value specified in this locale.
The possible values for 'p_sign_posn' and 'n_sign_posn' are given below.
Value
Explanation

Currency and value are surrounded by parentheses.
Nothing is specified in this locale.
The function temporarily sets the LC_CTYPE locale to the LC_NUMERIC locale or the LC_MONETARY locale if
locales are different and numeric or monetary strings are non-ASCII.
This temporary change affects other threads.
The function now temporarily sets the LC_CTYPE locale to the LC_NUMERIC locale in
23.2.
This function is not available on all systems, and the set of
possible options might also vary across platforms.
The possible argument values are numbers, for which symbolic
constants are available in the locale module.
The nl_langinfo() function accepts one of the following keys.
Most descriptions are taken from the corre-
sponding description in the GNU C library.
CODESET
Get a string with the name of the character encoding used in the selected locale.
locale.
D_T_FMT
Get a string that can be used as a format string for time.strftime() to represent date and time in a
locale-specific way.
D_FMT
Get a string that can be used as a format string for time.strftime() to represent a date in a locale-
specific way.
T_FMT
Get a string that can be used as a format string for time.strftime() to represent a time in a locale-
specific way.
locale.
T_FMT_AMPM
Get a format string for time.strftime() to represent time in the am/pm format.
DAY_1 ...
DAY_7
Get the name of the n-th day of the week.
Note: This follows the US convention of DAY_1 being Sunday, not the international convention (ISO 8601)
that Monday is the first day of the week.
ABDAY_1 ...
ABDAY_7
Get the abbreviated name of the n-th day of the week.
MON_1 ...
MON_12
Get the name of the n-th month.
ABMON_1 ...
ABMON_12
Get the abbreviated name of the n-th month.
locale.
RADIXCHAR
Get the radix character (decimal dot, decimal comma, etc.).
THOUSEP
Get the separator character for thousands (groups of three digits).
YESEXPR
Get a regular expression that can be used with the regex function to recognize a positive response to a yes/no
question.
locale.
NOEXPR
Get a regular expression that can be used with the regex(3) function to recognize a negative response to a
yes/no question.
ERA
Get a string that represents the era used in the current locale.
Most locales do not define this value.
An example of a locale which does define this value is the Japanese
one.
In Japan, the traditional representation of dates includes the name of the era corresponding to the then-
emperor’s reign.
Normally it should not be necessary to use this value directly.
Specifying the E modifier in their format strings
causes the time.strftime() function to use this information.
The format of the returned string is not
specified, and therefore you should not assume knowledge of it on different systems.
locale.
ERA_D_T_FMT
Get a format string for time.strftime() to represent date and time in a locale-specific era-based way.
ERA_D_FMT
Get a format string for time.strftime() to represent a date in a locale-specific era-based way.
ERA_T_FMT
Get a format string for time.strftime() to represent a time in a locale-specific era-based way.
ALT_DIGITS
Get a representation of up to 100 values used to represent the values 0 to 99.
locale.getdefaultlocale([envvars])
Tries to determine the default locale settings and returns them as a tuple of the form (language code,
encoding).
According to POSIX, a program which has not called setlocale(LC_ALL, '') runs using the portable 'C'
locale.
Calling setlocale(LC_ALL, '') lets it use the default locale as defined by the LANG variable.
Since
we do not want to interfere with the current locale setting we thus emulate the behavior in the way described above.
To maintain compatibility with other platforms, not only the LANG variable is tested, but a list of variables given
as envvars parameter.
The first found to be defined will be used.
The GNU gettext search path contains 'LC_ALL',
'LC_CTYPE', 'LANG' and 'LANGUAGE', in that order.
Except for the code 'C', the language code corresponds to RFC 1766.
Deprecated since version 3.11, will be removed in version 3.15.
It defaults to LC_CTYPE.
Except for the code 'C', the language code corresponds to RFC 1766.
User preferences are expressed dif-
ferently on different systems, and might not be available programmatically on some systems, so this function only
returns a guess.
On some systems, it is necessary to invoke setlocale() to obtain the user preferences, so this function is not
thread-safe.
If invoking setlocale is not necessary or desired, do_setlocale should be set to False.
On Android or if the Python UTF-8 Mode is enabled, always return 'utf-8', the locale encoding and the
do_setlocale argument are ignored.
The Python preinitialization configures the LC_CTYPE locale.
See also the filesystem encoding and error handler.
Changed in version 3.7: The function now always returns "utf-8" on Android or if the Python UTF-8 Mode is
enabled.
On Android and VxWorks, return "utf-8".
Return
"utf-8"
if
nl_langinfo(CODESET) returns an empty string: for example, if the current LC_CTYPE locale is not
supported.
On Windows, return the ANSI code page.
The Python preinitialization configures the LC_CTYPE locale.
See also the filesystem encoding and error handler.
This function is similar to getpreferredencoding(False) except this function ignores the Python UTF-8
Mode.
Returns a normalized locale code for the given locale name.
The returned locale code is formatted for use with
setlocale().
If normalization fails, the original name is returned unchanged.
If the given encoding is not known, the function defaults to the default encoding for the locale code just like
setlocale().
The default setting is determined by calling getdefaultlocale().
As any other compare function, returns a
negative, or a positive value, or 0, depending on whether string1 collates before or after string2 or is equal to it.
locale.strxfrm(string)
Transforms a string to one that can be used in locale-aware comparisons.
This function can be used when the same string is
compared repeatedly, e.g. when collating a sequence of strings.
The format follows the conventions of the
% operator.
For floating point values, the decimal point is modified if appropriate.
If grouping is True, also takes
the grouping into account.
If monetary is true, the conversion uses monetary thousands separator and grouping strings.
Processes formatting specifiers as in format % val, but takes the current locale settings into account.
The monetary keyword parameter was added.
The returned string includes the currency symbol if symbol is true, which is the default.
If grouping is True
(which is not the default), grouping is done with the value.
If international is True (which is not the default), the
international currency symbol is used.
Note: This function will not work with the ‘C’ locale, so you have to set a locale via setlocale() first.
Converts a string into a normalized number string, following the LC_NUMERIC settings.
New in version 3.5.
New in version 3.10.
locale.atof(string, func=float)
Converts a string to a number, following the LC_NUMERIC settings, by calling func on the result of calling
delocalize() on string.
Converts a string to an integer, following the LC_NUMERIC conventions.
LC_CTYPE
Locale category for the character type functions.
Most importantly, this category defines the text encoding, i.e.
how bytes are interpreted as Unicode codepoints.
See PEP 538 and PEP 540 for how this variable might be
automatically coerced to C.UTF-8 to avoid issues created by invalid settings in containers or incompatible settings
passed over remote SSH connections.
Python doesn’t internally use locale-dependent character transformation functions from ctype.h.
Instead, an
internal pyctype.h provides locale-independent equivalents like Py_TOLOWER.
locale.
LC_COLLATE
Locale category for sorting strings.
The functions strcoll() and strxfrm() of the locale module are
affected.
locale.
LC_TIME
Locale category for the formatting of time.
The function time.strftime() follows these conventions.
locale.
Locale category for formatting of monetary values.
The available options are available from the localeconv()
function.
LC_MESSAGES
Locale category for message display.
Python currently does not support application specific locale-aware messages.
Messages displayed by the operating system, like those returned by os.strerror() might be affected by this
category.
This value may not be available on operating systems not conforming to the POSIX standard, most notably Win-
dows.
LC_NUMERIC
Locale category for formatting numbers.
The functions format_string(), atoi(), atof() and str()
of the locale module are affected by that category.
All other numeric formatting operations are not affected.
LC_ALL
Combination of all locale settings.
If this flag is used when the locale is changed, setting the locale for all categories
is attempted.
If that fails for any category, no category is changed at all.
When the locale is retrieved using this
flag, a string indicating the setting for all categories is returned.
This string can be later used to restore the settings.
locale.
This is a symbolic constant used for different values returned by localeconv().
Example:
# get current locale
# use German locale; name might vary with platform
# compare a string containing an umlaut
# use user's preferred locale
# use default (C) locale
# restore saved locale
23.2.1 Background, details, hints, tips and caveats
The C standard defines the locale as a program-wide property that may be relatively expensive to change.
On top of that,
some implementations are broken in such a way that frequent locale changes may cause core dumps.
This makes the
locale somewhat painful to use correctly.
Initially, when a program is started, the locale is the C locale, no matter what the user’s preferred locale is.
There is one
exception: the LC_CTYPE category is changed at startup to set the current locale encoding to the user’s preferred locale
encoding.
The program must explicitly say that it wants the user’s preferred locale settings for other categories by calling
setlocale(LC_ALL, '').
It is generally a bad idea to call setlocale() in some library routine, since as a side effect it affects the entire program.
Saving and restoring it is almost as bad: it is expensive and affects other threads that happen to run before the settings
have been restored.
If, when coding a module for general use, you need a locale independent version of an operation that is affected by the
locale (such as certain formats used with time.strftime()), you will have to find a way to do it without using the
standard library routine.
Even better is convincing yourself that using locale settings is okay.
Only as a last resort should
you document that your module is not compatible with non-C locale settings.
The only way to perform numeric operations according to the locale is to use the special functions defined by this module:
atof(), atoi(), format_string(), str().
There is no way to perform case conversions and character classifications according to the locale.
But since the return
value can only be used portably to restore it, that is not very useful (except perhaps to find out whether or not the locale
is C).
When Python code uses the locale module to change the locale, this also affects the embedding application.
If the
embedding application doesn’t want this to happen, it should remove the _locale extension module (which does all
the work) from the table of built-in modules in the config.c file, and make sure that the _locale module is not
accessible as a shared library.
The locale module exposes the C library’s gettext interface on systems that provide this interface.
It consists
of the functions gettext(), dgettext(), dcgettext(), textdomain(), bindtextdomain(), and
bind_textdomain_codeset().
These are similar to the same functions in the gettext module, but use the
C library’s binary format for message catalogs, and the C library’s search algorithms for locating message catalogs.
Python applications should normally find no need to invoke these functions, and should use gettext instead.
A known
exception to this rule are applications that link with additional C libraries which internally invoke gettext() or
dcgettext().
For these applications, it may be necessary to bind the text domain, so that the libraries can prop-
erly locate their message catalogs.
The modules described in this chapter are frameworks that will largely dictate the structure of your program.
Currently
the modules described here are all oriented toward writing command-line interfaces.
Introduction
Turtle graphics is an implementation of the popular geometric drawing tools introduced in Logo, developed by Wally
Feurzeig, Seymour Papert and Cynthia Solomon in 1967.
Turtle star
Turtle can draw intricate shapes using programs that repeat simple moves.
It’s an effective and well-proven way for learners to encounter programming concepts and interaction with software, as it
provides instant, visible feedback.
It also provides convenient access to graphical output in general.
Turtle drawing was originally created as an educational tool, to be used by teachers in the classroom.
For the programmer
who needs to produce some graphical output it can be a way to do that without the overhead of introducing more complex
or external libraries into their work.
In this tutorial we’ll explore some of the basics of turtle drawing.
You should see (most likely, in a new window on your display) a line drawn by the turtle, heading East.
Let’s continue by drawing a triangle:
forward(100)
left(120)
forward(100)
Notice how the turtle, represented by an arrow, points in different directions as you steer it.
Experiment with those commands, and also with backward() and right().
Pen control
Try changing the color - for example, color('blue') - and width of the line - for example, width(3) - and then
drawing again.
You can also move the turtle around without drawing, by lifting up the pen: up() before moving.
To start drawing again,
use down().
The home position is at the center of the turtle’s screen.
If you ever need to know them, get the turtle’s x-y co-ordinates
with:
pos()
Home is at (0, 0).
Let’s draw the star shape at the top of this page.
Get started as quickly as possible
One of the joys of turtle graphics is the immediate, visual feedback that’s available from simple commands - it’s an
excellent way to introduce children to programming ideas, with a minimum of overhead (not just children, of course).
The turtle module makes this possible by exposing all its basic functionality as functions, available with from turtle
import *.
The turtle graphics tutorial covers this approach.
It’s worth noting that many of the turtle commands also have even more terse equivalents, such as fd() for forward().
These are especially useful when working with learners for whom typing is not a skill.
You’ll need to have the Tk interface package installed on your system for turtle graphics to work.
Be warned that this is not always straightforward, so check this in advance if you’re planning to use turtle
graphics with a learner.
The solution is to use import turtle - fd() becomes turtle.fd(), width() becomes turtle.width()
and so on.
Use turtle graphics in a script
It’s recommended to use the turtle module namespace as described immediately above, for example:
import turtle as t
from random import random
for i in range(100):
steps = int(random() * 100)
angle = int(random() * 360)
t.right(angle)
t.fd(steps)
Another step is also required though - as soon as the script ends, Python will also close the turtle’s window.
Add:
t.mainloop()
to the end of the script.
The script will now wait to be dismissed and will not exit until it is terminated, for example by
closing the turtle graphics window.
Use object-oriented turtle graphics
See also:
Explanation of the object-oriented interface
Other than for very basic introductory purposes, or for trying things out as quickly as possible, it’s more usual and much
more powerful to use the object-oriented approach to turtle graphics.
For example, this allows multiple turtles on screen
at once.
In this approach, the various turtle commands are methods of objects (mostly of Turtle objects).
You can use the
object-oriented approach in the shell, but it would be more typical in a Python script.
The example above then becomes:
from turtle import Turtle
from random import random
t = Turtle()
for i in range(100):
steps = int(random() * 100)
angle = int(random() * 360)
t.right(angle)
t.fd(steps)
t.screen.mainloop()
24.1.
Turtle graphics reference
Note: In the following documentation the argument list for functions is given.
Methods, of course, have the additional
first argument self which is omitted here.
Turtle state
Visibility
showturtle() | st()
hideturtle() | ht()
isvisible()
Appearance
shape()
resizemode()
shapesize() | turtlesize()
shearfactor()
settiltangle()
tiltangle()
tilt()
shapetransform()
get_shapepoly()
Using events
onclick()
onrelease()
ondrag()
Special Turtle methods
begin_poly()
end_poly()
get_poly()
24.1.
Turtle motion
turtle.forward(distance)
turtle.fd(distance)
Parameters distance – a number (integer or float)
Move the turtle forward by the specified distance, in the direction the turtle is headed.
Parameters distance – a number
Move the turtle backward by distance, opposite to the direction the turtle is headed.
Do not change the turtle’s
heading.
Turn turtle right by angle units.
Angle orientation depends on the turtle mode, see mode().
22.0
337.0
turtle.left(angle)
turtle.lt(angle)
Parameters angle – a number (integer or float)
24.1.
Angle orientation depends on the turtle mode, see mode().
22.0
67.0
turtle.goto(x, y=None)
turtle.setpos(x, y=None)
turtle.setposition(x, y=None)
Parameters
• x – a number or a pair/vector of numbers
• y – a number or None
If y is None, x must be a pair of coordinates or a Vec2D (e.g. as returned by pos()).
Move turtle to an absolute position.
If the pen is down, draw line.
Do not change the turtle’s orientation.
Unlike goto(x, y), a line will not be drawn.
The turtle’s orientation does
not change.
If currently filling, the polygon(s) teleported from will be filled after leaving, and filling will begin
again after teleporting.
This can be disabled with fill_gap=True, which makes the imaginary line traveled during
teleporting act as a fill barrier like in goto(x, y).
Parameters y – a number (integer or float)
Set the turtle’s second coordinate to y, leave first coordinate unchanged.
Parameters to_angle – a number (integer or float)
Set the orientation of the turtle to to_angle.
Here are some common directions in degrees:
standard mode
logo mode
0 - east
0 - north
90 - north
90 - east
180 - west
180 - south
270 - south
270 - west
90.0
turtle.home()
Move turtle to the origin – coordinates (0,0) – and set its heading to its start-orientation (which depends on the
mode, see mode()).
The center is radius units left of the turtle; extent – an angle – determines which
part of the circle is drawn.
If extent is not given, draw the entire circle.
If extent is not a full circle, one endpoint
of the arc is the current pen position.
Draw the arc in counterclockwise direction if radius is positive, otherwise in
clockwise direction.
Finally the direction of the turtle is changed by the amount of extent.
As the circle is approximated by an inscribed regular polygon, steps determines the number of steps to use.
If not
given, it will be calculated automatically.
May be used to draw regular polygons.
Parameters
• size – an integer >= 1 (if given)
• color – a colorstring or a numeric color tuple
Draw a circular dot with diameter size, using color.
If size is not given, the maximum of pensize+4 and 2*pensize
is used.
Parameters stampid – an integer, must be return value of previous stamp() call
Delete stamp with given stampid.
Undo (repeatedly) the last turtle action(s).
Number of available undo actions is determined by the size of the
undobuffer.

turtle.fd(50); turtle.lt(80)


turtle.undo()
turtle.speed(speed=None)
Parameters speed – an integer in the range 0..10 or a speedstring (see below)
Set the turtle’s speed to an integer value in the range 0..10.
If no argument is given, return current speed.
If input is a number greater than 10 or smaller than 0.5, speed is set to 0.
Speedstrings are mapped to speedvalues
as follows:
• “fastest”: 0
• “fast”: 10
• “normal”: 6
• “slow”: 3
24.1.
Attention: speed = 0 means that no animation takes place.
This depends on the turtle’s start orientation which depends on the mode - “standard”/”world” or “logo”.
225.0
turtle.xcor()
Return the turtle’s x coordinate.
Default value is 360 degrees.
90.0
Change angle measurement unit to grad (also known as gon,
grade, or gradian and equals 1/100-th of the right angle.)
100.0
90.0
24.1.
Equivalent to degrees(2*math.pi).
90.0
1.5707963267948966
Pen control
Drawing state
turtle.pendown()
turtle.pd()
turtle.down()
Pull the pen down – drawing when moving.
If resizemode is set to “auto” and turtleshape is a polygon, that polygon
is drawn with the same line thickness.
If no argument is given, the current pensize is returned.
Moreover
one or more of these attributes can be provided as keyword-arguments.
This can be used to set several pen attributes
in one statement.
False
True
Color control
turtle.pencolor(*args)
Return or set the pencolor.
Four input formats are allowed:
pencolor() Return the current pencolor as color specification string or as a tuple (see example).
May be used
as input to another color/pencolor/fillcolor call.
Set pencolor to the RGB color represented by the tuple of r, g, and b.
Each of r, g,
and b must be in the range 0..colormode, where colormode is either 1.0 or 255 (see colormode()).
Each of r, g, and b must be
in the range 0..colormode.
If turtleshape is a polygon, the outline of that polygon is drawn with the newly set pencolor.
Four input formats are allowed:
fillcolor() Return the current fillcolor as color specification string, possibly in tuple format (see example).
May be used as input to another color/pencolor/fillcolor call.
Set fillcolor to the RGB color represented by the tuple of r, g, and b.
Each of r, g,
and b must be in the range 0..colormode, where colormode is either 1.0 or 255 (see colormode()).
Each of r, g, and b must be
in the range 0..colormode.
If turtleshape is a polygon, the interior of that polygon is drawn with the newly set fillcolor.
Several input formats are allowed.
Equivalent
to
pencolor(colorstring1) and fillcolor(colorstring2) and analogously if the other input
format is used.
If turtleshape is a polygon, outline and interior of that polygon is drawn with the newly set colors.
Whether or not overlap regions for self-intersecting polygons or multiple shapes are filled depends on the operating
system graphics, type of overlap, and number of overlaps.
For example, the Turtle star above may be either all
yellow or have some white regions.
Delete the turtle’s drawings from the screen, re-center the turtle and set variables to the default values.
Do not move turtle.
State and position of the turtle as well as drawings
of other turtles are not affected.
Parameters
• arg – object to be written to the TurtleScreen
• move – True/False
• align – one of the strings “left”, “center” or right”
• font – a triple (fontname, fontsize, fonttype)
Write text - the string representation of arg - at the current turtle position according to align (“left”, “center” or
“right”) and with the given font.
If move is true, the pen is moved to the bottom-right corner of the text.
By default,
move is False.
Turtle state
Visibility
turtle.hideturtle()
turtle.ht()
Make the turtle invisible.
It’s a good idea to do this while you’re in the middle of doing some complex drawing,
because hiding the turtle speeds up the drawing observably.
Shape with
name must exist in the TurtleScreen’s shape dictionary.
Initially there are the following polygon shapes: “ar-
row”, “turtle”, “circle”, “square”, “triangle”, “classic”.
To learn about how to deal with shapes see Screen method
register_shape().
If rmode is not given, return current resizemode.
Different resizemodes have the following effects:
• “auto”: adapts the appearance of the turtle corresponding to the value of pensize.
Set resizemode to “user”.
If and only if resize-
mode is set to “user”, the turtle will be displayed stretched according to its stretchfactors: stretch_wid is stretch-
factor perpendicular to its orientation, stretch_len is stretchfactor in direction of its orientation, outline determines
the width of the shape’s outline.
Shear the turtleshape according to the given shearfactor shear, which is the
tangent of the shear angle.
Do not change the turtle’s heading (direction of movement).
If shear is not given: return
the current shearfactor, i. e. the tangent of the shear angle, by which lines parallel to the heading of the turtle are
sheared.
Parameters angle – a number
Rotate the turtleshape by angle from its current tilt-angle, but do not change the turtle’s heading (direction of
movement).
Do not change
the turtle’s heading (direction of movement).
Deprecated since version 3.1.
If angle is given, rotate the turtleshape to point in the direction specified by
angle, regardless of its current tilt-angle.
Do not change the turtle’s heading (direction of movement).
If none of the matrix elements are given, return the transformation matrix as a tuple of 4 elements.
Otherwise
set the given elements and transform the turtleshape according to the matrix consisting of first row t11, t12 and
second row t21, t22.
The determinant t11 * t22 - t12 * t21 must not be zero, otherwise an error is raised.
Return the current shape polygon as tuple of coordinate pairs.
Using events
turtle.onclick(fun, btn=1, add=None)
Parameters
• fun – a function with two arguments which will be called with the coordinates of the clicked
point on the canvas
• btn – number of the mouse-button, defaults to 1 (left mouse button)
• add – True or False – if True, a new binding will be added, otherwise it will replace a
former binding
24.1.
If fun is None, existing bindings are removed.
If fun is None, existing bindings are removed.
If fun is None, existing bindings are removed.
Every sequence of mouse-move-events on a turtle is preceded by a mouse-click event on that turtle.
Subsequently, clicking and dragging the Turtle will move it across the screen thereby producing handdrawings (if
pen is down).
Current turtle position is first vertex of polygon.
Current turtle position is last vertex of polygon.
This will be connected
with the first vertex.
Only reasonable use: as a function to return the “anonymous turtle”:
<turtle.
Turtle object at 0x...>
turtle.getscreen()
Return the TurtleScreen object the turtle is drawing on.
TurtleScreen methods can then be called for that
object.
If size is an integer, an empty undobuffer of given size is installed.
If size is None, the undobuffer is
disabled.
Create an empty Shape object of type “compound”.
Add as many components to this object as desired, using the addcomponent() method.
For example:
3.
Now add the Shape to the Screen’s shapelist and use it:
Note:
The Shape class is used internally by the register_shape() method in different ways.
The application
programmer has to deal with the Shape class only when using compound shapes like shown above!
Parameters args – a color string or three numbers in the range 0..colormode or a 3-tuple of such
numbers
Set or return background color of the TurtleScreen.
If picname is a filename, set the corresponding
image as background.
If picname is "nopic", delete background image, if present.
If picname is None, return
the filename of the current backgroundimage.
The
global function clear is a different one derived from the Turtle method clear.
Delete all drawings and all turtles from the TurtleScreen.
Reset the now empty TurtleScreen to its initial state:
white background, no background image, no event bindings and tracing on.
turtle.reset()
Note:
This TurtleScreen method is available as a global function only under the name resetscreen.
The
global function reset is another one derived from the Turtle method reset.
Else resize the canvas the turtles are drawing
on.
Do not alter the drawing window.
To observe hidden parts of the canvas, use the scrollbars.
With this method,
one can make visible those parts of a drawing which were outside the canvas before.
Parameters
• llx – a number, x-coordinate of lower left corner of canvas
24.1.
This performs a screen.
If mode “world” is already active, all drawings are redrawn according to the new coordinates.
The longer the drawing delay, the slower the animation.
Optional argument:


turtle.tracer(n=None, delay=None)
Parameters
• n – nonnegative integer
• delay – nonnegative integer
Turn turtle animation on/off and set delay for update drawings.
If n is given, only each n-th regular screen update is
really performed.
When called without arguments,
returns the currently stored value of n. Second argument sets delay value (see delay()).

fd(dist)

rt(90)

dist += 2
turtle.update()
Perform a TurtleScreen update.
To be used when tracer is turned off.
See also the RawTurtle/Turtle method speed().
If fun is None, event bindings are removed.
Remark: in order to be able to
register key-events, TurtleScreen must have the focus.
Bind fun to key-press event of key if key is given, or to any key-press-event if no key is given.
Remark: in order to
be able to register key-events, TurtleScreen must have focus.
If fun is None, existing bindings are removed.
Example for a TurtleScreen instance named screen and a Turtle instance named turtle:
# make the turtle move to the clicked point.
The
global function onclick is another one derived from the Turtle method onclick.
Tkinter’s mainloop function.
Must be the last statement in a turtle graphics program.
Must not be used if a script is run from within IDLE in -n mode (No subprocess) - for interactive use of turtle
graphics.
Input methods
turtle.textinput(title, prompt)
Parameters
• title – string
• prompt – string
Pop up a dialog window for input of a string.
Parameter title is the title of the dialog window, prompt is a text
mostly describing what information to input.
Return the string input.
The number input must be in the range minval ..
If not, a
hint is issued and the dialog remains open for correction.
Return the number input.
If the dialog is canceled, return
None.
Settings and special methods
turtle.mode(mode=None)
Parameters mode – one of the strings “standard”, “logo” or “world”
Set turtle mode (“standard”, “logo” or “world”) and perform reset.
If mode is not given, current mode is returned.
Mode “standard” is compatible with old turtle.
Mode “logo” is compatible with most Logo turtle graphics.
Mode “world” uses user-defined “world coordinates”.
Attention: in this mode angles appear distorted if x/y
unit-ratio doesn’t equal 1.
Mode
Initial turtle heading
positive angles
“standard”
to the right (east)
counterclockwise
“logo”
upward (north)
clockwise
# resets turtle heading to north
'logo'
turtle.colormode(cmode=None)
Parameters cmode – one of the values 1.0 or 255
Return the colormode or set it to 1.0 or 255.
Subsequently r, g, b values of color triples have to be in the range
0..
TurtleGraphicsError: bad color sequence: (240, 160, 80)
1.0

turtle.getcanvas()
Return the Canvas of this TurtleScreen.
Useful for insiders who know what to do with a Tkinter Canvas.
Note: Image shapes do not rotate when turning the turtle, so they do not display the heading of the turtle!
Add a turtle shape to TurtleScreen’s shapelist.
Only thusly registered shapes can be used by issuing the command
shape(shapename).
Bind bye() method to mouse clicks on the Screen.
If the value “using_IDLE” in the configuration dictionary is False (default value), also enter mainloop.
Remark:
If IDLE with the -n switch (no subprocess) is used, this value should be set to True in turtle.cfg.
In this
case IDLE’s own mainloop is active also for the client script.
turtle.setup(width=_CFG[’width’], height=_CFG[’height’], startx=_CFG[’leftright’], starty=_CFG[’topbottom’])
Set the size and position of the main window.
Default values of arguments are stored in the configuration dictionary
and can be changed via a turtle.cfg file.
Parameters
• width – if an integer, a size in pixels, if a float, a fraction of the screen; default is 50% of
screen
• height – if an integer, the height in pixels, if a float, a fraction of the screen; default is 75%
of screen
• startx – if positive, starting position in pixels from the left edge of the screen, if negative
from the right edge, if None, center window horizontally
• starty – if positive, starting position in pixels from the top edge of the screen, if negative
from the bottom edge, if None, center window vertically
# sets window to 200x200 pixels, in upper left of screen
# sets window to 75% of screen by 50% of screen and centers
turtle.title(titlestring)
Parameters titlestring – a string that is shown in the titlebar of the turtle graphics window
Set title of turtle window to titlestring.
RawTurtle(canvas)
class turtle.RawPen(canvas)
Parameters canvas – a tkinter.
Canvas, a ScrolledCanvas or a TurtleScreen
Create a turtle.
The turtle has all methods described above as “methods of Turtle/RawTurtle”.
Turtle
Subclass of RawTurtle, has the same interface but draws on a default Screen object created automatically when
needed for the first time.
TurtleScreen(cv)
Parameters cv – a tkinter.
Canvas
Provides screen oriented methods like bgcolor() etc.
Screen
Subclass of TurtleScreen, with four methods added.
Parameters master – some Tkinter widget to contain the ScrolledCanvas, i.e. a Tkinter-canvas with
scrollbars added
Used by class Screen, which thus automatically provides a ScrolledCanvas as playground for the turtles.
Parameters type_ – one of the strings “polygon”, “image”, “compound”
Data structure modeling shapes.
Example:
See Compound shapes.
Vec2D(x, y)
A two-dimensional vector class, used as a helper class for implementing turtle graphics.
May be useful for turtle
graphics programs too.
Derived from tuple, so a vector is a tuple!
A Turtle instance will automatically create a Screen instance if one is not already present.
Turtle is a subclass of RawTurtle, which doesn’t automatically create a drawing surface - a canvas will need to be
provided or created for it.
The canvas can be a tkinter.
Canvas, ScrolledCanvas or TurtleScreen.
TurtleScreen is the basic drawing surface for a turtle.
Screen is a subclass of TurtleScreen, and includes some
additional methods for managing its appearance (including size and title) and behaviour.
TurtleScreen’s constructor
needs a tkinter.
Canvas or a ScrolledCanvas as an argument.
The functional interface for turtle graphics uses the various methods of Turtle and TurtleScreen/Screen.
Be-
hind the scenes, a screen object is automatically created whenever a function derived from a Screen method is called.
Similarly, a turtle object is automatically created whenever any of the functions derived from a Turtle method is called.
To use multiple turtles on a screen, the object-oriented interface must be used.
The public methods of the Screen and Turtle classes are documented extensively via docstrings.
So these can be used as
online-help via the Python help facilities:
• When using IDLE, tooltips show the signatures and first lines of the docstrings of typed in function-/method calls.
Help on method bgcolor in module turtle:
bgcolor(self, *args) unbound turtle.
Screen method
Set or return backgroundcolor of the TurtleScreen.
Arguments (if given): a color string or three numbers
in the range 0..colormode or a 3-tuple of such numbers.
"orange"
"#800080"
Help on method penup in module turtle:
penup(self) unbound turtle.
Turtle method
Pull the pen up -- no drawing when moving.
The docstrings of the functions which are derived from methods have a modified form:
Help on function bgcolor in module turtle:
bgcolor(*args)
Set or return backgroundcolor of the TurtleScreen.
Arguments (if given): a color string or three numbers
in the range 0..colormode or a 3-tuple of such numbers.
Example::
"orange"
"#800080"
Help on function penup in module turtle:
penup()
Pull the pen up -- no drawing when moving.
These modified docstrings are created automatically together with the function definitions that are derived from the
methods at import time.
Parameters filename – a string, used as filename
Create and write docstring-dictionary to a Python script with the given filename.
This function has to be called
explicitly (it is not used by the turtle graphics classes).
The docstring dictionary will be written to the Python script
filename.py.
It is intended to serve as a template for translation of the docstrings into different languages.
If you (or your students) want to use turtle with online help in your native language, you have to translate the docstrings
and save the resulting file as e.g. turtle_docstringdict_german.py.
At the time of this writing there are docstring dictionaries in German and in Italian.
How to configure Screen and Turtles
The built-in default configuration mimics the appearance and behaviour of the old turtle module in order to retain best
possible compatibility with it.
If you want to use a different configuration which better reflects the features of this module or which better fits to your
needs, e.g. for use in a classroom, you can prepare a configuration file turtle.cfg which will be read at import time
and modify the configuration according to its settings.
The first four lines correspond to the arguments of the Screen.setup method.
For more info try help(shape).
If you set e.g. language = italian the docstringdict turtle_docstringdict_italian.py will
be loaded at import time (if present on the import path, e.g. in the same directory as turtle).
The entries exampleturtle and examplescreen define the names of these objects as they occur in the docstrings.
The
transformation of method-docstrings to function-docstrings will delete these names from the docstrings.
This will
prevent exitonclick() to enter the mainloop.
There can be a turtle.cfg file in the directory where turtle is stored and an additional one in the current working
directory.
The latter will override the settings of the first one.
The Lib/turtledemo directory contains a turtle.cfg file.
You can study it as an example and see its effects
when running the demos (preferably not from within the demo-viewer).
A demo viewer __main__.py which can be used to view the sourcecode of the scripts and run them at the same
time.
Multiple scripts demonstrating different features of the turtle module.
Examples can be accessed via the Ex-
amples menu.
They can also be run standalone.
A turtle.cfg file which serves as an example of how to write and use such files.
The methods Turtle.tracer, Turtle.window_width and Turtle.window_height have been
eliminated.
Methods with these names and functionality are now available only as methods of Screen.
The
functions derived from these remain available.
The method Turtle.fill() has been eliminated.
The behaviour of begin_fill() and end_fill()
have changed slightly: now every filling process must be completed with an end_fill() call.
A method Turtle.filling has been added.
It returns a boolean value: True if a filling process is under way,
False otherwise.
This behaviour corresponds to a fill() call without arguments in Python 2.6.
The Turtle methods shearfactor(), shapetransform() and get_shapepoly() have been added.
Thus the full range of regular linear transforms is now available for transforming turtle shapes.
The Screen method onkeypress() has been added as a complement to onkey().
As the latter binds actions
to the key release event, an alias: onkeyrelease() was also added for it.
The method Screen.mainloop has been added, so there is no longer a need to use the standalone
mainloop() function when working with Screen and Turtle objects.
Two input methods have been added: Screen.textinput and Screen.numinput.
These pop up input
dialogs and return strings and numbers respectively.
Lib/cmd.py
The Cmd class provides a simple framework for writing line-oriented command interpreters.
These are often useful for
test harnesses, administrative tools, and prototypes that will later be wrapped in a more sophisticated interface.
A Cmd instance or subclass instance is a line-oriented interpreter framework.
There is no good reason to instantiate
Cmd itself; rather, it’s useful as a superclass of an interpreter class you define yourself in order to inherit Cmd’s
methods and encapsulate action methods.
The optional argument completekey is the readline name of a completion key; it defaults to Tab.
If completekey
is not None and readline is available, command completion is done automatically.
The optional arguments stdin and stdout specify the input and output file objects that the Cmd instance or subclass
instance will use for input and output.
If not specified, they will default to sys.stdin and sys.stdout.
If you want a given stdin to be used, make sure to set the instance’s use_rawinput attribute to False, otherwise
stdin will be ignored.
Cmd Objects
A Cmd instance has the following methods:
Cmd.cmdloop(intro=None)
Repeatedly issue a prompt, accept input, parse an initial prefix off the received input, and dispatch to action methods,
passing them the remainder of the line as argument.
The optional argument is a banner or intro string to be issued before the first prompt (this overrides the intro
class attribute).
If the readline module is loaded, input will automatically inherit bash-like history-list editing (e.g.
Control-P scrolls back to the last command, Control-N forward to the next one, Control-F moves the
cursor to the right non-destructively, Control-B moves the cursor to the left non-destructively, etc.).
An end-of-file on input is passed back as the string 'EOF'.
An interpreter instance will recognize a command name foo if and only if it has a method do_foo().
As
a special case, a line beginning with the character '?' is dispatched to the method do_help().
As another
special case, a line beginning with the character '!' is dispatched to the method do_shell() (if such a method
is defined).
This method will return when the postcmd() method returns a true value.
The stop argument to postcmd()
is the return value from the command’s corresponding do_*() method.
If completion is enabled, completing commands will be done automatically, and completing of commands args is
done by calling complete_foo() with arguments text, line, begidx, and endidx.
All subclasses of Cmd inherit a predefined do_help().
This method, called with an argument 'bar', invokes
the corresponding method help_bar(), and if that is not present, prints the docstring of do_bar(), if avail-
able.
With no argument, do_help() lists all available help topics (that is, all commands with corresponding
help_*() methods or commands that have docstrings), and also lists any undocumented commands.
Cmd.onecmd(str)
Interpret the argument as though it had been typed in response to the prompt.
This may be overridden, but should
not normally need to be; see the precmd() and postcmd() methods for useful execution hooks.
The return
value is a flag indicating whether interpretation of commands by the interpreter should stop.
If there is a do_*()
method for the command str, the return value of that method is returned, otherwise the return value from the
default() method is returned.
Method called when an empty line is entered in response to the prompt.
If this method is not overridden, it repeats
the last nonempty command entered.
Method called on an input line when the command prefix is not recognized.
If this method is not overridden, it
prints an error message and returns.
Cmd.completedefault(text, line, begidx, endidx)
Method called to complete an input line when no command-specific complete_*() method is available.
By
default, it returns an empty list.
Cmd.columnize(list, displaywidth=80)
Method called to display a list of strings as a compact set of columns.
Each column is only as wide as necessary.
Hook method executed just before the command line line is interpreted, but after the input prompt is generated
and issued.
This method is a stub in Cmd; it exists to be overridden by subclasses.
The return value is used as the
command which will be executed by the onecmd() method; the precmd() implementation may re-write the
command or simply return line unchanged.
Hook method executed just after a command dispatch is finished.
This method is a stub in Cmd; it exists to be
overridden by subclasses.
The return value of this method will be used as the new value for the internal flag which corresponds to stop;
returning false will cause interpretation to continue.
Cmd.preloop()
Hook method executed once when cmdloop() is called.
This method is a stub in Cmd; it exists to be overridden
by subclasses.
Hook method executed once when cmdloop() is about to return.
This method is a stub in Cmd; it exists to be
overridden by subclasses.
Instances of Cmd subclasses have some public instance variables:
Cmd.prompt
The prompt issued to solicit input.
Cmd.identchars
The string of characters accepted for the command prefix.
The last nonempty command prefix seen.
Cmd.cmdqueue
A list of queued input lines.
The cmdqueue list is checked in cmdloop() when new input is needed; if it is
nonempty, its elements will be processed in order, as if entered at the prompt.
Cmd.intro
A string to issue as an intro or banner.
May be overridden by giving the cmdloop() method an argument.
Cmd.doc_header
The header to issue if the help output has a section for documented commands.
Cmd.misc_header
The header to issue if the help output has a section for miscellaneous help topics (that is, there are help_*()
methods without corresponding do_*() methods).
Cmd.undoc_header
The header to issue if the help output has a section for undocumented commands (that is, there are do_*()
methods without corresponding help_*() methods).
Cmd.ruler
The character used to draw separator lines under the help-message headers.
If empty, no ruler line is drawn.
It
defaults to '='.
Cmd.use_rawinput
A flag, defaulting to true.
If true, cmdloop() uses input() to display a prompt and read the next command;
if false, sys.stdout.write() and sys.stdin.readline() are used.
This section presents a simple example of how to build a shell around a few of the commands in the turtle module.
Basic turtle commands such as forward() are added to a Cmd subclass with method named do_forward().
The
argument is converted to a number and dispatched to the turtle module.
The docstring is used in the help utility provided
by the shell.
The example also includes a basic record and playback facility implemented with the precmd() method which is re-
sponsible for converting the input to lowercase and writing the commands to a file.
The do_playback() method reads
the file and adds the recorded commands to the cmdqueue for immediate playback:
import cmd, sys
from turtle import *
class TurtleShell(cmd.
Cmd):
intro = 'Welcome to the turtle shell.
Type help or ?
Here is a sample session with the turtle shell showing the help functions, using blank lines to repeat commands, and the
simple record and playback facility:
Welcome to the turtle shell.
Type help or ?
Documented commands (type help <topic>):
========================================
bye
color
goto
home
playback
record
right
circle
forward
heading
left
position
reset
undo
(turtle) help forward
Move the turtle forward by the specified distance:
FORWARD 10
(turtle) record spiral.cmd
(turtle) position
Current position is 0 0
(turtle) heading
Current heading is 0
(turtle) reset
(continues on next page)
24.2.
This will
often be useful for writing minilanguages, (for example, in run control files for Python applications) or for parsing quoted
strings.
The shlex module defines the following functions:
shlex.split(s, comments=False, posix=True)
Split the string s using shell-like syntax.
If comments is False (the default), the parsing of comments in the given
string will be disabled (setting the commenters attribute of the shlex instance to the empty string).
Concatenate the tokens of the list split_command and return a string.
This function is the inverse of split().
New in version 3.8.
The returned value is a string that can safely be used as one token in
a shell command line, for cases where you cannot use a list.
Warning: The shlex module is only designed for Unix shells.
The quote() function is not guaranteed to be correct on non-POSIX compliant shells or shells from other
operating systems such as Windows.
Executing commands quoted by this module on such shells can open up
the possibility of a command injection vulnerability.
Consider using functions that pass command arguments with lists such as subprocess.run() with
shell=False.
The shlex module defines the following class:
24.3.
A shlex instance or subclass instance is a lexical analyzer object.
The initialization argument, if present, specifies
where to read characters from.
It must be a file-/stream-like object with read() and readline() methods,
or a string.
If no argument is given, input will be taken from sys.stdin.
The second optional argument is a
filename string, which sets the initial value of the infile attribute.
If the instream argument is omitted or equal
to sys.stdin, this second argument defaults to “stdin”.
The posix argument defines the operational mode: when
posix is not true (default), the shlex instance will operate in compatibility mode.
When operating in POSIX
mode, shlex will try to be as close as possible to the POSIX shell parsing rules.
The punctuation_chars argument
provides a way to make the behaviour even closer to how real shells parse.
This can take a number of values: the
default value, False, preserves the behaviour seen under Python 3.5 and earlier.
If set to True, then parsing of
the characters ();<>|& is changed: any run of these characters (considered punctuation characters) is returned
as a single token.
If set to a non-empty string of characters, those characters will be used as the punctuation
characters.
Any characters in the wordchars attribute that appear in punctuation_chars will be removed from
wordchars.
See Improved Compatibility with Shells for more information.
The punctuation_chars parameter was added.
See also:
Module configparser Parser for configuration files similar to the Windows .ini files.
If tokens have been stacked using push_token(), pop a token off the stack.
Otherwise, read
one from the input stream.
If reading encounters an immediate end-of-file, eof is returned (the empty string ('')
in non-POSIX mode, and None in POSIX mode).
Ignore the pushback stack, and do not interpret source requests.
When shlex detects a source request (see source below) this method is given the following token as argument,
and expected to return a tuple consisting of a filename and an open file-like object.
Normally, this method first strips any quotes off the argument.
If the result is an absolute pathname, or there was
no previous source request in effect, or the previous source was a stream (such as sys.stdin), the result is left
alone.
Otherwise, if the result is a relative pathname, the directory part of the name of the file immediately before
it on the source inclusion stack is prepended (this behavior is like the way the C preprocessor handles #include
"file.h").
The result of the manipulations is treated as a filename, and returned as the first component of the tuple, with
open() called on it to yield the second component.
This hook is exposed so that you can use it to implement directory search paths, addition of file extensions, and
other namespace hacks.
There is no corresponding ‘close’ hook, but a shlex instance will call the close() method
of the sourced input stream when it returns EOF.
If the filename argument is specified it will later be available for
use in error messages.
This is the same method used internally by the sourcehook() method.
Pop the last-pushed input source from the input stack.
This is the same method used internally when the lexer
reaches EOF on a stacked input stream.
This method generates an error message leader in the format of a Unix C compiler error label; the format is '"%s",
line %d: ', where the %s is replaced with the name of the current source file and the %d with the current input
line number (the optional arguments can be used to override these).
This convenience is provided to encourage shlex users to generate error messages in the standard, parseable
format understood by Emacs and other Unix tools.
Instances of shlex subclasses have some public instance variables which either control lexical analysis or can be used
for debugging:
shlex.commenters
The string of characters that are recognized as comment beginners.
All characters from the comment beginner to
end of line are ignored.
Includes just '#' by default.
By default, includes all ASCII al-
phanumerics and underscore.
In POSIX mode, the accented characters in the Latin-1 set are also included.
If punctuation_chars is not empty, the characters ~-./*?=, which can appear in filename specifica-
tions and command line parameters, will also be included in this attribute, and any characters which appear in
punctuation_chars will be removed from wordchars if they are present there.
If whitespace_split
is set to True, this will have no effect.
Whitespace bounds tokens.
By default, includes space,
tab, linefeed and carriage-return.
This will be only used in POSIX mode, and includes just '\' by
default.
By default, includes ASCII single and double quotes.
This is only used in POSIX mode,
and includes just '"' by default.
This is useful, for example, for parsing command
lines with shlex, getting tokens in a similar way to shell arguments.
When used in combination with
punctuation_chars, tokens will be split on whitespace in addition to those characters.
Changed
in
version
3.8:
The
punctuation_chars
attribute
was
made
compatible
with
the
whitespace_split attribute.
It
may be useful to examine this when constructing error messages.
If you assign a string to it, that string will be recognized as a lexical-level inclusion
request similar to the source keyword in various shells.
That is, the immediately following token will be opened
as a filename and input will be taken from that stream until EOF, at which point the close() method of that
stream will be called and the input source will again become the original input stream.
Source requests may be
stacked any number of levels deep.
If
you need to use this, you can read the module source code to learn the details.
It may be useful to examine this when catching exceptions.
This will be set to the empty string (''), in non-POSIX mode, and to None
in POSIX mode.
Characters that will be considered punctuation.
Runs of punctuation characters will be
returned as a single token.
However, note that no semantic validity checking will be performed: for example, ‘»>’
could be returned as a token, even though it may not be recognised as such by shells.
New in version 3.6.
Parsing Rules
When operating in non-POSIX mode, shlex will try to obey to the following rules.
If whitespace_split is False, any character not declared to be a word character, whitespace, or a quote
will be returned as a single-character token.
It’s not possible to parse empty strings, even if quoted.
When operating in POSIX mode, shlex will try to obey to the following parsing rules.
The escape characters
retain its special meaning only when followed by the quote in use, or the escape character itself.
Otherwise the
escape character will be considered a normal character.
The shlex class provides compatibility with the parsing performed by common Unix shells like bash, dash, and
sh.
To take advantage of this compatibility, specify the punctuation_chars argument in the constructor.
This
defaults to False, which preserves pre-3.6 behaviour.
However, if it is set to True, then parsing of the characters
();<>|& is changed: any run of these characters is returned as a single token.
While this is short of a full parser for
shells (which would be out of scope for the standard library, given the multiplicity of shells out there), it does allow you
to perform processing of command lines more easily than you could otherwise.
Instead of passing True as the value for the punctuation_chars parameter, you can pass a string with specific characters,
which will be used to determine which characters constitute punctuation.
For example:
['a', '&', '&', 'b', '||', 'c']
Note: When punctuation_chars is specified, the wordchars attribute is augmented with the characters ~-./
*?=.
That is because these characters can appear in file names (including wildcards) and command-line arguments (e.g.
--color=auto).
For best effect, punctuation_chars should be set in conjunction with posix=True.
Program Frameworks
CHAPTER
TWENTYFIVE
GRAPHICAL USER INTERFACES WITH TK
Tk/Tcl has long been an integral part of Python.
It provides a robust and platform independent windowing toolkit,
that is available to Python programmers using the tkinter package, and its extension, the tkinter.tix and the
tkinter.ttk modules.
The tkinter package is a thin object-oriented layer on top of Tcl/Tk.
To use tkinter, you don’t need to write Tcl
code, but you will need to consult the Tk documentation, and occasionally the Tcl documentation.
Although its standard docu-
mentation is weak, good material is available, which includes: references, tutorials, a book and others.
Nevertheless, there are many
other GUI libraries that you could be interested in.
Lib/tkinter/__init__.py
The tkinter package (“Tk interface”) is the standard Python interface to the Tcl/Tk GUI toolkit.
Both Tk and
tkinter are available on most Unix platforms, including macOS, as well as on Windows systems.
Tkinter supports a range of Tcl/Tk versions, built either with or without thread support.
The official Python binary release
bundles Tcl/Tk 8.6 threaded.
See the source code for the _tkinter module for more information about supported
versions.
Tkinter is not a thin wrapper, but adds a fair amount of its own logic to make the experience more pythonic.
This
documentation will concentrate on these additions and changes, and refer to the official Tcl/Tk documentation for details
that are unchanged.
Note:
Tcl/Tk 8.5 (2007) introduced a modern set of themed user interface components along with a new API to use
them.
Both old and new APIs are still available.
Most documentation you will find online still uses the old API and can
be woefully outdated.
See also:
• TkDocs Extensive tutorial on creating user interfaces with Tkinter.
Tcl/Tk Resources:
• Tk commands Comprehensive reference to each of the underlying Tcl/Tk commands used by Tkinter.
• Tcl/Tk Home Page Additional documentation, and links to Tcl/Tk core development.
Books:
• Modern Tkinter for Busy Python Developers By Mark Roseman.
By Mark Lutz; has excellent coverage of Tkinter.
By John Ousterhout, inventor of Tcl/Tk, and Ken Jones; does not cover
Tkinter.
Architecture
Tcl/Tk is not a single library but rather consists of a few distinct modules, each with separate functionality and its own
official documentation.
Python’s binary releases also ship an add-on module together with it.
Tcl Tcl is a dynamic interpreted programming language, just like Python.
Though it can be used on its own as a general-
purpose programming language, it is most commonly embedded into C applications as a scripting engine or an
interface to the Tk toolkit.
The Tcl library has a C interface to create and manage one or more instances of a Tcl
interpreter, run Tcl commands and scripts in those instances, and add custom commands implemented in either
Tcl or C. Each interpreter has an event queue, and there are facilities to send events to it and process them.
Unlike
Python, Tcl’s execution model is designed around cooperative multitasking, and Tkinter bridges this difference (see
Threading model for details).
Tk Tk is a Tcl package implemented in C that adds custom commands to create and manipulate GUI widgets.
Each Tk
object embeds its own Tcl interpreter instance with Tk loaded into it.
Tk’s widgets are very customizable, though
at the cost of a dated appearance.
Tk uses Tcl’s event queue to generate and process GUI events.
Ttk Themed Tk (Ttk) is a newer family of Tk widgets that provide a much better appearance on different platforms than
many of the classic Tk widgets.
Ttk is distributed as part of Tk, starting with Tk version 8.5.
Python bindings are
provided in a separate module, tkinter.ttk.
Internally, Tk and Ttk use facilities of the underlying operating system, i.e., Xlib on Unix/X11, Cocoa on macOS, GDI
on Windows.
When your Python application uses a class in Tkinter, e.g., to create a widget, the tkinter module first assembles a
Tcl/Tk command string.
It passes that Tcl command string to an internal _tkinter binary module, which then calls
the Tcl interpreter to evaluate it.
Tkinter Modules
Support for Tkinter is spread across several modules.
Most applications will need the main tkinter module, as well as
the tkinter.ttk module, which provides the modern themed widget set and API:
from tkinter import *
from tkinter import ttk
class tkinter.
Tk(screenName=None, baseName=None, className=’Tk’, useTk=True, sync=False, use=None)
Construct a toplevel Tk widget, which is usually the main window of an application, and initialize a Tcl interpreter
for this widget.
Each instance has its own associated Tcl interpreter.
The Tk class is typically instantiated using all default values.
When given (as a string), sets the DISPLAY environment variable.
By default, baseName is derived from the program name (sys.argv[0]).
Used as a profile file and also as the name with which Tcl is invoked (argv0
in interp).
The tkinter.
Tcl() function sets this to False.
sync If True, execute all X server commands synchronously, so that errors are reported immediately.
Can be
used for debugging.
Note that on some platforms this will only work correctly if id refers to a Tk frame or toplevel that has its
-container option enabled.
The path for the profile files is
the HOME environment variable or, if that isn’t defined, then os.curdir.
This provides access to the Tcl interpreter.
Each widget
that is attached the same instance of Tk has the same value for its tk attribute.
For Tk, the master is None because it is the main window.
The
terms master and parent are similar and sometimes used interchangeably as argument names; however, calling
winfo_parent() returns a string of the widget name whereas master returns the object.
Tcl(screenName=None, baseName=None, className=’Tk’, useTk=False)
The Tcl() function is a factory function which creates an object much like that created by the Tk class, except that
it does not initialize the Tk subsystem.
This is most often useful when driving the Tcl interpreter in an environment
where one doesn’t want to create extraneous toplevel windows, or where one cannot (such as Unix/Linux systems
without an X server).
An object created by the Tcl() object can have a Toplevel window created (and the Tk
subsystem initialized) by calling its loadtk() method.
The modules that provide Tk support include:
25.1.
Common dialogs to allow the user to specify a file to open or save.
tkinter.font Utilities to help work with fonts.
A binary module that contains the low-level interface to Tcl/Tk.
It is automatically imported by the main
tkinter module, and should never be used directly by application programmers.
It is usually a shared library (or
DLL), but might in some cases be statically linked with the Python interpreter.
Based on tkinter.
Automatically imported by the main tkinter module.
This will become deprecated when it is replaced
with the Tk DND.
tkinter.tix (deprecated) An older third-party Tcl/Tk package that adds several new widgets.
Better alternatives for
most can be found in tkinter.ttk.
Tkinter Life Preserver
This section is not designed to be an exhaustive tutorial on either Tk or Tkinter.
For that, refer to one of the external
resources noted earlier.
Instead, this section provides a very quick orientation to what a Tkinter application looks like,
identifies foundational Tk concepts, and explains how the Tkinter wrapper is structured.
The remainder of this section will help you to identify the classes, methods, and options you’ll need in your Tkinter
application, and where to find more detailed documentation on them, including in the official Tcl/Tk reference manual.
We’ll start by walking through a “Hello World” application in Tkinter.
This isn’t the smallest one we could write, but has
enough to illustrate some key concepts you’ll need to know.
It also creates a toplevel window, known as the root window, which serves as the main window of the
application.
The following line creates a frame widget, which in this case will contain a label and a button we’ll create next.
The frame
is fit inside the root window.
The next line creates a label widget holding a static text string.
The grid() method is used to specify the relative layout
(position) of the label within its containing frame widget, similar to how tables in HTML work.
A button widget is then created, and placed to the right of the label.
When pressed, it will call the destroy() method
of the root window.
Finally, the mainloop() method puts everything on the display, and responds to user input until the program terminates.
Important Tk Concepts
Even this simple program illustrates the following key Tk concepts:
widgets A Tkinter user interface is made up of individual widgets.
Each widget is represented as a Python object,
instantiated from classes like ttk.
Frame, ttk.
Label, and ttk.
Button.
The label and button were contained within a frame, which in
turn was contained within the root window.
When creating each child widget, its parent widget is passed as the
first argument to the widget constructor.
Different classes of widgets will have different sets of options.
A geometry
manager like grid controls where in the user interface they are placed.
If your program isn’t running the event loop, your user interface won’t update.
Understanding How Tkinter Wraps Tcl/Tk
When your application uses Tkinter’s classes and methods, internally Tkinter is assembling strings representing Tcl/Tk
commands, and executing those commands in the Tcl interpreter attached to your application’s Tk instance.
Whether it’s trying to navigate reference documentation, trying to find the right method or option, adapting some existing
code, or debugging your Tkinter application, there are times that it will be useful to understand what those underlying
Tcl/Tk commands look like.
To illustrate, here is the Tcl/Tk equivalent of the main part of the Tkinter script above.
Without getting into too many details, notice the following:
• The commands used to create widgets (like ttk::frame) correspond to widget classes in Tkinter.
The pathname for the root window is just .
In Tkinter, the hierarchy is defined not by pathname
but by specifying the parent widget when creating each child widget.
As you’ll see shortly, at other times Tcl uses what appear to be method calls on
widget objects, which more closely mirror what would is used in Tkinter.
How do I…?
What option does…?
If you’re not sure how to do something in Tkinter, and you can’t immediately find it in the tutorial or reference documen-
tation you’re using, there are a few strategies that can be helpful.
First, remember that the details of how individual widgets work may vary across different versions of both Tkinter and
Tcl/Tk.
If you’re searching documentation, make sure it corresponds to the Python and Tcl/Tk versions installed on your
system.
When searching for how to use an API, it helps to know the exact name of the class, option, or method that you’re using.
Introspection, either in an interactive Python shell or with print(), can help you identify what you need.
To find out what configuration options are available on any widget, call its configure() method, which returns a
dictionary containing a variety of information about each object, including its default and current values.
Use keys()
to get just the names of each option.
As most widgets have many configuration options in common, it can be useful to find out which are specific to a particular
widget class.
Comparing the list of options to that of a simpler widget, like a frame, is one way to do that.
Similarly, you can find the available methods for a widget object using the standard dir() function.
If you try it, you’ll
see there are over 200 common widget methods, so again identifying those specific to a widget class is helpful.
Navigating the Tcl/Tk Reference Manual
As noted, the official Tk commands reference manual (man pages) is often the most accurate description of what specific
operations on widgets do.
Even when you know the name of the option or method that you need, you may still have a few
places to look.
While all operations in Tkinter are implemented as method calls on widget objects, you’ve seen that many Tcl/Tk opera-
tions appear as commands that take a widget pathname as its first parameter, followed by optional parameters, e.g.
destroy .
In the official Tcl/Tk reference documentation, you’ll find most operations that look like method calls on the man page
for a specific widget (e.g., you’ll find the invoke() method on the ttk::button man page), while functions that take a
widget as a parameter often have their own man page (e.g., grid).
You’ll find many common options and methods in the options or ttk::widget man pages, while others are found in the man
page for a specific widget class.
You’ll also find that many Tkinter methods have compound names, e.g., winfo_x(), winfo_height(),
winfo_viewable().
You’d find documentation for all of these in the winfo man page.
Note: Somewhat confusingly, there are also methods on all Tkinter widgets that don’t actually operate on the widget, but
operate at a global scope, independent of any widget.
Examples are methods for accessing the clipboard or the system
bell.
If you use threads, you may
need to be aware of this.
A Python interpreter may have many threads associated with it.
In Tcl, multiple threads can be created, but each thread
has a separate Tcl interpreter instance associated with it.
Threads can also create more than one interpreter instance,
though each interpreter instance can be used only by the one thread that created it.
Each Tk object created by tkinter contains a Tcl interpreter.
It also keeps track of which thread created that interpreter.
Calls to tkinter can be made from any Python thread.
Internally, if a call comes from a thread other than the one that
created the Tk object, an event is posted to the interpreter’s event queue, and when executed, the result is returned to the
calling Python thread.
Tcl/Tk applications are normally event-driven, meaning that after initialization, the interpreter runs an event loop (i.e. Tk.
mainloop()) and responds to events.
Because it is single-threaded, event handlers must respond quickly, otherwise
they will block other events from being processed.
To avoid this, any long-running computations should not run in an
event handler, but are either broken into smaller pieces using timers, or run in another thread.
This is different from many
GUI toolkits where the GUI runs in a completely separate thread from all application code including event handlers.
If the Tcl interpreter is not running the event loop and processing events, any tkinter calls made from threads other
than the one running the Tcl interpreter will fail.
A number of special cases exist:
• Tcl/Tk libraries can be built so they are not thread-aware.
In this case, tkinter calls the library from the origi-
nating Python thread, even if this is different than the thread that created the Tcl interpreter.
A global lock ensures
only one call occurs at a time.
While tkinter allows you to create more than one instance of a Tk object (with its own interpreter), all inter-
preters that are part of the same thread share a common event queue, which gets ugly fast.
In practice, don’t create
more than one instance of Tk at a time.
Otherwise, it’s best to create them in separate threads and ensure you’re
running a thread-aware Tcl/Tk build.
It is
even possible to run multiple nested event loops or abandon the event loop entirely.
If you’re doing anything tricky
when it comes to events or threads, be aware of these possibilities.
For a complete explanation of a given option and its behavior, see the Tk man pages for the widget in question.
Note that the man pages list “STANDARD OPTIONS” and “WIDGET SPECIFIC OPTIONS” for each widget.
The
former is a list of options that are common to many widgets, the latter are the options that are idiosyncratic to that
particular widget.
The Standard Options are documented on the options(3) man page.
No distinction between standard and widget-specific options is made in this document.
Some options don’t apply to some
kinds of widgets.
Whether a given widget responds to a particular option depends on the class of the widget; buttons have
a command option, labels do not.
The options supported by a given widget are listed in that widget’s man page, or can be queried at runtime by calling the
config() method without arguments, or by calling the keys() method on that widget.
The return value of these calls
is a dictionary whose key is the name of the option as a string (for example, 'relief') and whose values are 5-tuples.
Some options, like bg are synonyms for common options with long names (bg is shorthand for “background”).
Passing
the config() method the name of a shorthand option will return a 2-tuple, not 5-tuple.
The 2-tuple passed back will
contain the name of the synonym and the “real” option (such as ('bg', 'background')).
Index
Meaning
Example

option name
'relief'

option name for database lookup
'relief'

option class for database lookup
'Relief'

default value
'raised'

current value
'groove'
Example:
{'relief': ('relief', 'relief', 'Relief', 'raised', 'groove')}
Of course, the dictionary printed will include all the options available and their values.
The packer is one of Tk’s geometry-management mechanisms.
Geometry managers are used to specify the relative
positioning of widgets within their container - their mutual master.
In contrast to the more cumbersome placer (which is
used less commonly, and we do not cover here), the packer takes qualitative relationship specification - above, to the left
of, filling, etc - and works everything out to determine the exact placement coordinates for you.
The size of any master widget is determined by the size of the “slave widgets” inside.
The packer is used to control where
slave widgets appear inside the master into which they are packed.
You can pack widgets into frames, and frames into
other frames, in order to achieve the kind of layout you desire.
Additionally, the arrangement is dynamically adjusted to
accommodate incremental changes to the configuration, once it is packed.
Note that widgets do not appear until they have had their geometry specified with a geometry manager.
It’s a common
early mistake to leave out the geometry specification, and then be surprised when the widget is created but nothing appears.
A widget will appear only after it has had, for example, the packer’s pack() method applied to it.
The pack() method can be called with keyword-option/value pairs that control where the widget is to appear within its
container, and how it is to behave when the main application window is resized.
Here are some examples:
fred.pack()
# defaults to side = "top"
fred.pack(side="left")
fred.pack(expand=1)
Packer Options
For more extensive information on the packer and the options that it can take, see the man pages and page 183 of John
Ousterhout’s book.
Denotes where the packer is to place each slave in its parcel.
Coupling Widget Variables
The current-value setting of some widgets (like text entry widgets) can be connected directly to application variables by
using special options.
These options are variable, textvariable, onvalue, offvalue, and value.
This
connection works both ways: if the variable changes for any reason, the widget it’s connected to will be updated to reflect
the new value.
Unfortunately, in the current implementation of tkinter it is not possible to hand over an arbitrary Python variable to a
widget through a variable or textvariable option.
The only kinds of variables for which this works are variables
that are subclassed from a class called Variable, defined in tkinter.
There are many useful subclasses of Variable already defined:
StringVar, IntVar, DoubleVar, and
BooleanVar.
To read the current value of such a variable, call the get() method on it, and to change its value
you call the set() method.
If you follow this protocol, the widget will always track the value of the variable, with no
further intervention on your part.
For example:
25.1.
StringVar()
# Set it to some value.
The current entry content is:",
self.contents.get())
root = tk.
The Window Manager
In Tk, there is a utility command, wm, for interacting with the window manager.
Options to the wm command allow you to
control things like titles, placement, icon bitmaps, and the like.
In tkinter, these commands have been implemented
as methods on the Wm class.
Toplevel widgets are subclassed from the Wm class, and so can call the Wm methods directly.
To get at the toplevel window that contains a given widget, you can often just refer to the widget’s master.
Of course if the
widget has been packed inside of a frame, the master won’t represent a toplevel window.
To get at the toplevel window
that contains an arbitrary widget, you can call the _root() method.
This method begins with an underscore to denote
the fact that this function is part of the implementation, and not an interface to Tk functionality.
Here are some examples of typical usage:
import tkinter as tk
class App(tk.
To specify an X bitmap filename, give the full path to the file,
preceded with an @, as in "@/usr/contrib/bitmap/gumby.bit".
See page 160 of Ousterhout’s book for details.
For example to get
a hand cursor (XC_hand2), use the string "hand2".
You can also specify a bitmap and mask file of your own.
See page 179 of Ousterhout’s book.
distance Screen distances can be specified in either pixels or absolute distances.
Pixels are given as numbers and absolute
distances as strings, with the trailing character denoting units: c for centimetres, i for inches, m for millimetres, p
for printer’s points.
For example, 3.5 inches is expressed as "3.5i".
Font sizes with positive numbers are measured
in points; sizes with negative numbers are measured in pixels.
For example: fred["geometry"] = "200x100".
For example:
"2 3 4 5" and "3i 2i 4.5i 2i" and "3c 2c 4c 10.43c" are all legal regions.
Legal values are: "raised", "sunken", "flat",
"groove", and "ridge".
This is almost always the set() method of some scrollbar widget, but can be any widget method that
takes a single argument.
The bind method from the widget command allows you to watch for certain events and to have a callback function trigger
when that event type occurs.
The form of the bind method is:

sequence is a string that denotes the target kind of event.
An Event instance will be passed
as the argument.
Passing an empty string denotes that this binding is to replace any other bindings that
this event is associated with.
Passing a '+' means that this function is to be added to the list of functions bound
to this event type.
For example:

self.button.bind("<Enter>", self.turn_red)
Notice how the widget field of the event is being accessed in the turn_red() callback.
This field contains the widget
that caught the X event.
The following table lists the other event fields you can access, and how they are denoted in Tk,
which can be useful when referring to the Tk man pages.
Y
y_root
The index Parameter
A number of widgets require “index” parameters to be passed.
These are used to point at a specific place in a Text widget,
or to particular characters in an Entry widget, or to particular menu items in a Menu widget.
Entry widgets have options that refer to character positions in the text
being displayed.
You can use these tkinter functions to access these special points in text widgets:
Text widget indexes The index notation for Text widgets is very rich and is best described in the Tk man pages.
Some options and methods for menus manipulate specific
menu entries.
An integer preceded by @, as in @6, where the integer is interpreted as a y pixel coordinate in the menu’s
coordinate system;
• the string "none", which indicates no menu entry at all, most often used with menu.activate() to deactivate
all entries, and finally,
• a text string that is pattern matched against the label of the menu entry, as scanned from the top of the menu
to the bottom.
Note that this index type is considered after all the others, which means that matches for menu
items labelled last, active, or none may be interpreted as the above literals, instead.
Images
Images of different formats can be created through the corresponding subclass of tkinter.
Image:
• BitmapImage for images in XBM format.
PhotoImage for images in PGM, PPM, GIF and PNG formats.
The latter is supported starting with Tk 8.6.
Either type of image is created through either the file or the data option (other options are available as well).
The image object can then be used wherever an image option is supported by some widget (e.g. labels, buttons, menus).
In these cases, Tk will not keep a reference to the image.
When the last Python reference to the image object is deleted,
the image data is deleted as well, and Tk will display an empty box wherever the image was used.
See also:
The Pillow package adds support for formats such as BMP, JPEG, TIFF, and WebP, among others.
File Handlers
Tk allows you to register and unregister a callback function which will be called from the Tk mainloop when I/O is possible
on a file descriptor.
Only one handler may be registered per file descriptor.
Example code:
import tkinter
widget = tkinter.
Tk()
mask = tkinter.
READABLE | tkinter.
This feature is not available on Windows.
Since you don’t know how many bytes are available for reading, you may not want to use the BufferedIOBase or
TextIOBase read() or readline() methods, since these will insist on reading a predefined number of bytes.
For sockets, the recv() or recvfrom() methods will work fine; for other files, use raw reads or os.read(file.
fileno(), maxbytecount).
Registers the file handler callback function func.
The file argument may either be an object with a fileno()
method (such as a file or socket object), or an integer file descriptor.
The mask argument is an ORed combination
of any of the three constants below.
The callback is called as follows:
callback(file, mask)
25.1.
Unregisters a file handler.
READABLE
tkinter.
WRITABLE
tkinter.
EXCEPTION
Constants used in the mask arguments.
Color choosing dialog
Source code: Lib/tkinter/colorchooser.py
The tkinter.colorchooser module provides the Chooser class as an interface to the native color picker dialog.
Chooser implements a modal color choosing dialog window.
The Chooser class inherits from the Dialog class.
Chooser(master=None, **options)
tkinter.colorchooser.askcolor(color=None, **options)
Create a color choosing dialog.
A call to this method will show the window, wait for the user to make a selection,
and return the selected color (or None) to the caller.
Tkinter standard dialog module
25.3 tkinter.font — Tkinter font wrapper
Source code: Lib/tkinter/font.py
The tkinter.font module provides the Font class for creating and using named fonts.
The different font weights and slants are:
tkinter.font.
NORMAL
tkinter.font.
BOLD
tkinter.font.
ITALIC
tkinter.font.
ROMAN
class tkinter.font.
The Font class represents a named font.
Font instances are given unique names and can be specified by their
family, size, and style configuration.
Named fonts are Tk’s method of creating and identifying fonts as a single
object, rather than specifying a font by its attributes with each occurrence.
If size is positive it is interpreted as size in points.
If size is a negative number its absolute value is treated
as size in pixels.
weight - font emphasis (NORMAL, BOLD)
slant - ROMAN, ITALIC
underline - font underlining (0 - none, 1 - underline)
overstrike - font strikeout (0 - none, 1 - strikeout)
actual(option=None, displayof=None)
Return the attributes of the font.
Modify attributes of the font.
If
no display is specified then the main application window is assumed.
Options include:
ascent - distance between baseline and highest point that a character of the font can occupy
descent - distance between baseline and lowest point that a character of the font can occupy
linespace - minimum vertical separation necessary between any two characters of the font that ensures
no vertical overlap between lines.
The root parameter was added.
Lib/tkinter/simpledialog.py
The tkinter.simpledialog module contains convenience classes and functions for creating simple modal dialogs
to get a value from the user.
The above three functions provide dialogs that prompt the user to enter a value of the desired type.
class tkinter.simpledialog.
The base class for custom dialogs.
Override to construct the dialog’s interface and return the widget that should have initial focus.
Override for custom button layouts.
Lib/tkinter/filedialog.py
The tkinter.filedialog module provides classes and factory functions for creating file/directory selection win-
dows.
Native Load/Save Dialogs
The following classes and functions provide file dialog windows that combine a native look-and-feel with configuration
options to customize behaviour.
Prompt user to select a directory.
Additional keyword option:
mustexist - determines if selection must be an existing directory.
The above two classes provide native dialog windows for saving and loading files.
Convenience classes
The below classes are used for creating file/directory windows from scratch.
These do not emulate the native look-and-feel
of the platform.
Directory(master=None, **options)
Create a dialog prompting the user to select a directory.
Note: The FileDialog class should be subclassed for custom event handling and behaviour.
FileDialog(master, title=None)
Create a basic file selection dialog.
Event handler for double-click event on directory.
Event handler for click event on directory.
Event handler for double-click event on file.
Event handler for single-click event on file.
Update the current file selection to file.
A subclass of FileDialog that creates a dialog window for selecting an existing file.
A subclass of FileDialog that creates a dialog window for selecting a destination file.
Confirmation is required if an
already existing file is selected.
See also:
Modules tkinter.messagebox, tut-files
25.5 tkinter.messagebox — Tkinter message prompts
Source code: Lib/tkinter/messagebox.py
The tkinter.messagebox module provides a template base class as well as a variety of convenience methods for
commonly used configurations.
The message boxes are modal and will return a subset of (True, False, None, OK,
CANCEL, YES, NO) based on the user’s selection.
Common message box styles and layouts include but are not limited
to:
class tkinter.messagebox.
Message(master=None, **options)
Create a message window with an application-specified message, an icon and a set of buttons.
Each of the buttons
in the message window is identified by a unique symbolic name (see the type options).
The following options are supported:
command Specifies the function to invoke when the user closes the dialog.
The name of the button
clicked by the user to close the dialog is passed as argument.
This is only available on macOS.
default Gives the symbolic name of the default button for this message window (OK, CANCEL, and so
on).
If this option is not specified, the first button in the dialog will be made the default.
The message
detail will be presented beneath the main message and, where supported by the OS, in a less em-
phasized font than the main message.
If this option is not specified, then the INFO icon will be displayed.
The default value is an empty string.
The message box is displayed
on top of its parent window.
This option is ignored on macOS,
where platform guidelines forbid the use of a title on this kind of dialog.
Then return the symbolic name
of the selected button.
Keyword arguments can override options specified in the constructor.
Information message box
25.5.
Warning message boxes
tkinter.messagebox.showwarning(title=None, message=None, **options)
Creates and displays a warning message box with the specified title and message.
Question message boxes
tkinter.messagebox.askquestion(title=None, message=None, *, type=YESNO, **options)
Ask a question.
By default shows buttons YES and NO.
Returns the symbolic name of the selected button.
Shows buttons OK and CANCEL.
Returns True if the answer is ok and False
otherwise.
Shows buttons RETRY and CANCEL.
Return True if the answer is yes and
False otherwise.
Shows buttons YES and NO.
Returns True if the answer is yes and False otherwise.
Shows buttons YES, NO and CANCEL.
Return True if the answer is yes, None if cancelled, and
False otherwise.
Symbolic names of buttons:
tkinter.messagebox.
ABORT = 'abort'
tkinter.messagebox.
RETRY = 'retry'
tkinter.messagebox.
IGNORE = 'ignore'
tkinter.messagebox.
OK = 'ok'
tkinter.messagebox.
CANCEL = 'cancel'
tkinter.messagebox.
YES = 'yes'
tkinter.messagebox.
NO = 'no'
Predefined sets of buttons:
tkinter.messagebox.
ABORTRETRYIGNORE = 'abortretryignore'
Displays three buttons whose symbolic names are ABORT, RETRY and IGNORE.
OK = 'ok'
Displays one button whose symbolic name is OK.
OKCANCEL = 'okcancel'
Displays two buttons whose symbolic names are OK and CANCEL.
RETRYCANCEL = 'retrycancel'
Displays two buttons whose symbolic names are RETRY and CANCEL.
NO.
tkinter.messagebox.
YESNOCANCEL = 'yesnocancel'
Displays three buttons whose symbolic names are YES, NO and CANCEL.
Icon images:
tkinter.messagebox.
ERROR = 'error'
tkinter.messagebox.
INFO = 'info'
tkinter.messagebox.
QUESTION = 'question'
tkinter.messagebox.
Using the ScrolledText class is a lot easier than setting
up a text widget and scroll bar directly.
The text widget and scrollbar are packed together in a Frame, and the methods of the Grid and Pack geometry
managers are acquired from the Frame object.
This allows the ScrolledText widget to be used directly to achieve
most normal geometry management behavior.
Should more specific control be necessary, the following attributes are available:
class tkinter.scrolledtext.
ScrolledText(master=None, **kw)
frame
The frame which surrounds the text and scroll bar widgets.
This is experimental and due to be deprecated when it is replaced with the Tk DND.
The tkinter.dnd module provides drag-and-drop support for objects within a single application, within the same
window or between windows.
To enable an object to be dragged, you must create an event binding for it that starts the
drag-and-drop process.
Typically, you bind a ButtonPress event to a callback function that you write (see Bindings and
Events).
The function should call dnd_start(), where ‘source’ is the object to be dragged, and ‘event’ is the event that
invoked the call (the argument to your callback function).
Selection of a target object occurs as follows:
25.6.
Top-down search of area under mouse for target widget
• Target widget should have a callable dnd_accept attribute
• If dnd_accept is not present or returns None, search moves to parent widget
• If no target widget is found, then the target object is None
2.
Call to <target>.dnd_commit(source, event) to notify of drop
5.
Call to <source>.dnd_end(target, event) to signal end of drag-and-drop
class tkinter.dnd.
The DndHandler class handles drag-and-drop events tracking Motion and ButtonRelease events on the root of the
event widget.
Inspect area below mouse for target objects while drag is performed.
Signal end of drag when the release pattern is triggered.
Lib/tkinter/ttk.py
The tkinter.ttk module provides access to the Tk themed widget set, introduced in Tk 8.5.
It provides additional
benefits including anti-aliased font rendering under X11 and window transparency (requiring a composition window man-
ager on X11).
The basic idea for tkinter.ttk is to separate, to the extent possible, the code implementing a widget’s behavior from
the code implementing its appearance.
This has the direct benefit of using the new widgets which gives a better look and feel across platforms; however, the
replacement widgets are not completely compatible.
The main difference is that widget options such as “fg”, “bg” and
others related to widget styling are no longer present in Ttk widgets.
Instead, use the ttk.
Style class for improved
styling effects.
See also:
Converting existing applications to use Tile widgets A monograph (using Tcl terminology) about differences typi-
cally encountered when moving applications to use the new widgets.
Button, Checkbutton, Entry, Frame,
Label, LabelFrame, Menubutton, PanedWindow, Radiobutton, Scale, Scrollbar, and Spinbox.
The other six are new: Combobox, Notebook, Progressbar, Separator, Sizegrip and Treeview.
And
all them are subclasses of Widget.
Using the Ttk widgets gives the application an improved look and feel.
As discussed above, there are differences in how
the styling is coded.
Tk code:
l1 = tkinter.
Ttk code:
style = ttk.
Widget defines standard options and methods supported by Tk themed widgets and is not supposed to be directly
instantiated.
Standard Options
All the ttk Widgets accept the following options:
Option
Description
class
Specifies the window class.
The class is used when querying the option database for the window’s
other options, to determine the default bindtags for the window, and to select the widget’s default
layout and style.
This option is read-only, and may only be specified when the window is created.
If set to the empty string (the default), the cursor
is inherited for the parent widget.
If 0 is returned, it means that the window should be skipped entirely during keyboard
traversal.
If 1, it means that the window should receive the input focus as long as it is viewable.
And an
empty string means that the traversal scripts make the decision about whether or not to focus on the
window.
Scrollable Widget Options
The following options are supported by widgets that are controlled by a scrollbar.
Option
Description
xscrollcommand
Used to communicate with horizontal scrollbars.
When the view in the widget’s window change, the widget will generate a Tcl command based on
the scrollcommand.
Usually this option consists of the method Scrollbar.set() of some scrollbar.
This will
cause the scrollbar to be updated whenever the view in the window changes.
Option
Description
text
Specifies a text string to be displayed inside the widget.
The
underline character is used for mnemonic activation.
This is a list of 1 or more elements.
The first element
is the default image name.
The rest of the list if a sequence of statespec/value pairs
as defined by Style.map(), specifying different images to use when the widget is
in a particular state or a combination of states.
All images in the list should have the
same size.
Valid values are:
• text: display text only
• image: display image only
• top, bottom, left, right: display image above, below, left of, or right of the text,
respectively.
If zero or unspecified, the
natural width of the text label is used.
Compatibility Options
Option
Description
state
May be set to “normal” or “disabled” to control the “disabled” state bit.
This is a write-only option: setting
it changes the widget state, but the Widget.state() method does not affect this option.
Widget States
The widget state is a bitmap of independent state flags.
Flag
Description
active
The mouse cursor is over the widget and pressing a mouse button will cause some action to occur
disabled
Widget is disabled under program control
focus
Widget has keyboard focus
pressed
Widget is being pressed
selected
“On”, “true”, or “current” for things like Checkbuttons and radiobuttons
background
Windows and Mac have a notion of an “active” or foreground window.
The background state is set for
widgets in a background window, and cleared for those in the foreground window
readonly
Widget should not allow user modification
alternate
A widget-specific alternate display format
invalid
The widget’s value is invalid
A state specification is a sequence of state names, optionally prefixed with an exclamation point indicating that the bit is
off.
Besides the methods described below, the ttk.
Widget supports the methods tkinter.Widget.cget() and
tkinter.Widget.configure().
Widget
identify(x, y)
Returns the name of the element at position x y, or the empty string if the point does not lie within any
element.
If a callback is not specified, returns True if the widget state matches statespec and
False otherwise.
If callback is specified then it is called with args if widget state matches statespec.
If statespec is specified, sets the widget state according to it and return a new
statespec indicating which flags were changed.
If statespec is not specified, returns the currently enabled state
flags.
Combobox widget combines a text field with a pop-down list of values.
This widget is a subclass of Entry.
Besides the methods inherited from Widget:
Widget.cget(),
Widget.configure(),
Widget.
identify(), Widget.instate() and Widget.state(), and the following inherited from Entry:
Entry.bbox(), Entry.delete(), Entry.icursor(), Entry.index(), Entry.insert(), Entry.
selection(), Entry.xview(), it has some other methods, described at ttk.Combobox.
Options
This widget accepts the following specific options:
Option
Description
exportselection
Boolean value.
If set, the widget selection is linked to the Window Manager selection (which can
be returned by invoking Misc.selection_get, for example).
One of “left”, “center”, or “right”.
It may specify which values to display.
In the “readonly” state, the value may not be edited
directly, and the user can only selection of the values from the dropdown list.
In the “normal”
state, the text field is directly editable.
In the “disabled” state, no interaction is possible.
Whenever the value associated with
that name changes, the widget value is updated, and vice versa.
See tkinter.
StringVar.
Combobox
class tkinter.ttk.
If newindex is specified, sets the combobox value to the element position newindex.
Otherwise, returns the
index of the current value or -1 if the current value is not in the values list.
Spinbox
The ttk.
Spinbox widget is a ttk.
Entry enhanced with increment and decrement arrows.
It can be used for
numbers or lists of string values.
This widget is a subclass of Entry.
Besides the methods inherited from Widget:
Widget.cget(),
Widget.configure(),
Widget.
identify(), Widget.instate() and Widget.state(), and the following inherited from Entry:
Entry.bbox(), Entry.delete(), Entry.icursor(), Entry.index(), Entry.insert(), Entry.
xview(), it has some other methods, described at ttk.
Spinbox.
Options
This widget accepts the following specific options:
Option
Description
from
Float value.
If set, this is the minimum value to which the decrement button will decrement.
Must be
spelled as from_ when used as an argument, since from is a Python keyword.
to
Float value.
If set, this is the maximum value to which the increment button will increment.
Specifies the amount which the increment/decrement buttons change the value.
Defaults to
1.0.
values
Sequence of string or float values.
If specified, the increment/decrement buttons will cycle through the
items in this sequence rather than incrementing or decrementing numbers.
If True, increment and decrement buttons will cycle from the to value to the from
value or the from value to the to value, respectively.
format
String value.
This specifies the format of numbers set by the increment/decrement buttons.
It must be
in the form “%W.Pf”, where W is the padded width of the value, P is the precision, and ‘%’ and ‘f’ are
literal.
Will be called with no arguments whenever either of the increment or decrement
buttons are pressed.
25.8.
Spinbox
class tkinter.ttk.
Spinbox
get()
Returns the current value of the spinbox.
set(value)
Sets the value of the spinbox to value.
Each child window is associated
with a tab, which the user may select to change the currently displayed window.
Options
This widget accepts the following specific options:
Option
Description
height
If present and greater than zero, specifies the desired height of the pane area (not including internal
padding or tabs).
Otherwise, the maximum height of all panes is used.
The padding is a list up to
four length specifications left top right bottom.
If fewer than four elements are specified, bottom defaults
to top, right defaults to left, and top defaults to left.
Otherwise, the maximum width of all panes is used.
If “disabled”, then the tab is not selectable.
If “hidden”, then
the tab is not shown.
Value is a string containing zero or
more of the characters “n”, “s”, “e” or “w”.
Each letter refers to a side (north, south, east or west) that
the child window will stick to, as per the grid() geometry manager.
Syntax is the same as
for the option padding used by this widget.
See the option image described in Widget.
See Label Options for legal values.
The underlined character is
used for mnemonic activation if Notebook.enable_traversal() is called.
Tab Identifiers
The tab_id present in several methods of ttk.
Virtual Events
This widget generates a «NotebookTabChanged» virtual event after a new tab is selected.
Notebook
class tkinter.ttk.
Notebook
add(child, **kw)
Adds a new tab to the notebook.
If window is currently managed by the notebook but hidden, it is restored to its previous position.
Hides the tab specified by tab_id.
The tab will not be displayed, but the associated window remains managed by the notebook and its configu-
ration remembered.
Hidden tabs may be restored with the add() command.
If child is already managed
by the notebook, moves it to the specified position.
See Tab Options for the list of available options.
The associated child window will be displayed, and the previously selected window (if different) is unmapped.
If tab_id is omitted, returns the widget name of the currently selected pane.
tab(tab_id, option=None, **kw)
Query or modify the options of the specific tab_id.
If kw is not given, returns a dictionary of the tab option values.
If option is specified, returns the value of that
option.
Otherwise, sets the options to the corresponding values.
tabs()
Returns a list of windows managed by the notebook.
This will extend the bindings for the toplevel window containing the notebook as follows:
• Control-Tab: selects the tab following the currently selected one.
Multiple notebooks in a single toplevel may be enabled for traversal, including nested notebooks.
However,
notebook traversal only works properly if all panes have the notebook they are in as master.
Progressbar
The ttk.
Progressbar widget shows the status of a long-running operation.
It can operate in two modes: 1) the deter-
minate mode which shows the amount completed relative to the total amount of work to be done and 2) the indeterminate
mode which provides an animated display to let the user know that work is progressing.
Specifies the orientation of the progress bar.
Defaults to 100.
In “determinate” mode, this represents the amount of work
completed.
In “indeterminate” mode, it is interpreted as modulo maximum; that is, the progress bar
completes one “cycle” when its value increases by maximum.
If specified, the value of the progress bar is automatically
set to the value of this name whenever the latter is modified.
The widget periodically increments the value of this option whenever its value is
greater than 0 and, in determinate mode, less than maximum.
This option may be used by the current
theme to provide additional animation effects.
Progressbar
class tkinter.ttk.
Progressbar
start(interval=None)
Begin autoincrement mode: schedules a recurring timer event that calls Progressbar.step() every
interval milliseconds.
If omitted, interval defaults to 50 milliseconds.
Separator widget displays a horizontal or vertical separator bar.
It has no other methods besides the ones inherited from ttk.Widget.
Options
This widget accepts the following specific option:
Option
Description
orient
One of “horizontal” or “vertical”.
Specifies the orientation of the separator.
25.8.
Sizegrip
The ttk.
Sizegrip widget (also known as a grow box) allows the user to resize the containing toplevel window by
pressing and dragging the grip.
This widget has neither specific options nor specific methods, besides the ones inherited from ttk.Widget.
On macOS, toplevel windows automatically include a built-in size grip by default.
Adding a Sizegrip is harmless,
since the built-in grip will just mask the widget.
This widget supports only “southeast” resizing.
Treeview widget displays a hierarchical collection of items.
Each item has a textual label, an optional image,
and an optional list of data values.
The data values are displayed in successive columns after the tree label.
The order in which data values are displayed may be controlled by setting the widget option displaycolumns.
The
tree widget can also display column headings.
Columns may be accessed by number or symbolic names listed in the
widget option columns.
See Column Identifiers.
Each item is identified by a unique name.
The widget will generate item IDs if they are not supplied by the caller.
There
is a distinguished root item, named {}.
The root item itself is not displayed; its children appear at the top level of the
hierarchy.
Each item also has a list of tags, which can be used to associate event bindings with individual items and control the
appearance of the item.
The Treeview widget supports horizontal and vertical scrolling, according to the options described in Scrollable Widget
Options and the methods Treeview.xview() and Treeview.yview().
Note: the requested width is
determined from the sum of the column widths.
The padding is a list of up to four length
specifications.
One of “extended”,
“browse” or “none”.
If set to “extended” (the default), multiple items may be selected.
If “browse”, only a single item will be selected at a time.
If “none”, the selection will
not be changed.
Note that the application code and tag bindings can set the selection however they
wish, regardless of the value of this option.
The default is “tree headings”, i.e., show all elements.
Note: Column #0 always refers to the tree column, even if show=”tree” is not speci-
fied.
Item Options
The following item options may be specified for items in the insert and item widget commands.
Option
Description
text
The textual label to display for the item.
Each item should have the same number of values as the widget option columns.
If there are fewer values
than columns, the remaining values are assumed empty.
If there are more values than columns, the extra
values are ignored.
Tag Options
The following options may be specified on tags:
Option
Description
foreground
Specifies the text foreground color.
A symbolic name from the list of columns option.
A string of the form #n, where n is an integer, specifying the nth display column.
Item’s option values may be displayed in a different order than the order in which they are stored.
A data column number is an index into an item’s option values list; a display column number is the column number in the
tree where the values are displayed.
Tree labels are displayed in column #0.
If option displaycolumns is not set, then data
column n is displayed in column #n+1.
Again, column #0 always refers to the tree column.
Virtual Events
The Treeview widget generates the following virtual events.
Event
Description
«TreeviewSelect»
Generated whenever the selection changes.
The Treeview.focus() and Treeview.selection() methods can be used to determine the affected item or
items.
Treeview
class tkinter.ttk.
Treeview
bbox(item, column=None)
Returns the bounding box (relative to the treeview widget’s window) of the specified item in the form (x, y,
width, height).
If column is specified, returns the bounding box of that cell.
If the item is not visible (i.e., if it is a descendant
of a closed item or is scrolled offscreen), returns an empty string.
If item is not specified, returns root children.
Children present in item that are not present in newchildren are detached from the tree.
No items in newchil-
dren may be an ancestor of item.
If kw is not given, returns a dict of the column option values.
If option is specified then the value for that
option is returned.
Otherwise, sets the options to the corresponding values.
The valid options/values are:
id Returns the column name.
This is a read-only option.
Specifies how the text in this column should be aligned
with respect to the cell.
The treeview widget will not make the
column any smaller than specified by this option when the widget is resized or the user drags a column.
Delete all specified items and all their descendants.
The root item may not be deleted.
Unlinks all of the specified items from the tree.
The items and all of their descendants are still present, and may be reinserted at another point in the tree, but
will not be displayed.
The root item may not be detached.
Returns True if the specified item is present in the tree.
If item is specified, sets the focus item to item.
Otherwise, returns the current focus item, or ‘’ if there is none.
heading(column, option=None, **kw)
Query or modify the heading options for the specified column.
If kw is not given, returns a dict of the heading option values.
If option is specified then the value for that
option is returned.
Otherwise, sets the options to the corresponding values.
The valid options/values are:
text: text The text to display in the column heading.
One of the standard Tk anchor values.
Returns a description of the specified component under the point given by x and y, or the empty string if no
such component is present at that position.
The tree column has ID #0.
Tk 8.6.
index(item)
Returns the integer index of item within its parent’s list of children.
If index is less
than or equal to zero, the new node is inserted at the beginning; if index is greater than or equal to the current
number of children, it is inserted at the end.
If iid is specified, it is used as the item identifier; iid must not
already exist in the tree.
Otherwise, a new unique identifier is generated.
See Item Options for the list of available points.
If no options are given, a dict with options/values for the item is returned.
If option is specified then the value
for that option is returned.
Moves item to position index in parent’s list of children.
It is illegal to move an item under one of its descendants.
If index is less than or equal to zero, item is moved to
the beginning; if greater than or equal to the number of children, it is moved to the end.
If item was detached
it is reattached.
An alias for Treeview.move().
see(item)
Ensure that item is visible.
Sets all of item’s ancestors open option to True, and scrolls the widget if necessary so that item is within the
visible portion of the tree.
Changed in version 3.8: selection() no longer takes arguments.
For changing the selection state use the
following selection methods.
selection_set(*items)
items becomes the new selection.
Add items to the selection.
Remove items from the selection.
Changed in version 3.6: items can be passed as separate arguments, not just as a single tuple.
Toggle the selection state of each item in items.
Changed in version 3.6: items can be passed as separate arguments, not just as a single tuple.
With two arguments,
returns the current value of the specified column.
With three arguments, sets the value of given column in
given item to the specified value.
tag_bind(tagname, sequence=None, callback=None)
Bind a callback for the given event sequence to the tag tagname.
When an event is delivered to an item, the
callbacks for each of the item’s tags option are called.
If kw is not given, returns a dict of the option settings for tagname.
If option is specified, returns the value for
that option for the specified tagname.
Otherwise, sets the options to the corresponding values for the given
tagname.
If item is specified, returns 1 or 0 depending on whether the specified item has the given tagname.
Tk 8.6
25.8.
By default the style name is the same as the widget’s class
name, but it may be overridden by the widget’s style option.
If you don’t know the class name of a widget, use the method
Misc.winfo_class() (somewidget.winfo_class()).
See also:
Tcl’2004 conference presentation This document explains how the theme engine works
class tkinter.ttk.
Style
This class is used to manipulate the style database.
Each key in kw is an option and each value is a string identifying the value for that option.
For example, to change every default button to be a flat button with some padding and a different background
color:
from tkinter import ttk
import tkinter
root = tkinter.
Tk()
ttk.Style().configure("TButton", padding=6, relief="flat",
background="#ccc")
btn = ttk.Button(text="Sample")
btn.pack()
root.mainloop()
map(style, query_opt=None, **kw)
Query or sets dynamic values of the specified option(s) in style.
Each key in kw is an option and each value should be a list or a tuple (usually) containing statespecs grouped
in tuples, lists, or some other preference.
A statespec is a compound of one or more states and then a value.
An example may make it more understandable:
import tkinter
from tkinter import ttk
root = tkinter.
Tk()
style = ttk.
If state is specified, it is expected to be a sequence of one or more states.
If the default argument is set, it is
used as a fallback value in case no specification for option is found.
If layoutspec is omitted, return the layout specification for given style.
To understand the format, see the following example (it is not intended to do anything useful):
from tkinter import ttk
import tkinter
root = tkinter.
Tk()
style = ttk.
The latter is only available in Tk 8.6a for Windows XP and Vista and is not described here.
If “image” is used, args should contain the default image name followed by statespec/value pairs (this is the
imagespec), and kw may have the following options:
25.8.
If less than zero, the base image’s height is used
as a default.
Defaults to border’s value if not specified.
If less than zero, the base image’s width is used
as a default.
If “from” is used as the value of etype, element_create() will clone an existing element.
If
this element to clone from is not specified, an empty element will be used.
It is an error if themename already exists.
If parent is specified, the new theme will inherit styles, elements
and layouts from the parent theme.
If settings are present they are expected to have the same syntax used for
theme_settings().
Temporarily sets the current theme to themename, apply specified settings and then restore the previous theme.
Each key in settings is a style and each value may contain the keys ‘configure’, ‘map’, ‘layout’ and ‘element
create’ and they are expected to have the same format as specified by the methods Style.configure(),
Style.map(), Style.layout() and Style.element_create() respectively.
As an example, let’s change the Combobox for the default theme a bit:
from tkinter import ttk
import tkinter
root = tkinter.
Tk()
style = ttk.
Style()
style.theme_settings("default", {
"TCombobox": {
"configure": {"padding": 5},
"map": {
"background": [("active", "green2"),
("!disabled", "green4")],
"fieldbackground": [("!disabled", "green3")],
"foreground": [("focus", "OliveDrab1"),
("!disabled", "OliveDrab2")]
}
}
})
combo = ttk.
If themename is not given, returns the theme in use.
Otherwise, sets the current theme to themename, refreshes
all widgets and emits a «ThemeChanged» event.
Layouts
A layout can be just None, if it takes no options, or a dict of options specifying how to arrange the element.
The layout
mechanism uses a simplified version of the pack geometry manager: given an initial cavity, each element is allocated a
parcel.
Valid options/values are:
side: whichside Specifies which side of the cavity to place the element; one of top, right, bottom or left.
If omitted, the
element occupies the entire cavity.
Each element is a tuple (or other
sequence type) where the first item is the layout name, and the other is a Layout.
This Tk extension is unmaintained and should not be used in new code.
Use tkinter.
The tkinter.tix (Tk Interface Extension) module provides an additional rich set of widgets.
Although the standard
Tk library has many useful widgets, they are far from complete.
The tkinter.tix library provides most of the
commonly needed widgets that are missing from standard Tk: HList, ComboBox, Control (a.k.a. SpinBox) and an
assortment of scrollable widgets.
With all these new widgets, you can introduce new interaction techniques into applications, creating more useful and more
intuitive user interfaces.
You can design your application by choosing the most appropriate widgets to match the special
needs of your application and users.
Tix Homepage The home page for Tix.
This includes links to additional documentation and downloads.
Tix Man Pages On-line version of the man pages and reference material.
Tix Programming Guide On-line version of the programmer’s reference material.
Tix Development Applications Tix applications for development of Tix and Tkinter programs.
Tide applications work
under Tk or Tkinter, and include TixInspect, an inspector to remotely modify and debug Tix/Tk/Tkinter ap-
plications.
Toplevel widget of Tix which represents mostly the main window of an application.
It has an associated Tcl inter-
preter.
Classes in the tkinter.tix module subclasses the classes in the tkinter.
The former imports the latter, so
to use tkinter.tix with Tkinter, all you need to do is to import one module.
In general, you can just import
tkinter.tix, and replace the toplevel call to tkinter.
Tk with tix.
Tk:
from tkinter import tix
from tkinter.constants import *
root = tix.
Tk()
To use tkinter.tix, you must have the Tix widgets installed, usually alongside your installation of the Tk widgets.
To test your installation, try the following:
from tkinter import tix
root = tix.
Tix Widgets
Tix introduces over 40 widget classes to the tkinter repertoire.
Basic Widgets
class tkinter.tix.
Balloon
A Balloon that pops up over a widget to provide help.
When the user moves the cursor inside a widget to which a
Balloon widget has been bound, a small pop-up window with a descriptive message will be shown on the screen.
ButtonBox
The ButtonBox widget creates a box of buttons, such as is commonly used for Ok Cancel.
ComboBox
The ComboBox widget is similar to the combo box control in MS Windows.
The user can select a choice by either
typing in the entry subwidget or selecting from the listbox subwidget.
Control
The Control widget is also known as the SpinBox widget.
The user can adjust the value by pressing the two arrow
buttons or by entering the value directly into the entry.
The new value will be checked against the user-defined
upper and lower limits.
LabelEntry
The LabelEntry widget packages an entry widget and a label into one mega widget.
It can be used to simplify the
creation of “entry-form” type of interface.
LabelFrame
The LabelFrame widget packages a frame widget and a label into one mega widget.
To create widgets inside a
LabelFrame widget, one creates the new widgets relative to the frame subwidget and manage them inside the
frame subwidget.
OptionMenu
The OptionMenu creates a menu button of options.
PopupMenu
The PopupMenu widget can be used as a replacement of the tk_popup command.
The advantage of the Tix
PopupMenu widget is it requires less application code to manipulate.
Select
The Select widget is a container of button subwidgets.
It can be used to provide radio-box or check-box style of
selection options for the user.
The StdButtonBox widget is a group of standard buttons for Motif-like dialog boxes.
File Selectors
class tkinter.tix.
DirList
The DirList widget displays a list view of a directory, its previous directories and its sub-directories.
The user can
choose one of the directories displayed in the list or change to another directory.
DirTree
The DirTree widget displays a tree view of a directory, its previous directories and its sub-directories.
The user can
choose one of the directories displayed in the list or change to another directory.
The user can use this
dialog window to navigate through the file system to select the desired directory.
The DirSelectBox is similar to the standard Motif(TM) directory-selection box.
It is generally used for the
user to choose a directory.
DirSelectBox stores the directories mostly recently selected into a ComboBox widget
so that they can be quickly selected again.
ExFileSelectBox
The ExFileSelectBox widget is usually embedded in a tixExFileSelectDialog widget.
It provides a convenient
method for the user to select files.
The style of the ExFileSelectBox widget is very similar to the standard
file dialog on MS Windows 3.1.
FileSelectBox
The FileSelectBox is similar to the standard Motif(TM) file-selection box.
It is generally used for the user to choose
a file.
FileSelectBox stores the files mostly recently selected into a ComboBox widget so that they can be quickly
selected again.
FileEntry
The FileEntry widget can be used to input a filename.
The user can type in the filename manually.
Alternatively,
the user can press the button widget that sits next to the entry, which will bring up a file selection dialog.
25.9.
HList
The HList widget can be used to display any data that have a hierarchical structure, for example, file system directory
trees.
The list entries are indented and connected by branch lines according to their places in the hierarchy.
CheckList
The CheckList widget displays a list of items to be selected by the user.
CheckList acts similarly to the Tk check-
button or radiobutton widgets, except it is capable of handling many more items than checkbuttons or radiobuttons.
Tree
The Tree widget can be used to display hierarchical data in a tree form.
The user can adjust the view of the tree by
opening or closing parts of the tree.
Tabular ListBox
class tkinter.tix.
The TList widget can be used to display data in a tabular format.
The list entries of a TList widget are similar
to the entries in the Tk listbox widget.
The main differences are (1) the TList widget can display the list entries
in a two dimensional format and (2) you can use graphical images as well as multiple colors and fonts for the list
entries.
Manager Widgets
class tkinter.tix.
PanedWindow
The PanedWindow widget allows the user to interactively manipulate the sizes of several panes.
The panes can be
arranged either vertically or horizontally.
The user changes the sizes of the panes by dragging the resize handle
between two panes.
The ListNoteBook widget is very similar to the TixNoteBook widget: it can be used to display many windows
in a limited space using a notebook metaphor.
The notebook is divided into a stack of pages (windows).
At one
time only one of these pages can be shown.
The user can navigate through these pages by choosing the name of
the desired page in the hlist subwidget.
NoteBook
The NoteBook widget can be used to display many windows in a limited space using a notebook metaphor.
The
notebook is divided into a stack of pages.
At one time only one of these pages can be shown.
The user can navigate
through these pages by choosing the visual “tabs” at the top of the NoteBook widget.
Image Types
The tkinter.tix module adds:
• pixmap capabilities to all tkinter.tix and tkinter widgets to create color images from XPM files.
For example, a compound image
can be used to display a bitmap and a text string simultaneously in a Tk Button widget.
InputOnly
The InputOnly widgets are to accept inputs from the user, which can be done with the bind command (Unix only).
Form Geometry Manager
In addition, tkinter.tix augments tkinter by providing:
class tkinter.tix.
Tix Commands
class tkinter.tix.tixCommand
The tix commands provide access to miscellaneous elements of Tix’s internal state and the Tix application context.
Most of the information manipulated by these methods pertains to the application as a whole, or to a screen or
display, rather than to a particular window.
To view the current settings, the common usage is:
from tkinter import tix
root = tix.
If no option is specified, returns a
dictionary all of the available options.
If option is specified with no value, then the method returns a list describing
the one named option (this list will be identical to the corresponding sublist of the value returned if no option is
specified).
If one or more option-value pairs are specified, then the method modifies the given option(s) to have the
given value(s); in this case the method returns an empty string.
Option may be any of the configuration options.
Returns the current value of the configuration option given by option.
Option may be any of the configuration
options.
By using tix_getbitmap(), you can avoid hard coding the pathnames
of the bitmap files in your application.
When successful, it returns the complete pathname of the bitmap file, pre-
fixed with the character @.
The returned value can be used to configure the bitmap option of the Tk and Tix
widgets.
Tix maintains a list of directories under which the tix_getimage() and tix_getbitmap() meth-
ods will search for image files.
The standard bitmap directory is $TIX_LIBRARY/bitmaps.
The
tix_addbitmapdir() method adds directory into this list.
By using this method, the image files of an appli-
cations can also be located using the tix_getimage() or tix_getbitmap() method.
This method
25.9.
This dialog will be returned by all sub-
sequent calls to tix_filedialog().
An optional dlgclass parameter can be passed as a string to speci-
fied what type of file selection dialog widget is desired.
Possible options are tix, FileSelectDialog or
tixExFileSelectDialog.
tixCommand.tix_getimage(self, name)
Locates an image file of the name name.xpm, name.xbm or name.ppm in one of the bitmap directories (see
the tix_addbitmapdir() method above).
If more than one file with the same name (but different extensions)
exist, then the image type is chosen according to the depth of the X display: xbm images are chosen on monochrome
displays and color images are chosen on color displays.
By using tix_getimage(), you can avoid hard coding
the pathnames of the image files in your application.
Gets the options maintained by the Tix scheme mechanism.
This affects only
those widgets created after this call.
Therefore, it is best to call the resetoptions method before the creation of any
widgets in a Tix application.
The optional parameter newScmPrio can be given to reset the priority level of the Tk options set by the Tix schemes.
Because of the way Tk handles the X option database, after Tix has been has imported and inited, it is not possible to
reset the color schemes and font sets using the tix_config() method.
Instead, the tix_resetoptions()
method must be used.
25.10 IDLE
Source code: Lib/idlelib/
IDLE is Python’s Integrated Development and Learning Environment.
It is possible to have multiple editor
windows simultaneously.
On Windows and Linux, each has its own top menu.
Each menu documented below indicates
which window type it is associated with.
Output windows, such as used for Edit => Find in Files, are a subtype of editor window.
They currently have the same
top menu but a different default title and context menu.
On macOS, there is one application menu.
It dynamically changes according to the window currently selected.
It has an
IDLE menu, and some entries described below are moved around to conform to Apple guidelines.
File menu (Shell and Editor)
New File Create a new file editing window.
Open… Open an existing file with an Open dialog.
Open Module… Open an existing module (searches sys.path).
Recent Files Open a list of recent files.
Click one to open it.
Module Browser Show functions, classes, and methods in the current Editor file in a tree structure.
In the shell, open a
module first.
Path Browser Show sys.path directories, modules, functions, classes and methods in a tree structure.
Save Save the current window to the associated file, if there is one.
Windows that have been changed since being opened
or last saved have a * before and after the window title.
If there is no associated file, do Save As instead.
Save As… Save the current window with a Save As dialog.
The file saved becomes the new associated file for the window.
Save Copy As… Save the current window to different file without changing the associated file.
Print Window Print the current window to the default printer.
Close Window Close the current window (if an unsaved editor, ask to save; if an unsaved Shell, ask to quit execution).
Calling exit() or close() in the Shell window also closes Shell.
If this is the only window, also exit IDLE.
Exit IDLE Close all windows and quit IDLE (ask to save unsaved edit windows).
Edit menu (Shell and Editor)
Undo Undo the last change to the current window.
A maximum of 1000 changes may be undone.
Redo Redo the last undone change to the current window.
Select All Select the entire contents of the current window.
Cut Copy selection into the system-wide clipboard; then delete the selection.
Copy Copy selection into the system-wide clipboard.
Paste Insert contents of the system-wide clipboard into the current window.
The clipboard functions are also available in context menus.
Find… Open a search dialog with many options
25.10.
Find Selection Search for the currently selected string, if there is one.
Find in Files… Open a file search dialog.
Put results in a new output window.
Replace… Open a search-and-replace dialog.
Go to Line Move the cursor to the beginning of the line requested and make that line visible.
A request past the end of
the file goes to the end.
Clear any selection and update the line and column status.
Show Completions Open a scrollable list allowing selection of existing names.
See Completions in the Editing and
navigation section below.
Expand Word Expand a prefix you have typed to match a full word in the same window; repeat to get a different
expansion.
After an unclosed parenthesis for a function, open a small window with function parameter hints.
See
Calltips in the Editing and navigation section below.
Show Surrounding Parens Highlight the surrounding parenthesis.
Format menu (Editor window only)
Format Paragraph Reformat the current blank-line-delimited paragraph in comment block or multiline string or se-
lected line in a string.
All lines in the paragraph will be formatted to less than N columns, where N defaults to
72.
Indent Region Shift selected lines right by the indent width (default 4 spaces).
Dedent Region Shift selected lines left by the indent width (default 4 spaces).
Comment Out Region Insert ## in front of selected lines.
Uncomment Region Remove leading # or ## from selected lines.
Tabify Region Turn leading stretches of spaces into tabs.
Untabify Region Turn all tabs into the correct number of spaces.
Toggle Tabs Open a dialog to switch between indenting with spaces and tabs.
New Indent Width Open a dialog to change indent width.
The accepted default by the Python community is 4 spaces.
Strip Trailing Chitespace Remove trailing space and other whitespace characters after the last non-whitespace charac-
ter of a line by applying str.rstrip to each line, including lines within multiline strings.
Except for Shell windows,
remove extra newlines at the end of the file.
Run menu (Editor window only)
Run Module Do Check Module.
If no error, restart the shell to clean the environment, then execute the module.
Output
is displayed in the Shell window.
Note that output requires use of print or write.
When execution is complete,
the Shell retains focus and displays a prompt.
At this point, one may interactively explore the result of execution.
This is similar to executing a file with python -i file at a command line.
Run… Customized Same as Run Module, but run the module with customized settings.
Command Line Arguments
extend sys.argv as if passed on a command line.
The module can be run in the Shell without restarting.
Check Module Check the syntax of the module currently open in the Editor window.
If the module has not been saved
IDLE will either prompt the user to save or autosave, as selected in the General tab of the Idle Settings dialog.
Shell menu (Shell window only)
View Last Restart Scroll the shell window to the last Shell restart.
Restart Shell Restart the shell to clean the environment and reset display and exception handling.
Previous History Cycle through earlier commands in history which match the current entry.
Next History Cycle through later commands in history which match the current entry.
Interrupt Execution Stop a running program.
Debug menu (Shell window only)
Go to File/Line Look on the current line.
If found,
open the file if not already open, and show the line.
Use this to view source lines referenced in an exception
traceback and lines found by Find in Files.
Also available in the context menu of the Shell window and Output
windows.
Debugger (toggle) When activated, code entered in the Shell or run from an Editor will run under the debugger.
In the
Editor, breakpoints can be set with the context menu.
This feature is still incomplete and somewhat experimental.
Stack Viewer Show the stack traceback of the last exception in a tree widget, with access to locals and globals.
Auto-open Stack Viewer Toggle automatically opening the stack viewer on an unhandled exception.
Options menu (Shell and Editor)
Configure IDLE Open a configuration dialog and change preferences for the following: fonts, indentation, keybindings,
text color themes, startup windows and size, additional help sources, and extensions.
On macOS, open the con-
figuration dialog by selecting Preferences in the application menu.
For more details, see Setting preferences under
Help and preferences.
Most configuration options apply to all windows or all future windows.
The option items below only apply to the active
window.
Show/Hide Code Context (Editor Window only) Open a pane at the top of the edit window which shows the block
context of the code which has scrolled above the top of the window.
See Code Context in the Editing and Navigation
section below.
Open a column to the left of the edit window which shows the num-
ber of each line of text.
The default is off, which may be changed in the preferences (see Setting preferences).
Zoom/Restore Height Toggles the window between normal size and maximum height.
The initial size defaults to 40
lines by 80 chars unless changed on the General tab of the Configure IDLE dialog.
The maximum height for a
screen is determined by momentarily maximizing a window the first time one is zoomed on the screen.
Changing
screen settings may invalidate the saved height.
This toggle has no effect when a window is maximized.
25.10.
Help menu (Shell and Editor)
About IDLE Display version, copyright, license, credits, and more.
IDLE Help Display this IDLE document, detailing the menu options, basic editing and navigation, and other tips.
Python Docs Access local Python documentation, if installed, or start a web browser and open docs.python.org showing
the latest Python documentation.
Turtle Demo Run the turtledemo module with example Python code and turtle drawings.
Additional help sources may be added here with the Configure IDLE dialog under the General tab.
See the Help sources
subsection below for more on Help menu choices.
Context menus
Open a context menu by right-clicking in a window (Control-click on macOS).
Context menus have the standard clipboard
functions also on the Edit menu.
Cut Copy selection into the system-wide clipboard; then delete the selection.
Copy Copy selection into the system-wide clipboard.
Paste Insert contents of the system-wide clipboard into the current window.
Editor windows also have breakpoint functions.
Lines with a breakpoint set are specially marked.
Breakpoints only have
an effect when running under the debugger.
Breakpoints for a file are saved in the user’s .idlerc directory.
Set Breakpoint Set a breakpoint on the current line.
Clear Breakpoint Clear the breakpoint on that line.
Shell and Output windows also have the following.
Go to file/line Same as in Debug menu.
The Shell window also has an output squeezing facility explained in the Python Shell window subsection below.
Squeeze If the cursor is over an output line, squeeze all the output between the code above and the prompt below down
to a ‘Squeezed text’ label.
Thereafter, use the File
menu.
There can be only one open editor window for a given file.
The title bar contains the name of the file, the full path, and the version of Python and IDLE running the window.
The
status bar contains the line number (‘Ln’) and column number (‘Col’).
Keybindings are selected in the Configure IDLE
dialog.
Automatic indentation
After a block-opening statement, the next line is indented by 4 spaces (in the Python Shell window by one tab).
In leading indentation, Backspace deletes up to 4 spaces
if they are there.
Tab inserts spaces (in the Python Shell window one tab), number depends on Indent width.
Currently,
tabs are restricted to four spaces due to Tcl/Tk limitations.
See also the indent/dedent region commands on the Format menu.
Search and Replace
Any selection becomes a search target.
However, only selections within a line work because searches are only performed
within lines with the terminal newline removed.
If [x] Regular expression is checked, the target is interpreted
according to the Python re module.
25.10.
Each request method displays a completion box with existing names.
For
any box, change the name being completed and the item highlighted in the box by typing and deleting characters; by
hitting Up, Down, PageUp, PageDown, Home, and End keys; and by a single click within the box.
Close the box with
Escape, Enter, and double Tab keys or clicks outside the box.
A double click within the box selects and closes.
One way to open a box is to type a key character and wait for a predefined interval.
This defaults to 2 seconds; customize
it in the settings dialog.
For imported module names or class or function attributes, type ‘.’.
For filenames in the root directory, type os.sep or
os.altsep immediately after an opening quote.
Move into subdirectories
by typing a directory name and a separator.
Instead of waiting, or after a box is closed, open a completion box immediately with Show Completions on the Edit menu.
The default hot key is C-space.
If one types a prefix for the desired name before opening the box, the first match or
near miss is made visible.
The result is the same as if one enters a prefix after the box is displayed.
Show Completions
after a quote completes filenames in the current directory instead of a root directory.
Hitting Tab after a prefix usually has the same effect as Show Completions.
However, if
there is only one match to the prefix, that match is immediately added to the editor text without opening a box.
When editing code in an editor (as oppose to Shell), increase the available module-level names by running your code and
not restarting the Shell thereafter.
This is especially useful after adding imports at the top of a file.
This also increases
possible attribute completions.
Completion boxes initially exclude names beginning with ‘_’ or, for modules, not included in ‘__all__’.
The hidden names
can be accessed by typing ‘_’ after ‘.’, either before or after the box is opened.
Calltips
A calltip is shown automatically when one types ( after the name of an accessible function.
A function name expression
may include dots and subscripts.
A calltip remains until it is clicked, the cursor is moved out of the argument area, or ) is
typed.
Whenever the cursor is in the argument part of a definition, select Edit and “Show Call Tip” on the menu or enter
its shortcut to display a calltip.
The calltip consists of the function’s signature and docstring up to the latter’s first blank line or the fifth non-blank line.
A ‘/’ or ‘*’ in the signature indicates that the preceding or following
arguments are passed by position or name (keyword) only.
Details are subject to change.
In Shell, the accessible functions depends on what modules have been imported into the user process, including those
imported by Idle itself, and which definitions have been run, all since the last restart.
For example, restart the Shell and enter itertools.count(.
A calltip appears because Idle imports itertools into
the user process for its own use.
Enter turtle.write( and nothing appears.
Idle does not itself
import turtle.
The menu entry and shortcut also do nothing.
Enter import turtle.
Thereafter, turtle.write(
will display a calltip.
In an editor, import statements have no effect until one runs the file.
One might want to run a file after writing import
statements, after adding function definitions, or after opening an existing file.
When shown, this pane freezes the opening lines for block code, such as those beginning with class, def,
or if keywords, that would have otherwise scrolled out of view.
The size of the pane will be expanded and contracted
as needed to show the all current levels of context, up to the maximum number of lines defined in the Configure IDLE
dialog (which defaults to 15).
If there are no current context lines and the feature is toggled on, a single blank line will
display.
Clicking on a line in the context pane will move that line to the top of the editor.
The text and background colors for the context pane can be configured under the Highlights tab in the Configure IDLE
dialog.
Shell window
In IDLE’s Shell, enter, edit, and recall complete statements.
Submit a single-line statement for execution by hitting Return with the cursor anywhere on the line.
If a line is extended
with Backslash (\), the cursor must be on the last physical line.
Submit a multi-line compound statement by entering a
blank line after the statement.
When one pastes code into Shell, it is not compiled and possibly executed until one hits Return, as specified above.
One
may edit pasted code first.
If one pastes more than one statement into Shell, the result will be a SyntaxError when
multiple statements are compiled as if they were one.
Lines containing RESTART mean that the user execution process has been re-started.
This occurs when the user execution
process has crashed, when one requests a restart on the Shell menu, or when one runs code in an editor window.
The editing features described in previous subsections work when entering code interactively.
IDLE’s Shell window also
responds to the following keys.
On macOS use C-p.
– Alt-n retrieves next.
On macOS use C-n.
– Return while the cursor is on any previous command retrieves that command
Text colors
Idle defaults to black on white text, but colors text with special meanings.
For the shell, these are shell output, shell error,
user output, and user error.
For Python code, at the shell prompt or in an editor, these are keywords, builtin class and
function names, names following class and def, strings, and comments.
For any text window, these are the cursor
(when present), found text (when possible), and selected text.
IDLE also highlights the soft keywords match, case, and _ in pattern-matching statements.
Text coloring is done in the background, so uncolorized text is occasionally visible.
To change the color scheme, use the
Configure IDLE dialog Highlighting tab.
The marking of debugger breakpoint lines in the editor and text in popups and
dialogs is not user-configurable.
25.10.
IDLE first checks for IDLESTARTUP; if IDLESTARTUP is present the file referenced is run.
If
IDLESTARTUP is not present, IDLE checks for PYTHONSTARTUP.
Files referenced by these environment variables
are convenient places to store functions that are used frequently from the IDLE shell, or for executing import statements
to import common modules.
In addition, Tk also loads a startup file if it is present.
Note that the Tk file is loaded unconditionally.
This additional file
is .Idle.py and is looked for in the user’s home directory.
Statements in this file will be executed in the Tk namespace,
so this file is not useful for importing functions to be used from IDLE’s Python shell.
If there are arguments:
• If -, -c, or r is used, all arguments are placed in sys.argv[1:...] and sys.argv[0] is set to '', '-c',
or '-r'.
No editor window is opened, even if that is the default set in the Options dialog.
Startup failure
IDLE uses a socket to communicate between the IDLE GUI process and the user code execution process.
A connection
must be established whenever the Shell starts or restarts.
If the user process fails to connect to the GUI process, it usually displays a Tk error box with a ‘cannot connect’ message
that directs the user here.
It then exits.
One specific connection failure on Unix systems results from misconfigured masquerading rules somewhere in a system’s
network setup.
When IDLE is started from a terminal, one will see a message starting with ** Invalid host:.
The
valid value is 127.0.0.1 (idlelib.rpc.LOCALHOST).
A common cause of failure is a user-written file with the same name as a standard library module, such as random.py and
tkinter.py.
When such a file is located in the same directory as a file that is about to be run, IDLE cannot import the stdlib
file.
The current fix is to rename the user file.
Though less common than in the past, an antivirus or firewall program may stop the connection.
If the program cannot
be taught to allow the connection, then it must be turned off for IDLE to work.
It is safe to allow this internal connection
because no data is visible on external ports.
A similar problem is a network mis-configuration that blocks connections.
Python installation issues occasionally stop IDLE: multiple versions can clash, or a single installation might need admin
access.
On Windows, use Task Manager to check for one and stop it if there
is.
Sometimes a restart initiated by a program crash or Keyboard Interrupt (control-C) may fail to connect.
Dismissing
the error box or using Restart Shell on the Shell menu may fix a temporary problem.
When IDLE first starts, it attempts to read user configuration files in ~/.idlerc/ (~ is one’s home directory).
If there
is a problem, an error message should be displayed.
Leaving aside random disk glitches, this can be prevented by never
editing the files by hand.
Instead, use the configuration dialog, under Options.
Once there is an error in a user configuration
file, the best solution may be to delete it and start over with the settings dialog.
If IDLE quits with no message, and it was not started from a console, try starting it from a console or terminal (python
-m idlelib) and see if this results in an error message.
On Unix-based systems with tcl/tk older than 8.6.11 (see About IDLE) certain characters of certain fonts can cause
a tk failure with a message to the terminal.
This can happen either if one starts IDLE to edit a file with such a character or
later when entering such a character.
If one cannot upgrade tcl/tk, then re-configure IDLE to use a font that works better.
Running user code
With rare exceptions, the result of executing Python code with IDLE is intended to be the same as executing the same code
by the default method, directly with Python in a text-mode system console or terminal window.
However, the different
interface and operation occasionally affect visible results.
For instance, sys.modules starts with more entries, and
threading.active_count() returns 2 instead of 1.
By default, IDLE runs user code in a separate OS process rather than in the user interface process that runs the shell and
editor.
In the execution process, it replaces sys.stdin, sys.stdout, and sys.stderr with objects that get input
from and send output to the Shell window.
The original values stored in sys.__stdin__, sys.__stdout__, and
sys.__stderr__ are not touched, but may be None.
Sending print output from one process to a text widget in another is slower than printing to a system terminal in the same
process.
This has the most effect when printing multiple arguments, as the string for each argument, each separator, the
newline are sent separately.
For development, this is usually not a problem, but if one wants to print faster in IDLE,
format and join together everything one wants displayed together and then print a single string.
Both format strings and
str.join() can help combine fields and lines.
IDLE’s standard stream replacements are not inherited by subprocesses created in the execution process, whether directly
by user code or by modules such as multiprocessing.
If such subprocess use input from sys.stdin or print or write
to sys.stdout or sys.stderr, IDLE should be started in a command line window.
The secondary subprocess will then be attached to that window for input and output.
If sys is reset by user code, such as with importlib.reload(sys), IDLE’s changes are lost and input from the
keyboard and output to the screen will not work correctly.
When Shell has the focus, it controls the keyboard and screen.
This is normally transparent, but functions that directly
access the keyboard and screen will not work.
These include system-specific functions that determine whether a key has
been pressed and if so, which.
The IDLE code running in the execution process adds frames to the call stack that would not be there otherwise.
IDLE
wraps sys.getrecursionlimit and sys.setrecursionlimit to reduce the effect of the additional stack
frames.
When user code raises SystemExit either directly or by calling sys.exit, IDLE returns to a Shell prompt instead of exiting.
When IDLE executes user
code, sys.stdout and sys.stderr are connected to the display area of IDLE’s Shell.
Some of its features are
inherited from the underlying Tk Text widget.
Others are programmed additions.
Where it matters, Shell is designed for
development rather than production runs.
For instance, Shell never throws away output.
A program that sends unlimited output to Shell will eventually fill memory,
resulting in a memory error.
In contrast, some system text windows only keep the last n lines of output.
A Windows
console, for instance, keeps a user-settable 1 to 9999 lines, with 300 the default.
A Tk Text widget, and hence IDLE’s Shell, displays characters (codepoints) in the BMP (Basic Multilingual Plane)
subset of Unicode.
Which characters are displayed with a proper glyph and which with a replacement box depends on
the operating system and installed fonts.
Tab characters cause the following text to begin after the next tab stop.
Newline characters cause following text to appear on a new line.
Other control characters are
ignored or displayed as a space, box, or something else, depending on the operating system and font.
Try it.
The repr function is used for interactive echo of expression values.
It returns an altered version of the input string in
which control codes, some BMP codepoints, and all non-BMP codepoints are replaced with escape codes.
As demon-
strated above, it allows one to identify the characters in a string, regardless of how they are displayed.
Normal and error output are generally kept separate (on separate lines) from code input and each other.
They each get
different highlight colors.
For SyntaxError tracebacks, the normal ‘^’ marking where the error was detected is replaced by coloring the text with an
error highlight.
When code run from a file causes other exceptions, one may right click on a traceback line to jump to the
corresponding line in an IDLE editor.
The file will be opened if necessary.
Shell has a special facility for squeezing output lines down to a ‘Squeezed text’ label.
This is done automatically for output
over N lines (N = 50 by default).
N can be changed in the PyShell section of the General page of the Settings dialog.
Output with fewer lines can be squeezed by right clicking on the output.
This can be useful lines long enough to slow
down scrolling.
Squeezed output is expanded in place by double-clicking the label.
It can also be sent to the clipboard or a separate view
window by right-clicking the label.
Developing tkinter applications
IDLE is intentionally different from standard Python in order to facilitate development of tkinter programs.
Enter
import tkinter as tk; root = tk.
Tk() in standard Python and nothing appears.
Enter the same in IDLE
and a tk window appears.
In standard Python, one must also enter root.update() to see the window.
IDLE does
the equivalent in the background, about 20 times a second, which is about every 50 milliseconds.
Next enter b = tk.
Button(root, text='button'); b.pack().
Again, nothing visibly changes in standard Python until one
enters root.update().
Most tkinter programs run root.mainloop(), which usually does not return until the tk app is destroyed.
When running a tkinter program from an IDLE editor, one can comment out the mainloop call.
One then gets a shell
prompt immediately and can interact with the live application.
One just has to remember to re-enable the mainloop call
when running in standard Python.
Running without a subprocess
By default, IDLE executes user code in a separate subprocess via a socket, which uses the internal loopback interface.
This connection is not externally visible and no data is sent to or received from the internet.
If firewall software complains
anyway, you can ignore it.
If the attempt to make the socket connection fails, Idle will notify you.
Such failures are sometimes transient, but if
persistent, the problem may be either a firewall blocking the connection or misconfiguration of a particular system.
Until
the problem is fixed, one can run Idle with the -n command line switch.
If IDLE is started with the -n command line switch it will run in a single process and will not create the subprocess which
runs the RPC Python execution server.
This can be useful if Python cannot create the subprocess or the RPC socket
interface on your platform.
However, in this mode user code is not isolated from IDLE itself.
Also, the environment
is not restarted when Run/Run Module (F5) is selected.
If your code has been modified, you must reload() the affected
modules and re-import any specific items (e.g. from foo import baz) if the changes are to take effect.
For these reasons,
it is preferable to run IDLE with the default subprocess if at all possible.
Deprecated since version 3.4.
The
result, in a read-only tkinter text window, is close to what one sees in a web browser.
Navigate through the text with a
mousewheel, the scrollbar, or up and down arrow keys held down.
Or click the TOC (Table of Contents) button and select
a section header in the opened box.
Help menu entry “Python Docs” opens the extensive sources of help, including tutorials, available at docs.python.
If your system has an off-line copy of the docs (this may
be an installation option), that will be opened instead.
Selected URLs can be added or removed from the help menu at any time using the General tab of the Configure IDLE
dialog.
Setting preferences
The font preferences, highlighting, keys, and general preferences can be changed via Configure IDLE on the Option menu.
Non-default user settings are saved in a .idlerc directory in the user’s home directory.
Problems caused by bad user
configuration files are solved by editing or deleting one or more of the files in .idlerc.
On the Font tab, see the text sample for the effect of font face and size on multiple characters in multiple languages.
Edit the sample to add other characters of personal interest.
Use the sample to select monospaced fonts.
If particular
characters have problems in Shell or an editor, add them to the top of the sample and try changing first size and then font.
On the Highlights and Keys tab, select a built-in or custom color theme and key set.
To use a newer built-in color theme
or key set with older IDLEs, save it as a new custom theme or key set and it well be accessible to older IDLEs.
25.10.
This setting is not
compatible with the tk/tkinter GUI framework used by IDLE, and it breaks a few IDLE features.
Extensions
IDLE contains an extension facility.
Preferences for extensions can be changed with the Extensions tab of the preferences
dialog.
See the beginning of config-extensions.def in the idlelib directory for further information.
The only current default
extension is zzdummy, an example also used for testing.
See the rest of this page for how to use IDLE.
The files in idlelib are described in idlelib/README.txt.
Access it either in idlelib or click Help => About IDLE on the
IDLE menu.
This file also maps IDLE menu items to the code that implements the item.
Except for files listed under
‘Startup’, the idlelib code is ‘private’ in sense that feature changes can be backported (see PEP 434).
The modules described in this chapter help you write software.
For example, the pydoc module takes a module and
generates documentation based on the module’s contents.
The doctest and unittest modules contains frameworks
for writing unit tests that automatically exercise code and verify that the expected output is produced.
The list of modules described in this chapter is:
26.1 typing — Support for type hints
New in version 3.5.
The Python runtime does not enforce function and variable type annotations.
They can be used by third party
tools such as type checkers, IDEs, linters, etc.
This module provides runtime support for type hints.
For the original specification of the typing system, see PEP 484.
For a simplified introduction to type hints, see PEP 483.
The function below takes and returns a string and is annotated as follows:
def greeting(name: str) -> str:

In the function greeting, the argument name is expected to be of type str and the return type str.
Subtypes are
accepted as arguments.
New features are frequently added to the typing module.
The typing_extensions package provides backports of these
new features to older versions of Python.
For a summary of deprecated features and a deprecation timeline, please see Deprecation Timeline of Major Features.
The Python typing system is standardised via PEPs, so this ref-
erence should broadly apply to most Python type checkers.
Structural subtyping (static duck typing) Introducing
Protocol
and
the
@runtime_checkable decorator
• PEP 585: Type Hinting Generics In Standard Collections Introducing types.
GenericAlias and the
ability to use standard library classes as generic types
• PEP 586: Literal Types Introducing Literal
• PEP 589: TypedDict: Type Hints for Dictionaries with a Fixed Set of Keys Introducing TypedDict
• PEP 591: Adding a final qualifier to typing Introducing Final and the @final decorator
• PEP 593: Flexible function and variable annotations Introducing Annotated
• PEP 604: Allow writing union types as X | Y Introducing types.
Type Parameter Syntax Introducing builtin syntax for creating generic functions, classes, and type
aliases.
In this example,
Vector and list[float] will be treated equivalently by static type checkers:
type Vector = list[float]
def scale(scalar: float, vector: Vector) -> Vector:

# passes type checking; a list of floats qualifies as a Vector.
The type statement is new in Python 3.12.
The static type checker will treat the new type as if it were a subclass of the original type.
You may still perform all int operations on a variable of type UserId, but the result will always be of type int.
This lets you pass in a UserId wherever an int might be expected, but will prevent you from accidentally creating a
UserId in an invalid way:
# 'output' is of type 'int', not 'UserId'
output = UserId(23413) + UserId(54341)
26.1.
That means the expression Derived(some_value) does not create a new class or introduce much overhead
beyond that of a regular function call.
More precisely, the expression some_value is Derived(some_value) is always true at runtime.
It is invalid to create a subtype of Derived:
from typing import NewType
UserId = NewType('UserId', int)
# Fails at runtime and does not pass type checking
class AdminUserId(UserId): pass
However, it is possible to create a NewType based on a ‘derived’ NewType:
from typing import NewType
UserId = NewType('UserId', int)
ProUserId = NewType('ProUserId', UserId)
and typechecking for ProUserId will work as expected.
See PEP 484 for more details.
Note: Recall that the use of a type alias declares two types to be equivalent to one another.
Doing type Alias =
Original will make the static type checker treat Alias as being exactly equivalent to Original in all cases.
This
is useful when you want to simplify complex type signatures.
In contrast, NewType declares one type to be a subtype of another.
Doing Derived = NewType('Derived',
Original) will make the static type checker treat Derived as a subclass of Original, which means a value of
type Original cannot be used in places where a value of type Derived is expected.
This is useful when you want to
prevent logic errors with minimal runtime cost.
New in version 3.5.2.
NewType is now a class rather than a function.
As a result, there is some additional runtime
cost when calling NewType over a regular function.
The performance of calling NewType has been restored to its level in Python 3.9.
Annotating callable objects
Functions – or other callable objects – can be annotated using collections.abc.
Callable or typing.
Callable. Callable[[int], str] signifies a function that takes a single parameter of type int and returns a
str.
The subscription syntax must always be used with exactly two values: the argument list and the return type.
The argument
list must be a list of types, a ParamSpec, Concatenate, or an ellipsis.
The return type must be a single type.
Argument 2 has incompatible type because of
# different name and kind in the callback
Callables which take other callables as arguments may indicate that their parameter types are dependent on each other
using ParamSpec.
Additionally, if that callable adds or removes arguments from other callables, the Concatenate
operator may be used.
They take the form Callable[ParamSpecVariable, ReturnType] and
Callable[Concatenate[Arg1Type, Arg2Type, ..., ParamSpecVariable], ReturnType]
respectively.
Callable now supports ParamSpec and Concatenate.
See PEP 612 for more details.
See also:
The documentation for ParamSpec and Concatenate provides examples of usage in Callable.
26.1.
Annotating tuples
For most containers in Python, the typing system assumes that all elements in the container will be of the same type.
For this reason, tuples are special-cased in Python’s typing system.
To denote an empty tuple, use tuple[()].
The type of class objects
A variable annotated with C may accept a value of type C.
In contrast, a variable annotated with type[C] (or typing.
Type[C]) may accept values that are classes themselves – specifically, it will accept the class object of C. For example:
a = 3
# Has type ``int``
b = int
# Has type ``type[int]``
c = type(a)
# Also has type ``type[int]``
Note that type[C] is covariant:
class User: ...
Error: ``type[int]`` is not a subtype of ``type[User]``
The only legal parameters for type are classes, Any, type variables, and unions of any of these types.
Also an error
type[Any] is equivalent to type, which is the root of Python’s metaclass hierarchy.
This syntax indicates that the class LoggedVar is parameterised around a single type variable T .
This also makes T
valid as a type within the class body.
Generic classes implicitly inherit from Generic.
A generic type can have any number of type variables.
Each type variable argument to Generic must be distinct.
In this case MyDict has a single parameter, T.
Using a generic class without specifying type parameters assumes Any for each position.
In the following example,
MyIterable is not generic but implicitly inherits from Iterable[Any]:
from collections.abc import Iterable
class MyIterable(Iterable): # Same as Iterable[Any]

26.1.
Generic no longer has a custom metaclass.
Changed in version 3.12: Syntactic support for generics and type aliases is new in version 3.12.
Previously, generic classes
had to explicitly inherit from Generic or contain a type variable in one of their bases.
The behavior is consistent with type variables’ described above as parameter specification variables are treated
by the typing module as a specialized type variable.
In this case, ** is
not used:
from typing import ParamSpec, Generic
P = ParamSpec('P')
class Z(Generic[P]):

Another difference between TypeVar and ParamSpec is that a generic with only one parameter specification variable
will accept parameter lists in the forms X[[Type1, Type2, ...
See ParamSpec and PEP
612 for more details.
A user-defined generic class can have ABCs as base classes without a metaclass conflict.
Generic metaclasses are not
supported.
The outcome of parameterizing generics is cached, and most types in the typing module are hashable and
comparable for equality.
The Any type
A special kind of type is Any.
A static type checker will treat every type as being compatible with Any and Any as being
compatible with every type.
Notice that no type checking is performed when assigning a value of type Any to a more precise type.
For example,
the static type checker did not report an error when assigning a to s even though s was declared to be of type str and
receives an int value at runtime!
This behavior allows Any to be used as an escape hatch when you need to mix dynamically and statically typed code.
Contrast the behavior of Any with the behavior of object.
Similar to Any, every type is a subtype of object.
However, unlike Any, the reverse is not true: object is not a subtype of every other type.
For example:
def hash_a(item: object) -> int:
# Fails type checking; an object does not have a 'magic' method.
Use object to indicate that a value could be any type in a typesafe manner.
Use Any to indicate that a value is
dynamically typed.
This means that a class A is allowed
where a class B is expected if and only if A is a subclass of B.
This requirement previously also applied to abstract base classes, such as Iterable.
The problem with this approach
is that a class had to be explicitly marked to support them, which is unpythonic and unlike what one would normally do
in idiomatic dynamically typed Python code.
For example, this conforms to PEP 484:
from collections.abc import Sized, Iterable, Iterator
class Bucket(Sized, Iterable[int]):

def __len__(self) -> int: ...
def __iter__(self) -> Iterator[int]: ...
PEP 544 allows to solve this problem by allowing users to write the above code without explicit base classes in the class
definition, allowing Bucket to be implicitly considered a subtype of both Sized and Iterable[int] by static type
checkers.
Special typing primitives
Special types
These can be used as types in annotations.
They do not support subscription using [].
typing.
Any
Special type indicating an unconstrained type.
Every type is compatible with Any.
Any is compatible with every type.
Changed in version 3.11: Any can now be used as a base class.
This can be useful for avoiding type checker errors
with classes that can duck type anywhere or are highly dynamic.
AnyStr
A constrained type variable.
Definition:
AnyStr = TypeVar('AnyStr', str, bytes)
AnyStr is meant to be used for functions that may accept str or bytes arguments but cannot allow the two to
mix.
LiteralString
Special type that includes only literal strings.
Any string literal is compatible with LiteralString, as is another LiteralString.
However, an object
typed as just str is not.
A string created by composing LiteralString-typed objects is also acceptable as a
LiteralString.
26.1.
LiteralString is useful for sensitive APIs where arbitrary user-generated strings could generate problems.
For example, the two cases above that generate type checker errors could be vulnerable to an SQL injection attack.
See PEP 675 for more details.
New in version 3.11.
Never
The bottom type, a type that has no members.
On older Python versions, NoReturn may be used to express the same concept.
Never
was added to make the intended meaning more explicit.
NoReturn
Special type indicating that a function never returns.
NoReturn can also be used as a bottom type, a type that has no values.
Starting in Python 3.11, the Never type
should be used for this concept instead.
Type checkers should treat the two equivalently.
New in version 3.5.4.
Self
Special type to represent the current enclosed class.
If
Foo.return_self was annotated as returning "Foo", then the type checker would infer the object returned
from SubclassOfFoo.return_self as being of type Foo rather than SubclassOfFoo.
Other common use cases include:
• classmethods that are used as alternative constructors and return instances of the cls parameter.
Annotating an __enter__() method which returns self.
You should not use Self as the return annotation if the method is not guaranteed to return an instance of a subclass
when the class is subclassed:
class Eggs:
# Self would be an incorrect return annotation here,
# as the object returned is always an instance of Eggs,
# even in subclasses
def returns_eggs(self) -> "Eggs":

See PEP 673 for more details.
New in version 3.11.
TypeAlias
Special annotation for explicitly declaring a type alias.
For example:
from typing import TypeAlias
Factors: TypeAlias = list[int]
26.1.
TypeAlias = "Box[str]"
class Box(Generic[T]):

def make_box_of_strings(cls) -> BoxOfStrings: ...
See PEP 613 for more details.
New in version 3.10.
Deprecated since version 3.12: TypeAlias is deprecated in favor of the type statement, which creates in-
stances of TypeAliasType and which natively supports forward references.
Note that while TypeAlias and
TypeAliasType serve similar purposes and have similar names, they are distinct and the latter is not the type
of the former.
Removal of TypeAlias is not currently planned, but users are encouraged to migrate to type
statements.
Special forms
These can be used as types in annotations.
They all support subscription using [], but each has a unique syntax.
typing.
Union
Union type; Union[X, Y] is equivalent to X | Y and means either X or Y.
To define a union, use e.g. Union[int, str] or the shorthand int | str.
Using that shorthand is recom-
mended.
The arguments must be types and there must be at least one.
Changed in version 3.10: Unions can now be written as X | Y. See union type expressions.
Optional
Optional[X] is equivalent to X | None (or Union[X, None]).
Note that this is not the same concept as an optional argument, which is one that has a default.
An optional
argument with a default does not require the Optional qualifier on its type annotation just because it is optional.
None:

On the other hand, if an explicit value of None is allowed, the use of Optional is appropriate, whether the
argument is optional or not.
Optional can now be written as X | None.
See union type expressions.
Concatenate
Special form for annotating higher-order functions.
Concatenate can be used in conjunction with Callable and ParamSpec to annotate a higher-order
callable which adds, removes, or transforms parameters of another callable.
Usage is in the form
Concatenate[Arg1Type, Arg2Type, ..., ParamSpecVariable].
Concatenate is cur-
rently only valid when used as the first argument to a Callable.
The last parameter to Concatenate must be a
ParamSpec or ellipsis (...).
For example, to annotate a decorator with_lock which provides a threading.
Lock to the decorated func-
tion, Concatenate can be used to indicate that with_lock expects a callable which takes in a Lock as the
first argument, and returns a callable with a different type signature.
In this case, the ParamSpec indicates that
the returned callable’s parameter types are dependent on the parameter types of the callable being passed in:
from collections.abc import Callable
from threading import Lock
from typing import Concatenate
# Use this lock to ensure that only one thread is executing a function
# at any time.
See also:
• PEP

–
Parameter
Specification
Variables
(the
PEP
which
introduced
ParamSpec
and
Concatenate)
• ParamSpec
• Annotating callable objects
typing.
Literal
Special typing form to define “literal types”.
Literal can be used to indicate to type checkers that the annotated object has a value equivalent to one of the
provided literals.
At runtime, an arbitrary value is allowed as type argument to Literal[.
..], but type checkers may impose restrictions.
See PEP 586 for more details about literal types.
New in version 3.8.
Literal now de-duplicates parameters.
Equality comparisons of Literal objects
are no longer order dependent.
Literal objects will now raise a TypeError exception during equality com-
parisons if one of their parameters are not hashable.
typing.
ClassVar
Special type construct to mark class variables.
As introduced in PEP 526, a variable annotation wrapped in ClassVar indicates that a given attribute is intended
to be used as a class variable and should not be set on instances of that class.
Usage:
class Starship:
stats: ClassVar[dict[str, int]] = {} # class variable
damage: int = 10
# instance variable
ClassVar accepts only types and cannot be further subscribed.
ClassVar is not a class itself, and should not be used with isinstance() or issubclass().
ClassVar
does not change Python runtime behavior, but it can be used by third-party type checkers.
Final
Special typing construct to indicate final names to type checkers.
Final names cannot be reassigned in any scope.
Final names declared in class scopes cannot be overridden in
subclasses.
TIMEOUT: Final[int] = 10
class FastConnector(Connection):
TIMEOUT = 1
# Error reported by type checker
There is no runtime checking of these properties.
See PEP 591 for more details.
New in version 3.8.
Required
Special typing construct to mark a TypedDict key as required.
This is mainly useful for total=False TypedDicts.
See TypedDict and PEP 655 for more details.
New in version 3.11.
NotRequired
Special typing construct to mark a TypedDict key as potentially missing.
See TypedDict and PEP 655 for more details.
New in version 3.11.
Annotated
Special typing form to add context-specific metadata to an annotation.
Add metadata x to a given type T by using the annotation Annotated[T, x].
Metadata added using
Annotated can be used by static analysis tools or at runtime.
At runtime, the metadata is stored in a
__metadata__ attribute.
If a library or tool encounters an annotation Annotated[T, x] and has no special logic for the metadata, it
should ignore the metadata and simply treat the annotation as T. As such, Annotated can be useful for code that
wants to use annotations for purposes outside Python’s static typing system.
Using Annotated[T, x] as an annotation still allows for static typechecking of T, as type checkers will simply
ignore the metadata x.
In this way, Annotated differs from the @no_type_check decorator, which can also
be used for adding annotations outside the scope of the typing system, but completely disables typechecking for a
function or class.
The responsibility of how to interpret the metadata lies with the tool or library encountering an Annotated anno-
tation.
A tool or library encountering an Annotated type can scan through the metadata elements to determine
if they are of interest (e.g., using isinstance()).
Annotated[<type>, <metadata>]
Here is an example of how you might use Annotated to add metadata to type annotations if you were doing
range analysis:
26.1.
It is up to the tool consuming the annotations to decide whether the client is allowed to add multiple metadata
elements to one annotation and how to merge those annotations.
The order of the metadata elements is preserved and matters for equality checks:
assert Annotated[int, ValueRange(3, 10), ctype("char")] !
This would be invalid: only one type should be passed to Annotated.
Pass include_extras=True
to have the metadata preserved:

{'x': <class 'int'>, 'return': <class 'NoneType'>}
{'x': typing.
At runtime, the metadata associated with an Annotated type can be retrieved via the __metadata__
attribute:
typing.
Annotated[int, 'very', 'important', 'metadata']
('very', 'important', 'metadata')
See also:
PEP 593 - Flexible function and variable annotations The PEP introducing Annotated to the standard li-
brary.
New in version 3.9.
TypeGuard
Special typing construct for marking user-defined type guard functions.
TypeGuard can be used to annotate the return type of a user-defined type guard function.
TypeGuard only
accepts a single type argument.
At runtime, functions marked this way should return a boolean.
TypeGuard aims to benefit type narrowing – a technique used by static type checkers to determine a more precise
type of an expression within a program’s code flow.
Usually type narrowing is done by analyzing conditional code
flow and applying the narrowing to a block of code.
Sometimes it would be convenient to use a user-defined boolean function as a type guard.
Such a function should
use TypeGuard[...] as its return type to alert static type checkers to this intention.
26.1.
TypeGuard tells the static type checker that for a given function:
1.
The return value is a boolean.
If the return value is True, the type of its argument is the type inside TypeGuard.
If is_str_list is a class or instance method, then the type in TypeGuard maps to the type of the second
parameter after cls or self.
In short, the form def foo(arg: TypeA) -> TypeGuard[TypeB]: ..., means that if foo(arg)
returns True, then arg narrows from TypeA to TypeB.
Note: TypeB need not be a narrower form of TypeA – it can even be a wider form.
The main reason is to allow
for things like narrowing list[object] to list[str] even though the latter is not a subtype of the former,
since list is invariant.
The responsibility of writing type-safe type guards is left to the user.
TypeGuard also works with type variables.
See PEP 647 for more details.
New in version 3.10.
Unpack
Typing operator to conceptually mark an object as having been unpacked.
Ts = TypeVarTuple('Ts')
tup: tuple[*Ts]
# Effectively does:
tup: tuple[Unpack[Ts]]
In fact, Unpack can be used interchangeably with * in the context of typing.
TypeVarTuple and
builtins.tuple types.
You might see Unpack being used explicitly in older versions of Python, where
* couldn’t be used in certain places:
# In older versions of Python, TypeVarTuple and Unpack
# are located in the `typing_extensions` backports package.
New in version 3.11.
The following classes should not be used directly as annotations.
Their intended purpose is to be building blocks for
creating generic types and type aliases.
These objects can be created through special syntax (type parameter lists and the type statement).
For compatibility
with Python 3.11 and earlier, they can also be created without the dedicated syntax, as documented below.
Generic
Abstract base class for generic types.
Such a class implicitly inherits from Generic.
The runtime semantics of this syntax are discussed in the Language
Reference.
This class can then be used as follows:
def lookup_name[X, Y](mapping: Mapping[X, Y], key: X, default: Y) -> Y:
try:

except KeyError:

Here the brackets after the function name indicate a generic function.
For backwards compatibility, generic classes can also be declared by explicitly inheriting from Generic.
TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False,
infer_variance=False)
Type variable.
T = TypeVar('T')
# Can be anything
S = TypeVar('S', bound=str)
# Can be any subtype of str
A = TypeVar('A', str, bytes)
# Must be exactly str or bytes
Type variables exist primarily for the benefit of static type checkers.
They serve as the parameters for generic types
as well as for generic function and type alias definitions.
See Generic for more information on generic types.
A:
"""Add two strings or bytes objects together.
The variance of type variables is inferred by type checkers when they are created through the type parameter
syntax or when infer_variance=True is passed.
Manually created type variables may be explicitly marked
covariant or contravariant by passing covariant=True or contravariant=True.
By default, manually
created type variables are invariant.
See PEP 484 and PEP 695 for more details.
Bound type variables and constrained type variables have different semantics in several important ways.
U = TypeVar('U', bound=str|bytes)
# Can be any subtype of the union str|bytes
V = TypeVar('V', bound=SupportsAbs)
# Can be anything with an __abs__ method
Using a constrained type variable, however, means that the TypeVar can only ever be solved as being exactly one
of the constraints given:
a = concatenate('one', 'two')
reveal_type(a)
# revealed type is str
b = concatenate(StringSubclass('one'), StringSubclass('two'))
reveal_type(b)
# revealed type is str, despite StringSubclass being passed in
c = concatenate('one', b'two')
# error: type variable 'A' can be either str or␣
�→bytes in a function call, but not both
At runtime, isinstance(x, T) will raise TypeError.
The name of the type variable.
Whether the type var has been explicitly marked as covariant.
__contravariant__
Whether the type var has been explicitly marked as contravariant.
__infer_variance__
Whether the type variable’s variance should be inferred by type checkers.
The bound of the type variable, if any.
Changed in version 3.12: For type variables created through type parameter syntax, the bound is evaluated
only when the attribute is accessed, not when the type variable is created (see lazy-evaluation).
A tuple containing the constraints of the type variable, if any.
Changed in version 3.12: For type variables created through type parameter syntax, the constraints are eval-
uated only when the attribute is accessed, not when the type variable is created (see lazy-evaluation).
Changed in version 3.12: Type variables can now be declared using the type parameter syntax introduced by PEP
695.
The infer_variance parameter was added.
Type variable tuple.
A specialized form of type variable that enables variadic generics.
A normal type variable enables parameterization with a single type.
A type variable tuple, in contrast, allows
parameterization with an arbitrary number of types by acting like an arbitrary number of type variables wrapped
in a tuple.
Note the use of the unpacking operator * in tuple[T, *Ts].
Conceptually, you can think of Ts as a tuple of
type variables (T1, T2, ...).
Type variable tuples must always be unpacked.
This helps distinguish type variable tuples from normal type vari-
ables:
x: Ts
# Not valid
x: tuple[Ts]
# Not valid
x: tuple[*Ts]
# The correct way to do it
Type variable tuples can be used in the same contexts as normal type variables.
For example, in class definitions,
arguments, and return types:
class Array[*Shape]:
def __getitem__(self, key: tuple[*Shape]) -> float: ...
def __abs__(self) -> "Array[*Shape]": ...
def get_shape(self) -> tuple[*Shape]: ...
Here, this allows
us to ensure the types of the *args passed to call_soon match the types of the (positional) arguments of
callback.
The name of the type variable tuple.
New in version 3.11.
Changed in version 3.12: Type variable tuples can now be declared using the type parameter syntax introduced by
PEP 695.
ParamSpec(name, *, bound=None, covariant=False, contravariant=False)
Parameter specification variable.
A specialized version of type variables.
Parameter specification variables exist primarily for the benefit of static type checkers.
They are used to forward
the parameter types of one callable to another callable – a pattern commonly found in higher order functions and
decorators.
They are only valid when used in Concatenate, or as the first argument to Callable, or as
parameters for user-defined Generics.
See Generic for more information on generic types.
For example, to add basic logging to a function, one can create a decorator add_logging to log function calls.
However this causes two problems:
1.
The type checker can’t type check the inner function because *args and **kwargs have to be typed
Any.
P.args represents the tuple of positional parameters in a given
call and should only be used to annotate *args.
P.kwargs represents the mapping of keyword parameters
to their values in a given call, and should be only be used to annotate **kwargs.
Both attributes require
the annotated parameter to be in scope.
The name of the parameter specification.
Parameter specification variables created with covariant=True or contravariant=True can be used to
declare covariant or contravariant generic types.
The bound argument is also accepted, similar to TypeVar.
However the actual semantics of these keywords are yet to be decided.
New in version 3.10.
Changed in version 3.12: Parameter specifications can now be declared using the type parameter syntax introduced
by PEP 695.
Note: Only parameter specification variables defined in global scope can be pickled.
ParamSpecArgs
typing.
ParamSpecKwargs
Arguments and keyword arguments attributes of a ParamSpec.
The P.args attribute of a ParamSpec is an
instance of ParamSpecArgs, and P.kwargs is an instance of ParamSpecKwargs.
They are intended for
runtime introspection and have no special meaning to static type checkers.
True
True
New in version 3.10.
class typing.
The type of type aliases created through the type statement.
Example:
<class 'typing.
The type alias’s value.
This is lazily evaluated, so names used in the definition of the alias are not resolved
until the __value__ attribute is accessed:
26.1.
Their intended purpose is to be building blocks
for creating and declaring types.
NamedTuple
Typed version of collections.namedtuple().
This is equivalent to:
Employee = collections.namedtuple('Employee', ['name', 'id'])
To give a field a default value, you can assign to it in the class body:
class Employee(NamedTuple):
name: str
id: int = 3
employee = Employee('Guido')
assert employee.id == 3
Fields with a default value must come after any fields without a default.
The resulting class has an extra attribute __annotations__ giving a dict that maps the field names to the field
types.
NamedTuple subclasses can also have docstrings and methods:
class Employee(NamedTuple):
"""Represents an employee.
Changed in version 3.6: Added support for PEP 526 variable annotation syntax.
Added support for default values, methods, and docstrings.
Changed in version 3.9:
Removed the _field_types attribute in favor of the more standard
__annotations__ attribute which has the same information.
Changed in version 3.11: Added support for generic namedtuples.
NewType(name, tp)
Helper class to create low-overhead distinct types.
A NewType is considered a distinct type by a typechecker.
At runtime, however, calling a NewType returns its
argument unchanged.
The module in which the new type is defined.
The type that the new type is based on.
New in version 3.5.2.
NewType is now a class rather than a function.
Base class for protocol classes.
Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing),
for example:
26.1.
Protocol classes decorated with runtime_checkable() (described later) act
as simple-minded runtime protocols that check only the presence of given attributes, ignoring their type signatures.
New in version 3.8.
Such a protocol can be used with isinstance() and issubclass().
This raises TypeError when ap-
plied to a non-protocol class.
This allows a simple-minded structural check, very similar to “one trick ponies” in
collections.abc such as Iterable.
For example:

class Closable(Protocol):

assert isinstance(open('/some/file'), Closable)

class Named(Protocol):
name: str
import threading
assert isinstance(threading.
Note: runtime_checkable() will check only the presence of the required methods or attributes, not their
type signatures or types.
For example, ssl.SSLObject is a class, therefore it passes an issubclass() check
against Callable.
However, the ssl.SSLObject.__init__ method exists only to raise a TypeError with
a more informative message, therefore making it impossible to call (instantiate) ssl.
SSLObject.
Note:
An isinstance() check against a runtime-checkable protocol can be surprisingly slow compared to
an isinstance() check against a non-protocol class.
New in version 3.8.
The internal implementation of isinstance() checks against runtime-checkable
protocols now uses inspect.getattr_static() to look up attributes (previously, hasattr() was used).
As a result, some objects which used to be considered instances of a runtime-checkable protocol may no longer be
considered instances of that protocol on Python 3.12+, and vice versa.
Most users are unlikely to be affected by
this change.
Changed in version 3.12: The members of a runtime-checkable protocol are now considered “frozen” at runtime as
soon as the class has been created.
Monkey-patching attributes onto a runtime-checkable protocol will still work,
but will have no impact on isinstance() checks comparing objects to the protocol.
See “What’s new in Python
3.12” for more details.
Special construct to add type hints to a dictionary.
At runtime it is a plain dict.
TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each
key is associated with a value of a consistent type.
This expectation is not checked at runtime but is only enforced
by type checkers.
The keyword-argument syntax is deprecated in
3.11 and will be removed in 3.13.
It may also be unsupported by static type checkers.
The functional syntax should also be used when any of the keys are not valid identifiers, for example because they
are keywords or contain hyphens.
By default, all keys must be present in a TypedDict.
It is possible to mark individual keys as non-required using
NotRequired:
26.1.
This means that a Point2D TypedDict can have the label key omitted.
This means that a Point2D TypedDict can have any of the keys omitted.
A type checker is only expected to
support a literal False or True as the value of the total argument.
True is the default, and makes all items
defined in the class body required.
It is possible for a TypedDict type to inherit from one or more other TypedDict types using the class-based
syntax.
Usage:
class Point3D(Point2D):
z: int
Point3D has three items: x, y and z.
It is equivalent to this definition:
class Point3D(TypedDict):
x: int
y: int
z: int
A TypedDict cannot inherit from a non-TypedDict class, except for Generic.
Point2D.__total__ gives the value of the total argument.
Example:
True
False
True
This attribute reflects only the value of the total argument to the current TypedDict class, not whether
the class is semantically total.
For example, a TypedDict with __total__ set to True may have keys
marked with NotRequired, or it may inherit from another TypedDict with total=False.
Therefore,
it is generally better to use __required_keys__ and __optional_keys__ for introspection.
Keys marked with Required will always appear in __required_keys__ and keys marked with
NotRequired will always appear in __optional_keys__.
For backwards compatibility with Python 3.10 and below, it is also possible to use inheritance to declare both
required and non-required keys in the same TypedDict .
This is done by declaring a TypedDict with
one value for the total argument and then inheriting from it in another TypedDict with a different value
for total:
26.1.
True
True
New in version 3.9.
Therefore, the runtime introspection that
__required_keys__ and __optional_keys__ rely on may not work properly, and the values of
the attributes may be incorrect.
See PEP 589 for more examples and detailed rules of using TypedDict.
New in version 3.8.
Changed in version 3.11: Added support for marking individual keys as Required or NotRequired.
See PEP
655.
Changed in version 3.11: Added support for generic TypedDicts.
Protocols
The following protocols are provided by the typing module.
All are decorated with @runtime_checkable.
SupportsBytes
An ABC with one abstract method __bytes__.
SupportsComplex
An ABC with one abstract method __complex__.
SupportsFloat
An ABC with one abstract method __float__.
SupportsIndex
An ABC with one abstract method __index__.
New in version 3.8.
class typing.
SupportsInt
An ABC with one abstract method __int__.
IO
class typing.
TextIO
class typing.BinaryIO
Generic type IO[AnyStr] and its subclasses TextIO(IO[str]) and BinaryIO(IO[bytes]) represent
the types of I/O streams such as returned by open().
Functions and decorators
typing.cast(typ, val)
Cast a value to a type.
This returns the value unchanged.
To the type checker this signals that the return value has the designated type, but
at runtime we intentionally don’t check anything (we want this to be as fast as possible).
At runtime this does nothing: it returns the first argument unchanged with no checks or side effects, no matter the
actual type of the argument.
None:
match arg:
case int():
print("It's an int")
case str():
print("It's a str")
case _ as unreachable:
assert_never(unreachable)
Here, the annotations allow the type checker to infer that the last case can never execute, because arg is either an
int or a str, and both options are covered by earlier cases.
If a type checker finds that a call to assert_never() is reachable, it will emit an error.
For example, if the
type annotation for arg was instead int | str | float, the type checker would emit an error pointing out
that unreachable is of type float.
For a call to assert_never to pass type checking, the inferred type
of the argument passed in must be the bottom type, Never, and nothing else.
At runtime, this throws an exception when called.
See also:
Unreachable Code and Exhaustiveness Checking has more information about exhaustiveness checking with static
typing.
New in version 3.11.
When a static type checker encounters a call to this function, it emits a diagnostic with the type of the argument.
For example:
x: int = 1
reveal_type(x)
# Revealed type is "builtins.int"
This can be useful when you want to debug how your type checker handles a particular piece of code.
The function returns its argument unchanged, which allows using it within an expression:
x = reveal_type(1)
# Revealed type is "builtins.int"
Most type checkers support reveal_type() anywhere, even if the name is not imported from typing.
Im-
porting the name from typing allows your code to run without runtime errors and communicates intent more
clearly.
At runtime, this function prints the runtime type of its argument to stderr and returns it unchanged:
x = reveal_type(1)
# prints "Runtime type is int"
print(x)
# prints "1"
New in version 3.11.
The presence of @dataclass_transform() tells a static type checker that the decorated object performs
runtime “magic” that transforms a class in a similar way to @dataclasses.dataclass.
For example, type checkers will assume these classes have __init__ methods
that accept id and name.
The decorated class, metaclass, or function may accept the following bool arguments which type checkers will
assume have the same effect as they would have on the @dataclasses.dataclass decorator: init, eq,
order, unsafe_hash, frozen, match_args, kw_only, and slots.
It must be possible for the value of
these arguments (True or False) to be statically evaluated.
The arguments to the dataclass_transform decorator can be used to customize the default behaviors of the
decorated class, metaclass, or function:
Parameters
• eq_default (bool) – Indicates whether the eq parameter is assumed to be True or
False if it is omitted by the caller.
Defaults to True.
• order_default (bool) – Indicates whether the order parameter is assumed to be True
or False if it is omitted by the caller.
Defaults to False.
• kw_only_default (bool) – Indicates whether the kw_only parameter is assumed to
be True or False if it is omitted by the caller.
Defaults to False.
• frozen_default (bool) – Indicates whether the frozen parameter is assumed to be
True or False if it is omitted by the caller.
Defaults to False.
New in version 3.12.
• field_specifiers (tuple[Callable[..., Any], ...]) – Specifies a static
list of supported classes or functions that describe fields, similar to dataclasses.
Defaults to ().
Type checkers recognize the following optional parameters on field specifiers:
26.1.
If
unspecified, init defaults to True.
default
Provides the default value for the field.
default_factoryProvides a runtime callback that returns the default value for the field.
If neither
default nor default_factory are specified, the field is assumed to have no de-
fault value and must be provided a value when the class is instantiated.
If True, the field will
be keyword-only.
If False, it will not be keyword-only.
If unspecified, the value
of the kw_only parameter on the object decorated with dataclass_transform
will be used, or if that is unspecified, the value of kw_only_default on
dataclass_transform will be used.
This alternative name is used in the synthesized
__init__ method.
At runtime, this decorator records its arguments in the __dataclass_transform__ attribute on the deco-
rated object.
It has no other runtime effect.
See PEP 681 for more details.
New in version 3.11.
The @overload decorator allows describing functions and methods that support multiple different combina-
tions of argument types.
A series of @overload-decorated definitions must be followed by exactly one non-
@overload-decorated definition (for the same function/method).
The non-@overload-decorated definition, meanwhile, will be used at
runtime but should be ignored by a type checker.
At runtime, calling an @overload-decorated function directly
will raise NotImplementedError.
Changed in version 3.11: Overloaded functions can now be introspected at runtime using get_overloads().
For example, given the defini-
tion of process in the documentation for @overload, get_overloads(process) will return a se-
quence of three function objects for the three defined overloads.
If called on a function with no overloads,
get_overloads() returns an empty sequence.
New in version 3.11.
This can be used to reclaim the memory used by the registry.
New in version 3.11.
Decorating a method with @final indicates to a type checker that the method cannot be overridden in a subclass.
Decorating a class with @final indicates that it cannot be subclassed.
There is no runtime checking of these properties.
See PEP 591 for more details.
New in version 3.8.
Changed in version 3.11: The decorator will now attempt to set a __final__ attribute to True on the decorated
object.
If the decorated object does not support setting attributes,
the decorator returns the object unchanged without raising an exception.
This works as a class or function decorator.
With a class, it applies recursively to all methods and classes defined in
that class (but not to methods defined in its superclasses or subclasses).
Type checkers will ignore all annotations
in a function or class with this decorator.
This wraps the decorator with something that wraps the decorated function in no_type_check().
26.1.
Type checkers should emit an error if a method decorated with @override does not, in fact, override anything.
This helps prevent bugs that may occur when a base class is changed without an equivalent change to a child class.
There is no runtime checking of this property.
The decorator will attempt to set an __override__ attribute to True on the decorated object.
Thus, a check like
if getattr(obj, "__override__", False) can be used at runtime to determine whether an object
obj has been marked as an override.
If the decorated object does not support setting attributes, the decorator
returns the object unchanged without raising an exception.
See PEP 698 for more details.
New in version 3.12.

Decorator to mark a class or function as unavailable at runtime.
This decorator is itself not available at runtime.
It is mainly intended to mark classes that are defined in type stub
files if an implementation returns an instance of a private class:

class Response:
# private or not available at runtime
code: int
def get_header(self, name: str) -> str: ...
def fetch_response() -> Response: ...
Note that returning instances of private classes is not recommended.
It is usually preferable to make such classes
public.
Introspection helpers
typing.get_type_hints(obj, globalns=None, localns=None, include_extras=False)
Return a dictionary containing type hints for a function, method, module or class object.
This is often the same as obj.__annotations__.
In addition, forward references encoded as string literals are
handled by evaluating them in globals and locals namespaces.
For a class C, return a dictionary constructed
by merging all the __annotations__ along C.__mro__ in reverse order.
The function recursively replaces all Annotated[T, ...] with T, unless include_extras is set to True
(see Annotated for more information).
Enabling
postponed evaluation of annotations (PEP 563) may remove the need for most forward references.
Changed in version 3.9: Added include_extras parameter as part of PEP 593.
See the documentation on
Annotated for more information.
Changed in version 3.11: Previously, Optional[t] was added for function and method annotations if a default
value equal to None was set.
Now the annotation is returned unchanged.
If X is a typing-module alias for a builtin or collections class, it will be normalized to the original class.
If X is
an instance of ParamSpecArgs or ParamSpecKwargs, return the underlying ParamSpec.
Return None
for unsupported objects.
Examples:
assert get_origin(str) is None
assert get_origin(Dict[str, int]) is dict
assert get_origin(Union[int, str]) is Union
P = ParamSpec('P')
assert get_origin(P.args) is P
assert get_origin(P.kwargs) is P
New in version 3.8.
Return () for unsupported objects.
For example:
26.1.
ForwardRef
Class used for internal typing representation of string forward references.
For example, List["SomeClass"] is implicitly transformed into List[ForwardRef("SomeClass")].
ForwardRef should not be instantiated by a user, but may be used by introspection tools.
Note:
PEP 585 generic types such as list["SomeClass"] will not be implicitly transformed into
list[ForwardRef("SomeClass")] and thus will not automatically resolve to list[SomeClass].
New in version 3.7.4.
Constant
typing.
TYPE_CHECKING
A special constant that is assumed to be True by 3rd party static type checkers.
It is False at runtime.
None:
local_var: expensive_mod.
The first type annotation must be enclosed in quotes, making it a “forward reference”, to hide the
expensive_mod reference from the interpreter runtime.
Type annotations for local variables are not evalu-
ated, so the second annotation does not need to be enclosed in quotes.
Instead, they are stored as strings in __annotations__.
This makes it unnecessary to use
quotes around the annotation (see PEP 563).
These were originally included
in the typing module in order to support parameterizing these generic classes using [].
The redundant types are deprecated as of Python 3.9.
However, while the aliases may be removed at some point, removal
of these aliases is not currently planned.
As such, no deprecation warnings are currently issued by the interpreter for these
aliases.
If at some point it is decided to remove these deprecated aliases, a deprecation warning will be issued by the interpreter
for at least two releases prior to removal.
The aliases are guaranteed to remain in the typing module without deprecation
warnings until at least Python 3.14.
Type checkers are encouraged to flag uses of the deprecated types if the program they are checking targets a minimum
Python version of 3.9 or newer.
Aliases to built-in types
class typing.
Dict(dict, MutableMapping[KT, VT])
Deprecated alias to dict.
Note that to annotate arguments, it is preferred to use an abstract collection type such as Mapping rather than to
use dict or typing.
Dict.
This type can be used as follows:
def count_words(text: str) -> Dict[str, int]:

Deprecated since version 3.9: builtins.dict now supports subscripting ([]).
See PEP 585 and Generic
Alias Type.
class typing.
List(list, MutableSequence[T])
Deprecated alias to list.
Note that to annotate arguments, it is preferred to use an abstract collection type such as Sequence or Iterable
rather than to use list or typing.
List.
This type may be used as follows:
def vec2[T: (int, float)](x: T, y: T) -> List[T]:

def keep_positives[T: (int, float)](vector: Sequence[T]) -> List[T]:

Deprecated since version 3.9: builtins.list now supports subscripting ([]).
See PEP 585 and Generic
Alias Type.
class typing.
Set(set, MutableSet[T])
Deprecated alias to builtins.set.
Note that to annotate arguments, it is preferred to use an abstract collection type such as AbstractSet rather
than to use set or typing.
Set.
Deprecated since version 3.9: builtins.set now supports subscripting ([]).
See PEP 585 and Generic Alias
Type.
26.1.
FrozenSet(frozenset, AbstractSet[T_co])
Deprecated alias to builtins.frozenset.
Deprecated since version 3.9: builtins.frozenset now supports subscripting ([]).
See PEP 585 and
Generic Alias Type.
typing.
Tuple
Deprecated alias for tuple.
Deprecated since version 3.9: builtins.tuple now supports subscripting ([]).
See PEP 585 and Generic
Alias Type.
class typing.
Type(Generic[CT_co])
Deprecated alias to type.
See The type of class objects for details on using type or typing.
Type in type annotations.
New in version 3.5.2.
Deprecated since version 3.9: builtins.type now supports subscripting ([]).
See PEP 585 and Generic
Alias Type.
Aliases to types in collections
class typing.
DefaultDict(collections.defaultdict, MutableMapping[KT, VT])
Deprecated alias to collections.defaultdict.
New in version 3.5.2.
Deprecated since version 3.9: collections.defaultdict now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
OrderedDict(collections.
OrderedDict, MutableMapping[KT, VT])
Deprecated alias to collections.
OrderedDict.
New in version 3.7.2.
Deprecated since version 3.9: collections.
OrderedDict now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
ChainMap(collections.
ChainMap, MutableMapping[KT, VT])
Deprecated alias to collections.
ChainMap.
New in version 3.5.4.
New in version 3.6.1.
Deprecated since version 3.9: collections.
ChainMap now supports subscripting ([]).
See PEP 585 and
Generic Alias Type.
class typing.
Counter(collections.
Counter, Dict[T, int])
Deprecated alias to collections.
Counter.
New in version 3.5.4.
New in version 3.6.1.
Deprecated since version 3.9: collections.
Counter now supports subscripting ([]).
Deque(deque, MutableSequence[T])
Deprecated alias to collections.deque.
New in version 3.5.4.
New in version 3.6.1.
Deprecated since version 3.9: collections.deque now supports subscripting ([]).
See PEP 585 and Generic
Alias Type.
The typing.io namespace is deprecated
and will be removed.
These types should be directly imported from typing instead.
Pattern
class typing.
Match
Deprecated aliases corresponding to the return types from re.compile() and re.match().
These types (and the corresponding functions) are generic over AnyStr.
Pattern can be specialised as
Pattern[str] or Pattern[bytes]; Match can be specialised as Match[str] or Match[bytes].
The typing.re namespace is deprecated and
will be removed.
These types should be directly imported from typing instead.
Deprecated since version 3.9: Classes Pattern and Match from re now support [].
See PEP 585 and Generic
Alias Type.
class typing.
Text
Deprecated alias for str.
Text is provided to supply a forward compatible path for Python 2 code: in Python 2, Text is an alias for
unicode.
Python 2 is no longer supported, and most type checkers also no longer support
type checking Python 2 code.
Removal of the alias is not currently planned, but users are encouraged to use str
instead of Text.
Aliases to container ABCs in collections.abc
class typing.
AbstractSet(Collection[T_co])
Deprecated alias to collections.abc.
Set.
Deprecated since version 3.9: collections.abc.
Set now supports subscripting ([]).
See PEP 585 and
Generic Alias Type.
26.1.
This type represents the types bytes, bytearray, and memoryview of byte sequences.
Prefer collections.abc.
Buffer, or a union
like bytes | bytearray | memoryview.
Collection.
New in version 3.6.0.
Deprecated since version 3.9: collections.abc.
Collection now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
Container(Generic[T_co])
Deprecated alias to collections.abc.
Container.
Deprecated since version 3.9: collections.abc.
Container now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
ItemsView(MappingView, AbstractSet[tuple[KT_co, VT_co]])
Deprecated alias to collections.abc.
ItemsView.
Deprecated since version 3.9: collections.abc.
ItemsView now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
KeysView(MappingView, AbstractSet[KT_co])
Deprecated alias to collections.abc.
KeysView.
Deprecated since version 3.9: collections.abc.
KeysView now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
Mapping(Collection[KT], Generic[KT, VT_co])
Deprecated alias to collections.abc.
Mapping.
This type can be used as follows:
def get_position_in_index(word_list: Mapping[str, int], word: str) -> int:

Deprecated since version 3.9: collections.abc.
Mapping now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
MappingView(Sized)
Deprecated alias to collections.abc.
MappingView.
Deprecated since version 3.9: collections.abc.
MappingView now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
MutableMapping(Mapping[KT, VT])
Deprecated alias to collections.abc.
MutableMapping.
Deprecated since version 3.9: collections.abc.
MutableMapping now supports subscripting ([]).
See
PEP 585 and Generic Alias Type.
class typing.
MutableSequence(Sequence[T])
Deprecated alias to collections.abc.
MutableSequence.
Deprecated since version 3.9: collections.abc.
MutableSequence now supports subscripting ([]).
MutableSet(AbstractSet[T])
Deprecated alias to collections.abc.
MutableSet.
Deprecated since version 3.9: collections.abc.
MutableSet now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
Sequence(Reversible[T_co], Collection[T_co])
Deprecated alias to collections.abc.
Sequence.
Deprecated since version 3.9: collections.abc.
Sequence now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
ValuesView(MappingView, Collection[_VT_co])
Deprecated alias to collections.abc.
ValuesView.
Deprecated since version 3.9: collections.abc.
ValuesView now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
Aliases to asynchronous ABCs in collections.abc
class typing.
Coroutine(Awaitable[ReturnType], Generic[YieldType, SendType, ReturnType])
Deprecated alias to collections.abc.
Coroutine.
None:
y = await c
# Inferred type of 'y' is int
New in version 3.5.3.
Deprecated since version 3.9: collections.abc.
Coroutine now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
AsyncGenerator(AsyncIterator[YieldType], Generic[YieldType, SendType])
Deprecated alias to collections.abc.
AsyncGenerator.
An async generator can be annotated by the generic type AsyncGenerator[YieldType, SendType].
For example:
async def echo_round() -> AsyncGenerator[int, float]:
sent = yield 0
while sent >= 0.0:
rounded = await round(sent)
sent = yield rounded
Unlike normal generators, async generators cannot return a value, so there is no ReturnType type parameter.
As with Generator, the SendType behaves contravariantly.
If your generator will only yield values, set the SendType to None:
async def infinite_stream(start: int) -> AsyncGenerator[int, None]:
while True:

start = await increment(start)
26.1.
Deprecated since version 3.9: collections.abc.
AsyncGenerator now supports subscripting ([]).
See
PEP 585 and Generic Alias Type.
class typing.
AsyncIterable(Generic[T_co])
Deprecated alias to collections.abc.
AsyncIterable.
New in version 3.5.2.
Deprecated since version 3.9: collections.abc.
AsyncIterable now supports subscripting ([]).
See
PEP 585 and Generic Alias Type.
class typing.
AsyncIterator(AsyncIterable[T_co])
Deprecated alias to collections.abc.
AsyncIterator.
New in version 3.5.2.
Deprecated since version 3.9: collections.abc.
AsyncIterator now supports subscripting ([]).
See
PEP 585 and Generic Alias Type.
class typing.
Awaitable(Generic[T_co])
Deprecated alias to collections.abc.
Awaitable.
New in version 3.5.2.
Deprecated since version 3.9: collections.abc.
Awaitable now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
Aliases to other ABCs in collections.abc
class typing.
Iterable(Generic[T_co])
Deprecated alias to collections.abc.
Iterable.
Deprecated since version 3.9: collections.abc.
Iterable now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
class typing.
Iterator(Iterable[T_co])
Deprecated alias to collections.abc.
Iterator.
Deprecated since version 3.9: collections.abc.
Iterator now supports subscripting ([]).
See PEP 585
and Generic Alias Type.
typing.
Callable
Deprecated alias to collections.abc.
Callable.
See Annotating callable objects for details on how to use collections.abc.
Callable and typing.
Callable in type annotations.
Deprecated since version 3.9: collections.abc.
Callable now supports subscripting ([]).
Callable now supports ParamSpec and Concatenate.
See PEP 612 for more
details.
Generator(Iterator[YieldType], Generic[YieldType, SendType, ReturnType])
Deprecated alias to collections.abc.
Generator.
A generator can be annotated by the generic type Generator[YieldType, SendType, ReturnType].
For example:
def echo_round() -> Generator[int, float, str]:
sent = yield 0
while sent >= 0:
sent = yield round(sent)

Note that unlike many other generics in the typing module, the SendType of Generator behaves contravari-
antly, not covariantly or invariantly.
Generator now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
Hashable
Deprecated alias to collections.abc.
Hashable.
Deprecated since version 3.12: Use collections.abc.
Hashable directly instead.
Reversible(Iterable[T_co])
Deprecated alias to collections.abc.
Reversible.
Deprecated since version 3.9: collections.abc.
Reversible now supports subscripting ([]).
See PEP
585 and Generic Alias Type.
class typing.
Sized
Deprecated alias to collections.abc.
Sized.
Deprecated since version 3.12: Use collections.abc.
Sized directly instead.
ContextManager(Generic[T_co])
Deprecated alias to contextlib.
AbstractContextManager.
New in version 3.5.4.
New in version 3.6.0.
Deprecated since version 3.9: contextlib.
AbstractContextManager now supports subscripting ([]).
See PEP 585 and Generic Alias Type.
class typing.
AsyncContextManager(Generic[T_co])
Deprecated alias to contextlib.
AbstractAsyncContextManager.
New in version 3.5.4.
New in version 3.6.2.
See PEP 585 and Generic Alias Type.
The following table
summarizes major deprecations for your convenience.
This is subject to change, and not all deprecations are listed.
PEP 585
typing.
ByteString
3.9
3.14
gh-

typing.
Text
3.11
Undecided
gh-

typing.
Hashable
and
typing.
Sized
3.12
Undecided
gh-

typing.
Lib/pydoc.py
The pydoc module automatically generates documentation from Python modules.
The documentation can be presented
as pages of text on the console, served to a web browser, or saved to HTML files.
For modules, classes, functions and methods, the displayed documentation is derived from the docstring (i.e.
the
__doc__ attribute) of the object, and recursively of its documentable members.
If there is no docstring, pydoc tries
to obtain a description from the block of comment lines just above the definition of the class, function or method in the
source file, or at the top of the module (see inspect.getcomments()).
The built-in function help() invokes the online help system in the interactive interpreter, which uses pydoc to generate
its documentation as text on the console.
The same text documentation can also be viewed from outside the Python
interpreter by running pydoc as a script at the operating system’s command prompt.
For example, running
python -m pydoc sys
at a shell prompt will display documentation on the sys module, in a style similar to the manual pages shown by the Unix
man command.
The argument to pydoc can be the name of a function, module, or package, or a dotted reference to a
class, method, or function within a module or module in a package.
If the argument to pydoc looks like a path (that is,
it contains the path separator for your operating system, such as a slash in Unix), and refers to an existing Python source
file, then documentation is produced for that file.
In order to find objects and their documentation, pydoc imports the module(s) to be documented.
Therefore,
any code on module level will be executed on that occasion.
When printing output to the console, pydoc attempts to paginate the output for easier reading.
If the PAGER environment
variable is set, pydoc will use its value as a pagination program.
Specifying a -w flag before the argument will cause HTML documentation to be written out to a file in the current
directory, instead of displaying text on the console.
Specifying a -k flag before the argument will search the synopsis lines of all available modules for the keyword given as
the argument, again in a manner similar to the Unix man command.
The synopsis line of a module is the first line of its
documentation string.
You can also use pydoc to start an HTTP server on the local machine that will serve documentation to visiting web
browsers.
Specifying 0 as the port number will
select an arbitrary unused port.
By default the hostname
is ‘localhost’ but if you want the server to be reached from other machines, you may want to change the host name that
the server responds to.
During development this is especially useful if you want to run pydoc from within a container.
Each served
page has a navigation bar at the top where you can Get help on an individual item, Search all modules with a keyword in
their synopsis line, and go to the Module index, Topics and Keywords pages.
When pydoc generates documentation, it uses the current environment and path to locate modules.
Thus, invoking
pydoc spam documents precisely the version of the module you would get if you started the Python interpreter and
typed import spam.
Module docs for core modules are assumed to reside in https://docs.python.org/X.Y/library/ where
X and Y are the major and minor version numbers of the Python interpreter.
This can be overridden by setting the
PYTHONDOCS environment variable to a different URL or to a local directory containing the Library Reference Manual
pages.
Changed in version 3.2: Added the -b option.
The -g command line option was removed.
Changed in version 3.4: pydoc now uses inspect.signature() rather than inspect.getfullargspec()
to extract signature information from callables.
Added the -n option.
The Python Development Mode introduces additional runtime checks that are too expensive to be enabled by default.
It should not be more verbose than the default if the code is correct; new warnings are only emitted when an issue is
detected.
It can be enabled using the -X dev command line option or by setting the PYTHONDEVMODE environment variable to
1.
See also Python debug build.
The following warnings are shown:
– DeprecationWarning
– ImportWarning
– PendingDeprecationWarning
– ResourceWarning
Normally, the above warnings are filtered by the default warning filters.
It behaves as if the -W default command line option is used.
Use the -W error command line option or set the PYTHONWARNINGS environment variable to error to treat
warnings as errors.
Install debug hooks on memory allocators to check for:
– Buffer underflow
– Buffer overflow
– Memory allocator API violation
– Unsafe usage of the GIL
See the PyMem_SetupDebugHooks() C function.
It behaves as if the PYTHONMALLOC environment variable is set to debug.
To enable the Python Development Mode without installing debug hooks on memory allocators, set the
PYTHONMALLOC environment variable to default.
It behaves as if the -X faulthandler command line option is used or if the PYTHONFAULTHANDLER en-
vironment variable is set to 1.
• Enable asyncio debug mode.
For example, asyncio checks for coroutines that were not awaited and logs them.
Examples: open(), str.
encode() and bytes.decode().
By default, for best performance, the errors argument is only checked at the first encoding/decoding error and the
encoding argument is sometimes ignored for empty strings.
IOBase destructor logs close() exceptions.
The Python Development Mode does not enable the tracemalloc module by default, because the overhead cost (to
performance and memory) would be too large.
Enabling the tracemalloc module provides additional information on
the origin of some errors.
For example, ResourceWarning logs the traceback where the resource was allocated, and
a buffer overflow error logs the traceback where the memory block was allocated.
The Python Development Mode does not prevent the -O command line option from removing assert statements nor
from setting __debug__ to False.
The Python Development Mode can only be enabled at the Python startup.
Its value can be read from sys.flags.
Changed in version 3.8: The io.
IOBase destructor now logs close() exceptions.
The encoding and errors arguments are now checked for string encoding and decoding operations.
The script does not close the file explicitly.
By default, Python does not emit any warning.
ResourceWarning: unclosed file <_io.
In addition, enabling tracemalloc shows the line where the file was opened:
26.3.
ResourceWarning: unclosed file <_io.
The fix is to close explicitly the file.
Example using a context manager:

# Close the file explicitly when exiting the with block
with open(sys.argv[1]) as fp:
nlines = len(fp.readlines())
print(nlines)
Not closing a resource explicitly can leave a resource open for way longer than expected; it can cause severe issues upon
exiting Python.
It is bad in CPython, but it is even worse in PyPy.
Closing resources explicitly makes an application more
deterministic and more reliable.
ResourceWarning: unclosed file <_io.
Enable tracemalloc to get the object allocation traceback
Exception ignored in: <_io.
When the file object finalizer tries to close the file descriptor
again, it fails with the Bad file descriptor error.
A file descriptor must be closed only once.
In the worst case
scenario, closing it twice can lead to a crash (see bpo-18748 for an example).
The fix is to remove the os.close(fp.fileno()) line, or open the file with closefd=False.
26.4 doctest — Test interactive Python examples
Source code: Lib/doctest.py
The doctest module searches for pieces of text that look like interactive Python sessions, and then executes those
sessions to verify that they work exactly as shown.
There are several common ways to use doctest:
• To check that a module’s docstrings are up-to-date by verifying that all interactive examples still work as docu-
mented.
Depending on
whether the examples or the expository text are emphasized, this has the flavor of “literate testing” or “executable
documentation”.
Here’s a complete but small example module:
"""
This is the "example" module.
The example module supplies one function, factorial().
OverflowError: n too large
"""
import math
if not n >= 0:
raise ValueError("n must be >= 0")
if math.floor(n) !
If you run example.py directly from the command line, doctest works its magic:
$ python example.py
$
There’s no output!
That’s normal, and it means all the examples worked.
Test passed.
Jump in.
The following sections provide full
details.
Note that there are many examples of doctests in the standard Python test suite and libraries.
Especially useful
examples can be found in the standard test file Lib/test/test_doctest.py.
This won’t display anything unless an example fails, in which case the failing example(s) and the cause(s) of the failure(s)
are printed to stdout, and the final line of output is ***Test Failed*** N failures., where N is the number
of examples that failed.
Run it with the -v switch instead:
python M.py -v
and a detailed report of all examples tried is printed to standard output, along with assorted summaries at the end.
You can force verbose mode by passing verbose=True to testmod(), or prohibit it by passing verbose=False.
In either of those cases, sys.argv is not examined by testmod() (so passing -v or not has no effect).
There is also a command line shortcut for running testmod().
This will import example.py as a standalone module and run testmod() on it.
Note that this may not work correctly
if the file is part of a package and imports other submodules from that package.
For more information on testmod(), see section Basic API.
That short script executes and verifies any interactive Python examples contained in the file example.txt.
The file
content is treated as if it were a single giant docstring; the file doesn’t need to contain a Python program!
This is an example text file in reStructuredText format.
First import
``factorial`` from the ``example`` module:
Now use it:

Running doctest.testfile("example.txt") then finds the error in this documentation:
File "./example.txt", line 14, in example.txt
Failed example:
factorial(6)
Expected:

Got:

As with testmod(), testfile() won’t display anything unless an example fails.
If an example does fail, then the
failing example(s) and the cause(s) of the failure(s) are printed to stdout, using the same format as testmod().
By default, testfile() looks for files in the calling module’s directory.
See section Basic API for a description of the
optional arguments that can be used to tell it to look for files in other locations.
Like testmod(), testfile()’s verbosity can be set with the -v command-line switch or with the optional keyword
argument verbose.
There is also a command line shortcut for running testfile().
How It Works
This section examines in detail how doctest works: which docstrings it looks at, how it finds interactive examples, what
execution context it uses, how it handles exceptions, and how option flags can be used to control its behavior.
This is
the information that you need to know to write doctest examples; for information about actually running doctest on these
examples, see the following sections.
Which Docstrings Are Examined?
The module docstring, and all function, class and method docstrings are searched.
Objects imported into the module are
not searched.
In addition, there are cases when you want tests to be part of a module but not part of the help text, which requires that the
tests not be included in the docstring.
Doctest looks for a module-level variable called __test__ and uses it to locate
other tests.
If M.__test__ exists and is truthy, it must be a dict, and each entry maps a (string) name to a function
object, class object, or string.
It is important to note that the value can be mapped to a function, class object, or module; if so, doctest searches them
recursively for docstrings, which are then scanned for tests.
Any classes found are recursively searched similarly, to test docstrings in their contained methods and nested classes.
How are Docstring Examples Recognized?
Any expected output must immediately follow the final 'output (if any) extends to the next 'The fine print:
• Expected output cannot contain an all-whitespace line, since such a line is taken to signal the end of expected
output.
If expected output does contain a blank line, put <BLANKLINE> in your doctest example each place a
blank line is expected.
Tabs in output generated by the tested
code are not modified.
Because any hard tabs in the sample output are expanded, this means that if the code output
includes hard tabs, the only way the doctest can pass is if the NORMALIZE_WHITESPACE option or directive is
in effect.
Alternatively, the test can be rewritten to capture the output and compare it to an expected value as part
of the test.
This handling of tabs in the source was arrived at through trial and error, and has proven to be the least
error prone way of handling them.
It is possible to use a different algorithm for handling tabs by writing a custom
DocTestParser class.
If you continue a line via backslashing in an interactive session, or for any other reason use a backslash, you should
use a raw docstring, which will preserve your backslashes exactly as you type them:

r'''Backslashes in a raw docstring: m\n'''

Backslashes in a raw docstring: m\n
Otherwise, the backslash will be interpreted as part of the string.
For example, the \n above would be interpreted
as a newline character.
What’s the Execution Context?
By default, each time doctest finds a docstring to test, it uses a shallow copy of M’s globals, so that running tests doesn’t
change the module’s real globals, and so that one test in M can’t leave behind crumbs that accidentally allow another test to
work.
This means examples can freely use any names defined at top-level in M, and names defined earlier in the docstring
being run.
Examples cannot see names defined in other docstrings.
You can force use of your own dict as the execution context by passing globs=your_dict to testmod() or
testfile() instead.
What About Exceptions?
No problem, provided that the traceback is the only output produced by the example: just paste in the traceback.1 Since
tracebacks contain details that are likely to change rapidly (for example, exact file paths and line numbers), this is one
case where doctest works hard to be flexible in what it accepts.
File "<stdin>", line 1, in <module>
ValueError: list.remove(x): x not in list
That doctest succeeds if ValueError is raised, with the list.remove(x): x not in list detail as shown.
The expected output for an exception must start with a traceback header, which may be either of the following two lines,
indented the same as the first line of the example:
Traceback (most recent call last):
Traceback (innermost last):
The traceback header is followed by an optional traceback stack, whose contents are ignored by doctest.
The traceback
stack is typically omitted, or copied verbatim from an interactive session.
The traceback stack is followed by the most interesting part: the line(s) containing the exception type and detail.
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: multi
line
detail
The last three lines (starting with ValueError) are compared against the exception’s type and detail, and the rest are
ignored.
Best practice is to omit the traceback stack, unless it adds significant documentation value to the example.
Traceback (most recent call last):

ValueError: multi
(continues on next page)
1 Examples containing both expected output and an exception are not supported.
Trying to guess where one ends and the other begins is too
error-prone, and that also makes for a confusing test.
In particular, in the rewritten example, the use of ... is independent
of doctest’s ELLIPSIS option.
The ellipsis in that example could be left out, or could just as well be three (or three
hundred) commas or digits, or an indented transcript of a Monty Python skit.
Some details you should read once, but won’t need to remember:
• Doctest can’t guess whether your expected output came from an exception traceback or from ordinary printing.
So, e.g., an example that expects ValueError: 42 is prime will pass whether ValueError is actually
raised or if the example merely prints that traceback text.
In practice, ordinary output rarely begins with a traceback
header line, so this doesn’t create real problems.
The first line following the traceback header indented the same and starting
with an alphanumeric is taken to be the start of the exception detail.
Of course this does the right thing for genuine
tracebacks.
The interactive shell omits the traceback header line for some SyntaxErrors.
But doctest uses the trace-
back header line to distinguish exceptions from non-exceptions.
So in the rare case where you need to test a
SyntaxError that omits the traceback header, you will need to manually add the traceback header line to your
test example.
For some exceptions, Python displays the position of the error using ^ markers and tildes:
File "<stdin>", line 1
1 + None
~~^~~~~~
TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
Since the lines showing the position of the error come before the exception type and detail, they are not checked
by doctest.
File "<stdin>", line 1
1 + None
^~~~~~~~
TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
Option Flags
A number of option flags control various aspects of doctest’s behavior.
Symbolic names for the flags are supplied as
module constants, which can be bitwise ORed together and passed to various functions.
The names can also be used in
doctest directives, and may be passed to the doctest command line interface via the -o option.
The -o command line option.
By default, if an expected output block contains just 1, an actual output block containing just 1 or just True is
considered to be a match, and similarly for 0 versus False.
When DONT_ACCEPT_TRUE_FOR_1 is specified,
neither substitution is allowed.
The default behavior caters to that Python changed the return type of many functions
from integer to boolean; doctests expecting “little integer” output still work in these cases.
This option will probably
go away, but not for several years.
By default, if an expected output block contains a line containing only the string <BLANKLINE>, then that line
will match a blank line in the actual output.
Because a genuinely blank line delimits the expected output, this is
the only way to communicate that a blank line is expected.
When DONT_ACCEPT_BLANKLINE is specified, this
substitution is not allowed.
NORMALIZE_WHITESPACE
When specified, all sequences of whitespace (blanks and newlines) are treated as equal.
Any sequence of whitespace
within the expected output will match any sequence of whitespace within the actual output.
By default, whitespace
must match exactly.
NORMALIZE_WHITESPACE is especially useful when a line of expected output is very long,
and you want to wrap it across multiple lines in your source.
doctest.
ELLIPSIS
When specified, an ellipsis marker (...) in the expected output can match any substring in the actual output.
This includes substrings that span line boundaries, and empty substrings, so it’s best to keep usage of this simple.
IGNORE_EXCEPTION_DETAIL
When specified, doctests expecting exceptions pass so long as an exception of the expected type is raised, even if
the details (message and fully qualified exception name) don’t match.
For example, an example expecting ValueError: 42 will pass if the actual exception raised is ValueError:
3*14, but will fail if, say, a TypeError is raised instead.
It will also ignore any fully qualified name included
before the exception class, which can vary between implementations and versions of Python and the code/libraries
in use.
Hence, all three of these variations will work with the flag specified:
Traceback (most recent call last):
Exception: message
Traceback (most recent call last):
builtins.
SKIP
When specified, do not run the example at all.
This can be useful in contexts where doctest examples serve as both
documentation and test cases, and an example should be included for documentation purposes, but should not be
checked.
E.g., the example’s output might be random; or the example might depend on resources which would be
unavailable to the test driver.
The SKIP flag can also be used for temporarily “commenting out” examples.
doctest.
COMPARISON_FLAGS
A bitmask or’ing together all the comparison flags above.
The second group of options controls how test failures are reported:
doctest.
REPORT_UDIFF
When specified, failures that involve multi-line expected and actual outputs are displayed using a unified diff.
REPORT_CDIFF
When specified, failures that involve multi-line expected and actual outputs will be displayed using a context diff.
doctest.
REPORT_NDIFF
When specified, differences are computed by difflib.
Differ, using the same algorithm as the popular
ndiff.py utility.
This is the only method that marks differences within lines as well as across lines.
For example,
if a line of expected output contains digit 1 where actual output contains letter l, a line is inserted with a caret
marking the mismatching column positions.
REPORT_ONLY_FIRST_FAILURE
When specified, display the first failing example in each doctest, but suppress output for all remaining examples.
This will prevent doctest from reporting correct examples that break because of earlier failures; but it might also
hide incorrect examples that fail independently of the first failure.
When REPORT_ONLY_FIRST_FAILURE is
specified, the remaining examples are still run, and still count towards the total number of failures reported; only
the output is suppressed.
FAIL_FAST
When specified, exit after the first failing example and don’t attempt to run the remaining examples.
Thus, the
number of failures reported will be at most 1.
This flag may be useful during debugging, since examples after the
first failure won’t even produce debugging output.
The doctest command line accepts the option -f as a shorthand for -o FAIL_FAST.
New in version 3.4.
doctest.
REPORTING_FLAGS
A bitmask or’ing together all the reporting flags above.
Create a new option flag with a given name, and return the new flag’s integer value.
Doctest directives are special Python
comments following an example’s source code:
directive
::=
"#" "doctest:" directive_options
directive_options
::=
directive_option ("," directive_option)*
directive_option
::=
on_or_off directive_option_name
on_or_off
::=
"+" | "-"
directive_option_name
::=
"DONT_ACCEPT_BLANKLINE" | "NORMALIZE_WHITESPACE" | ...
Whitespace is not allowed between the + or - and the directive option name.
The directive option name can be any of
the option flag names explained above.
An example’s doctest directives modify doctest’s behavior for that single example.
Use + to enable the named behavior,
or - to disable it.
For example, this test passes:
# doctest: +NORMALIZE_WHITESPACE
[0,
1,
2,
3,
4,
5,
6,
7,
8,
9,
10,
11, 12, 13, 14, 15, 16, 17, 18, 19]
Without the directive it would fail, both because the actual output doesn’t have two blanks before the single-digit list
elements, and because the actual output is on a single line.
This test also passes, and also requires a directive to do so:
# doctest: +ELLIPSIS
[0, 1, ..., 18, 19]
Multiple directives can be used on a single physical line, separated by commas:
# doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
[0,
1, ...,
18,
19]
If multiple directive comments are used for a single example, then they are combined:
# doctest: +ELLIPSIS

# doctest: +NORMALIZE_WHITESPACE
[0,
1, ...,
18,
19]
As the previous example shows, you can add ... lines to your example containing only directives.
However, option flags can also be passed to functions that run
doctests, establishing different defaults.
In such cases, disabling an option via - in a directive can be useful.
If even a single character doesn’t match, the test
fails.
This will probably surprise you a few times, as you learn exactly what Python does and doesn’t guarantee about
output.
For example, when printing a set, Python doesn’t guarantee that the element is printed in any particular order, so
a test like
{"Hermione", "Harry"}
is vulnerable!
One workaround is to do
True
instead.
Another is to do
['Harry', 'Hermione']
There are others, but you get the idea.
The ELLIPSIS directive gives a nice approach for the last example:
# doctest: +ELLIPSIS
<C object at 0x...>
Floating-point numbers are also subject to small output variations across platforms, because Python defers to the platform
C library for float formatting, and C libraries vary widely in quality here.
Basic API
The functions testmod() and testfile() provide a simple interface to doctest that should be sufficient for most
basic uses.
All arguments except filename are optional, and should be specified in keyword form.
Test examples in the file named filename.
Return (failure_count, test_count).
If module_relative is True (the default), then filename specifies an OS-independent module-relative path.
By
default, this path is relative to the calling module’s directory; but if the package argument is specified, then
it is relative to that package.
To ensure OS-independence, filename should use / characters to separate path
segments, and may not be an absolute path (i.e., it may not begin with /).
If module_relative is False, then filename specifies an OS-specific path.
The path may be absolute or
relative; relative paths are resolved with respect to the current working directory.
Optional argument name gives the name of the test; by default, or if None, os.path.basename(filename)
is used.
Optional argument package is a Python package or the name of a Python package whose directory should be used
as the base directory for a module-relative filename.
If no package is specified, then the calling module’s directory
is used as the base directory for module-relative filenames.
It is an error to specify package if module_relative is
False.
Optional argument globs gives a dict to be used as the globals when executing examples.
A new shallow copy of
this dict is created for the doctest, so its examples start with a clean slate.
By default, or if None, a new empty dict
is used.
Optional argument extraglobs gives a dict merged into the globals used to execute examples.
This works like dict.
update(): if globs and extraglobs have a common key, the associated value in extraglobs appears in the combined
dict.
By default, or if None, no extra globals are used.
This is an advanced feature that allows parameterization of
doctests.
For example, a doctest can be written for a base class, using a generic name for the class, then reused to
test any number of subclasses by passing an extraglobs dict mapping the generic name to the subclass to be tested.
Optional argument verbose prints lots of stuff if true, and prints only failures if false; by default, or if None, it’s
true if and only if '-v' is in sys.argv.
Optional argument report prints a summary at the end when true, else prints nothing at the end.
In verbose mode,
the summary is detailed, else the summary is very brief (in fact, empty if all tests passed).
Optional argument optionflags (default value 0) takes the bitwise OR of option flags.
See section Option Flags.
Optional argument raise_on_error defaults to false.
If true, an exception is raised upon the first failure or unexpected
exception in an example.
This allows failures to be post-mortem debugged.
Default behavior is to continue running
examples.
Optional argument parser specifies a DocTestParser (or subclass) that should be used to extract tests from the
files.
It defaults to a normal parser (i.e., DocTestParser()).
Optional argument encoding specifies an encoding that should be used to convert the file to unicode.
All arguments are optional, and all except for m should be specified in keyword form.
Also test examples reachable from dict m.__test__, if it exists and is not None.
Only docstrings attached to objects belonging to module m are searched.
Return (failure_count, test_count).
Optional argument name gives the name of the module; by default, or if None, m.__name__ is used.
Optional argument exclude_empty defaults to false.
If true, objects for which no doctests are found are excluded
from consideration.
The default is a backward compatibility hack, so that code still using doctest.master.
summarize() in conjunction with testmod() continues to get output for objects with no tests.
The ex-
clude_empty argument to the newer DocTestFinder constructor defaults to true.
Optional arguments extraglobs, verbose, report, optionflags, raise_on_error, and globs are the same as for function
testfile() above, except that globs defaults to m.__dict__.
A shallow copy of dictionary argument globs is used for the execution context.
Optional argument name is used in failure messages, and defaults to "NoName".
If optional argument verbose is true, output is generated even if there are no failures.
By default, output is generated
only in case of an example failure.
Optional argument compileflags gives the set of flags that should be used by the Python compiler when running the
examples.
By default, or if None, flags are deduced corresponding to the set of future features found in globs.
Optional argument optionflags works as for function testfile() above.
Unittest API
As your collection of doctest’ed modules grows, you’ll want a way to run all their doctests systematically.
TestSuite instances from text files and modules with doctests:
doctest.
DocFileSuite(*paths, module_relative=True, package=None, setUp=None, tearDown=None,
globs=None, optionflags=0, parser=DocTestParser(), encoding=None)
Convert doctest tests from one or more text files to a unittest.
The returned unittest.
TestSuite is to be run by the unittest framework and runs the interactive examples in
each file.
If an example in any file fails, then the synthesized unit test fails, and a failureException exception
is raised showing the name of the file containing the test and a (sometimes approximate) line number.
Pass one or more paths (as strings) to text files to be examined.
Options may be provided as keyword arguments:
Optional argument module_relative specifies how the filenames in paths should be interpreted:
• If module_relative is True (the default), then each filename in paths specifies an OS-independent module-
relative path.
By default, this path is relative to the calling module’s directory; but if the package argument
is specified, then it is relative to that package.
To ensure OS-independence, each filename should use /
characters to separate path segments, and may not be an absolute path (i.e., it may not begin with /).
If module_relative is False, then each filename in paths specifies an OS-specific path.
The path may be
absolute or relative; relative paths are resolved with respect to the current working directory.
Optional argument package is a Python package or the name of a Python package whose directory should be
used as the base directory for module-relative filenames in paths.
If no package is specified, then the calling
module’s directory is used as the base directory for module-relative filenames.
It is an error to specify package if
module_relative is False.
Optional argument setUp specifies a set-up function for the test suite.
This is called before running the tests in each
file.
The setUp function will be passed a DocTest object.
The setUp function can access the test globals as the
globs attribute of the test passed.
Optional argument tearDown specifies a tear-down function for the test suite.
This is called after running the tests
in each file.
The tearDown function will be passed a DocTest object.
The setUp function can access the test
globals as the globs attribute of the test passed.
Optional argument globs is a dictionary containing the initial global variables for the tests.
A new copy of this
dictionary is created for each test.
By default, globs is a new empty dictionary.
Optional argument optionflags specifies the default doctest options for the tests, created by or-ing together individual
option flags.
See section Option Flags.
See function set_unittest_reportflags() below for a better way
to set reporting options.
Optional argument parser specifies a DocTestParser (or subclass) that should be used to extract tests from the
files.
It defaults to a normal parser (i.e., DocTestParser()).
Optional argument encoding specifies an encoding that should be used to convert the file to unicode.
TestSuite.
The returned unittest.
TestSuite is to be run by the unittest framework and runs each doctest in the module.
If any of the doctests fail, then the synthesized unit test fails, and a failureException exception is raised
showing the name of the file containing the test and a (sometimes approximate) line number.
Optional argument module provides the module to be tested.
It can be a module object or a (possibly dotted) module
name.
If not specified, the module calling this function is used.
Optional argument globs is a dictionary containing the initial global variables for the tests.
A new copy of this
dictionary is created for each test.
By default, globs is a new empty dictionary.
Optional argument extraglobs specifies an extra set of global variables, which is merged into globs.
By default, no
extra globals are used.
Optional arguments setUp, tearDown, and optionflags are the same as for function DocFileSuite() above.
This function uses the same search technique as testmod().
Changed in version 3.5: DocTestSuite() returns an empty unittest.
TestSuite if module contains no
docstrings instead of raising ValueError.
Under the covers, DocTestSuite() creates a unittest.
TestSuite out of doctest.
DocTestCase in-
stances, and DocTestCase is a subclass of unittest.
TestCase.
DocTestCase isn’t documented here (it’s an
internal detail), but studying its code can answer questions about the exact details of unittest integration.
Similarly, DocFileSuite() creates a unittest.
TestSuite out of doctest.
DocFileCase instances, and
DocFileCase is a subclass of DocTestCase.
So both ways of creating a unittest.
TestSuite run instances of DocTestCase.
This is important for a subtle
reason: when you run doctest functions yourself, you can control the doctest options in use directly, by passing
option flags to doctest functions.
However, if you’re writing a unittest framework, unittest ultimately controls
when and how tests get run.
The framework author typically wants to control doctest reporting options (perhaps, e.g.,
specified by command line options), but there’s no way to pass options through unittest to doctest test runners.
For this reason, doctest also supports a notion of doctest reporting flags specific to unittest support, via this
function:
doctest.set_unittest_reportflags(flags)
Set the doctest reporting flags to use.
Argument flags takes the bitwise OR of option flags.
See section Option Flags.
Only “reporting flags” can be used.
This is a module-global setting, and affects all future doctests run by module unittest: the runTest()
method of DocTestCase looks at the option flags specified for the test case when the DocTestCase in-
stance was constructed.
If no reporting flags were specified (which is the typical and expected case), doctest’s
unittest reporting flags are bitwise ORed into the option flags, and the option flags so augmented are passed
to the DocTestRunner instance created to run the doctest.
If any reporting flags were specified when the
DocTestCase instance was constructed, doctest’s unittest reporting flags are ignored.
The value of the unittest reporting flags in effect before the function was called is returned by the function.
It is fairly flexible, and should meet most
users’ needs; however, if you require more fine-grained control over testing, or wish to extend doctest’s capabilities, then
you should use the advanced API.
The advanced API revolves around two container classes, which are used to store the interactive examples extracted from
doctest cases:
• Example: A single Python statement, paired with its expected output.
A collection of doctest examples that should be run in a single namespace.
The constructor arguments are used to
initialize the attributes of the same names.
DocTest defines the following attributes.
They are initialized by the constructor, and should not be modified
directly.
This is a dictionary mapping names to
values.
Any changes to the namespace made by the examples (such as binding new variables) will be reflected
in globs after the test is run.
Typically, this is the name of the object or file that the test was
extracted from.
This line number is zero-based with respect to the beginning of the file.
A single interactive example, consisting of a Python statement and its expected output.
The constructor arguments
are used to initialize the attributes of the same names.
Example defines the following attributes.
They are initialized by the constructor, and should not be modified
directly.
This source code consists of a single Python statement, and
always ends with a newline; the constructor adds a newline when necessary.
want
The expected output from running the example’s source code (either from stdout, or a traceback in case of
exception).
The
constructor adds a newline when necessary.
This exception message is compared against the return
value of traceback.format_exception_only().
The constructor adds a newline if needed.
This line number is
zero-based with respect to the beginning of the containing string.
Any option flags not contained in this dictionary are left at their default value (as specified by the
DocTestRunner’s optionflags).
By default, no options are set.
DocTestFinder objects
class doctest.
A processing class used to extract the DocTests that are relevant to a given object, from its docstring and the
docstrings of its contained objects.
DocTests can be extracted from modules, classes, functions, methods, stat-
icmethods, classmethods, and properties.
The optional argument verbose can be used to display the objects searched by the finder.
It defaults to False (no
output).
The optional argument parser specifies the DocTestParser object (or a drop-in replacement) that is used to
extract doctests from docstrings.
If the optional argument recurse is false, then DocTestFinder.find() will only examine the given object,
and not any contained objects.
If the optional argument exclude_empty is false, then DocTestFinder.find() will include tests for objects
with empty docstrings.
The optional argument name specifies the object’s name; this name will be used to construct names for the
returned DocTests.
If name is not specified, then obj.__name__ is used.
The optional parameter module is the module that contains the given object.
If the module is not specified or
is None, then the test finder will attempt to automatically determine the correct module.
As a default namespace, if globs is not specified.
If module is False, no attempt to find the module will be made.
This is obscure, of use mostly in test-
ing doctest itself: if module is False, or is None but cannot be found automatically, then all objects are
considered to belong to the (non-existent) module, so all contained objects will (recursively) be searched for
doctests.
The globals for each DocTest is formed by combining globs and extraglobs (bindings in extraglobs override
bindings in globs).
A new shallow copy of the globals dictionary is created for each DocTest.
If extraglobs is not specified,
then it defaults to {}.
DocTestParser objects
class doctest.
DocTestParser
A processing class used to extract interactive examples from a string, and use them to create a DocTest object.
Extract all doctest examples from the given string, and collect them into a DocTest object.
See the documentation for
DocTest for more information.
Extract all doctest examples from the given string, and return them as a list of Example objects.
Line
numbers are 0-based.
The optional argument name is a name identifying this string, and is only used for error
messages.
Line numbers for the Examples are 0-based.
The optional argument name is a name identifying
this string, and is only used for error messages.
A processing class used to execute and verify the interactive examples in a DocTest.
The comparison between expected outputs and actual outputs is done by an OutputChecker.
This comparison
may be customized with a number of option flags; see section Option Flags for more information.
If the option
flags are insufficient, then the comparison may also be customized by passing a subclass of OutputChecker to
the constructor.
The test runner’s display output can be controlled in two ways.
First, an output function can be passed to
TestRunner.run(); this function will be called with strings that should be displayed.
It defaults to sys.
stdout.write.
If capturing the output is not sufficient, then the display output can be also customized
by subclassing DocTestRunner, and overriding the methods report_start(), report_success(),
report_unexpected_exception(), and report_failure().
The optional keyword argument checker specifies the OutputChecker object (or drop-in replacement) that
should be used to compare the expected outputs to the actual outputs of doctest examples.
The optional keyword argument verbose controls the DocTestRunner’s verbosity.
If verbose is True, then
information is printed about each example, as it is run.
If verbose is False, then only failures are printed.
If
verbose is unspecified, or None, then verbose output is used iff the command-line switch -v is used.
The optional keyword argument optionflags can be used to control how the test runner compares expected output
to actual output, and how it displays failures.
For more information, see section Option Flags.
DocTestParser defines the following methods:
report_start(out, test, example)
Report that the test runner is about to process the given example.
This method is provided to allow subclasses
of DocTestRunner to customize their output; it should not be called directly.
Report that the given example ran successfully.
This method is provided to allow subclasses of
DocTestRunner to customize their output; it should not be called directly.
Report that the given example failed.
This method is provided to allow subclasses of DocTestRunner to
customize their output; it should not be called directly.
This method is provided to allow subclasses
of DocTestRunner to customize their output; it should not be called directly.
The examples are run in the namespace test.globs.
If clear_globs is true (the default), then this names-
pace will be cleared after the test runs, to help with garbage collection.
If you would like to examine the
namespace after the test completes, then use clear_globs=False.
compileflags gives the set of flags that should be used by the Python compiler when running the examples.
If
not specified, then it will default to the set of future-import flags that apply to globs.
The output of each example is checked using the DocTestRunner’s output checker, and the results are
formatted by the DocTestRunner.report_*() methods.
The optional verbose argument controls how detailed the summary is.
If the verbosity is not specified, then
the DocTestRunner’s verbosity is used.
OutputChecker objects
class doctest.
OutputChecker
A class used to check the whether the actual output from a doctest example matches the expected output.
OutputChecker defines two methods: check_output(), which compares a given pair of outputs, and
returns True if they match; and output_difference(), which returns a string describing the differences
between two outputs.
OutputChecker defines the following methods:
check_output(want, got, optionflags)
Return True iff the actual output from an example (got) matches the expected output (want).
These strings
are always considered to match if they are identical; but depending on what option flags the test runner is
using, several non-exact match types are also possible.
See section Option Flags for more information about
option flags.
output_difference(example, got, optionflags)
Return a string describing the differences between the expected output for a given example (example) and the
actual output (got).
The DebugRunner class is a subclass of DocTestRunner that raises an exception for the first failing example,
containing information about that example.
This information can be used to perform post-mortem debugging on
the example.
Then you can inspect current values of variables, and so on.
For example, suppose a.py
contains just this module docstring:
26.4.
Convert text with examples to a script.
Argument s is a string containing doctest examples.
The string is converted to a Python script, where doctest
examples in s are converted to regular code, and everything else is converted to Python comments.
The generated
script is returned as a string.
For example,
import doctest
print(doctest.script_from_examples(r"""
Set x and y to 1 and 2.
Argument module is a module object, or dotted name of a module, containing the object whose doctests are of inter-
est.
Argument name is the name (within the module) of the object with the doctests of interest.
The result is a string,
containing the object’s docstring converted to a Python script, as described for script_from_examples()
above.
For example, if module a.py contains a top-level function f(), then
import a, doctest
print(doctest.testsource(a, "a.f"))
prints a script version of function f()’s docstring, with doctests converted to code, and the rest placed in comments.
Debug the doctests for an object.
The module and name arguments are the same as for function testsource() above.
The synthesized Python
script for the named object’s docstring is written to a temporary file, and then that file is run under the control of
the Python debugger, pdb.
A shallow copy of module.__dict__ is used for both local and global execution context.
Optional argument pm controls whether post-mortem debugging is used.
If pm has a true value, the script file is
run directly, and the debugger gets involved only if the script terminates via raising an unhandled exception.
If
it does, then post-mortem debugging is invoked, via pdb.post_mortem(), passing the traceback object from
the unhandled exception.
If pm is not specified, or is false, the script is run under the debugger from the start, via
passing an appropriate exec() call to pdb.run().
This is like function debug() above, except that a string containing doctest examples is specified directly, via the
src argument.
Optional argument pm has the same meaning as in function debug() above.
Optional argument globs gives a dictionary to use as both local and global execution context.
If not specified, or
None, an empty dictionary is used.
If specified, a shallow copy of the dictionary is used.
The DebugRunner class, and the special exceptions it may raise, are of most interest to testing framework authors,
and will only be sketched here.
A subclass of DocTestRunner that raises an exception as soon as a failure is encountered.
If an unexpected
exception occurs, an UnexpectedException exception is raised, containing the test, the example, and the
original exception.
If the output doesn’t match, then a DocTestFailure exception is raised, containing the
test, the example, and the actual output.
For information about the constructor parameters and methods, see the documentation for DocTestRunner in
section Advanced API.
There are two exceptions that may be raised by DebugRunner instances:
exception doctest.
An exception raised by DocTestRunner to signal that a doctest example’s actual output did not match its ex-
pected output.
The constructor arguments are used to initialize the attributes of the same names.
DocTestFailure defines the following attributes:
DocTestFailure.test
The DocTest object that was being run when the example failed.
The Example that failed.
DocTestFailure.got
The example’s actual output.
UnexpectedException(test, example, exc_info)
An exception raised by DocTestRunner to signal that a doctest example raised an unexpected exception.
The
constructor arguments are used to initialize the attributes of the same names.
UnexpectedException defines the following attributes:
UnexpectedException.test
The DocTest object that was being run when the example failed.
UnexpectedException.example
The Example that failed.
UnexpectedException.exc_info
A tuple containing information about the unexpected exception, as returned by sys.exc_info().
Checking examples in docstrings.
Executable documentation / literate testing.
These uses have different requirements, and it is important to distinguish them.
In particular, filling your docstrings with
obscure test cases makes for bad documentation.
When writing a docstring, choose docstring examples with care.
There’s an art to this that needs to be learned—it may
not be natural at first.
Examples should add genuine value to the documentation.
A good example can often be worth
many words.
If done with care, the examples will be invaluable for your users, and will pay back the time it takes to
collect them many times over as the years go by and things change.
I’m still amazed at how often one of my doctest
examples stops working after a “harmless” change.
By inter-
leaving prose and examples, it becomes much easier to keep track of what’s actually being tested, and why.
When a test
fails, good prose can make it much easier to figure out what the problem is, and how it should be fixed.
It’s true that
you could write extensive comments in code-based testing, but few programmers do.
Many have found that using doctest
approaches instead leads to much clearer tests.
Perhaps this is simply because doctest makes writing prose a little easier
than writing code, while writing comments in code is a little harder.
I think it goes deeper than just that: the natural
attitude when writing a doctest-based test is that you want to explain the fine points of your software, and illustrate them
with examples.
This in turn naturally leads to test files that start with the simplest features, and logically progress to com-
plications and edge cases.
A coherent narrative is the result, instead of a collection of isolated functions that test isolated
bits of functionality seemingly at random.
It’s a different attitude, and produces different results, blurring the distinction
between testing and explaining.
Regression testing is best confined to dedicated objects or files.
There are several options for organizing tests:
• Write text files containing test cases as interactive examples, and test the files using testfile() or
DocFileSuite().
This is recommended, although is easiest to do for new projects, designed from the start
to use doctest.
These functions can be included in the same file as the module, or separated out into a separate test file.
When you have placed your tests in a module, the module can itself be the test runner.
When a test fails, you can arrange
for your test runner to re-run only the failing doctest while you debug the problem.
REPORT_NDIFF|doctest.
The unittest unit testing framework was originally inspired by JUnit and has a similar flavor as major unit testing
frameworks in other languages.
It supports test automation, sharing of setup and shutdown code for tests, aggregation of
tests into collections, and independence of the tests from the reporting framework.
To achieve this, unittest supports some important concepts in an object-oriented way:
26.5.
A test fixture represents the preparation needed to perform one or more tests, and any associated cleanup
actions.
This may involve, for example, creating temporary or proxy databases, directories, or starting a server
process.
test case A test case is the individual unit of testing.
It checks for a specific response to a particular set of inputs.
It is used to aggregate tests that should be executed
together.
The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of
executing the tests.
See also:
Module doctest Another test-support module with a very different flavor.
Simple Smalltalk Testing: With Patterns Kent Beck’s original paper on testing frameworks using the pattern shared
by unittest.
For example, assert func(10)
== 42.
The Python Testing Tools Taxonomy An extensive list of Python testing tools including functional testing frameworks
and mock object libraries.
A special-interest-group for discussion of testing, and testing tools, in Python.
The script Tools/unittestgui/unittestgui.py in the Python source distribution is a GUI tool for test dis-
covery and execution.
This is intended largely for ease of use for those new to unit testing.
For production environments
it is recommended that tests be driven by a continuous integration system such as Buildbot, Jenkins, GitHub Actions, or
AppVeyor.
Basic example
The unittest module provides a rich set of tools for constructing and running tests.
This section demonstrates that a
small subset of the tools suffice to meet the needs of most users.
Here is a short script to test three string methods:
import unittest
class TestStringMethods(unittest.
TestCase.
The three individual tests are defined with methods whose
names start with the letters test.
This naming convention informs the test runner about which methods represent tests.
The crux of each test is a call to assertEqual() to check for an expected result; assertTrue() or
assertFalse() to verify a condition; or assertRaises() to verify that a specific exception gets raised.
These
methods are used instead of the assert statement so the test runner can accumulate all test results and produce a report.
The setUp() and tearDown() methods allow you to define instructions that will be executed before and after each
test method.
They are covered in more detail in the section Organizing test code.
The final block shows a simple way to run the tests.
When run from the command line, the above script produces an output that looks like this:

----------------------------------------------------------------------
Ran 3 tests in 0.000s
OK
Passing the -v option to your test script will instruct unittest.main() to enable a higher level of verbosity, and
produce the following output:
test_isupper (__main__.TestStringMethods.test_isupper) ...
The above examples show the most commonly used unittest features which are sufficient to meet many everyday
testing needs.
The remainder of the documentation explores the full feature set from first principles.
The behavior of returning a value from a test method (other than the default None value), is
now deprecated.
TestClass
python -m unittest test_module.
TestClass.test_method
You can pass in a list with any combination of module names, and fully qualified class or method names.
This allows you to use the shell filename completion to specify the test module.
The file specified must still be importable
as a module.
If you
want to execute a test file that isn’t importable as a module you should execute the file directly instead.
In earlier versions it was only possible to run individual test methods and not modules or classes.
Command-line options
unittest supports these command-line options:
-b, --buffer
The standard output and standard error streams are buffered during the test run.
Output during a passing test is
discarded.
Output is echoed normally on test fail or error and is added to the failure messages.
A second
Control-C raises the normal KeyboardInterrupt exception.
See Signal Handling for the functions that provide this functionality.
This option may be used multiple times, in
which case all test cases that match any of the given patterns are included.
Patterns that contain a wildcard character (*) are matched against the test name using fnmatch.
Patterns are matched against the fully qualified test method name as imported by the test loader.
For example, -k foo matches foo_tests.
SomeTest.test_something, bar_tests.
SomeTest.
The command-line options -b, -c and -f were added.
The command-line option --locals.
The command-line option -k.
The command-line option --durations.
Unittest supports simple test discovery.
In order to be compatible with test discovery, all of the test files must be modules or
packages importable from the top-level directory of the project (this means that their filenames must be valid identifiers).
Test discovery is implemented in TestLoader.discover(), but can also be used from the command line.
The
basic command-line usage is:
cd project_directory
python -m unittest discover
Note: As a shortcut, python -m unittest is the equivalent of python -m unittest discover.
If you
want to pass arguments to test discovery the discover sub-command must be used explicitly.
The -s, -p, and -t options can be passed in as positional arguments in that order.
The package name you supply will then be imported and its location on the filesystem will be used as the
start directory.
Caution:
Test discovery loads tests by importing them.
Once test discovery has found all the test files from the
start directory you specify it turns the paths into package names to import.
For example foo/bar/baz.py will be
imported as foo.bar.baz.
If you have a package installed globally and attempt test discovery on a different copy of the package then the import
could happen from the wrong place.
If this happens test discovery will warn you and exit.
If you supply the start directory as a package name rather than a path to a directory then discover assumes that
whichever location it imports from is the location you intended, so you will not get the warning.
Test modules and packages can customize test loading and discovery by through the load_tests protocol.
Test discovery supports namespace packages for the start directory.
Note that you need to specify
the top level directory too (e.g. python -m unittest discover -s root/namespace -t root).
Python 3.11 dropped the namespace packages support.
It has been broken since Python 3.7.
Start directory and subdirectories containing tests must be regular package that have __init__.py file.
In this case, you need to specify start directory
as dotted package name, and target directory explicitly.
For example:
# proj/
<-- current directory
#
namespace/
#
mypkg/
#
__init__.py
#
test_mypkg.py
python -m unittest discover -s namespace.mypkg -t .
The basic building blocks of unit testing are test cases — single scenarios that must be set up and checked for correctness.
In unittest, test cases are represented by unittest.
TestCase instances.
To make your own test cases you must
write subclasses of TestCase or use FunctionTestCase.
The testing code of a TestCase instance should be entirely self contained, such that it can be run either in isolation or
in arbitrary combination with any number of other test cases.
The simplest TestCase subclass will simply implement a test method (i.e. a method whose name starts with test) in
order to perform specific testing code:
import unittest
class DefaultWidgetSizeTestCase(unittest.
Note that in order to test something, we use one of the assert*() methods provided by the TestCase base class.
If the test fails, an exception will be raised with an explanatory message, and unittest will identify the test case as a
failure.
Any other exceptions will be treated as errors.
Tests can be numerous, and their set-up can be repetitive.
Note: The order in which the various tests will be run is determined by sorting the test method names with respect to
the built-in ordering for strings.
If the setUp() method raises an exception while the test is running, the framework will consider the test to have suffered
an error, and the test method will not be executed.
If setUp() succeeded, tearDown() will be run whether the test method succeeded or not.
Such a working environment for the testing code is called a test fixture.
A new TestCase instance is created as a unique
test fixture used to execute each individual test method.
Thus setUp(), tearDown(), and __init__() will be
called once per test.
It is recommended that you use TestCase implementations to group tests together according to the features they test.
In most
cases, calling unittest.main() will do the right thing and collect all the module’s test cases for you and execute
them.
The test module can be run standalone from the command line.
The test code can more easily be separated from shipped code.
If the testing strategy changes, there is no need to change the source code.
Re-using old test code
Some users will find that they have existing test code that they would like to run from unittest, without converting
every old test function to a TestCase subclass.
For this reason, unittest provides a FunctionTestCase class.
This subclass of TestCase can be used to wrap
an existing test function.
Set-up and tear-down functions can also be provided.
Given the following test function:

assert something.name is not None
# ...
one can create an equivalent test case instance as follows, with optional set-up and tear-down methods:
testcase = unittest.
Note: Even though FunctionTestCase can be used to quickly convert an existing test base over to a unittest-
based system, this approach is not recommended.
Taking the time to set up proper TestCase subclasses will make
future test refactorings infinitely easier.
In some cases, the existing tests may have been written using the doctest module.
If so, doctest provides a
DocTestSuite class that can automatically build unittest.
TestSuite instances from the existing doctest-
based tests.
Unittest supports skipping individual test methods and even whole classes of tests.
In addition, it supports marking a test
as an “expected failure,” a test that is broken and will fail, but shouldn’t be counted as a failure on a TestResult.
Skipping a test is simply a matter of using the skip() decorator or one of its conditional variants, calling TestCase.
Basic skipping looks like this:
class MyTestCase(unittest.
TestCase):

TestCase.setUp() can also skip the test.
This is useful when a resource that needs to be set up is not available.
Expected failures use the expectedFailure() decorator.
It’s easy to roll your own skipping decorators by making a decorator that calls skip() on the test when it wants it to be
skipped.
The following decorators and exception implement test skipping and expected failures:
@unittest.skip(reason)
Unconditionally skip the decorated test.
If the test fails or errors in the test function itself (rather than in one
of the test fixture methods) then it will be considered a success.
If the test passes, it will be considered a failure.
This exception is raised to skip a test.
Usually you can use TestCase.skipTest() or one of the skipping decorators instead of raising this directly.
Skipped tests will not have setUp() or tearDown() run around them.
Skipped classes will not
have setUpClass() or tearDownClass() run.
Skipped modules will not have setUpModule() or
tearDownModule() run.
Distinguishing test iterations using subtests
New in version 3.4.
When there are very small differences among your tests, for instance some parameters, unittest allows you to distinguish
them inside the body of a test method using the subTest() context manager.
For example, the following test:
class NumbersTest(unittest.
TestCase):

Test that numbers between 0 and 5 are all even.
This section describes in depth the API of unittest.
Test cases
class unittest.
Instances of the TestCase class represent the logical test units in the unittest universe.
This class is intended
to be used as a base class, with specific tests being implemented by concrete subclasses.
This class implements the
interface needed by the test runner to allow it to drive the tests, and methods that the test code can use to check for
and report various kinds of failure.
Each instance of TestCase will run a single base method: the method named methodName.
In most uses of
TestCase, you will neither change the methodName nor reimplement the default runTest() method.
Changed in version 3.2: TestCase can be instantiated successfully without providing a methodName.
This makes
it easier to experiment with TestCase from the interactive interpreter.
TestCase instances provide three groups of methods: one group used to run the test, another used by the test
implementation to check conditions and report failures, and some inquiry methods allowing information about the
test itself to be gathered.
Method called to prepare the test fixture.
This is called immediately before calling the test method; other than
AssertionError or SkipTest, any exception raised by this method will be considered an error rather
than a test failure.
The default implementation does nothing.
This is called even
if the test method raised an exception, so the implementation in subclasses may need to be particularly careful
about checking internal state.
Any exception, other than AssertionError or SkipTest, raised by this
method will be considered an additional error rather than a test failure (thus increasing the total number of
reported errors).
This method will only be called if the setUp() succeeds, regardless of the outcome of
the test method.
The default implementation does nothing.
A class method called before tests in an individual class are run.
See Class and Module Fixtures for more details.
New in version 3.2.
See Class and Module Fixtures for more details.
New in version 3.2.
If result is omitted or None,
a temporary result object is created (by calling the defaultTestResult() method) and used.
The result
object is returned to run()’s caller.
The same effect may be had by simply calling the TestCase instance.
Changed in version 3.3: Previous versions of run did not return the result.
Neither did calling an instance.
skipTest(reason)
Calling this during a test method or setUp() skips the current test.
See Skipping tests and expected failures
for more information.
New in version 3.1.
A test case can contain any number of subtest declarations, and they can be arbitrarily nested.
See Distinguishing test iterations using subtests for more information.
New in version 3.4.
This allows exceptions raised by the test to be propagated to the
caller, and can be used to support running tests under a debugger.
The TestCase class provides several assert methods to check for and report failures.
Note that the msg keyword argument can be passed to assertRaises(),
assertRaisesRegex(), assertWarns(), assertWarnsRegex() only when they are used as a con-
text manager.
assertEqual(first, second, msg=None)
Test that first and second are equal.
If the values do not compare equal, the test will fail.
In addition, if first and second are the exact same type and one of list, tuple, dict, set, frozenset or str or any
type that a subclass registers with addTypeEqualityFunc() the type-specific equality function will be
called in order to generate a more useful default error message (see also the list of type-specific methods).
Changed in version 3.1: Added the automatic calling of type-specific equality function.
Changed in version 3.2: assertMultiLineEqual() added as the default type equality function for
comparing strings.
If the values do compare equal, the test will fail.
Note
that
this
is
equivalent
to
bool(expr) is True
and
not
to
expr is True
(use
assertIs(expr, True) for the latter).
This method should also be avoided when more specific
methods are available (e.g. assertEqual(a, b) instead of assertTrue(a == b)), because they
provide a better error message in case of failure.
None.
New in version 3.1.
New in version 3.1.
To check for the exact type, use assertIs(type(obj), cls).
New in version 3.2.
The with block does not log on
logger with minimum level
3.10
assertRaises(exception, callable, *args, **kwds)
assertRaises(exception, *, msg=None)
Test that an exception is raised when callable is called with any positional or keyword arguments that are
also passed to assertRaises().
The test passes if exception is raised, is an error if another exception
is raised, or fails if no exception is raised.
To catch any of a group of exceptions, a tuple containing the
exception classes may be passed as exception.
If only the exception and possibly the msg arguments are given, return a context manager so that the code
under test can be written inline rather than as a function:
with self.assertRaises(SomeException):
do_something()
When used as a context manager, assertRaises() accepts the additional keyword argument msg.
The context manager will store the caught exception object in its exception attribute.
Added the exception attribute.
Added the msg keyword argument when used as a context manager.
assertRaisesRegex(exception, regex, callable, *args, **kwds)
assertRaisesRegex(exception, regex, *, msg=None)
Like assertRaises() but also tests that regex matches on the string representation of the raised exception.
Examples:
self.assertRaisesRegex(ValueError, "invalid literal for.*XYZ'$",
int, 'XYZ')
or:
with self.assertRaisesRegex(ValueError, 'literal'):
int('XYZ')
New in version 3.1: Added under the name assertRaisesRegexp.
Changed in version 3.2: Renamed to assertRaisesRegex().
Added the msg keyword argument when used as a context manager.
The test passes if warning is triggered and fails if it isn’t.
Any exception is an
error.
To catch any of a group of warnings, a tuple containing the warning classes may be passed as warnings.
If only the warning and possibly the msg arguments are given, return a context manager so that the code under
test can be written inline rather than as a function:
with self.assertWarns(SomeWarning):
do_something()
When used as a context manager, assertWarns() accepts the additional keyword argument msg.
The context manager will store the caught warning object in its warning attribute, and the source line which
triggered the warnings in the filename and lineno attributes.
This method works regardless of the warning filters in place when it is called.
New in version 3.2.
Added the msg keyword argument when used as a context manager.
New in version 3.2.
Added the msg keyword argument when used as a context manager.
A context manager to test that at least one message is logged on the logger or one of its children, with at least
the given level.
If given, logger should be a logging.
Logger object or a str giving the name of a logger.
The default is
the root logger, which will catch all messages that were not blocked by a non-propagating descendent logger.
If given, level should be either a numeric logging level or its string equivalent (for example either "ERROR"
or logging.
ERROR).
The default is logging.
INFO.
The test passes if at least one message emitted inside the with block matches the logger and level conditions,
otherwise it fails.
The object returned by the context manager is a recording helper which keeps tracks of the matching log
messages.
It has two attributes:
records
A list of logging.
LogRecord objects of the matching log messages.
A context manager to test that no messages are logged on the logger or one of its children, with at least the
given level.
If given, logger should be a logging.
Logger object or a str giving the name of a logger.
The default is
the root logger, which will catch all messages.
If given, level should be either a numeric logging level or its string equivalent (for example either "ERROR"
or logging.
ERROR).
The default is logging.
INFO.
There are also other methods used to perform more specific checks, such as:
Method
Checks that
New
in
assertAlmostEqual(a,
b)
round(a-b, 7) == 0
assertNotAlmostEqual(a,
b)
round(a-b, 7) !
Note that these methods round
the values to the given number of decimal places (i.e. like the round() function) and not significant digits.
If delta is supplied instead of places then the difference between first and second must be less or equal to (or
greater than) delta.
Supplying both delta and places raises a TypeError.
Changed in version 3.2: assertAlmostEqual() automatically considers almost equal objects that com-
pare equal.
Added the
delta keyword argument.
AssertionError: "3" unexpectedly not greater than or equal to "4"
New in version 3.1.
assertRegex(text, regex, msg=None)
assertNotRegex(text, regex, msg=None)
Test that a regex search matches (or does not match) text.
In case of failure, the error message will include the
pattern and the text (or the pattern and the part of text that unexpectedly matched).
New in version 3.1: Added under the name assertRegexpMatches.
New in version 3.2: assertNotRegex().
When they don’t, an
error message listing the differences between the sequences will be generated.
Duplicate elements are not ignored when comparing first and second.
It verifies whether each element
has the same count in both sequences.
New in version 3.2.
The assertEqual() method dispatches the equality check for objects of the same type to different type-specific
methods.
Registers a type-specific method called by assertEqual() to check if two objects of exactly the same
typeobj (not subclasses) compare equal.
It must raise self.failureException(msg)
when inequality between the first two parameters is detected – possibly providing useful information and
explaining the inequalities in details in the error message.
New in version 3.1.
The list of type-specific methods automatically used by assertEqual() are summarized in the following table.
Note that it’s usually not necessary to invoke these methods directly.
Method
Used to compare
New in
assertMultiLineEqual(a, b)
strings
3.1
assertSequenceEqual(a, b)
sequences
3.1
assertListEqual(a, b)
lists
3.1
assertTupleEqual(a, b)
tuples
3.1
assertSetEqual(a, b)
sets or frozensets
3.1
assertDictEqual(a, b)
dicts
3.1
assertMultiLineEqual(first, second, msg=None)
Test that the multiline string first is equal to the string second.
When not equal a diff of the two strings
highlighting the differences will be included in the error message.
This method is used by default when
comparing strings with assertEqual().
New in version 3.1.
If a seq_type is supplied, both first and second must be instances of
seq_type or a failure will be raised.
If the sequences are different an error message is constructed that shows
the difference between the two.
This
method
is
not
called
directly
by
assertEqual(),
but
it’s
used
to
implement
assertListEqual() and assertTupleEqual().
New in version 3.1.
If not, an error message is constructed that shows only the differences
between the two.
An error is also raised if either of the parameters are of the wrong type.
These methods
are used by default when comparing lists or tuples with assertEqual().
New in version 3.1.
If not, an error message is constructed that lists the differences between the sets.
This method is used by default when comparing sets or frozensets with assertEqual().
Fails if either of first or second does not have a set.difference() method.
New in version 3.1.
assertDictEqual(first, second, msg=None)
Test that two dictionaries are equal.
If not, an error message is constructed that shows the differences in the
dictionaries.
This method will be used by default to compare dictionaries in calls to assertEqual().
New in version 3.1.
Finally the TestCase provides the following methods and attributes:
fail(msg=None)
Signals a test failure unconditionally, with msg or None for the error message.
This class attribute gives the exception raised by the test method.
If a test framework needs to use a specialized
exception, possibly to carry additional information, it must subclass this exception in order to “play fair” with
the framework.
The initial value of this attribute is AssertionError.
True is the default value.
In this case, the custom message is appended to the
end of the standard failure message.
When set to False, the custom message replaces the standard message.
The class setting can be overridden in individual test methods by assigning an instance attribute,
self.longMessage, to True or False before calling the assert methods.
The class setting gets reset before each test call.
New in version 3.1.
It
defaults to 80*8 characters.
Assert methods affected by this attribute are assertSequenceEqual()
(including all the sequence comparison methods that delegate to it), assertDictEqual() and
assertMultiLineEqual().
Setting maxDiff to None means that there is no maximum length of diffs.
New in version 3.2.
Testing frameworks can use the following methods to collect information on the test:
countTestCases()
Return the number of tests represented by this test object.
For TestCase instances, this will always be 1.
26.5.
For TestCase instances, this will always be an instance of TestResult; subclasses of TestCase should
override this as necessary.
This is usually the full name of the test method, including
the module and class name.
The default implementation
of this method returns the first line of the test method’s docstring, if available, or None.
Changed in version 3.1: In 3.1 this was changed to add the test name to the short description even in the
presence of a docstring.
This caused compatibility issues with unittest extensions and adding the test name
was moved to the TextTestResult in Python 3.2.
Functions will be
called in reverse order to the order they are added (LIFO).
They are called with any arguments and keyword
arguments passed into addCleanup() when they are added.
If setUp() fails, meaning that tearDown() is not called, then any cleanup functions added will still be
called.
New in version 3.1.
If successful, also add its __exit__() method as a cleanup function
by addCleanup() and return the result of the __enter__() method.
New in version 3.11.
This method is called unconditionally after tearDown(), or after setUp() if setUp() raises an excep-
tion.
It is responsible for calling all the cleanup functions added by addCleanup().
If you need cleanup functions
to be called prior to tearDown() then you can call doCleanups() yourself.
New in version 3.1.
classmethod addClassCleanup(function, /, *args, **kwargs)
Add a function to be called after tearDownClass() to cleanup resources used during the test class.
Func-
tions will be called in reverse order to the order they are added (LIFO).
They are called with any arguments
and keyword arguments passed into addClassCleanup() when they are added.
If setUpClass() fails, meaning that tearDownClass() is not called, then any cleanup functions added
will still be called.
New in version 3.8.
This method is called unconditionally after tearDownClass(),
or after setUpClass() if
setUpClass() raises an exception.
It is responsible for calling all the cleanup functions added by addClassCleanup().
If you need cleanup
functions to be called prior to tearDownClass() then you can call doClassCleanups() yourself.
New in version 3.8.
This class provides an API similar to TestCase and also accepts coroutines as test functions.
New in version 3.8.
Method called to prepare the test fixture.
This is called after setUp().
This is called immediately before
calling the test method; other than AssertionError or SkipTest, any exception raised by this method
will be considered an error rather than a test failure.
Method called immediately after the test method has been called and the result recorded.
This is called
before tearDown().
This is called even if the test method raised an exception, so the implementation
in subclasses may need to be particularly careful about checking internal state.
Any exception, other than
AssertionError or SkipTest, raised by this method will be considered an additional error rather than
a test failure (thus increasing the total number of reported errors).
This method will only be called if the
asyncSetUp() succeeds, regardless of the outcome of the test method.
The default implementation does
nothing.
This method accepts a coroutine that can be used as a cleanup function.
If successful, also add its __aexit__() method as a
cleanup function by addAsyncCleanup() and return the result of the __aenter__() method.
New in version 3.11.
If result is omitted or None, a temporary result object is created (by calling the defaultTestResult()
method) and used.
The result object is returned to run()’s caller.
At the end of the test all the tasks in the
event loop are cancelled.
This class implements the portion of the TestCase interface which allows the test runner to drive the test, but
does not provide the methods which test code can use to check and report errors.
This is used to create test cases
using legacy test code, allowing it to be integrated into a unittest-based test framework.
Grouping tests
class unittest.
This class represents an aggregation of individual test cases and test suites.
The class presents the interface needed
by the test runner to allow it to be run as any other test case.
Running a TestSuite instance is the same as
iterating over the suite, running each test individually.
If tests is given, it must be an iterable of individual test cases or other test suites that will be used to build the suite
initially.
Additional methods are provided to add test cases and suites to the collection later on.
TestSuite objects behave much like TestCase objects, except they do not actually implement a test.
Instead,
they are used to aggregate tests into groups of tests that should be run together.
Some additional methods are
available to add tests to TestSuite instances:
addTest(test)
Add a TestCase or TestSuite to the suite.
This is equivalent to iterating over tests, calling addTest() for each element.
Note
that unlike TestCase.run(), TestSuite.run() requires the result object to be passed in.
This allows exceptions raised by the test
to be propagated to the caller and can be used to support running tests under a debugger.
Subclasses can lazily provide tests by
overriding __iter__().
Note that this method may be called several times on a single suite (for ex-
ample when counting tests or comparing for equality) so the tests returned by repeated iterations before
TestSuite.run() must be the same for each call iteration.
After TestSuite.run(), callers should
not rely on the tests returned by this method unless the caller uses a subclass that overrides TestSuite.
_removeTestAtIndex() to preserve test references.
In earlier versions the TestSuite accessed tests directly rather than through iter-
ation, so overriding __iter__() wasn’t sufficient for providing tests.
In earlier versions the TestSuite held references to each TestCase
after TestSuite.run().
Subclasses can restore that behavior by overriding TestSuite.
_removeTestAtIndex().
In the typical usage of a TestSuite object, the run() method is invoked by a TestRunner rather than by
the end-user test harness.
Loading and running tests
class unittest.
TestLoader
The TestLoader class is used to create test suites from classes and modules.
Normally, there is no need to
create an instance of this class; the unittest module provides an instance that can be shared as unittest.
Using a subclass or instance, however, allows customization of some configurable prop-
erties.
TestLoader objects have the following attributes:
errors
A list of the non-fatal errors encountered while loading tests.
Not reset by the loader at any point.
Fatal errors
are signalled by the relevant method raising an exception to the caller.
Non-fatal errors are also indicated by
a synthetic test that will raise the original error when run.
New in version 3.5.
TestLoader objects have the following methods:
loadTestsFromTestCase(testCaseClass)
Return a suite of all test cases contained in the TestCase-derived testCaseClass.
A test case instance is created for each method named by getTestCaseNames().
By default these
are the method names beginning with test.
If getTestCaseNames() returns no methods, but the
runTest() method is implemented, a single test case is created for that method instead.
This method searches module for classes derived
from TestCase and creates an instance of the class for each test method defined for the class.
Note:
While using a hierarchy of TestCase-derived classes can be convenient in sharing fixtures and
helper functions, defining test methods on base classes that are not intended to be instantiated directly does
not play well with this method.
Doing so, however, can be useful when the fixtures are different and defined
in subclasses.
If a module provides a load_tests function it will be called to load the tests.
This allows modules to
customize test loading.
This is the load_tests protocol.
The pattern argument is passed as the third argument
to load_tests.
Changed in version 3.2: Support for load_tests added.
Changed in version 3.5: Support for a keyword-only argument pattern has been added.
Changed in version 3.12: The undocumented and unofficial use_load_tests parameter has been removed.
The specifier name is a “dotted name” that may resolve either to a module, a test case class, a test method within
a test case class, a TestSuite instance, or a callable object which returns a TestCase or TestSuite
instance.
These checks are applied in the order listed here; that is, a method on a possible test case class will
be picked up as “a test method within a test case class”, rather than “a callable object”.
For
example,
if
you
have
a
module
SampleTests
containing
a
TestCase-derived
class
SampleTestCase with three test methods (test_one(), test_two(), and test_three()), the
specifier 'SampleTests.
SampleTestCase' would cause this method to return a suite which will run
all three test methods.
Using the specifier 'SampleTests.
SampleTestCase.test_two' would
cause it to return a test suite which will run only the test_two() test method.
The specifier can refer to
modules and packages which have not been imported; they will be imported as a side-effect.
The method optionally resolves name relative to the given module.
If an ImportError or AttributeError occurs while traversing name then
a synthetic test that raises that error when run will be returned.
These errors are included in the errors
accumulated by self.errors.
The return
value is a test suite which supports all the tests defined for each name.
Only test files that match pattern will be loaded.
Only module names that are importable (i.e. are valid Python identifiers) will be loaded.
All test modules must be importable from the top level of the project.
If the start directory is not the top level
directory then the top level directory must be specified separately.
If importing a module fails, for example due to a syntax error, then this will be recorded as a single error and
discovery will continue.
If a package (a directory containing a file named __init__.py) is found, the package will be checked
for a load_tests function.
If this exists then it will be called package.load_tests(loader,
tests, pattern).
Test discovery takes care to ensure that a package is only checked for tests once
during an invocation, even if the load_tests function itself calls loader.discover.
If load_tests exists then discovery does not recurse into the package, load_tests is responsible for
loading all tests in the package.
The pattern is deliberately not stored as a loader attribute so that packages can continue discovery themselves.
New in version 3.2.
Modules that raise SkipTest on import are recorded as skips, not errors.
Paths are sorted before being imported so that execution order is the same even if
the underlying file system’s ordering is not dependent on file name.
Changed in version 3.5: Found packages are now checked for load_tests regardless of whether their path
matches pattern, because it is impossible for a package name to match the default pattern.
Changed in version 3.11: start_dir can not be a namespace packages.
It has been broken since Python 3.7
and Python 3.11 officially remove it.
The following attributes of a TestLoader can be configured either by subclassing or assignment on an instance:
testMethodPrefix
String giving the prefix of method names which will be interpreted as test methods.
The default value is
'test'.
This affects getTestCaseNames() and all the loadTestsFrom*() methods.
No methods on the resulting object are needed.
The default value is the TestSuite class.
This affects all the loadTestsFrom*() methods.
If this attribute is not None (the default), all test methods to be included in test suites must match one of the
patterns in this list.
Note that matches are always performed using fnmatch.fnmatchcase(), so unlike
patterns passed to the -k option, simple substring patterns will have to be converted using * wildcards.
This affects all the loadTestsFrom*() methods.
New in version 3.7.
TestResult
This class is used to compile information about which tests have succeeded and which have failed.
The TestCase and TestSuite classes ensure that
results are properly recorded; test authors do not need to worry about recording the outcome of tests.
Testing frameworks built on top of unittest may want access to the TestResult object generated by running
a set of tests for reporting purposes; a TestResult instance is returned by the TestRunner.run() method
for this purpose.
TestResult instances have the following attributes that will be of interest when inspecting the results of running
a set of tests:
errors
A list containing 2-tuples of TestCase instances and strings holding formatted tracebacks.
Each tuple
represents a test which raised an unexpected exception.
Each tuple
represents a test where a failure was explicitly signalled using the TestCase.assert*() methods.
New in version 3.1.
Each tuple
represents an expected failure or error of the test case.
New in version 3.12.
shouldStop
Set to True when the execution of tests should stop by stop().
Collected output will only be echoed onto the real sys.stdout and sys.
Any output is also attached to the failure / error message.
New in version 3.2.
New in version 3.2.
Returns False if there were any unexpectedSuccesses from tests marked
with the expectedFailure() decorator.
This method can be called to signal that the set of tests being run should be aborted by setting the
shouldStop attribute to True.
TestRunner objects should respect this flag and return without running
any additional tests.
For example, this feature is used by the TextTestRunner class to stop the test framework when the user
signals an interrupt from the keyboard.
Interactive tools which provide TestRunner implementations can
use this in a similar manner.
The following methods of the TestResult class are used to maintain the internal data structures, and may be
extended in subclasses to support additional reporting requirements.
This is particularly useful in building tools
which support interactive reporting while tests are being run.
startTest(test)
Called when the test case test is about to be run.
New in version 3.1.
New in version 3.1.
The default implementation appends a tuple (test, formatted_err) to the instance’s errors at-
tribute, where formatted_err is a formatted traceback derived from err.
addFailure(test, err)
Called when the test case test signals a failure.
The default implementation appends a tuple (test, formatted_err) to the instance’s failures
attribute, where formatted_err is a formatted traceback derived from err.
The default implementation does nothing.
The default implementation appends a tuple (test, reason) to the instance’s skipped attribute.
The
default
implementation
appends
a
tuple
(test, formatted_err)
to
the
instance’s
expectedFailures attribute, where formatted_err is a formatted traceback derived from err.
The default implementation appends the test to the instance’s unexpectedSuccesses attribute.
addSubTest(test, subtest, outcome)
Called when a subtest finishes.
If outcome is None, the subtest succeeded.
Otherwise, it failed with an exception where outcome is a tuple
of the form returned by sys.exc_info(): (type, value, traceback).
The default implementation does nothing when the outcome is a success, and records subtest failures as normal
failures.
New in version 3.4.
New in version 3.12.
A concrete implementation of TestResult used by the TextTestRunner.
Subclasses should accept
**kwargs to ensure compatibility as the interface changes.
New in version 3.2.
New in version 3.12: Added durations keyword argument.
If no customization of the TestLoader is needed,
this instance can be used instead of repeatedly creating new instances.
If stream is None, the default, sys.stderr
is used as the output stream.
This class has a few configurable parameters, but is essentially very simple.
Graphical
applications which run test suites should provide alternate implementations.
Such implementations should accept
**kwargs as the interface to construct runners changes when features are added to unittest.
By
default
this
runner
shows
DeprecationWarning,
PendingDeprecationWarning,
ResourceWarning and ImportWarning even if they are ignored by default.
This behavior can be
overridden using Python’s -Wd or -Wa options (see Warning control) and leaving warnings to None.
Changed in version 3.2: Added the warnings parameter.
Changed in version 3.2: The default stream is set to sys.stderr at instantiation time rather than import time.
Added the tb_locals parameter.
Changed in version 3.12: Added the durations parameter.
This method returns the instance of TestResult used by run().
It is not intended to be called directly,
but can be overridden in subclasses to provide a custom TestResult.
_makeResult() instantiates the class or callable passed in the TextTestRunner constructor as the
resultclass argument.
It defaults to TextTestResult if no resultclass is provided.
This method is the main public interface to the TextTestRunner.
This method takes a TestSuite or
TestCase instance.
A TestResult is created by calling _makeResult() and the test(s) are run and
the results printed to stdout.
The defaultTest argument is either the name of a single test or an iterable of test names to run if no test names are
specified via argv.
If not specified or None and no test names are provided via argv, all tests found in module are
run.
The argv argument can be a list of options passed to the program, with the first element being the program name.
If not specified or None, the values of sys.argv are used.
The testRunner argument can either be a test runner class or an already created instance of it.
By default main calls
sys.exit() with an exit code indicating success (0) or failure (1) of the tests run.
An exit code of 5 indicates
that no tests were run.
The testLoader argument has to be a TestLoader instance, and defaults to defaultTestLoader.
This displays
the result on standard output without calling sys.exit():
The failfast, catchbreak and buffer parameters have the same effect as the same-name command-line options.
The warnings argument specifies the warning filter that should be used while running the tests.
If it’s not speci-
fied, it will remain None if a -W option is passed to python (see Warning control), otherwise it will be set to
'default'.
Calling main actually returns an instance of the TestProgram class.
This stores the result of the tests run as
the result attribute.
Changed in version 3.1: The exit parameter was added.
The verbosity, failfast, catchbreak, buffer and warnings parameters were added.
The defaultTest parameter was changed to also accept an iterable of test names.
Modules or packages can customize how tests are loaded from them during normal test runs or test discovery by imple-
menting a function called load_tests.
If a test module defines load_tests it will be called by TestLoader.loadTestsFromModule() with the
following arguments:
load_tests(loader, standard_tests, pattern)
where pattern is passed straight through from loadTestsFromModule.
It defaults to None.
It should return a TestSuite.
loader is the instance of TestLoader doing the loading.
It is common for test modules to only want to add or remove tests from the standard set of tests.
The
third argument is used when loading packages as part of test discovery.
A typical load_tests function that loads tests from a specific set of TestCase classes may look like:
test_cases = (TestCase1, TestCase2, TestCase3)

for test_class in test_cases:
tests = loader.loadTestsFromTestCase(test_class)
suite.addTests(tests)

If discovery is started in a directory containing a package, either from the command line or by calling TestLoader.
discover(), then the package __init__.py will be checked for load_tests.
If that function does not exist,
discovery will recurse into the package as though it were just another directory.
This should return a TestSuite representing all the tests from the package.
Because the pattern is passed into load_tests the package is free to continue (and potentially modify) test discovery.
Discovery no longer checks package names for matching pattern due to the impossibility of
package names matching the default pattern.
When the test suite encounters a test from a new class
then tearDownClass() from the previous class (if there is one) is called, followed by setUpClass() from the
new class.
Similarly if a test is from a different module from the previous test then tearDownModule from the previous module
is run, followed by setUpModule from the new module.
After all the tests have run the final tearDownClass and tearDownModule are run.
Note that shared fixtures do not play well with [potential] features like test parallelization and they break test isolation.
They should be used with care.
The default ordering of tests created by the unittest test loaders is to group all tests from the same modules and classes
together.
This will lead to setUpClass / setUpModule (etc) being called exactly once per class and module.
If you
randomize the order, so that tests from different modules and classes are adjacent to each other, then these shared fixture
functions may be called multiple times in a single test run.
Shared fixtures are not intended to work with suites with non-standard ordering.
A BaseTestSuite still exists for
frameworks that don’t want to support shared fixtures.
If there are any exceptions raised during one of the shared fixture functions the test is reported as an error.
Because there
is no corresponding test instance an _ErrorHolder object (that has the same interface as a TestCase) is created to
represent the error.
If you are just using the standard unittest test runner then this detail doesn’t matter, but if you are a
framework author it may be relevant.
If you want the setUpClass and tearDownClass on base classes called then you must call up to them yourself.
The implementations in TestCase are empty.
If an exception is raised during a setUpClass then the tests in the class are not run and the tearDownClass is not
run.
Skipped classes will not have setUpClass or tearDownClass run.
If the exception is a SkipTest exception
then the class will be reported as having been skipped instead of as an error.
If an exception is raised in a setUpModule then none of the tests in the module will be run and the tearDownModule
will not be run.
If the exception is a SkipTest exception then the module will be reported as having been skipped instead
of as an error.
To add cleanup code that must be run even in the case of an exception, use addModuleCleanup:
unittest.addModuleCleanup(function, /, *args, **kwargs)
Add a function to be called after tearDownModule() to cleanup resources used during the test class.
Functions
will be called in reverse order to the order they are added (LIFO).
They are called with any arguments and keyword
arguments passed into addModuleCleanup() when they are added.
If setUpModule() fails, meaning that tearDownModule() is not called, then any cleanup functions added
will still be called.
New in version 3.8.
If successful, also add its __exit__() method as a cleanup function by
addModuleCleanup() and return the result of the __enter__() method.
New in version 3.11.
This function is called unconditionally after tearDownModule(),
or after setUpModule() if
setUpModule() raises an exception.
It is responsible for calling all the cleanup functions added by addModuleCleanup().
If you need cleanup
functions to be called prior to tearDownModule() then you can call doModuleCleanups() yourself.
New in version 3.8.
The -c/--catch command-line option to unittest, along with the catchbreak parameter to unittest.main(),
provide more friendly handling of control-C during a test run.
With catch break behavior enabled control-C will allow
the currently running test to complete, and the test run will then end and report all the results so far.
A second control-c
will raise a KeyboardInterrupt in the usual way.
The control-c handling signal handler attempts to remain compatible with code or tests that install their own signal.
SIGINT handler.
If the unittest handler is called but isn’t the installed signal.
SIGINT handler, i.e. it has been
replaced by the system under test and delegated to, then it calls the default handler.
This will normally be the expected
behavior by code that replaces an installed handler and delegates to it.
When a signal.
Register a TestResult object for control-c handling.
Registering a result stores a weak reference to it, so it
doesn’t prevent the result from being garbage collected.
Registering a TestResult object has no side-effects if control-c handling is not enabled, so test frameworks can
unconditionally register all results they create independently of whether or not handling is enabled.
Remove a registered result.
Once a result has been removed then stop() will no longer be called on that result
object in response to a control-c.
unittest.removeHandler(function=None)
When called without arguments this function removes the control-c handler if it has been installed.
This function
can also be used as a test decorator to temporarily remove the handler while the test is being executed:


26.6 unittest.mock — mock object library
New in version 3.3.
Source code: Lib/unittest/mock.py
unittest.mock is a library for testing in Python.
It allows you to replace parts of your system under test with mock
objects and make assertions about how they have been used.
After performing an action, you can make assertions about which methods / attributes were used and arguments they were
called with.
You can also specify return values and set needed attributes in the normal way.
Additionally, mock provides a patch() decorator that handles patching module and class level attributes within the
scope of a test, along with sentinel for creating unique objects.
See the quick guide for some examples of how to use
Mock, MagicMock and patch().
Mock is designed for use with unittest and is based on the ‘action -> assertion’ pattern instead of ‘record -> replay’
used by many mocking frameworks.
There is a backport of unittest.mock for earlier versions of Python, available as mock on PyPI.
Mock has many other ways you can configure it and control its behaviour.
For example the spec argument configures the
mock to take its specification from another object.
Attempting to access attributes or methods on the mock that don’t
exist on the spec will fail with an AttributeError.
The patch() decorator / context manager makes it easy to mock classes or objects in a module under test.
The object
you specify will be replaced with a mock (or other object) during the test and restored when the test ends:
... @patch('module.
ClassName1()

module.
ClassName1

assert MockClass2 is module.
Note: When you nest patch decorators the mocks are passed in to the decorated function in the same order they applied
(the normal Python order that decorators are applied).
This means from the bottom up, so in the example above the mock
for module.
ClassName1 is passed in first.
This is normally straight-
forward, but for a quick guide read where to patch.
Mock supports the mocking of Python magic methods.
The easiest way of using magic methods is with the MagicMock
class.
Mock allows you to assign functions (or other Mock instances) to magic methods and they will be called appropriately.
The MagicMock class is just a Mock variant that has all of the magic methods pre-created for you (well, all the useful
ones anyway).
The following is an example of using magic methods with the ordinary Mock class:
'wheeeeee'
For ensuring that the mock objects in your tests have the same api as the objects they are replacing, you can use auto-
speccing.
Auto-speccing can be done through the autospec argument to patch, or the create_autospec() function.
Auto-speccing creates mock objects that have the same attributes and methods as the objects they are replacing, and any
functions and methods (including constructors) have the same call signature as the real object.
This ensures that your mocks will fail in the same way as your production code if they are used incorrectly:


'fishy'
(continues on next page)
26.6.
The Mock Class
Mock is a flexible mock object intended to replace the use of stubs and test doubles throughout your code.
Mocks are
callable and create attributes as new mocks when you access them1.
Accessing the same attribute will always return the
same mock.
Mocks record how you use them, allowing you to make assertions about what your code has done to them.
MagicMock is a subclass of Mock with all the magic methods pre-created and ready to use.
NonCallableMock and
NonCallableMagicMock
The patch() decorators makes it easy to temporarily replace classes in a particular module with a Mock object.
By
default patch() will create a MagicMock for you.
You can specify an alternative class of Mock using the new_callable
argument to patch().
This can be either a list of strings or an existing object (a class or instance) that acts as the specification
for the mock object.
If you pass in an object then a list of strings is formed by calling dir on the object
(excluding unsupported magic attributes and methods).
Accessing any attribute not in this list will raise an
AttributeError.
This
allows mocks to pass isinstance() tests.
If used, attempting to set or get an attribute on the mock that isn’t on the
object passed as spec_set will raise an AttributeError.
See the side_effect attribute.
Useful
for raising exceptions or dynamically changing return values.
The function is called with the same arguments
as the mock, and unless it returns DEFAULT, the return value of this function is used as the return value.
Alternatively side_effect can be an exception class or instance.
In this case the exception will be raised when
the mock is called.
If side_effect is an iterable then each call to the mock will return the next value from the iterable.
A side_effect can be cleared by setting it to None.
• return_value: The value returned when the mock is called.
By default this is a new Mock (created on first
access).
See the return_value attribute.
Passing unsafe=True will allow access to these attributes.
New in version 3.5.
The only exceptions are magic methods and attributes (those that have leading and trailing double underscores).
Mock doesn’t create these but
instead raises an AttributeError.
This is because the interpreter will often implicitly request these methods, and gets very confused to get a new
Mock object when it expects a magic method.
If wraps is not None then calling the Mock will pass the call
through to the wrapped object (returning the real result).
Attribute access on the mock will return a Mock
object that wraps the corresponding attribute of the wrapped object (so attempting to access an attribute that
doesn’t exist will raise an AttributeError).
If the mock has an explicit return_value set then calls are not passed to the wrapped object and the return_value
is returned instead.
This can be useful for debugging.
The name is propagated to child mocks.
Mocks can also be called with arbitrary keyword arguments.
These will be used to set attributes on the mock after
it is created.
Assert that the mock was called at least once.
Assert that the mock was called exactly once.
Called 2 times.
New in version 3.6.
Assert that the mock was called exactly once and that call was with the specified arguments.
Traceback (most recent call last):

AssertionError: Expected 'mock' to be called once.
Called 2 times.
The assert passes if the mock has ever been called,
unlike assert_called_with() and
assert_called_once_with() that only pass if the call is the most recent one, and in the case of
assert_called_once_with() it must also be the only call.
The mock_calls list is checked for the calls.
If any_order is false then the calls must be sequential.
There can be extra calls before or after the specified
calls.
Assert the mock was never called.
Traceback (most recent call last):

AssertionError: Expected 'hello' to not have been called.
Called 1 times.
New in version 3.5.
The reset_mock method resets all the call attributes on a mock object:
True
False
Changed in version 3.6: Added two keyword-only arguments to the reset_mock function.
This can be useful where you want to make a series of assertions that reuse the same object.
In case you want to reset return_value or side_effect, then pass the
corresponding parameter as True.
Child mocks and the return value mock (if any) are reset as well.
Note: return_value, and side_effect are keyword-only arguments.
Only attributes on the spec can be
fetched as attributes from the mock.
If spec_set is true then only attributes on the spec can be set.
Calls to the attached mock will be
recorded in the method_calls and mock_calls attributes of this one.
configure_mock(**kwargs)
Set attributes on the mock through keyword arguments.
KeyError
The same thing can be achieved in the constructor call to mocks:
'eggs'

Traceback (most recent call last):

KeyError
configure_mock() exists to make it easier to do configuration after the mock has been created.
Mock objects limit the results of dir(some_mock) to useful results.
For mocks with a spec this includes
all the permitted attributes for the mock.
See FILTER_DIR for what this filtering does, and how to switch it off.
By default child mocks will be the same type as the
parent.
Subclasses of Mock may want to override this to customize the way child mocks are made.
For non-callable mocks the callable variant will be used (rather than any custom subclass).
The default return value is a mock object and you can configure it in the normal way:
<Mock name='mock()()' id='...'>
return_value can also be set in the constructor:


side_effect
This can either be a function to be called when the mock is called, an iterable or an exception (class or instance)
to be raised.
If you pass in a function it will be called with same arguments as the mock and unless the function returns
the DEFAULT singleton the call to the mock will then return whatever the function returns.
If the function
returns DEFAULT then the mock will return its normal value (from the return_value).
If you pass in an iterable, it is used to retrieve an iterator which must yield a value on every call.
This value can
either be an exception instance to be raised, or a value to be returned from the call to the mock (DEFAULT
handling is identical to the function case).
Using side_effect to return a sequence of values:
(3, 2, 1)
Using a callable:




side_effect can be set in the constructor.
Traceback (most recent call last):

KeyError

call_args
This is either None (if the mock hasn’t been called), or the arguments that the mock was last called with.
This will be in the form of a tuple: the first member, which can also be accessed through the args property,
is any ordered arguments the mock was called with (or an empty tuple) and the second member, which can
also be accessed through the kwargs property, is any keyword arguments (or an empty dictionary).
None
(continues on next page)
26.6.
These are tuples, so they can be unpacked to get at the individual arguments and make
more complex assertions.
See calls as tuples.
Changed in version 3.8: Added args and kwargs properties.
This is a list of all the calls made to the mock object in sequence (so the length of the list is the number of
times it has been called).
Before any calls have been made it is an empty list.
The call object can be used
for conveniently constructing lists of calls to compare with call_args_list.
These can be unpacked as tuples to get at the individual
arguments.
See calls as tuples.
See calls as tuples.
These can be unpacked as tuples to get at the individual
arguments.
See calls as tuples.
For a mock object with a spec,
__class__ returns the spec class instead.
True
class unittest.mock.
A non-callable version of Mock.
The constructor parameters have the same meaning of Mock, with the exception
of return_value and side_effect which have no meaning on a non-callable mock.
26.6.
True
True
The Mock classes have support for mocking magic methods.
See magic methods for the full details.
The mock classes and the patch() decorators all take arbitrary keyword arguments for configuration.
For the patch()
decorators the keywords are passed to the constructor of the mock being created.
The return value and side effect of child mocks can be set in the same way, using dotted notation.
As you can’t use dotted
names directly in a call you have to create a dictionary and unpack it using **:
'eggs'

Traceback (most recent call last):

KeyError
A callable mock which was created with a spec (or a spec_set) will introspect the specification object’s signature when
matching calls to the mock.
This applies to assert_called_with(), assert_called_once_with(), assert_has_calls() and
assert_any_call().
When Autospeccing, it will also apply to method calls on the mock object.
Changed in version 3.4: Added signature introspection on specced and autospecced mock objects.
A mock intended to be used as a property, or other descriptor, on a class.
PropertyMock provides __get__()
and __set__() methods so you can specify a return value when it is fetched.
Fetching a PropertyMock instance from an object calls the mock, with no args.
Setting it calls the mock with
the value being set.
Instead you
can attach it to the mock type object:

class unittest.mock.
An asynchronous version of MagicMock.
The AsyncMock object will behave so the object is recognized as an
async function, and the result of a call is an awaitable.
True
True
The result of mock() is an async function which will have the outcome of side_effect or return_value
after it has been awaited:
• if side_effect is a function, the async function will return the result of that function,
• if side_effect is an exception, the async function will raise the exception,
• if side_effect is an iterable, the async function will return the next value of the iterable, however, if the
sequence of result is exhausted, StopAsyncIteration is raised immediately,
• if side_effect is not defined, the async function will return the value defined by return_value,
hence, by default, the async function returns a new AsyncMock object.
Setting the spec of a Mock or MagicMock to an async function will result in a coroutine object being returned
after calling.
Assert that the mock was awaited at least once.
True
Traceback (most recent call last):

AssertionError: Expected mock to have been awaited.
Assert that the mock was awaited exactly once.
Traceback (most recent call last):

AssertionError: Expected mock to have been awaited once.
Assert that the last await was with the specified arguments.
Traceback (most recent call last):

AssertionError: expected call not found.
Assert that the mock was awaited exactly once and with the specified arguments.
Traceback (most recent call last):

AssertionError: Expected mock to have been awaited once.
Awaited 2 times.
Assert the mock has been awaited with the specified calls.
The await_args_list list is checked for the
awaits.
If any_order is false then the awaits must be sequential.
There can be extra calls before or after the specified
awaits.
Awaits not found.
Also sets await_count to 0, await_args to None, and clears the
await_args_list.
This is a list of all the awaits made to the mock object in sequence (so the length of the list is the number of
times it has been awaited).
Before any awaits have been made it is an empty list.
The call will return the value set as the return_value attribute.
The default return value
is a new Mock object; it is created the first time the return value is accessed (either explicitly or by calling the Mock) -
but it is stored and the same one returned each time.
Calls made to the object will be recorded in the attributes like call_args and call_args_list.
If side_effect is set then it will be called after the call has been recorded, so if side_effect raises an exception
the call is still recorded.
If side_effect is a function then whatever that function returns is what calls to the mock return.
The side_effect
function is called with the same arguments as the mock.
This allows you to vary the return value of the call dynamically,
based on the input:


(continues on next page)
26.6.
If you want the mock to still return the default return value (a new mock), or any set return value, then there are two ways
of doing this.
Either return mock.return_value from inside side_effect, or return DEFAULT:








To remove a side_effect, and return to the default behaviour, set the side_effect to None:





The side_effect can also be any iterable object.
ValueError

Deleting Attributes
Mock objects create attributes on demand.
This allows them to pretend to be objects of any type.
You may want a mock object to return False to a hasattr() call, or raise an AttributeError when an attribute
is fetched.
You can do this by providing an object as a spec for a mock, but that isn’t always convenient.
You “block” attributes by deleting them.
Once deleted, accessing an attribute will raise an AttributeError.
AttributeError: f
Mock names and the name attribute
Since “name” is an argument to the Mock constructor, if you want your mock object to have a “name” attribute you can’t
just pass it in at creation time.
There are two alternatives.
One option is to use configure_mock():
'my_name'
A simpler option is to simply set the “name” attribute after mock creation:
26.6.
Calls to the child are recorded in the method_calls and mock_calls attributes of the parent.
This allows you to prevent the “parenting” if for some reason you don’t
want it to happen.
The patchers
The patch decorators are used for patching objects only within the scope of the function they decorate.
They automatically
handle the unpatching for you, even if exceptions are raised.
Inside the body of the function or
with statement, the target is patched with a new object.
When the function/with statement exits the patch is undone.
If new is omitted, then the target is replaced with an AsyncMock if the patched object is an async function or a
MagicMock otherwise.
If patch() is used as a decorator and new is omitted, the created mock is passed in
as an extra argument to the decorated function.
If patch() is used as a context manager the created mock is
returned by the context manager.
The target is imported and the spec-
ified object replaced with the new object, so the target must be importable from the environment you are calling
patch() from.
The target is imported when the decorated function is executed, not at decoration time.
The spec and spec_set keyword arguments are passed to the MagicMock if patch is creating one for you.
In addition you can pass spec=True or spec_set=True, which causes patch to pass in the object being
mocked as the spec/spec_set object.
By default AsyncMock is used for async functions and MagicMock for the rest.
A more powerful form of spec is autospec.
If you set autospec=True then the mock will be created with a spec
from the object being replaced.
All attributes of the mock will also have the spec of the corresponding attribute of
the object being replaced.
Methods and functions being mocked will have their arguments checked and will raise
a TypeError if they are called with the wrong signature.
For mocks replacing a class, their return value (the
‘instance’) will have the same spec as the class.
See the create_autospec() function and Autospeccing.
Instead of autospec=True you can pass autospec=some_object to use an arbitrary object as the spec
instead of the one being replaced.
By default patch() will fail to replace attributes that don’t exist.
If you pass in create=True, and the attribute
doesn’t exist, patch will create the attribute for you when the patched function is called, and delete it again after the
patched function has exited.
This is useful for writing tests against attributes that your production code creates at
runtime.
It is off by default because it can be dangerous.
With it switched on you can write passing tests against
APIs that don’t actually exist!
If you are patching builtins in a module then you don’t need to pass
create=True, it will be added by default.
Patch can be used as a TestCase class decorator.
It works by decorating each test method in the class.
This
reduces the boilerplate code when your test methods share a common patchings set.
TEST_PREFIX.
By default this is 'test', which matches the
way unittest finds tests.
You can specify an alternative prefix by setting patch.
TEST_PREFIX.
Patch can be used as a context manager, with the with statement.
Here the patching applies to the indented block
after the with statement.
If you use “as” then the patched object will be bound to the name after the “as”; very
useful if patch() is creating a mock object for you.
These will be passed to AsyncMock if the patched object is
asynchronous, to MagicMock otherwise or to new_callable if specified.
True
Patching a class replaces the class with a MagicMock instance.
If the class is instantiated in the code under test then it
will be the return_value of the mock that will be used.
If the class is instantiated multiple times you could use side_effect to return a new mock each time.
Alternatively
you can set the return_value to be anything you want.
To configure return values on methods of instances on the patched class you must do this on the return_value.
If you use spec or spec_set and patch() is replacing a class, then the return value of the created mock will have the
same spec.
The new_callable argument is useful where you want to use an alternative class to the default MagicMock for the created
mock.
When patch() is creating a mock for you, it is common that the first thing you need to do is to configure the mock.
Some of that configuration can be done in the call to patch.
Changed in version 3.8: patch() now returns an AsyncMock if the target is an async function.
Arguments new, spec,
create, spec_set, autospec and new_callable have the same meaning as for patch().
Like patch(), patch.
object() takes arbitrary keyword arguments for configuring the mock object it creates.
You can either call patch.object() with three arguments or two arguments.
The three argument form takes the
object to be patched, the attribute name and the object to replace the attribute with.
When calling with the two argument form you omit the replacement object, and a mock is created for you and passed in
as an extra argument to the decorated function:
... def test(mock_method):

SomeClass.class_method(3)

mock_method.assert_called_with(3)

spec, create and the other arguments to patch.object() have the same meaning as they do for patch().
patch.dict
patch.dict(in_dict, values=(), clear=False, **kwargs)
Patch a dictionary, or dictionary like object, and restore the dictionary to its original state after the test.
If it is a mapping then it must at least support getting,
setting and deleting items plus iterating over keys.
If clear is true then the dictionary will be cleared before the new values are set.
Changed in version 3.8: patch.dict() now returns the patched dictionary when used as a context manager.
If you want to use a different prefix for your test, you can inform the patchers of the different prefix by setting patch.
TEST_PREFIX.
For more details about how to change the value of see TEST_PREFIX.
patch.dict() can be used to add members to a dictionary, or simply let a test change a dictionary, and ensure the
dictionary is restored when the test ends.
At the very minimum they
must support item getting, setting, deleting and either iteration or membership test.
It takes the object to be patched (either as an object or a string to fetch
the object by importing) and keyword arguments for the patches:
with patch.multiple(settings, FIRST_PATCH='one', SECOND_PATCH='two'):

Use DEFAULT as the value if you want patch.multiple() to create mocks for you.
In this case the created
mocks are passed into a decorated function by keyword, and a dictionary is returned when patch.multiple()
is used as a context manager.
The arguments spec,
spec_set, create, autospec and new_callable have the same meaning as for patch().
These arguments will be
applied to all patches done by patch.multiple().
If you want patch.multiple() to create mocks for you, then you can use DEFAULT as the value.
If patch.multiple() is used as a context manager, the value returned by the context manager is a dictionary where
created mocks are keyed by name:

assert 'other' in repr(values['other'])

assert 'thing' in repr(values['thing'])

assert values['thing'] is thing

assert values['other'] is other

patch methods: start and stop
All the patchers have start() and stop() methods.
These make it simpler to do patching in setUp methods or
where you want to do multiple patches without nesting decorators or with statements.
To use them call patch(), patch.object() or patch.dict() as normal and keep a reference to the returned
patcher object.
You can then call start() to put the patch in place and stop() to undo it.
If you are using patch() to create a mock for you then it will be returned by the call to patcher.start.
Class1')

self.patcher2 = patch('package.module.
Class2')

self.
Class1 is self.
MockClass1

assert package.module.
Class2 is self.
MockClass2

Caution:
If you use this technique you must ensure that the patching is “undone” by calling stop.
This can
be fiddlier than you might think, because if an exception is raised in the setUp then tearDown is not called.
unittest.
TestCase.addCleanup() makes this easier:


patcher = patch('package.module.
Class')
26.6.
MockClass = patcher.start()

self.addCleanup(patcher.stop)



assert package.module.
Class is self.
As an added bonus you no longer need to keep a reference to the patcher object.
It is also possible to stop all patches which have been started by using patch.stopall().
Only stops patches started with start.
The following example patches builtin ord():
... def test(mock_ord):

mock_ord.return_value = 101

print(ord('c'))


TEST_PREFIX
All of the patchers can be used as class decorators.
When used in this way they wrap every test method on the class.
The
patchers recognise methods that start with 'test' as being test methods.
This is the same way that the unittest.
TestLoader finds test methods by default.
It is possible that you want to use a different prefix for your tests.
You can inform the patchers of the different prefix by
setting patch.
Note that the decorators are applied from the bottom upwards.
This is the standard way that Python applies decorators.
The order of the created mocks passed into your test function matches this order.
Where to patch
patch() works by (temporarily) changing the object that a name points to with another one.
There can be many names
pointing to any individual object, so for patching to work you must ensure that you patch the name used by the system
under test.
The basic principle is that you patch where an object is looked up, which is not necessarily the same place as where it is
defined.
A couple of examples will help to clarify this.
Imagine we have a project that we want to test with the following structure:
a.py
-> Defines SomeClass
b.py
-> from a import SomeClass
-> some_function instantiates SomeClass
Now we want to test some_function but we want to mock out SomeClass using patch().
The problem is that
when we import module b, which we will have to do then it imports SomeClass from module a.
If we use patch() to
mock out a.SomeClass then it will have no effect on our test; module b already has a reference to the real SomeClass
and it looks like our patching had no effect.
The key is to patch out SomeClass where it is used (or where it is looked up).
In this case some_function will
actually look up SomeClass in module b, where we have imported it.
The patching should look like:
@patch('b.
SomeClass')
However, consider the alternative scenario where instead of from a import SomeClass module b does import
a and some_function uses a.SomeClass.
Both of these import forms are common.
In this case the class we want
to patch is being looked up in the module and so we have to patch a.SomeClass instead:
@patch('a.
SomeClass')
26.6.
You
should patch these on the class rather than an instance.
They also work with some objects that proxy attribute access, like
the django settings object.
MagicMock and magic method support
Mocking Magic Methods
Mock supports mocking the Python protocol methods, also known as “magic methods”.
This allows mock objects to
replace containers or other objects that implement Python protocols.
Because magic methods are looked up differently from normal methods2, this support has been specially implemented.
This means that only specific magic methods are supported.
The supported list includes almost all of them.
If there are
any missing that you need please let us know.
You mock magic methods by setting the method you are interested in to a function or a mock instance.
If you are using
a function then it must take self as the first argument3.
Calls to magic methods do not appear in method_calls, but they are recorded in mock_calls.
Different versions of Python are inconsistent about applying this rule.
The supported protocol methods should work with all supported versions of Python.
The function is basically hooked up to the class, but each Mock instance is kept isolated from the others.
File system path representation: __fspath__
• Asynchronous iteration methods: __aiter__ and __anext__
Changed in version 3.8: Added support for os.PathLike.__fspath__().
Changed in version 3.8: Added support for __aenter__, __aexit__, __aiter__ and __anext__.
Magic Mock
There are two MagicMock variants: MagicMock and NonCallableMagicMock.
MagicMock is a subclass of Mock with default implementations of most of the magic methods.
You can use
MagicMock without having to configure the magic methods yourself.
The constructor parameters have the same meaning as for Mock.
If you use the spec or spec_set arguments then only magic methods that exist in the spec will be created.
NonCallableMagicMock(*args, **kw)
A non-callable version of MagicMock.
The constructor parameters have the same meaning as for MagicMock, with the exception of return_value and
side_effect which have no meaning on a non-callable mock.
26.6.
The magic methods are setup with MagicMock objects, so you can configure them and use them in the usual way:
'result'
By default many of the protocol methods are required to return objects of a specific type.
These methods are preconfigured
with a default return value, so that they can be used without you having to do anything if you aren’t interested in the return
value.
You
can still set these up if you want.
Attributes are created on demand when you access them by name.
Accessing the same attribute will always return
the same object.
The objects returned have a sensible repr so that test failure messages are readable.
The sentinel attributes now preserve their identity when they are copied or
pickled.
26.6.
Sometimes when testing you need to test that a specific object is passed as an argument to another method, or returned.
It can be common to create named sentinel objects to test this.
In this example we monkey patch method to return sentinel.some_object:
sentinel.some_object
DEFAULT
unittest.mock.
The DEFAULT object is a pre-created sentinel (actually sentinel.DEFAULT).
It can be used by
side_effect functions to indicate that the normal return value should be used.
For a call object that represents multiple calls, call_list() returns a list of all the intermediate calls as well as
the final call.
A chained call is multiple calls on a single
line of code.
This results in multiple entries in mock_calls on a mock.
Manually constructing the sequence of calls
can be tedious.
A call object is either a tuple of (positional args, keyword args) or (name, positional args, keyword args) depending on
how it was constructed.
When you construct them yourself this isn’t particularly interesting, but the call objects that
are in the Mock.call_args, Mock.call_args_list and Mock.mock_calls attributes can be introspected
to get at the individual arguments they contain.
The call objects in Mock.call_args and Mock.call_args_list are two-tuples of (positional args, keyword
args) whereas the call objects in Mock.mock_calls, along with ones you construct yourself, are three-tuples of
(name, positional args, keyword args).
You can use their “tupleness” to pull out the individual arguments for more complex introspection and assertions.
The
positional arguments are a tuple (an empty tuple if there are no positional arguments) and the keyword arguments are a
dictionary:
(1, 2, 3)
{'arg': 'one', 'arg2': 'two'}
True
True
<MagicMock name='mock.foo()' id='...'>
'foo'
(4, 5, 6)
{'arg': 'two', 'arg2': 'three'}
True
create_autospec
unittest.mock.create_autospec(spec, spec_set=False, instance=False, **kwargs)
Create a mock object using another object as a spec.
Attributes on the mock will use the corresponding attribute
on the spec object as their spec.
Functions or methods being mocked will have their arguments checked to ensure that they are called with the correct
signature.
If spec_set is True then attempting to set attributes that don’t exist on the spec object will raise an
AttributeError.
If a class is used as a spec then the return value of the mock (the instance of the class) will have the same spec.
You can use a class as the spec for an instance object by passing instance=True.
The returned mock will only
be callable if instances of the mock are callable.
create_autospec() also takes arbitrary keyword arguments that are passed to the constructor of the created
mock.
26.6.
Changed in version 3.8: create_autospec() now returns an AsyncMock if the target is an async function.
ANY
unittest.mock.
ANY
Sometimes you may need to make assertions about some of the arguments in a call to mock, but either not care about
some of the arguments or want to pull them individually out of call_args and make more complex assertions on them.
To
ignore
certain
arguments
you
can

objects
that
compare
equal
to
everything.
Calls
to
assert_called_with() and assert_called_once_with() will then succeed no matter what was
passed in.
ANY can also be used in comparisons with call lists like mock_calls:
True
FILTER_DIR
unittest.mock.
FILTER_DIR
FILTER_DIR is a module level variable that controls the way mock objects respond to dir().
The default is True,
which uses the filtering described below, to only show useful members.
If you dislike this filtering, or need to switch it
off for diagnostic purposes, then set mock.
FILTER_DIR = False.
With filtering on, dir(some_mock) shows only useful attributes and will include any dynamically created attributes
that wouldn’t normally be shown.
FILTER_DIR.
A helper function to create a mock to replace the use of open().
It works for open() called directly or used as
a context manager.
The mock argument is the mock object to configure.
If None (the default) then a MagicMock will be created for
you, with the API limited to methods or attributes available on standard file handles.
Calls to those methods will take data from read_data until it is depleted.
The mock of these methods is pretty
simplistic: every time the mock is called, the read_data is rewound to the start.
If you need more control over
the data that you are feeding to the tested code you will need to customize this mock for yourself.
When that is
insufficient, one of the in-memory filesystem packages on PyPI can offer a realistic filesystem for testing.
Changed in version 3.4: Added readline() and readlines() support.
The mock of read() changed to
consume read_data rather than returning it on each call.
Changed in version 3.5: read_data is now reset on each call to the mock.
Changed in version 3.8: Added __iter__() to implementation so that iteration (such as in for loops) correctly
consumes read_data.
The issue is that even if you mock out the call to open() it is the returned object that is used as a context manager (and
has __enter__() and __exit__() called).
Mocking context managers with a MagicMock is common enough and fiddly enough that a helper function is useful.
Autospeccing
Autospeccing is based on the existing spec feature of mock.
It limits the api of mocks to the api of an original object
(the spec), but it is recursive (implemented lazily) so that attributes of mocks only have the same api as the attributes of
the spec.
In addition mocked functions / methods have the same call signature as the original so they raise a TypeError
if they are called incorrectly.
Before I explain how auto-speccing works, here’s why it is needed.
Mock is a very powerful and flexible object, but it suffers from two flaws when used to mock out objects from a system
under test.
One of these flaws is specific to the Mock api and the other is a more general problem with using mock objects.
First
the
problem
specific
to
Mock.
Mock
has
two
assert
methods
that
are
extremely
handy:
assert_called_with() and assert_called_once_with().
Traceback (most recent call last):

AssertionError: Expected 'mock' to be called once.
Called 2 times.
Because mocks auto-create attributes on demand, and allow you to call them with arbitrary arguments, if you misspell
one of these assert methods then your assertion is gone:
# Intentional typo!
The second issue is more general to mocking.
If you refactor some of your code, rename members and so on, any tests
for code that is still using the old api but uses mocks instead of the real objects will still pass.
This means your tests can
all pass even though your code is broken.
Note that this is another reason why you need integration tests as well as unit tests.
Testing everything in isolation is all
fine and dandy, but if you don’t test how your units are “wired together” there is still lots of room for bugs that tests might
have caught.
If you use a class or instance as the spec for a mock
then you can only access attributes on the mock that exist on the real class:
# Intentional typo!
Mock object has no attribute 'assret_called_with'
The spec only applies to the mock itself, so we still have the same issue with any methods on the mock:
<mock.
Mock object at 0x...>
# Intentional typo!
Auto-speccing solves this problem.
You can either pass autospec=True to patch() / patch.object() or use
the create_autospec() function to create a mock with a spec.
If you use the autospec=True argument to
patch() then the object that is being replaced will be used as the spec object.
Because the speccing is done “lazily”
(the spec is created as attributes on the mock are accessed) you can use it with very complex or deeply nested objects
(like modules that import modules that import modules) without a big performance hit.
Here’s an example of it in use:
True
<MagicMock name='request.
You can see that request.
Request has a spec.
Request takes two arguments in the constructor (one
of which is self).
The spec also applies to instantiated classes (i.e. the return value of specced mocks):
<NonCallableMagicMock name='request.
Request()' spec='Request' id='...'>
Request objects are not callable, so the return value of instantiating our mocked out request.
Request is a non-
callable mock.
With the spec in place any typos in our asserts will raise the correct error:
26.6.
Request().add_header()' id='...'>
# Intentional typo!
Mock object has no attribute 'assret_called_with'
In many cases you will just be able to add autospec=True to your existing patch() calls and then be protected
against bugs due to typos and api changes.
As well as using autospec through patch() there is a create_autospec() for creating autospecced mocks directly:
<NonCallableMagicMock name='mock.
This isn’t without caveats and limitations however, which is why it is not the default behaviour.
In order to know what
attributes are available on the spec object, autospec has to introspect (access attributes) the spec.
As you traverse attributes
on the mock a corresponding traversal of the original object is happening under the hood.
If any of your specced objects
have properties or descriptors that can trigger code execution then you may not be able to use autospec.
On the other
hand it is much better to design your objects so that introspection is safe4.
A more serious problem is that it is common for instance attributes to be created in the __init__() method and not
to exist on the class at all.
Mock object has no attribute 'a'
There are a few different ways of resolving this problem.
The easiest, but not necessarily the least annoying, way is to
simply set the required attributes on the mock after creation.
There is a more aggressive version of both spec and autospec that does prevent you setting non-existent attributes.
Calling a mocked class to create a mock instance does not create a real instance.
It is
only attribute lookups - along with calls to dir() - that are done.
Mock object has no attribute 'a'
Probably the best way of solving the problem is to add class attributes as default values for instance members initialised
in __init__().
Note that if you are only setting default attributes in __init__() then providing them via class
attributes (shared between instances of course) is faster too.
This brings up another issue.
It is relatively common to provide a default value of None for members that will later be an
object of a different type.
None would be useless as a spec because it wouldn’t let you access any attributes or methods on
it.
As None is never going to be useful as a spec, and probably indicates a member that will normally of some other type,
autospec doesn’t use a spec for members that are set to None.
If modifying your production classes to add defaults isn’t to your liking then there are more options.
One of these is simply
to use an instance as the spec rather than the class.
The other is to create a subclass of the production class and add the
defaults to the subclass without affecting the production class.
Both of these require you to use an alternative object as
the spec.
Seal will disable the automatic creation of mocks when accessing an attribute of the mock being sealed or any of
its attributes that are already mocks recursively.
If a mock instance with a name or a spec is assigned to an attribute it won’t be considered in the sealing chain.
This
allows one to prevent seal from fixing part of the mock object.
New in version 3.7.
Once our mock has been used (real.method in this example) it has methods and attributes that allow you to make
assertions about how it has been used.
Note: In most of these examples the Mock and MagicMock classes are interchangeable.
As the MagicMock is the
more capable class it makes a sensible one to use by default.
Once the mock has been called its called attribute is set to True.
More importantly we can use the
assert_called_with() or assert_called_once_with() method to check that it was called with the cor-
rect arguments.
Another common use
case is to pass an object into a method (or some part of the system under test) and then check that it is used in the correct
way.
The simple ProductionClass below has a closer method.
If it is called with an object then it calls close on it.
So to test it we need to pass in an object with a close method and check that it was called correctly.
We don’t have to do any work to provide the ‘close’ method on our mock.
Accessing close creates it.
So, if ‘close’
hasn’t already been called then accessing it in the test will create it, but assert_called_with() will raise a failure
exception.
Mocking Classes
A common use case is to mock out classes instantiated by your code under test.
When you patch a class, then that class is
replaced with a mock.
Instances are created by calling the class.
This means you access the “mock instance” by looking
at the return value of the mocked class.
In the example below we have a function some_function that instantiates Foo and calls a method on it.
The call to
patch() replaces the class Foo with a mock.
It can be useful to give your mocks a name.
The name is shown in the repr of the mock and can be helpful when the mock
appears in test failure messages.
The name is also propagated to attributes or methods of the mock:
<MagicMock name='foo' id='...'>
<MagicMock name='foo.method' id='...'>
26.7.
Often you want to track more than a single call to a method.
The mock_calls attribute records all calls to child
attributes of the mock - and also to their children.
If we wanted this call to return a list, then we have to configure the result of the nested
call.
True
It is the call to .call_list() that turns our call object into a list of calls representing the chained calls.
Raising exceptions with mocks
A useful attribute is side_effect.
If you set this to an exception class or instance then the exception will be raised
when the mock is called.
Traceback (most recent call last):

Exception: Boom!
Side effect functions and iterables
side_effect can also be set to a function or an iterable.
The use case for side_effect as an iterable is where your
mock is going to be called several times, and you want each call to return a different value.
When you set side_effect
to an iterable every call to the mock returns the next value from the iterable:



For more advanced use cases, like dynamically varying the return values depending on what the mock is called with,
side_effect can be a function.
The function will be called with the same arguments as the mock.
Whatever the
function returns is what the call returns:
26.7.
The
return_value attribute of __aiter__ can be used to set the return values to be used for iteration.
Suppose you have a class that implements some_method.
In a test for another class, you provide a mock of
this object that also provides some_method.
If later you refactor the first class, so that it no longer has some_method
- then your tests will continue to pass even though your code is now broken!
Mock allows you to provide an object as a specification for the mock, using the spec keyword argument.
Accessing
methods / attributes on the mock that don’t exist on your specification object will immediately raise an attribute error.
If
you change the implementation of your specification, then tests that use that class will start failing immediately without
you having to instantiate the class in those tests.
If you want this smarter matching to also work with method calls on the mock, you can use auto-speccing.
If you want a stronger form of specification that prevents the setting of arbitrary attributes as well as the getting of them
then you can use spec_set instead of spec.
Using side_effect to return per file content
mock_open() is used to patch open() method.
This can be used to return different contents per file stored in a dictionary:
DEFAULT = "default"
data_dict = {"file1": "data1",
"file2": "data2"}

with patch("builtins.open", side_effect=open_side_effect):
with open("file1") as file1:
assert file1.read() == "data1"
with open("file2") as file2:
assert file2.read() == "data2"
with open("file3") as file2:
assert file2.read() == "default"
26.7.
This is normally
straightforward, but for a quick guide read where to patch.
A common need in tests is to patch a class attribute or a module attribute, for example patching a builtin or patching a
class in a module to test that it is instantiated.
Modules and classes are effectively global, so patching on them has to be
undone after the test or the patch will persist into other tests and cause hard to diagnose problems.
It also optionally takes a value that you want the attribute (or class or whatever) to be replaced with.
If you want to patch with a Mock, you can use patch() with only one argument (or patch.object() with two
arguments).
You can stack up multiple patch decorators using this pattern:

@patch('package.module.ClassName1')

@patch('package.module.ClassName2')


self.assertIs(package.module.
ClassName1, MockClass1)

self.assertIs(package.module.
When you nest patch decorators the mocks are passed in to the decorated function in the same order they applied (the
normal Python order that decorators are applied).
This means from the bottom up, so in the example above the mock for
test_module.ClassName2 is passed in first.
There is also patch.dict() for setting values in a dictionary just during a scope and restoring the dictionary to its
original state when the test ends:

assert foo == {'newkey': 'newvalue'}

patch, patch.object and patch.dict can all be used as context managers.
As an alternative patch, patch.object and patch.dict can be used as class decorators.
When used in this way
it is the same as applying the decorator individually to every method whose name starts with “test”.
Further Examples
Here are some more examples for some slightly more advanced scenarios.
Mocking chained calls
Mocking chained calls is actually straightforward with mock once you understand the return_value attribute.
When
a mock is called for the first time, or you fetch its return_value before it has been called, a new Mock is created.
This means that you can see how the object returned from a call to a mocked object has been used by interrogating the
return_value mock:
<Mock name='mock().foo()' id='...'>
From here it is a simple step to configure and then make assertions about chained calls.
Specifically, we want to test
that the code section # more code uses the response object in the correct way.
As this chain of calls is made from an instance attribute we can monkey patch the backend attribute on a Something
instance.
Let’s assume the object it returns is ‘file-like’, so we’ll ensure that our response object uses
the builtin open() as its spec.
To do this we create a mock instance as our mock backend and create a mock response object for it.
A chained call is several calls in one line of
code, so there will be several entries in mock_calls.
We can use call.call_list() to create this list of calls
for us:
Partial mocking
In some tests I wanted to mock out a call to datetime.date.today() to return a known date, but I didn’t want to
prevent the code under test from creating new date objects.
Unfortunately datetime.date is written in C, and so I
couldn’t just monkey-patch out the static datetime.date.today() method.
I found a simple way of doing this that involved effectively wrapping the date class with a mock, but passing through calls
to the constructor to the real class (and returning real instances).
The patch decorator is used here to mock out the date class in the module under test.
The side_effect
attribute on the mock date class is then set to a lambda function that returns a real date.
See where to patch.
When date.today() is called a known date is returned, but calls to the date(...) constructor still return normal
dates.
Without this you can find yourself having to calculate an expected result using exactly the same algorithm as the
code under test, which is a classic testing anti-pattern.
Calls to the date constructor are recorded in the mock_date attributes (call_count and friends) which may also be
useful for your tests.
An alternative way of dealing with mocking dates, or other builtin classes, is discussed in this blog entry.
Mocking a Generator Method
A Python generator is a function or method that uses the yield statement to return a series of values when iterated
over1.
A generator method / function is called to return the generator object.
It is the generator object that is then iterated over.
The protocol method for iteration is __iter__(), so we can mock this using a MagicMock.
There are also generator expressions and more advanced uses of generators, but we aren’t concerned about them here.
A very good introduction
to generators and how powerful they are is: Generator Tricks for Systems Programmers.
To configure the values returned from the iteration (implicit in the call to list), we need to configure the object returned
by the call to foo.iter().
This can feel like unnecessary repetition.
Instead, you can use patch() (in all its various forms) as a class
decorator.
This applies the patches to all test methods on the class.
TestCase):



self.assertIs(mymodule.
SomeClass, MockSomeClass)



self.assertIs(mymodule.
If you use this technique you must ensure that the patching is “undone” by calling stop.
This can be fiddlier than you
might think, because if an exception is raised in the setUp then tearDown is not called.
TestCase.
I needed self to be passed in as the first argument because I want to make asserts about which objects were
calling this particular method.
The issue is that you can’t patch with a mock for this, because if you replace an unbound
method with a mock it doesn’t become a bound method when fetched from the instance, and so it doesn’t get self passed
in.
The workaround is to patch the unbound method with a real function instead.
The patch() decorator makes it so
simple to patch out methods with a mock that having to create a real function becomes a nuisance.
If you pass autospec=True to patch then it does the patching with a real function object.
This function object has the
same signature as the one it is replacing, but delegates to a mock under the hood.
You still get your mock auto-created
in exactly the same way as before.
What it means though, is that if you use it to patch out an unbound method on a class
the mocked function will be turned into a bound method if it is fetched from an instance.
If we don’t use autospec=True then the unbound method is patched out with a Mock instance instead, and isn’t called
with self.
If your mock is only being called once you can use the assert_called_once_with() method that also asserts
that the call_count is one.
Traceback (most recent call last):

AssertionError: Expected to be called once.
Called 2 times.
Both assert_called_with and assert_called_once_with make assertions about the most recent call.
You can build up a list of expected calls and compare
it to call_args_list.
This looks remarkably similar to the repr of the call_args_list:
True
Coping with mutable arguments
Another situation is rare, but can bite you, is when your mock is called with mutable arguments.
If the arguments are mutated by the code under test then you can
no longer make assertions about what the values were when the mock was called.
Here’s some example code that shows the problem.
One possibility would be for mock to copy the arguments you pass in.
This could then cause problems if you do assertions
that rely on object identity for equality.
Here’s one solution that uses the side_effect functionality.
If you provide a side_effect function for a mock
then side_effect will be called with the same args as the mock.
This gives us an opportunity to copy the arguments
and store them for later assertions.
In this example I’m using another mock to store the arguments so that I can use the
mock methods for doing the assertion.
It returns a new mock that we do the assertion on.
The
side_effect function makes a copy of the args and calls our new_mock with the copy.
Note:
If your mock is only going to be used once there is an easier way of checking arguments at the point they are
called.
You can simply do the checking inside a side_effect function.
AssertionError
26.7.
When you subclass Mock or MagicMock all dynamically created attributes, and the return_value will use your
subclass automatically.
That means all children of a CopyingMock will also have the type CopyingMock.
Nesting Patches
Using patch as a context manager is nice, but if you do multiple patches you can end up with nested with statements
indenting further and further to the right:



with patch('mymodule.
Foo') as mock_foo:

with patch('mymodule.
Bar') as mock_bar:

with patch('mymodule.
Spam') as mock_spam:

assert mymodule.
Foo is mock_foo

assert mymodule.
Bar is mock_bar

assert mymodule.
Spam is mock_spam

With unittest cleanup functions and the patch methods: start and stop we can achieve the same effect without the nested
indentation.
Foo')

mock_bar = self.create_patch('mymodule.
Bar')

mock_spam = self.create_patch('mymodule.
Spam')


assert mymodule.
Foo is mock_foo

assert mymodule.
Bar is mock_bar

assert mymodule.
Spam is mock_spam

Mocking a dictionary with MagicMock
You may want to mock a dictionary, or other container object, recording all access to it whilst having it still behave like
a dictionary.
We can do this with MagicMock, which will behave like a dictionary, and using side_effect to delegate dictionary
access to a real underlying dictionary that is under our control.
We can also control
what is returned.
A third option is to use MagicMock but passing in dict as the spec (or spec_set) argument so that the MagicMock
created only has dictionary magic methods available:
With these side effect functions in place, the mock will behave like a normal dictionary but recording the access.
It even
raises a KeyError if you try to access a key that doesn’t exist.
26.7.
One reason might be to add helper methods.
Here’s a
silly example:




<MyMock id='...'>
False
True
The standard behaviour for Mock instances is that attributes and the return value mocks are of the same type as the mock
they are accessed on.
This ensures that Mock attributes are Mocks and MagicMock attributes are MagicMocks2.
So if you’re subclassing to add helper methods then they’ll also be available on the attributes and return value mock of
instances of your subclass.
An exception to this rule are the non-callable mocks.
True
Sometimes this is inconvenient.
For example, one user is subclassing mock to created a Twisted adaptor.
Having this
applied to attributes too actually causes errors.
You can prevent your subclass being used for attributes by overriding this method.
The signature is that it
takes arbitrary keyword arguments (**kwargs) which are then passed onto the mock constructor:




<MagicMock name='mock.foo' id='...'>
Mocking imports with patch.dict
One situation where mocking can be hard is where you have a local import inside a function.
These are harder to mock
because they aren’t using an object from the module namespace that we can patch out.
Generally local imports are to be avoided.
They are sometimes done to prevent circular dependencies, for which there is
usually a much better way to solve the problem (refactor the code) or to prevent “up front costs” by delaying the import.
This can also be solved in better ways than an unconditional local import (store the module as a class or module attribute
and only do the import on first use).
That aside there is a way to use mock to affect the results of an import.
Importing fetches an object from the sys.
Note that it fetches an object, which need not be a module.
Importing a module for the first time
results in a module object being put in sys.modules, so usually when you import something you get a module back.
This need not be the case however.
This means you can use patch.dict() to temporarily put a mock in place in sys.modules.
Any imports whilst
this patch is active will fetch the mock.
When the patch is complete (the decorated function exits, the with statement
body is complete or patcher.stop() is called) then whatever was there previously will be restored safely.
As you can see the import fooble succeeds, but on exit there is no ‘fooble’ left in sys.modules.
This also works for the from module import name form:
26.7.
Tracking order of calls and less verbose call assertions
The Mock class allows you to track the order of method calls on your mock objects through the method_calls
attribute.
This doesn’t allow you to track the order of calls between separate mock objects, however we can use
mock_calls to achieve the same effect.
Because mocks track calls to child mocks in mock_calls, and accessing an arbitrary attribute of a mock creates a child
mock, we can create our separate mocks from a parent one.
True
If patch is creating, and putting in place, your mocks then you can attach them to a manager mock using the
attach_mock() method.
After attaching calls will be recorded in mock_calls of the manager.

with patch('mymodule.
MockClass2().bar()
<MagicMock name='mock.
MockClass1().foo()' id='...'>
<MagicMock name='mock.
MockClass1().foo(),
call.MockClass2(),
call.
If many calls have been made, but you’re only interested in a particular sequence of them then an alternative is to use the
assert_has_calls() method.
This takes a list of calls (constructed with the call object).
If that sequence of
calls are in mock_calls then the assert succeeds.
Even though the chained call m.one().two().three() aren’t the only calls that have been made to the mock, the
assert still succeeds.
Sometimes a mock may have several calls made to it, and you are only interested in asserting about some of those calls.
You may not even care about the order.
In this case you can pass any_order=True to assert_has_calls:
(...)
More complex argument matching
Using the same basic concept as ANY we can implement matchers to do more complex assertions on objects used as
arguments to mocks.
Suppose we expect some object to be passed to a mock that by default compares equal based on object identity (which
is the Python default for user defined classes).
To use assert_called_with() we would need to pass in the exact
same object.
If we are only interested in some of the attributes of this object then we can create a matcher that will check
these attributes for us.
Traceback (most recent call last):
(continues on next page)
26.7.
A comparison function for our Foo class might look something like this:

if not type(self) == type(other):



if self.a !
Putting all this together:
The Matcher is instantiated with our compare function and the Foo object we want to compare against.
In
assert_called_with the Matcher equality method will be called, which compares the object the mock was
called with against the one we created our matcher with.
With a bit of tweaking you could have the comparison function raise the AssertionError directly and provide a
more useful failure message.
As of version 1.5, the Python testing library PyHamcrest provides similar functionality, that may be useful here, in the
form of its equality matcher (hamcrest.library.integration.match_equality).
The standard library contains a rich set of fixers that will handle almost all code.
The lib2to3 module was marked pending for depreca-
tion in Python 3.9 (raising PendingDeprecationWarning on import) and fully deprecated in Python 3.11 (raising
DeprecationWarning).
The 2to3 tool is part of that.
It will be removed in Python 3.13.
It is also located in the Tools/scripts directory
of the Python root.
The directories are recursively traversed for Python
sources.
Here is a sample Python 2.x source file, example.py:

print "What's your name?
It can be converted to Python 3.x code via 2to3 on the command line:
$ 2to3 example.py
A diff against the original source file is printed.
By default, 2to3 runs a set of predefined fixers.
The -l flag lists all available fixers.
An explicit set of fixers to run can be
given with -f.
Likewise the -x explicitly disables a fixer.
Here, in
addition to the default fixers, the idioms fixer is run:
26.8.
Sometimes 2to3 will find a place in your source code that needs to be changed, but 2to3 cannot fix automatically.
In this
case, 2to3 will print a warning beneath the diff for a file.
You should address the warning in order to have compliant 3.x
code.
To enable this mode, use the -d flag.
Note that only doctests will be refactored.
This
also doesn’t require the module to be valid Python.
For example, doctest like examples in a reST document could also be
refactored with this option.
The -v option enables output of more information on the translation process.
Since some print statements can be parsed as function calls or statements, 2to3 cannot always read files containing the
print function.
This change can also be enabled manually
with the -p flag.
Use -p to run fixers on code that already has had its print statements converted.
Also -e can be used
to make exec() a function.
The -o or --output-dir option allows specification of an alternate directory for processed output files to be written
to.
The -n flag is required when using this as backup files do not make sense when not overwriting the input files.
The -o option was added.
The -W or --write-unchanged-files flag tells 2to3 to always write output files even if no changes were required
to the file.
This option implies the -w flag as it would not make sense otherwise.
New in version 3.2.3: The -W flag was added.
The --add-suffix option specifies a string to append to all output filenames.
The -n flag is required when specifying
this as backups are not necessary when writing to different filenames.
New in version 3.2.3: The --add-suffix option was added.
To translate an entire project from one directory tree to another use:
$ 2to3 --output-dir=python3-version/mycode -W -n python2-version/mycode
26.8.2 Fixers
Each step of transforming code is encapsulated in a fixer.
The command 2to3 -l lists them.
As documented above,
each can be turned on and off individually.
They are described here in more detail.
For example apply(function, *args, **kwargs) is converted to
function(*args, **kwargs).
This fixer is optional because the memoryview API is similar but not
exactly the same as that of buffer.
Similarly, dict.
viewitems(), dict.viewkeys() and dict.viewvalues() are converted respectively to dict.
items(), dict.keys() and dict.values().
It also wraps existing usages of dict.items(), dict.
keys(), and dict.values() in a call to list.
except
Converts except X, T to except X as T.
exec
Converts the exec statement to the exec() function.
The argument to execfile() is wrapped in calls to open(), compile(),
and exec().
For example, my_function.func_closure is converted
to my_function.__closure__.
Type compar-
isons like type(x) is SomeClass and type(x) == SomeClass are converted to isinstance(x,
SomeClass).
This fixer also tries to make use of sorted() in appropri-
ate places.
For example, this block
L = list(some_iterable)
L.sort()
is changed to
L = sorted(some_iterable)
import
Detects sibling imports and converts them to relative imports.
It is separate from the imports fixer only because of
technical limitations.
For example, isinstance(x, (int,
int)) is converted to isinstance(x, int) and isinstance(x, (int, float, int)) is con-
verted to isinstance(x, (int, float)).
Im-
ports of itertools.ifilterfalse() are also changed to itertools.filterfalse().
It also changes map(None, x) to list(x).
Using from
future_builtins import map disables this fixer.
For example, meth.im_func is converted to meth.__func__.
It also renames next() methods to
__next__().
When needed,
the appropriate import statements are added, e.g. import collections.abc.
The following mapping are
made:
From
To
operator.isCallable(obj)
callable(obj)
operator.
Mapping)
operator.isNumberType(obj)
isinstance(obj, numbers.
Number)
operator.repeat(obj, n)
operator.mul(obj, n)
operator.irepeat(obj, n)
operator.imul(obj, n)
paren
Add extra parenthesis where they are required in list comprehensions.
For example, [x for x in 1, 2]
becomes [x for x in (1, 2)].
If E is a tuple, the translation will be incorrect because substituting tuples for
exceptions has been removed in 3.0.
This fixer is optional.
This fixer inserts temporary variables.
Removes excess whitespace from comma separated items.
This fixer is optional.
This is disabled when from future_builtins import zip ap-
pears.
Python 3.10 includes new language syntax that is not
parsable by lib2to3’s LL(1) parser (see PEP 634).
The lib2to3 module was marked pending for deprecation
in Python 3.9 (raising PendingDeprecationWarning on import) and fully deprecated in Python 3.11 (raising
DeprecationWarning).
It will be removed from the standard library in Python 3.13.
Consider third-party alterna-
tives such as LibCST or parso.
Note: The lib2to3 API should be considered unstable and may change drastically in the future.
It is documented for the benefit of the core developers
of Python.
Any use of this package outside of Python’s standard library is discouraged as code mentioned here can change
or be removed without notice between releases of Python.
The test package contains all regression tests for Python as well as the modules test.support and test.
Each module in the test package whose name starts with test_ is a testing suite for a specific module or feature.
All
new tests should be written using the unittest or doctest module.
Some older tests are written using a “traditional”
testing style that compares output printed to sys.stdout; this style of test is considered deprecated.
See also:
Module unittest Writing PyUnit regression tests.
Module doctest Tests embedded in documentation strings.
One is to name the test module by starting
it with test_ and end it with the name of the module being tested.
The test methods in the test module should start
with test_ and end with a description of what the method is testing.
This is needed so that the methods are recognized
by the test driver as test methods.
Also, no documentation string for the method should be included.
A comment (such
as # Tests function returns only True or False) should be used to provide documentation for test
methods.
This is done because documentation strings get printed out if they exist and thus what test is being run is not
stated.
A basic boilerplate is often used:
import unittest
from test import support
class MyTestCase1(unittest.
This code pattern allows the testing suite to be run by test.regrtest, on its own as a script that supports the
unittest CLI, or via the python -m unittest CLI.
The goal for regression testing is to try to break code.
The testing suite should exercise all classes, functions, and constants.
This includes not just the external API that
is to be presented to the outside world but also “private” code.
Blackbox testing
(testing only the published user interface) is not complete enough to make sure all boundary and edge cases are
tested.
This makes sure that not only all valid values are
acceptable but also that improper values are handled correctly.
Test where branching occurs and thus tailor input to make sure as many
different paths through the code are taken.
This will make sure that the error does not crop
up again if the code is changed in the future.
If a test is dependent on a specific condition of the operating system then verify the condition already exists before
attempting the test.
This minimizes external dependencies of tests and
also minimizes possible anomalous behavior from side-effects of importing a module.
On occasion, tests will vary by something as small as what type of input is used.
TestCase):
arg = (1, 2, 3)
When using this pattern, remember that all classes that inherit from unittest.
TestCase are run as tests.
The
Mixin class in the example above does not have any data and so can’t be run by itself, thus it does not inherit from
unittest.
The test package can be run as a script to drive Python’s regression test suite, thanks to the -m option: python -m
test.
Under the hood, it uses test.regrtest; the call python -m test.regrtest used in previous Python
versions still works.
Running the script by itself automatically starts running all regression tests in the test package.
It does this by finding all modules in the package whose name starts with test_, importing them, and executing the
function test_main() if present or loading the tests via unittest.
TestLoader.loadTestsFromModule if test_main
does not exist.
The names of tests to execute may also be passed to the script.
Specifying a single regression test (python
-m test test_spam) will minimize output and only print whether the test passed or failed.
Running test directly allows what resources are available for tests to use to be set.
You do this by using the -u command-
line option.
Specifying all as the value for the -u option enables all possible resources: python -m test -uall.
If all but one resource is desired (a more common case), a comma-separated list of resources that are not desired may
be listed after all.
The command python -m test -uall,-audio,-largefile will run test with all
resources except the audio and largefile resources.
For a list of all resources and more command-line options, run
python -m test -h.
Some other ways to execute the regression tests depend on what platform the tests are being executed on.
On Unix, you
can run make test at the top-level directory where Python was built.
On Windows, executing rt.bat from your
PCbuild directory will run all regression tests.
Note: test.support is not a public module.
It is documented here to help Python developers write tests.
The API
of this module is subject to change without backwards compatibility concerns between releases.
This module defines the following exceptions:
exception test.support.
TestFailed
Exception to be raised when a test fails.
This is deprecated in favor of unittest-based tests and unittest.
TestCase’s assertion methods.
ResourceDenied
Subclass of unittest.
SkipTest.
Raised when a resource (such as a network connection) is not available.
Raised by the requires() function.
The test.support module defines the following constants:
test.support.verbose
True when verbose output is enabled.
Should be checked when more detailed information is desired about a
running test.
Timeout in seconds for tests using a network server listening on the network local loopback interface like 127.0.
0.1.
The timeout is long enough to prevent test failure: it takes into account that the client and the server can run in
different threads or even different processes.
The timeout should be long enough for connect(), recv() and send() methods of socket.socket.
Its default value is 5 seconds.
See also INTERNET_TIMEOUT.
test.support.
INTERNET_TIMEOUT
Timeout in seconds for network requests going to the internet.
The timeout is short enough to prevent a test to wait for too long if the internet request is blocked for whatever
reason.
Usually, a timeout using INTERNET_TIMEOUT should not mark a test as failed, but skip the test instead: see
transient_internet().
Its default value is 1 minute.
See also LOOPBACK_TIMEOUT.
test.support.
SHORT_TIMEOUT
Timeout in seconds to mark a test as failed if the test takes “too long”.
The timeout value depends on the regrtest --timeout command line option.
If a test using SHORT_TIMEOUT starts to fail randomly on slow buildbots, use LONG_TIMEOUT instead.
Its default value is 30 seconds.
test.support.
LONG_TIMEOUT
Timeout in seconds to detect when a test hangs.
It is long enough to reduce the risk of test failure on the slowest Python buildbots.
It should not be used to mark
a test as failed if the test takes “too long”.
The timeout value depends on the regrtest --timeout command line
option.
Its default value is 5 minutes.
See also LOOPBACK_TIMEOUT, INTERNET_TIMEOUT and SHORT_TIMEOUT.
test.support.
PGO
Set when tests can be skipped when they are not useful for PGO.
test.support.
PIPE_MAX_SIZE
A constant that is likely larger than the underlying OS pipe buffer size, to make writes blocking.
test.support.
Py_DEBUG
True if Python is built with the Py_DEBUG macro defined: if Python is built in debug mode (./configure
--with-pydebug).
New in version 3.12.
test.support.
SOCK_MAX_SIZE
A constant that is likely larger than the underlying OS socket buffer size, to make writes blocking.
TEST_SUPPORT_DIR
Set to the top level directory that contains test.support.
test.support.
TEST_HOME_DIR
Set to the top level directory for the test package.
TEST_DATA_DIR
Set to the data directory within the test package.
test.support.
MAX_Py_ssize_t
Set to sys.maxsize for big memory tests.
Limited by MAX_Py_ssize_t.
Not limited by MAX_Py_ssize_t.
test.support.
MISSING_C_DOCSTRINGS
Set to True if Python is built without docstrings (the WITH_DOC_STRINGS macro is not defined).
See the
configure --without-doc-strings option.
See also the HAVE_DOCSTRINGS variable.
test.support.HAVE_DOCSTRINGS
Set to True if function docstrings are available.
See the python -OO option, which strips docstrings of functions
implemented in Python.
See also the MISSING_C_DOCSTRINGS variable.
test.support.
TEST_HTTP_URL
Define the URL of a dedicated HTTP server for the network tests.
test.support.
ALWAYS_EQ
Object that is equal to anything.
Used to test mixed type comparison.
test.support.
NEVER_EQ
Object that is not equal to anything (even to ALWAYS_EQ).
Used to test mixed type comparison.
test.support.
LARGEST
Object that is greater than anything (except itself).
Used to test mixed type comparison.
test.support.
SMALLEST
Object that is less than anything (except itself).
Used to test mixed type comparison.
The test.support module defines the following functions:
test.support.busy_retry(timeout, err_msg=None, /, *, error=True)
Run the loop body until break stops the loop.
After timeout seconds, raise an AssertionError if error is true, or just stop the loop if error is false.
Example:
for _ in support.busy_retry(support.
SHORT_TIMEOUT):
if check():
break
Example of error=False usage:
26.10.
SHORT_TIMEOUT, error=False):
if check():
break
else:
raise RuntimeError('my custom error')
test.support.sleeping_retry(timeout, err_msg=None, /, *, init_delay=0.010, max_delay=1.0,
error=True)
Wait strategy that applies exponential backoff.
Run the loop body until break stops the loop.
Sleep at each loop iteration, but not at the first iteration.
The sleep
delay is doubled at each iteration (up to max_delay seconds).
See busy_retry() documentation for the parameters usage.
Example raising an exception after SHORT_TIMEOUT seconds:
for _ in support.sleeping_retry(support.
SHORT_TIMEOUT):
if check():
break
Example of error=False usage:
for _ in support.sleeping_retry(support.
SHORT_TIMEOUT, error=False):
if check():
break
else:
raise RuntimeError('my custom error')
test.support.is_resource_enabled(resource)
Return True if resource is enabled and available.
The list of available resources is only set when test.
Always returns True if called by a function whose __name__ is '__main__'.
Used when tests are executed
by test.regrtest.
test.support.sortdict(dict)
Return a repr of dict with keys sorted.
test.support.findfile(filename, subdir=None)
Return the path to the file named filename.
If no match is found filename is returned.
This does not equal a failure
since it could be the path to the file.
Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.
Get size of a page in bytes.
Defines a minimum interval for Android systems
to prevent the system from hanging.
This function returns True or False depending on the host platform.
Example usage:
check_impl_detail()
# Only on CPython (default).
It is meant to hold the stdout at the time the regrtest began.
A context managers that temporarily replaces the named stream with io.StringIO object.
Example use with output streams:
with captured_stdout() as stdout, captured_stderr() as stderr:
print("hello")
print("error", file=sys.stderr)
assert stdout.getvalue() == "hello\n"
assert stderr.getvalue() == "error\n"
Example use with input stream:
with captured_stdin() as stdin:
stdin.write('hello\n')
stdin.seek(0)
# call test code that consumes from sys.stdin
captured = input()
self.assertEqual(captured, "hello")
test.support.disable_faulthandler()
A context manager that temporary disables faulthandler.
This is needed because timely deallocation is not guaranteed by
the garbage collector.
This means that __del__ methods may be called later than expected and weakrefs may
remain alive for longer than expected.
On exit, the garbage collector is restored to its prior
state.
This will set obj.attr to 5 for the duration of the with block, restoring the old value at the end of the block.
If attr doesn’t exist on obj, it will be created and then deleted at the end of the block.
The old value (or None if it doesn’t exist) will be assigned to the target of the “as” clause, if there is one.
This will set obj["item"] to 5 for the duration of the with block, restoring the old value at the end of the
block.
If item doesn’t exist on obj, it will be created and then deleted at the end of the block.
The old value (or None if it doesn’t exist) will be assigned to the target of the “as” clause, if there is one.
It can be used to make sure that the
logs order is consistent before writing into stderr.
New in version 3.11.
Print a warning into sys.__stderr__.
Format the message as: f"Warning -- {msg}".
If msg is made
of multiple lines, add "Warning -- " prefix to each line.
New in version 3.9.
Raise an AssertionError if the process exit code is not equal to exitcode.
If the process runs longer than timeout seconds (SHORT_TIMEOUT by default), kill the process and raise an
AssertionError.
The timeout feature is not available on Windows.
New in version 3.9.
The returned value includes the
size of the Python object header and alignment.
test.support.calcvobjsize(fmt)
Return the size of the PyVarObject whose structure members are defined by fmt.
A decorator to conditionally mark tests with unittest.expectedFailure().
A decorator that skips the decorated test on TLS certification validation failures.
The locales passed will be tried sequentially, and the first valid
locale will be used.
Decorator for the minimum version when running test on FreeBSD.
If the FreeBSD version is less than the mini-
mum, the test is skipped.
Decorator for the minimum version when running test on Linux.
If the Linux version is less than the minimum,
the test is skipped.
Decorator for the minimum version when running test on macOS.
If the macOS version is less than the minimum,
the test is skipped.
If that returns False, then uses msg as the reason
for skipping the test.
The decorator does not run the test if it is not run by CPython.
Any trace function is unset for the duration of the test to prevent unexpected refcounts caused by the trace function.
For example, a test that needs two byte buffers, of 4 GiB each, could be decorated
with @bigmemtest(size=_4G, memuse=2).
The size argument is normally passed to the decorated test method as an extra argument.
If dry_run is True, the
value passed to the test method may be less than the requested value.
Decorator for tests that fill the address space.
If lineno is not None, compares to the line of the exception.
If offset is not None, compares to the offset of the
exception.
If open fails, raises TestFailed.
test.support.reap_children()
Use this at the end of test_main whenever sub-processes are started.
This will help ensure that no extra children
(zombies) stick around to hog resources and create problems when looking for refleaks.
test.support.get_attribute(obj, name)
Get an attribute, raising unittest.
SkipTest if AttributeError is raised.
Storing the exception value (cm.unraisable.exc_value) creates a reference cycle.
The reference cycle is
broken explicitly when the context manager exits.
Storing the object (cm.unraisable.object) can resurrect it if it is set to an object which is being finalized.
Exiting the context manager clears the stored object.
Generic implementation of the unittest load_tests protocol for use in test packages.
In
simple cases, the test package’s __init__.py can be the following:
import os
from test.support import load_package_tests

test.support.detect_api_mismatch(ref_api, other_api, *, ignore=())
Returns the set of attributes, functions or methods of ref_api not found on other_api, except for a defined list of
items to be ignored in this check specified in ignore.
New in version 3.5.
Override object_to_patch.attr_name with new_value.
Also add cleanup procedure to test_instance to restore ob-
ject_to_patch for attr_name.
The attr_name should be a valid attribute for object_to_patch.
Run code in subinterpreter.
Raise unittest.
SkipTest if tracemalloc is enabled.
Assert instances of cls are deallocated after iterating.
Assert that the __all__ variable of module contains all public names.
The module’s public names (its API) are detected automatically based on whether they match the public name
convention and were defined in module.
The name_of_module argument can specify (as a string or tuple thereof) what module(s) an API could be defined
in order to be detected as a public API.
One case for this is when module imports part of its public API from other
modules, possibly a C backend (like csv and its _csv).
The extra argument can be a set of names that wouldn’t otherwise be automatically detected as “public”, like objects
without a proper __module__ attribute.
If provided, it will be added to the automatically detected ones.
The not_exported argument can be a set of names that must not be treated as part of the public API even though
their names indicate otherwise.
Example use:
26.10.
TestCase):

class OtherTestCase(unittest.TestCase):

not_exported = {'baz'}
# Undocumented name.
Skip tests if the multiprocessing.synchronize module is missing, if there is no available semaphore
implementation, or if creating a lock raises an OSError.
Assert that type tp cannot be instantiated using args and kwds.
This function returns a context manager that will change the global sys.set_int_max_str_digits()
setting for the duration of the context to allow execution of test code that needs a different limit on the number of
digits when converting between an integer and string.
New in version 3.11.
The test.support module defines the following classes:
class test.support.
SuppressCrashReport
A context manager used to try to prevent crash dialog popups on tests that are expected to crash a subprocess.
On Windows, it disables Windows Error Reporting dialogs using SetErrorMode.
On UNIX, resource.setrlimit() is used to set resource.
RLIMIT_CORE’s soft limit to 0 to prevent
coredump file creation.
On both platforms, the old value is restored by __exit__().
SaveSignals
Class to save and restore signal handlers registered by the Python signal handler.
save(self)
Save the signal handlers to a dictionary mapping signal numbers to the current signal handler.
restore(self)
Set the signal numbers from the save() dictionary to the saved handler.
The test.support.socket_helper module provides support for socket tests.
New in version 3.9.
IPV6_ENABLED
Set to True if IPv6 is enabled on this host, False otherwise.
AF_INET,
socktype=socket.SOCK_STREAM)
Returns an unused port that should be suitable for binding.
This is achieved by creating a temporary socket with
the same family and type as the sock parameter (default is AF_INET, SOCK_STREAM), and binding it to the
specified host address (defaults to 0.0.0.0) with the port set to 0, eliciting an unused ephemeral port from the
OS.
The temporary socket is then closed and deleted, and the ephemeral port is returned.
Either this method or bind_port() should be used for any tests where a server socket needs to be bound to
a particular port for the duration of the test.
Which one to use depends on whether the calling code is creating a
Python socket, or if an unused port needs to be provided in a constructor or passed to an external program (i.e. the
-accept argument to openssl’s s_server mode).
Always prefer bind_port() over find_unused_port()
where possible.
Using a hard coded port is discouraged since it can make multiple instances of the test impossible
to run simultaneously, which is a problem for buildbots.
Relies on ephemeral ports in order to ensure we are using
an unbound port.
This is important as many tests may be running simultaneously, especially in a buildbot environ-
ment.
This method raises an exception if the sock.family is AF_INET and sock.type is SOCK_STREAM,
and the socket has SO_REUSEADDR or SO_REUSEPORT set on it.
Tests should never set these socket options
for TCP/IP sockets.
The only case for setting these options is testing multicasting via multiple UDP sockets.
Additionally, if the SO_EXCLUSIVEADDRUSE socket option is available (i.e. on Windows), it will be set on the
socket.
This will prevent anyone else from binding to our host/port for the duration of the test.
test.support.socket_helper.bind_unix_socket(sock, addr)
Bind a Unix socket, raising unittest.
SkipTest if PermissionError is raised.
A context manager that raises ResourceDenied when various issues with the internet connection manifest
themselves as exceptions.
26.11.
The test.support.script_helper module provides support for Python’s script execution tests.
This is designed to be used with @unittest.skipIf() to annotate tests that need to use an
assert_python*() function to launch an isolated mode (-I) or no environment mode (-E) sub-interpreter
process.
A normal build & test does not run into this situation but it can happen when trying to run the standard library test
suite from an interpreter that doesn’t have an obvious home with Python’s current home finding logic.
Setting PYTHONHOME is one way to get most of the testsuite to run in that situation.
PYTHONPATH or
PYTHONUSERSITE are other common environment variables that might impact whether or not the interpreter
can start.
The values can include
__isolated, __cleanenv, __cwd, and TERM.
The function no longer strips whitespaces from stderr.
Assert that running the interpreter with args and optional environment variables env_vars succeeds (rc == 0)
and return a (return code, stdout, stderr) tuple.
If the __cleanenv keyword-only parameter is set, env_vars is used as a fresh environment.
Python is started in isolated mode (command line option -I), except if the __isolated keyword-only parameter is
set to False.
The function no longer strips whitespaces from stderr.
Assert that running the interpreter with args and optional environment variables env_vars fails (rc !
See assert_python_ok() for more options.
The function no longer strips whitespaces from stderr.
PIPE,
stderr=subprocess.
STDOUT, **kw)
Run a Python subprocess with the given arguments.
Popen().
Returns a subprocess.
Popen object.
test.support.script_helper.kill_python(p)
Run the given subprocess.
Popen process until completion and return stdout.
If omit_suffix is False, append .py to
the name.
Return the full script path.
Return a tuple containing (full path, full path of archive name).
If compiled is True, both source files will be compiled and added to
the zip package.
Return a tuple of the full zip path and the archive name for the zip file.
New in version 3.9.
The module defines the following class:
class test.support.bytecode_helper.
BytecodeTestCase(unittest.
This class has custom assertion methods for inspecting bytecode.
BytecodeTestCase.get_disassembly_as_string(co)
Return the disassembly of co as string.
BytecodeTestCase.assertInBytecode(x, opname, argval=_UNSPECIFIED)
Return instr if opname is found, otherwise throws AssertionError.
Throws AssertionError if opname is found.
New in version 3.10.
Raise an AssertionError if thread is still alive after timeout seconds.
Designed to emit a warning if a test leaves running threads in
the background.
Thread exception using threading.excepthook().
Attributes set when an exception is caught:
• exc_type
• exc_value
• exc_traceback
• thread
See threading.excepthook() documentation.
These attributes are deleted at the context manager exit.
New in version 3.8.
26.15 test.support.os_helper — Utilities for os tests
The test.support.os_helper module provides support for os tests.
New in version 3.10.
test.support.os_helper.
FS_NONASCII
A non-ASCII character encodable by os.fsencode().
test.support.os_helper.
SAVEDCWD
Set to os.getcwd().
TESTFN
Set to a name that is safe to use as the name of a temporary file.
Any temporary file that is created should be closed
and unlinked (removed).
TESTFN_NONASCII
Set to a filename containing the FS_NONASCII character, if it exists.
This guarantees that if the filename exists,
it can be encoded and decoded with the default filesystem encoding.
This allows tests that require a non-ASCII
filename to be easily skipped on platforms where they can’t work.
test.support.os_helper.
TESTFN_UNENCODABLE
Set to a filename (str type) that should not be able to be encoded by file system encoding in strict mode.
It may be
None if it’s not possible to generate such a filename.
test.support.os_helper.
TESTFN_UNDECODABLE
Set to a filename (bytes type) that should not be able to be decoded by file system encoding in strict mode.
It may
be None if it’s not possible to generate such a filename.
test.support.os_helper.
TESTFN_UNICODE
Set to a non-ASCII name for a temporary file.
EnvironmentVarGuard
Class used to temporarily set or unset environment variables.
Instances can be used as a context manager and have
a complete dictionary interface for querying/modifying the underlying os.environ.
After exit from the context
manager all changes to environment variables done through this instance will be rolled back.
Changed in version 3.1: Added dictionary interface.
Simple path-like object.
It implements the __fspath__() method which just returns the path argument.
If path
is an exception, it will be raised in __fspath__().
Temporarily unset the environment variable envvar.
A context manager that temporarily changes the current working directory to path and yields the directory.
If quiet is False, the context manager raises an exception on error.
Otherwise, it issues only a warning and keeps
the current working directory the same.
If it already exists, truncate it.
Return True if the file system for directory is case-insensitive.
test.support.os_helper.make_bad_fd()
Create an invalid file descriptor by opening and closing a temporary file, and returning its descriptor.
26.15.
Call os.rmdir() on filename.
On Windows platforms, this is wrapped with a wait loop that checks for the
existence of the file, which is needed due to antivirus programs that can hold files open and prevent deletion.
As with rmdir(), on Windows platforms this is wrapped with a wait loop that checks for the existence of the
files.
The context manager creates a temporary directory in the current directory with name name before temporarily
changing the current working directory.
If name is None, the temporary directory is created using tempfile.
If quiet is False and it is not possible to create or change the CWD, an error is raised.
Otherwise, only a warning
is raised and the original CWD is used.
If path is None, the temporary directory is created using tempfile.mkdtemp().
If quiet is False, the
context manager raises an exception on error.
Otherwise, if path is specified and cannot be created, only a warning
is issued.
A context manager that temporarily sets the process umask.
As with rmdir(), on Windows platforms, this is wrapped with a wait loop
that checks for the existence of the file.
Remove the module named module_name from sys.modules and delete any byte-compiled files of the module.
This function imports and returns a fresh copy of the named Python module by removing the named module from
sys.modules before doing the import.
Note that unlike reload(), the original module is not affected by this
operation.
Module and package deprecation messages are suppressed during this import if deprecated is True.
This function will raise ImportError if the named module cannot be imported.
Example use:
# Get copies of the warnings module for testing without affecting the
# version being used by the rest of the test suite.
This function imports and returns the named module.
Unlike a normal import, this function raises unittest.
SkipTest if the module cannot be imported.
Module and package deprecation messages are suppressed during this import if deprecated is True.
If a module
is required on a platform but optional for others, set required_on to an iterable of platform prefixes which will be
compared against sys.platform.
New in version 3.1.
Delete name from sys.modules.
The source value is the file system path to the source file.
It does not need to exist, however the PEP 3147/488 pyc
file must exist.
A context manager to force import to return a new module reference.
This is useful for testing module-level
behaviors, such as the emission of a DeprecationWarning on import.
Example usage:
with CleanImport('foo'):
importlib.import_module('foo')
# New reference.
A context manager to temporarily add directories to sys.path.
This makes a copy of sys.path, appends any directories given as positional arguments, then reverts sys.path
to the copied settings when the context ends.
Note that all sys.path modifications in the body of the context manager, including replacement of the object,
will be reverted at the end of the block.
26.16.
New in version 3.10.
Roughly
equivalent
to
warnings.catch_warnings()
with
warnings.simplefilter('ignore',
category=category).
For example:
@warning_helper.ignore_warnings(category=DeprecationWarning)

# do something
New in version 3.8.
test.support.warnings_helper.check_no_resource_warning(testcase)
Context manager to check that no ResourceWarning was raised.
You must remove the object which may emit
ResourceWarning before the end of the context manager.
Test also that the SyntaxWarning
is emitted only once, and that it will be converted to a SyntaxError when turned into error.
If lineno is not None, compares to the line of the
warning and exception.
If offset is not None, compares to the offset of the exception.
New in version 3.8.
A convenience wrapper for warnings.catch_warnings() that makes it easier to test that a warning was
correctly raised.
It is approximately equivalent to calling warnings.catch_warnings(record=True)
with warnings.simplefilter() set to always and with the option to automatically validate the results
that are recorded.
If one or more filters are provided, or if the optional keyword argument quiet is False, it checks
to make sure the warnings are as expected: each specified filter must match at least one of the warnings raised by
the enclosed code or the test fails, and if any warnings are raised that do not match any of the specified filters the
test fails.
To disable the first of these checks, set quiet to True.
If no arguments are specified, it defaults to:
check_warnings(("", Warning), quiet=True)
In this case all warnings are caught and no errors are raised.
On entry to the context manager, a WarningRecorder instance is returned.
The underlying warnings list from
catch_warnings() is available via the recorder object’s warnings attribute.
As a convenience, the attributes
of the object representing the most recent warning can also be accessed directly through the recorder object (see
example below).
If no warning has been raised, then any of the attributes that would otherwise be expected on an
object representing a warning will return None.
In this case if either warning was not raised, or some other warning was raised, check_warnings() would
raise an error.
Changed in version 3.2: New optional arguments filters and quiet.
WarningsRecorder
Class used to record warnings for unit tests.
See documentation of check_warnings() above for more details.
26.17.
Development Tools
CHAPTER
TWENTYSEVEN
DEBUGGING AND PROFILING
These libraries help you with Python development: the debugger enables you to step through code, analyze stack frames
and set breakpoints etc., and the profilers run code and give you a detailed breakdown of execution times, allowing you
to identify bottlenecks in your programs.
Auditing events provide visibility into runtime behaviors that would otherwise
require intrusive debugging or patching.
27.1 Audit events table
This table contains all events raised by sys.audit() or PySys_Audit() calls throughout the CPython runtime and
the standard library.
These calls were added in 3.8.0 or later (see PEP 578).
See sys.addaudithook() and PySys_AddAuditHook() for information on handling these events.
CPython implementation detail: This table is generated from the CPython documentation, and may not represent events
raised by other implementations.
See your runtime specific documentation for actual events raised.
Audit event
Arguments
_thread.start_new_thread
function, args, kwargs
array.__new__
typecode, initializer
builtins.breakpoint
breakpointhook
builtins.id
id
builtins.input
prompt
builtins.input/result
result
code.__new__
code, filename, name, argcount, posonlyargcount, kwonlyargcount, nlocals
compile
source, filename
cpython.
PyInterpreterState_Clear
cpython.
Pdb
pickle.find_class
module, name
poplib.connect
self, host, port
poplib.putline
self, line
pty.spawn
argv
resource.prlimit
pid, resource, limits
resource.setrlimit
resource, limits
setopencodehook
shutil.chown
path, user, group
shutil.copyfile
src, dst
shutil.copymode
src, dst
shutil.copystat
src, dst
shutil.copytree
src, dst
shutil.make_archive
base_name, format, root_dir, base_dir
shutil.move
src, dst
shutil.rmtree
path, dir_fd
27.1.
Popen
executable, args, cwd, env
sys._current_exceptions
sys._current_frames
sys._getframe
frame
sys._getframemodulename
depth
sys.addaudithook
sys.excepthook
hook, type, value, traceback
sys.set_asyncgen_hooks_finalizer
sys.set_asyncgen_hooks_firstiter
sys.setprofile
sys.settrace
sys.unraisablehook
hook, unraisable
syslog.closelog
syslog.openlog
ident, logoption, facility
syslog.setlogmask
maskpri
syslog.syslog
priority, message
telnetlib.Telnet.open
self, host, port
telnetlib.Telnet.write
self, buffer
tempfile.mkdtemp
fullpath
tempfile.mkstemp
fullpath
urllib.
Request
fullurl, data, headers, method
webbrowser.open
url
winreg.
ConnectRegistry
computer_name, key
winreg.
CreateKey
key, sub_key, access
winreg.
DeleteKey
key, sub_key, access
winreg.
DeleteValue
key, value
winreg.
DisableReflectionKey
key
winreg.
EnumKey
key, index
winreg.
EnumValue
key, index
winreg.
ExpandEnvironmentStrings
str
winreg.
LoadKey
key, sub_key, file_name
winreg.
OpenKey
key, sub_key, access
winreg.
OpenKey/result
key
winreg.
PyHKEY.Detach
key
winreg.
QueryInfoKey
key
winreg.
QueryReflectionKey
key
winreg.
QueryValue
key, sub_key, value_name
winreg.
SaveKey
key, file_name
winreg.
SetValue
key, sub_key, type, value
The following events are raised internally and do not correspond to any public API of CPython:
Audit event
Arguments
_winapi.
CreateFile
file_name,
desired_access,
share_mode,
creation_disposition,
flags_and_attributes
_winapi.
CreateJunction src_path, dst_path
_winapi.
CreateNamedPipe
name, open_mode, pipe_mode
_winapi.
CreatePipe
_winapi.
CreateProcess application_name, command_line, current_directory
_winapi.
OpenProcess
process_id, desired_access
_winapi.
TerminateProcesshandle, exit_code
ctypes.
Lib/bdb.py
The bdb module handles basic debugger functions, like setting breakpoints or managing execution via the debugger.
The following exception is defined:
exception bdb.
BdbQuit
Exception raised by the Bdb class for quitting the debugger.
The bdb module also defines two classes:
class bdb.
This class implements temporary breakpoints, ignore counts, disabling and (re-)enabling, and conditionals.
Breakpoints are indexed by number through a list called bpbynumber and by (file, line) pairs through
bplist.
The former points to a single instance of class Breakpoint.
The latter points to a list of such instances
since there may be more than one breakpoint per line.
When creating a breakpoint, its associated file name should be in canonical form.
If a funcname is defined,
a breakpoint hit will be counted when the first line of that function is executed.
A conditional breakpoint
always counts a hit.
Breakpoint instances have the following methods:
27.2.
If it is the last breakpoint in that position, it also
deletes the entry for the file/line.
New in version 3.2.
Breakpoint instances have the following attributes:
file
File name of the Breakpoint.
Bdb(skip=None)
The Bdb class acts as a generic Python debugger base class.
This class takes care of the details of the trace facility; a derived class should implement user interaction.
The
standard debugger class (pdb.Pdb) is an example.
The skip argument, if given, must be an iterable of glob-style module name patterns.
The debugger will not step into
frames that originate in a module that matches one of these patterns.
The skip argument.
The following methods of Bdb normally don’t need to be overridden.
Return canonical form of filename.
For real file names, the canonical form is an operating-system-dependent, case-normalized absolute
path.
A filename with angle brackets, such as "<stdin>" generated in interactive mode, is returned
unchanged.
This function is installed as the trace function of debugged frames.
Its return value is the new trace function
(in most cases, that is, itself).
The default implementation decides how to dispatch a frame, depending on the type of event (passed as a
string) that is about to be executed.
For the Python events, specialized functions (see below) are called.
For the C events, no action is taken.
The arg parameter depends on the previous event.
See the documentation for sys.settrace() for more information on the trace function.
If the debugger should stop on the current line, invoke the user_line() method (which should be over-
ridden in subclasses).
Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set
from user_line()).
Return a reference to the trace_dispatch() method for further tracing in that
scope.
If the debugger should stop on this function call, invoke the user_call() method (which should be over-
ridden in subclasses).
Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set
27.2.
Return a reference to the trace_dispatch() method for further tracing in that
scope.
If the debugger should stop on this function return, invoke the user_return() method (which should be
overridden in subclasses).
Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set
from user_return()).
If the debugger should stop at this exception, invokes the user_exception() method (which should be
overridden in subclasses).
Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set
from user_exception()).
Return a reference to the trace_dispatch() method for further tracing
in that scope.
Normally derived classes don’t override the following methods, but they may if they want to redefine the definition
of stopping and breakpoints.
Return True if module_name matches any skip pattern.
Delete temporary breakpoints based on
information from effective().
Derived classes should override these methods to gain control over debugger operation.
Handle how a breakpoint must be removed when it is a temporary one.
This method must be implemented by derived classes.
Derived classes and clients can call the following methods to affect the stepping state.
If frame is not specified, debugging starts from caller’s frame.
If there are no breakpoints, set the system trace function to None.
set_quit()
Set the quitting attribute to True.
This raises BdbQuit in the next call to one of the dispatch_*()
methods.
Derived classes and clients can call the following methods to manipulate breakpoints.
These methods return a string
containing an error message if something went wrong, or None if all is well.
If the lineno line doesn’t exist for the filename passed as argument, return an error
message.
The filename should be in canonical form, as described in the canonic() method.
Delete the breakpoints in filename and lineno.
If none were set, return an error message.
Delete the breakpoint which has the index arg in the Breakpoint.bpbynumber.
Delete all breakpoints in filename.
If none were set, return an error message.
clear_all_breaks()
Delete all existing breakpoints.
If none were set, return an error message.
get_bpbynumber(arg)
Return a breakpoint specified by the given number.
If arg is a string, it will be converted to a number.
If arg
is a non-numeric string, if the given breakpoint never existed or has been deleted, a ValueError is raised.
New in version 3.2.
Return True if there is a breakpoint for lineno in filename.
Return all breakpoints for lineno in filename, or an empty list if none are set.
Derived classes and clients can call the following methods to get a data structure representing a stack trace.
The most recently called frame is last in the list.
The size is the number of frames below the frame where the
debugger was invoked.
The canonical filename which contains the frame.
The function name or "<lambda>".
The input arguments.
The line of code (if it exists).
The following two methods can be called by clients to use a debugger to debug a statement, given as a string.
Debug a statement executed via the exec() function.
For backwards compatibility.
Debug a single function call, and return its result.
Finally, the module defines the following functions:
bdb.checkfuncname(b, frame)
Return True if we should break here, depending on the way the Breakpoint b was set.
If it was set via line number, it checks if b.line is the same as the one in frame.
If the breakpoint was set
via function name, we have to check we are in the right frame (the right function) and if we are on its first
executable line.
Return (active breakpoint, delete temporary flag) or (None, None) as the breakpoint to
act upon.
The active breakpoint is the first entry in bplist for the (file, line) (which must exist) that is enabled, for
which checkfuncname() is True, and that has neither a False condition nor positive ignore count.
The
flag, meaning that a temporary breakpoint should be deleted, is False only when the cond cannot be evaluated (in
which case, ignore count is ignored).
If no such entry exists, then (None, None) is returned.
This module contains functions to dump Python tracebacks explicitly, on a fault, after a timeout, or on a user signal.
Call
faulthandler.enable() to install fault handlers for the SIGSEGV, SIGFPE, SIGABRT, SIGBUS, and SIGILL
signals.
You can also enable them at startup by setting the PYTHONFAULTHANDLER environment variable or by using
the -X faulthandler command line option.
The fault handler is compatible with system fault handlers like Apport or the Windows fault handler.
The module uses an
alternative stack for signal handlers if the sigaltstack() function is available.
This allows it to dump the traceback
even on a stack overflow.
The fault handler is called on catastrophic cases and therefore can only use signal-safe functions (e.g. it cannot allocate
memory on the heap).
Only ASCII is supported.
The backslashreplace error handler is used on encoding.
The order is reversed: the most recent call is shown first.
By default, the Python traceback is written to sys.stderr.
To see tracebacks, applications must be run in the terminal.
A log file can alternatively be passed to faulthandler.enable().
The module is implemented in C, so tracebacks can be dumped on a crash or when Python is deadlocked.
The Python Development Mode calls faulthandler.enable() at Python startup.
See also:
Module pdb Interactive source code debugger for Python programs.
Module traceback Standard interface to extract, format and print stack traces of Python programs.
If all_threads is False, dump only the current thread.
See also:
traceback.print_tb(), which can be used to print a traceback object.
Changed in version 3.5: Added support for passing file descriptor to this function.
If all_threads is True, produce tracebacks for every running thread.
Otherwise,
dump only the current thread.
The file must be kept open until the fault handler is disabled: see issue with file descriptors.
Changed in version 3.5: Added support for passing file descriptor to this function.
On Windows, a handler for Windows exception is also installed.
The dump now mentions if a garbage collector collection is running if all_threads is true.
Disable the fault handler: uninstall the signal handlers installed by enable().
Dumping the tracebacks after a timeout
faulthandler.dump_traceback_later(timeout, repeat=False, file=sys.stderr, exit=False)
Dump the tracebacks of all threads, after a timeout of timeout seconds, or every timeout seconds if repeat is True.
If exit is True, call _exit() with status=1 after dumping the tracebacks.
If the function is called twice, the
new call replaces previous parameters and resets the timeout.
The timer has a sub-second resolution.
The file must be kept open until the traceback is dumped or cancel_dump_traceback_later() is called:
see issue with file descriptors.
This function is implemented using a watchdog thread.
This function is now always available.
Changed in version 3.5: Added support for passing file descriptor to this function.
Dumping the traceback on a user signal
faulthandler.register(signum, file=sys.stderr, all_threads=True, chain=False)
Register a user signal: install a handler for the signum signal to dump the traceback of all threads, or of the current
thread if all_threads is False, into file.
Call the previous handler if chain is True.
The file must be kept open until the signal is unregistered by unregister(): see issue with file descriptors.
Not available on Windows.
Changed in version 3.5: Added support for passing file descriptor to this function.
Return True if the
signal was registered, False otherwise.
If the
file is closed and its file descriptor is reused by a new file, or if os.dup2() is used to replace the file descriptor, the
traceback will be written into a different file.
Call these functions again each time that the file is replaced.
Segmentation fault
27.4 pdb — The Python Debugger
Source code: Lib/pdb.py
The module pdb defines an interactive source code debugger for Python programs.
It supports setting (conditional)
breakpoints and single stepping at the source line level, inspection of stack frames, source code listing, and evaluation of
arbitrary Python code in the context of any stack frame.
It also supports post-mortem debugging and can be called under
program control.
The debugger is extensible – it is actually defined as the class Pdb.
This is currently undocumented but easily understood
by reading the source.
The extension interface uses the modules bdb and cmd.
See also:
Module faulthandler Used to dump Python tracebacks explicitly, on a fault, after a timeout, or on a user signal.
Module traceback Standard interface to extract, format and print stack traces of Python programs.
The typical usage to break into the debugger is to insert:
import pdb; pdb.set_trace()
Or:
breakpoint()
at the location you want to break into the debugger, and then run the program.
You can then step through the code
following this statement, and continue running without the debugger using the continue command.
The built-in breakpoint(), when called with defaults, can be used instead of import pdb;
pdb.set_trace().
The debugger’s prompt is (Pdb), which is the indicator that you are in debug mode:
> ...(3)double()
-> return x * 2
(Pdb) p x

(Pdb) continue
3 * 2 is 6
Changed in version 3.3: Tab-completion via the readline module is available for commands and command arguments,
e.g. the current global and local names are offered as arguments of the p command.
You can also invoke pdb from the command line to debug other scripts.
For example:
python -m pdb myscript.py
When invoked as a module, pdb will automatically enter post-mortem debugging if the program being debugged exits
abnormally.
After post-mortem debugging (or after normal exit of the program), pdb will restart the program.
Automatic
restarting preserves pdb’s state (such as breakpoints) and in most cases is more useful than quitting the debugger upon
program’s exit.
New in version 3.2: -c option is introduced to execute commands as if given in a .pdbrc file, see Debugger Commands.
New in version 3.7: -m option is introduced to execute modules similar to the way python -m does.
As with a script,
the debugger will pause execution just before the first line of the module.
The module defines the following functions; each enters the debugger in a slightly different way:
pdb.run(statement, globals=None, locals=None)
Execute the statement (given as a string or a code object) under debugger control.
The debugger prompt appears
before any code is executed; you can set breakpoints and type continue, or you can step through the statement
using step or next (all these commands are explained below).
The optional globals and locals arguments specify
the environment in which the code is executed; by default the dictionary of the module __main__ is used.
When runeval() returns, it
returns the value of the expression.
Otherwise this function is similar to run().
When runcall() returns,
it returns whatever the function call returned.
The debugger prompt appears as soon as the function is entered.
This is useful to hard-code a breakpoint at a given point in a program,
even if the code is not otherwise being debugged (e.g. when an assertion fails).
If given, header is printed to the
console just before debugging begins.
The keyword-only argument header.
If no traceback is given, it uses the one of the exception
that is currently being handled (an exception must be being handled if the default is to be used).
The run* functions and set_trace() are aliases for instantiating the Pdb class and calling the method of the same
name.
The completekey, stdin and stdout arguments are passed to the underlying cmd.
Cmd class; see the description there.
The skip argument, if given, must be an iterable of glob-style module name patterns.
The debugger will not step
into frames that originate in a module that matches one of these patterns.1
By default, Pdb sets a handler for the SIGINT signal (which is sent when the user presses Ctrl-C on the console)
when you give a continue command.
This allows you to break into the debugger again by pressing Ctrl-C. If
you want Pdb not to touch the SIGINT handler, set nosigint to true.
The readrc argument defaults to true and controls whether Pdb will load .pdbrc files from the filesystem.
Example call to enable tracing with skip:
import pdb; pdb.Pdb(skip=['django.*']).set_trace()
Raises an auditing event pdb.
Pdb with no arguments.
The skip argument.
The nosigint argument.
The readrc argument.
Most commands can be abbreviated to one or two letters as
indicated; e.g. h(elp) means that either h or help can be used to enter the help command (but not he or hel, nor
H or Help or HELP).
Arguments to commands must be separated by whitespace (spaces or tabs).
Optional arguments
are enclosed in square brackets ([]) in the command syntax; the square brackets must not be typed.
Alternatives in the
command syntax are separated by a vertical bar (|).
Entering a blank line repeats the last command entered.
Exception: if the last command was a list command, the next
11 lines are listed.
Commands that the debugger doesn’t recognize are assumed to be Python statements and are executed in the context of
the program being debugged.
Python statements can also be prefixed with an exclamation point (!).
This is a powerful
way to inspect the program being debugged; it is even possible to change a variable or call a function.
When an exception
occurs in such a statement, the exception name is printed but the debugger’s state is not changed.
The debugger supports aliases.
Aliases can have parameters which allows one a certain level of adaptability to the context
under examination.
Multiple commands may be entered on a single line, separated by ;;.
No intelligence is applied to separating the commands;
the input is split at the first ;; pair, even if it is in the middle of a quoted string.
A workaround for strings with double
semicolons is to use implicit string concatenation ';'';' or ";"";".
To set a temporary global variable, use a convenience variable.
A convenience variable is a variable whose name starts with
$.
For example, $foo = 1 sets a global variable $foo which you can use in the debugger session.
The convenience
variables are cleared when the program resumes execution so it’s less likely to interfere with your program compared to
using normal variables like foo = 1.
There are three preset convenience variables:
• $_frame: the current frame you are debugging
• $_retval: the return value if the frame is returning
• $_exception: the exception if the frame is raising an exception
New in version 3.12.
If a file .pdbrc exists in the user’s home directory or in the current directory, it is read with 'utf-8' encoding and
executed as if it had been typed at the debugger prompt.
This is particularly useful for aliases.
If both files exist, the one
in the home directory is read first and aliases defined there can be overridden by the local file.
Changed in version 3.11: .pdbrc is now read with 'utf-8' encoding.
Previously, it was read with the system locale
encoding.
Changed in version 3.2: .pdbrc can now contain commands that continue debugging, such as continue or next.
With a command as argument, print help about that
command.
An arrow (>) indicates the current frame, which
determines the context of most commands.
With a function argument, set a break at the first
executable statement within that function.
The line number may be prefixed with a filename and a colon, to specify
a breakpoint in another file (probably one that hasn’t been loaded yet).
The file is searched on sys.path.
Note
that each breakpoint is assigned a number to which all the other breakpoint commands refer.
If a second argument is present, it is an expression which must evaluate to true before the breakpoint is honored.
Without argument, list all breaks, including for each breakpoint, the number of times that breakpoint has been hit,
the current ignore count, and the associated condition if any.
The arguments are the same as for
break.
With a space separated list of breakpoint
numbers, clear those breakpoints.
Without argument, clear all breaks (but first ask confirmation).
Disable the breakpoints given as a space separated list of breakpoint numbers.
If count is omitted, the ignore count is set to 0.
A breakpoint
becomes active when the ignore count is zero.
When non-zero, the count is decremented each time the breakpoint
is reached and the breakpoint is not disabled and any associated condition evaluates to true.
If condition is absent, any existing condition is removed; i.e., the breakpoint is made unconditional.
The commands themselves appear on the following
lines.
Type a line containing just end to terminate the commands.
An example:
(Pdb) commands 1
(com) p some_variable
(com) end
(Pdb)
To remove all commands from a breakpoint, type commands and follow it immediately with end; that is, give no
commands.
With no bpnumber argument, commands refers to the last breakpoint set.
Simply use the continue command, or
step, or any other command that resumes execution.
Specifying any command resuming execution (currently continue, step, next, return, jump, quit and
their abbreviations) terminates the command list (as if that command was immediately followed by end).
This is
because any time you resume execution (even with a simple next or step), you may encounter another breakpoint—
which could have its own command list, leading to ambiguities about which list to execute.
If you use the silent command in the command list, the usual message about stopping at a breakpoint is not
printed.
This may be desirable for breakpoints that are to print a specific message and then continue.
If none of
the other commands print anything, you see no sign that the breakpoint was reached.
s(tep)
Execute the current line, stop at the first possible occasion (either in a function that is called or on the next line in
the current function).
With lineno, continue execution until a line with a number greater or equal to lineno is reached.
In both cases, also
stop when the current frame returns.
Changed in version 3.2: Allow giving an explicit line number.
Continue execution, only stop when a breakpoint is encountered.
j(ump) lineno
Set the next line that will be executed.
Only available in the bottom-most frame.
This lets you jump back and
execute code again, or jump forward to skip code that you don’t want to run.
It should be noted that not all jumps are allowed – for instance it is not possible to jump into the middle of a for
loop or out of a finally clause.
Without arguments, list 11 lines around the current line or continue the previous
listing.
With .
With one argument, list 11 lines around at that
line.
With two arguments, list the given range; if the second argument is less than the first, it is interpreted as a
count.
The current line in the current frame is indicated by ->.
If an exception is being debugged, the line where the
exception was originally raised or propagated is indicated by >>, if it differs from the current line.
The >> marker.
Interesting lines are marked as for list.
New in version 3.2.
Note: print() can also be used, but is not a debugger command — this executes the Python print() function.
New in version 3.2.
Without expression, list all display expressions for the current frame.
Note: Display evaluates expression and compares to the result of the previous evaluation of expression, so when
the result is mutable, display may not be able to pick up the changes.
Without expression, clear all display expressions for the
current frame.
New in version 3.2.
New in version 3.2.
The command must not be enclosed in quotes.
Replaceable
parameters can be indicated by %1, %2, and so on, while %* is replaced by all the parameters.
If command is
omitted, the current alias for name is shown.
If no arguments are given, all aliases are listed.
Aliases may be nested and can contain anything that can be legally typed at the pdb prompt.
Note that internal pdb
commands can be overridden by aliases.
Such a command is then hidden until the alias is removed.
Aliasing is
recursively applied to the first word of the command line; all other words in the line are left alone.
The exclamation point can be omitted
unless the first word of the statement resembles a debugger command, e.g.:
(Pdb) !
If args is supplied, it is split with shlex and the result is used as the new
sys.argv. History, breakpoints, actions and debugger options are preserved.
The program being executed is aborted.
The Python Profilers
Source code: Lib/profile.py and Lib/pstats.py
27.5.1 Introduction to the profilers
cProfile and profile provide deterministic profiling of Python programs.
A profile is a set of statistics that describes
how often and for how long various parts of the program executed.
These statistics can be formatted into reports via the
pstats module.
The Python standard library provides two different implementations of the same profiling interface:
1.
Based on lsprof, contributed by Brett Rosen and Ted Czotter.
If you’re trying to extend the profiler in some way, the task might be easier with this module.
Originally designed and written by Jim Roskind.
Note:
The profiler modules are designed to provide an execution profile for a given program, not for benchmarking
purposes (for that, there is timeit for reasonably accurate results).
This particularly applies to benchmarking Python
code against C code: the profilers introduce overhead for Python code, but not for C-level functions, and so the C code
would seem faster than any Python one.
It provides a very brief overview, and allows a
user to rapidly perform profiling on an existing application.
The above action would run re.compile() and print profile results like the following:
27.5.
The first line indicates that 214 calls were monitored.
Of those calls, 207 were primitive, meaning that the call was not
induced via recursion.
The next line: Ordered by: cumulative time, indicates that the text string in the far
right column was used to sort the output.
The column headings include:
ncalls for the number of calls.
This figure is accurate even
for recursive functions.
The second value
is the number of primitive calls and the former is the total number of calls.
Note that when the function does not recurse,
these two values are the same, and only the single figure is printed.
The pstats.
Stats class reads profile results from a file and formats them in various ways.
The files cProfile and profile can also be invoked as a script to profile another script.
This only applies when -o is not supplied.
Added the -m option to cProfile.
New in version 3.8: Added the -m option to profile.
The strip_dirs() method removed the extraneous path from all the module names.
The sort_stats() method
sorted all the entries according to the standard module/line/name string that is printed.
The print_stats() method
printed out all the statistics.
The first call will actually sort the list by function name, and the second call will print out the statistics.
This sorts the profile by cumulative time in a function, and then only prints the ten most significant lines.
If you want to
understand what algorithms are taking time, the above line is what you would use.
If you were looking to see what functions were looping a lot, and taking a lot of time, you would do:
p.sort_stats(SortKey.TIME).print_stats(10)
to sort according to time spent within each function, and then print the statistics for the top ten functions.
You might also try:
p.sort_stats(SortKey.
This will sort all the statistics by file name, and then print out statistics for only the class init methods (since they are
spelled with __init__ in them).
As one final example, you could try:
p.sort_stats(SortKey.
TIME, SortKey.
This line sorts statistics with a primary key of time, and a secondary key of cumulative time, and then prints out some of
the statistics.
To be specific, the list is first culled down to 50% (re: .5) of its original size, then only lines containing
init are maintained, and that sub-sub-list is printed.
If you wondered what functions called the above functions, you could now (p is still sorted according to the last criteria)
do:
p.print_callers(.5, 'init')
and you would get a list of callers for each of the listed functions.
If you want more functionality, you’re going to have to read the manual, or guess what the following functions do:
p.print_callees()
p.add('restats')
Invoked as a script, the pstats module is a statistics browser for reading and examining profile dumps.
It has a simple
line-oriented interface (implemented using cmd) and interactive help.
27.5.
This function takes a single argument that can be passed to the exec() function, and an optional file name.
In all
cases this routine executes:
exec(command, __main__.__dict__, __main__.__dict__)
and gathers profiling statistics from the execution.
If no file name is present, then this function automatically
creates a Stats instance and prints a simple profiling report.
If the sort value is specified, it is passed to this
Stats instance to control how the results are sorted.
This function is similar to run(), with added arguments to supply the globals and locals dictionaries for the
command string.
This routine executes:
exec(command, globals, locals)
and gathers profiling statistics as in the run() function above.
class profile.
This class is normally only used if more precise control over profiling is needed than what the cProfile.run()
function provides.
A custom timer can be supplied for measuring how long code takes to run via the timer argument.
This must be a
function that returns a single number representing the current time.
If the number is an integer, the timeunit specifies
a multiplier that specifies the duration of each unit of time.
For example, if the timer returns times measured in
thousands of seconds, the time unit would be .001.
Directly using the Profile class allows formatting profile results without writing the profile data to a file:
import cProfile, pstats, io
from pstats import SortKey
pr = cProfile.
The Profile class can also be used as a context manager (supported only in cProfile module.
Profile() as pr:
# ... do something ...
pr.print_stats()
Changed in version 3.8: Added context manager support.
Only in cProfile.
Only in cProfile.
create_stats()
Stop collecting profiling data and record the results internally as the current profile.
Profile the cmd via exec().
Note that profiling will only work if the called command/function actually returns.
The Stats Class
Analysis of the profiler data is done using the Stats class.
This class constructor creates an instance of a “statistics object” from a filename (or list of filenames) or from a
Profile instance.
Output will be printed to the stream specified by stream.
The file selected by the above constructor must have been created by the corresponding version of profile or
cProfile.
To be specific, there is no file compatibility guaranteed with future versions of this profiler, and there
is no compatibility with files produced by other profilers, or the same profiler run on a different operating system.
If
several files are provided, all the statistics for identical functions will be coalesced, so that an overall view of several
processes can be considered in a single report.
If additional files need to be combined with data in an existing
Stats object, the add() method can be used.
Instead of reading the profile data from a file, a cProfile.
Profile or profile.
Profile object can be
used as the profile data source.
This method for the Stats class removes all leading path information from file names.
It is very useful in
reducing the size of the printout to fit within (close to) 80 columns.
This method modifies the object, and the
stripped information is lost.
After performing a strip operation, the object is considered to have its entries
in a “random” order, as it was just after object initialization and loading.
If strip_dirs() causes two
function names to be indistinguishable (they are on the same line of the same filename, and have the same
function name), then the statistics for these two entries are accumulated into a single entry.
This method of the Stats class accumulates additional profiling information into the current profiling ob-
ject.
Its arguments should refer to filenames created by the corresponding version of profile.run()
or cProfile.run().
Save the data loaded into the Stats object to a file named filename.
The file is created if it does not exist,
and is overwritten if it already exists.
This is equivalent to the method of the same name on the profile.
Profile and cProfile.
This method modifies the Stats object by sorting it according to the supplied criteria.
The argument can be
either a string or a SortKey enum identifying the basis of a sort (example: 'time', 'name', SortKey.
TIME or SortKey.NAME).
The SortKey enums argument have advantage over the string argument in that
it is more robust and less error prone.
When more than one key is provided, then additional keys are used as secondary criteria when there is equality
in all keys selected before them.
For example, sort_stats(SortKey.NAME, SortKey.FILE) will
sort all the entries according to their function name, and resolve all ties (identical function names) by sorting
by file name.
For the string argument, abbreviations can be used for any key names, as long as the abbreviation is unam-
biguous.
The following are the valid string and SortKey:
Valid String Arg
Valid enum Arg
Meaning
'calls'
SortKey.CALLS
call count
'cumulative'
SortKey.CUMULATIVE
cumulative time
'cumtime'
N/A
cumulative time
'file'
N/A
file name
'filename'
SortKey.
FILENAME
file name
'module'
N/A
file name
'ncalls'
N/A
call count
'pcalls'
SortKey.
PCALLS
primitive call count
'line'
SortKey.LINE
line number
'name'
SortKey.
NAME
function name
'nfl'
SortKey.
NFL
name/file/line
'stdname'
SortKey.
STDNAME
standard name
'time'
SortKey.
TIME
internal time
'tottime'
N/A
internal time
Note that all sorts on statistics are in descending order (placing most time consuming items first), where as
name, file, and line number searches are in ascending order (alphabetical).
The subtle distinction between
SortKey.
NFL and SortKey.
STDNAME is that the standard name is a sort of the name as printed, which
means that the embedded line numbers get compared in an odd way.
For example, lines 3, 20, and 40 would
(if the file names were the same) appear in the string order 20, 3 and 40.
In contrast, SortKey.
NFL
does a numeric compare of the line numbers.
In fact, sort_stats(SortKey.NFL) is the same as
sort_stats(SortKey.NAME, SortKey.FILENAME, SortKey.LINE).
For backward-compatibility reasons, the numeric arguments -1, 0, 1, and 2 are permitted.
They are inter-
preted as 'stdname', 'calls', 'time', and 'cumulative' respectively.
If this old style format
(numeric) is used, only one sort key (the numeric key) will be used, and additional arguments will be silently
ignored.
This method for the Stats class reverses the ordering of the basic list within the object.
This method for the Stats class prints out a report as described in the profile.run() definition.
The order of the printing is based on the last sort_stats() operation done on the object (subject to
caveats in add() and strip_dirs()).
The arguments provided (if any) can be used to limit the list down to the significant entries.
Initially, the list
is taken to be the complete set of profiled functions.
Each restriction is either an integer (to select a count of
lines), or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines), or a string that will
interpreted as a regular expression (to pattern match the standard name that is printed).
If several restrictions
are provided, then they are applied sequentially.
For example:
print_stats(.1, 'foo:')
would first limit the printing to first 10% of list, and then only print functions that were part of filename
.*foo:.
This method for the Stats class prints a list of all functions that called each function in the profiled database.
The ordering is identical to that provided by print_stats(), and the definition of the restricting argument
is also identical.
Each caller is reported on its own line.
The format differs slightly depending on the profiler
that produced the stats:
• With profile, a number is shown in parentheses after each caller to show how many times this specific
call was made.
For convenience, a second non-parenthesized number repeats the cumulative time spent
in the function at the right.
This method for the Stats class prints a list of all function that were called by the indicated function.
Aside
from this reversal of direction of calls (re: called vs was called by), the arguments and ordering are identical
to the print_callers() method.
This method returns an instance of StatsProfile, which contains a mapping of function names to instances of
FunctionProfile.
Each FunctionProfile instance holds information related to the function’s profile such as how
long the function took to run, how many times it was called, etc…
New in version 3.9: Added the following dataclasses: StatsProfile, FunctionProfile.
Added the following
function: get_stats_profile.
27.5.
What Is Deterministic Profiling?
Deterministic profiling is meant to reflect the fact that all function call, function return, and exception events are monitored,
and precise timings are made for the intervals between these events (during which time the user’s code is executing).
In
contrast, statistical profiling (which is not done by this module) randomly samples the effective instruction pointer, and
deduces where time is being spent.
The latter technique traditionally involves less overhead (as the code does not need to
be instrumented), but provides only relative indications of where time is being spent.
In Python, since there is an interpreter active during execution, the presence of instrumented code is not required in
order to do deterministic profiling.
Python automatically provides a hook (optional callback) for each event.
In addition,
the interpreted nature of Python tends to add so much overhead to execution, that deterministic profiling tends to only
add small processing overhead in typical applications.
The result is that deterministic profiling is not that expensive, yet
provides extensive run time statistics about the execution of a Python program.
Call count statistics can be used to identify bugs in code (surprising counts), and to identify possible inline-expansion
points (high call counts).
Internal time statistics can be used to identify “hot loops” that should be carefully optimized.
Cumulative time statistics should be used to identify high level errors in the selection of algorithms.
Note that the unusual
handling of cumulative times in this profiler allows statistics for recursive implementations of algorithms to be directly
compared to iterative implementations.
There is a fundamental problem with deterministic profilers
involving accuracy.
The most obvious restriction is that the underlying “clock” is only ticking at a rate (typically) of about
.001 seconds.
Hence no measurements will be more accurate than the underlying clock.
If enough measurements are
taken, then the “error” will tend to average out.
Unfortunately, removing this first error induces a second source of error.
The second problem is that it “takes a while” from when an event is dispatched until the profiler’s call to get the time
actually gets the state of the clock.
Similarly, there is a certain lag when exiting the profiler event handler from the
time that the clock’s value was obtained (and then squirreled away), until the user’s code is once again executing.
As a
result, functions that are called many times, or call many functions, will typically accumulate this error.
The error that
accumulates in this fashion is typically less than the accuracy of the clock (less than one clock tick), but it can accumulate
and become very significant.
The problem is more important with profile than with the lower-overhead cProfile.
For this reason, profile
provides a means of calibrating itself for a given platform so that this error can be probabilistically (on the average)
removed.
Do not be
alarmed by negative numbers in the profile.
They should only appear if you have calibrated your profiler, and the results
are actually better than without calibration.
Calibration
The profiler of the profile module subtracts a constant from each event handling time to compensate for the overhead
of calling the time function, and socking away the results.
By default, the constant is 0.
The following procedure can be
used to obtain a better constant for a given platform (see Limitations).
The method executes the number of Python calls given by the argument, directly and again under the profiler, measuring
the time for both.
It then computes the hidden overhead per profiler event, and returns that as a float.
The object of this exercise is to get a fairly consistent result.
If your computer is very fast, or your timer function has poor
resolution, you might have to pass 100000, or even 1000000, to get consistent results.
When you have a consistent answer, there are three ways you can use it:
import profile
# 1.
Apply computed bias to all Profile instances created hereafter.
Apply computed bias to a specific Profile instance.
Profile()
pr.bias = your_computed_bias
# 3.
Specify computed bias in instance constructor.
Profile(bias=your_computed_bias)
If you have a choice, you are better off choosing a smaller constant, and then your results will “less often” show up as
negative in profile statistics.
The resulting profiler will then call your_time_func.
Depending on whether you are using profile.
Profile
or cProfile.
Profile, your_time_func’s return value will be interpreted differently:
profile.
Profile your_time_func should return a single number, or a list of numbers whose sum is the current
time (like what os.times() returns).
If the function returns a single time number, or the list of returned numbers
has length 2, then you will get an especially fast version of the dispatch routine.
Be warned that you should calibrate the profiler class for the timer function that you choose (see Calibration).
For
most machines, a timer that returns a lone integer value will provide the best results in terms of low overhead during
profiling.
If you want to substitute a
better timer in the cleanest fashion, derive a class and hardwire a replacement dispatch method that best handles
your timer call, along with the appropriate calibration constant.
cProfile.
Profile your_time_func should return a single number.
If it returns integers, you can also in-
voke the class constructor with a second argument specifying the real duration of one unit of time.
For exam-
ple, if your_integer_time_func returns times measured in thousands of seconds, you would construct the
Profile instance as follows:
pr = cProfile.
As the cProfile.
Profile class cannot be calibrated, custom timer functions should be used with care and
should be as fast as possible.
For the best results with a custom timer, it might be necessary to hard-code it in the
C source of the internal _lsprof module.
Python 3.3 adds several new functions in time that can be used to make precise measurements of process or wall-clock
time.
For example, see time.perf_counter().
It has both a Command-Line Interface as well as
a callable one.
It avoids a number of common traps for measuring execution times.
See also Tim Peters’ introduction to
the “Algorithms” chapter in the second edition of Python Cookbook, published by O’Reilly.
In the Examples section you can find more advanced examples.
The module defines three convenience functions and a public class:
timeit.timeit(stmt=’pass’, setup=’pass’, timer=<default timer>, number=1000000, globals=None)
Create a Timer instance with the given statement, setup code and timer function and run its timeit() method
with number executions.
The optional globals argument specifies a namespace in which to execute the code.
The optional globals parameter was added.
The optional globals argument specifies a namespace in which
to execute the code.
The optional globals parameter was added.
The default timer, which is always time.perf_counter(), returns float seconds.
An alternative, time.perf_counter_ns,
returns integer nanoseconds.
Changed in version 3.3: time.perf_counter() is now the default timer.
Timer(stmt=’pass’, setup=’pass’, timer=<timer function>, globals=None)
Class for timing execution speed of small code snippets.
The constructor takes a statement to be timed, an additional statement used for setup, and a timer function.
Both
statements default to 'pass'; the timer function is platform-dependent (see the module doc string).
The statement will by default be executed within timeit’s namespace; this behavior can be controlled
by passing a namespace to globals.
To measure the execution time of the first statement, use the timeit() method.
The repeat() and
autorange() methods are convenience methods to call timeit() multiple times.
The execution time of setup is excluded from the overall timed execution run.
The stmt and setup parameters can also take objects that are callable without arguments.
This will embed calls to
them in a timer function that will then be executed by timeit().
Note that the timing overhead is a little larger
in this case because of the extra function calls.
The optional globals parameter was added.
This executes the setup statement once, and then returns the
time it takes to execute the main statement a number of times.
The default timer returns seconds as a float.
The argument is the number of times through the loop, defaulting to one million.
The main statement, the
setup statement and the timer function to be used are passed to the constructor.
Note: By default, timeit() temporarily turns off garbage collection during the timing.
The advantage of
this approach is that it makes independent timings more comparable.
The disadvantage is that GC may be an
important component of the performance of the function being measured.
If so, GC can be re-enabled as the
first statement in the setup string.
For example:
timeit.
Timer('for i in range(10): oct(i)', 'gc.enable()').timeit()
autorange(callback=None)
Automatically determine how many times to call timeit().
This is a convenience function that calls timeit() repeatedly so that the total time >= 0.2 second, returning
the eventual (number of loops, time taken for that number of loops).
It calls timeit() with increasing
numbers from the sequence 1, 2, 5, 10, 20, 50, … until the time taken is at least 0.2 seconds.
If callback is given and is not None, it will be called after each trial with two arguments:
callback(number, time_taken).
Call timeit() a few times.
This is a convenience function that calls the timeit() repeatedly, returning a list of results.
The first
argument specifies how many times to call timeit().
The second argument specifies the number argument
for timeit().
How-
ever, this is not very useful.
In a typical case, the lowest value gives a lower bound for how fast your machine
can run the given code snippet; higher values in the result vector are typically not caused by variability in
Python’s speed, but by other processes interfering with your timing accuracy.
So the min() of the result is
probably the only number you should be interested in.
After that, you should look at the entire vector and
apply common sense rather than statistics.
Default value of repeat changed from 3 to 5.
The advantage over the standard traceback is that source lines in the compiled template will be displayed.
The optional file argument directs where the traceback is sent; it defaults to sys.stderr.
Multiple -s options are treated similarly.
If -n is not given, a suitable number of loops is calculated by trying increasing numbers from the sequence 1, 2, 5, 10,
20, 50, … until the total time is at least 0.2 seconds.
The -r option is
good for this; the default of 5 repetitions is probably enough in most cases.
You can use time.process_time() to
measure CPU time.
Note: There is a certain baseline overhead associated with executing a pass statement.
The code here doesn’t try to hide
it, but you should be aware of it.
The baseline overhead can be measured by invoking the program without arguments,
and it might differ between Python versions.
The loop count, which tells you how many times the statement body was run per
timing loop repetition.
The repetition count (‘best of 5’) which tells you how many times the timing loop was repeated,
and finally the time the statement body took on average within the best repetition of the timing loop.
That is, the time the
fastest repetition took divided by the loop count.
Another option is to pass globals() to the globals parameter, which will cause the code to be executed within your
current global namespace.
It can be used in another program or from
the command line.
A popular third-party coverage tool that provides HTML output along with advanced features such as
branch coverage.
It can be as simple as
python -m trace --count -C .
The above will execute somefile.py and generate annotated listings of all Python modules imported during the
execution into the current directory.
New in version 3.8: Added --module option that allows to run an executable module.
Main options
At least one of the following options must be specified when invoking trace.
The --listfuncs option is mutu-
ally exclusive with the --trace and --count options.
When --listfuncs is provided, neither --count nor
--trace are accepted, and vice versa.
See also --coverdir, --file and --no-report below.
This does not
execute any code.
Modifiers
-f, --file=<file>
Name of a file to accumulate counts over several tracing runs.
Should be used with the --count option.
When generating annotated listings, mark lines which were not executed with -s, --summary
When using --count or --report, write a brief summary to stdout for each file processed.
This is useful if you intend to make several runs with --count, and then
produce a single set of annotated listings at the end.
Only used while tracing.
Filters
These options may be repeated multiple times.
The argument can be a list of names
separated by a comma.
The argument can be a list of directories
separated by os.pathsep.
Trace(count=1, trace=1, countfuncs=0, countcallers=0, ignoremods=(), ignoredirs=(), infile=None,
outfile=None, timing=False)
Create an object to trace execution of a single statement or expression.
All parameters are optional.
Execute the command and gather statistics from the execution with the current tracing parameters.
If not defined, globals and locals default to empty dictionaries.
runfunc(func, /, *args, **kwds)
Call func with the given arguments under control of the Trace object with the current tracing parameters.
results()
Return a CoverageResults object that contains the cumulative results of all previous calls to run,
runctx and runfunc for the given Trace instance.
Does not reset the accumulated trace results.
CoverageResults
A container for coverage results, created by Trace.results().
Should not be created directly by the user.
Set show_missing to show lines that had no hits.
Set summary to include in the output
the coverage summary per module.
If None, the results for each source file are placed in its directory.
A simple example demonstrating the use of the programmatic interface:
import sys
import trace
# create a Trace object, telling it what to ignore, and whether to
# do tracing or line-counting or both.
tracer = trace.
Trace(
ignoredirs=[sys.prefix, sys.exec_prefix],
trace=0,
count=1)
# run the new command using the given tracer
tracer.run('main()')
# make a report, placing output in the current directory
r = tracer.results()
r.write_results(show_missing=True, coverdir=".")
27.8 tracemalloc — Trace memory allocations
New in version 3.4.
Source code: Lib/tracemalloc.py
The tracemalloc module is a debug tool to trace memory blocks allocated by Python.
Traceback where an object was allocated
• Statistics on allocated memory blocks per filename and per line number: total size, number and average size of
allocated memory blocks
• Compute the differences between two snapshots to detect memory leaks
27.8.
The
tracemalloc.start() function can be called at runtime to start tracing Python memory allocations.
By default, a trace of an allocated memory block only stores the most recent frame (1 frame).
To store 25 frames at
startup: set the PYTHONTRACEMALLOC environment variable to 25, or use the -X tracemalloc=25 command line
option.
See Snapshot.statistics() for more options.
Similarly, the linecache module
has cached 940 KiB of Python source code to format tracebacks, all of it since the previous snapshot.
If the system has little free memory, snapshots can be written on disk using the Snapshot.dump() method to analyze
the snapshot offline.
Then use the Snapshot.load() method reload the snapshot.
27.8.
The traceback is where the importlib loaded data most recently: on the import pdb line
of the doctest module.
The traceback may change if a new module is loaded.
Pretty top
Code to display the 10 lines allocating the most memory with a pretty output, ignoring <frozen importlib.
Filter(False, "<frozen importlib._bootstrap>"),
tracemalloc.
Record the current and peak size of all traced memory blocks
The following code computes two sums like 0 + 1 + 2 + ...
This
list consumes a lot of memory temporarily.
Output:
first_size=664, first_peak=3592984
second_size=804, second_peak=29704
Using reset_peak() ensured we could accurately record the peak during the computation of small_sum, even
though it is much smaller than the overall peak size of memory blocks since the start() call.
Without the call
to reset_peak(), second_peak would still be the peak from the computation large_sum (that is, equal to
first_peak).
In this case, both peaks are much higher than the final memory usage, and which suggests we could
optimise (by removing the unnecessary call to list, and writing sum(range(...))).
See also stop().
Get the traceback where the Python object obj was allocated.
Return a Traceback instance, or None if the
tracemalloc module is not tracing memory allocations or did not trace the allocation of the object.
See also gc.get_referrers() and sys.getsizeof() functions.
The tracemalloc module must be tracing memory allocations to get the limit, otherwise an exception is raised.
The limit is set by the start() function.
Do nothing if the tracemalloc module is not tracing memory allocations.
This function only modifies the recorded peak size, and does not modify or clear any traces, unlike
clear_traces().
Snapshots taken with take_snapshot() before a call to reset_peak() can be
meaningfully compared to snapshots taken after the call.
See also get_traced_memory().
New in version 3.9.
Return an
int.
See also start() and stop() functions.
Collected tracebacks of
traces will be limited to nframe frames.
By default, a trace of a memory block only stores the most recent frame:
the limit is 1. nframe must be greater or equal to 1.
You can still read the original number of total frames that composed the traceback by looking at the Traceback.
total_nframe attribute.
Storing more than 1 frame is only useful to compute statistics grouped by 'traceback' or to compute cumulative
statistics: see the Snapshot.compare_to() and Snapshot.statistics() methods.
Storing more frames increases the memory and CPU overhead of the tracemalloc module.
Use the
get_tracemalloc_memory() function to measure how much memory is used by the tracemalloc mod-
ule.
See also stop(), is_tracing() and get_traceback_limit() functions.
tracemalloc.stop()
Stop tracing Python memory allocations: uninstall hooks on Python memory allocators.
Also clears all previously
collected traces of memory blocks allocated by Python.
Call take_snapshot() function to take a snapshot of traces before clearing them.
See also start(), is_tracing() and clear_traces() functions.
Return a new Snapshot instance.
The snapshot does not include memory blocks allocated before the tracemalloc module started to trace memory
allocations.
Tracebacks of traces are limited to get_traceback_limit() frames.
Use the nframe parameter of the
start() function to store more frames.
The tracemalloc module must be tracing memory allocations to take a snapshot, see the start() function.
See also the get_object_traceback() function.
DomainFilter
class tracemalloc.
Filter traces of memory blocks by their address space (domain).
New in version 3.6.
If inclusive is False (exclude), match memory blocks not allocated in the address space domain.
Read-only property.
Filter
class tracemalloc.
See the fnmatch.fnmatch() function for the syntax of filename_pattern.
The '.pyc' file extension is re-
placed with '.py'.
C extensions can use other
domains to trace other resources.
If inclusive is False (exclude), ignore memory blocks allocated in a file with a name matching
filename_pattern at line number lineno.
If lineno is None, the filter matches any line number.
Read-only property.
If all_frames is False, only the most recent
frame is checked.
This attribute has no effect if the traceback limit is 1.
See the get_traceback_limit() function and
Snapshot.traceback_limit attribute.
Frame
class tracemalloc.
Frame
Frame of a traceback.
The Traceback class is a sequence of Frame instances.
Snapshot
class tracemalloc.
Snapshot
Snapshot of traces of memory blocks allocated by Python.
The take_snapshot() function creates a snapshot instance.
Get statistics as a sorted list of StatisticDiff instances
grouped by key_type.
See the Snapshot.statistics() method for key_type and cumulative parameters.
The result is sorted from the biggest to the smallest by: absolute value of StatisticDiff.size_diff,
StatisticDiff.size, absolute value of StatisticDiff.count_diff, Statistic.count
and then by StatisticDiff.traceback.
27.8.
Use load() to reload the snapshot.
filter_traces(filters)
Create a new Snapshot instance with a filtered traces sequence, filters is a list of DomainFilter and
Filter instances.
If filters is an empty list, return a new Snapshot instance with a copy of the traces.
All inclusive filters are applied at once, a trace is ignored if no inclusive filters match it.
A trace is ignored if
at least one exclusive filter matches it.
DomainFilter instances are now also accepted in filters.
See also dump().
The cumulative mode can only be used with key_type equals to 'filename'
and 'lineno'.
The result is sorted from the biggest to the smallest by: Statistic.size, Statistic.count and
then by Statistic.traceback.
The sequence has an undefined order.
Use the Snapshot.statistics() method to get a sorted list of
statistics.
Statistic
class tracemalloc.
Statistic
Statistic on memory allocations.
Snapshot.statistics() returns a list of Statistic instances.
See also the StatisticDiff class.
StatisticDiff
class tracemalloc.
StatisticDiff
Statistic difference on memory allocations between an old and a new Snapshot instance.
Snapshot.compare_to() returns a list of StatisticDiff instances.
See also the Statistic class.
Trace
class tracemalloc.
Trace
Trace of a memory block.
The Snapshot.traces attribute is a sequence of Trace instances.
Added the domain attribute.
Read-only property.
tracemalloc uses the domain 0 to trace memory allocations made by Python.
C extensions can use other
domains to trace other resources.
Traceback
Sequence of Frame instances sorted from the oldest frame to the most recent frame.
A traceback contains at least 1 frame.
If the tracemalloc module failed to get a frame, the filename
"<unknown>" at line number 0 is used.
When a snapshot is taken, tracebacks of traces are limited to get_traceback_limit() frames.
See the
take_snapshot() function.
The original number of frames of the traceback is stored in the Traceback.
total_nframe attribute.
That allows to know if a traceback has been truncated by the traceback limit.
The Trace.traceback attribute is an instance of Traceback instance.
Frames are now sorted from the oldest to the most recent, instead of most recent to oldest.
This attribute can be set to None if
the information is not available.
The Traceback.total_nframe attribute was added.
Use the linecache module to retrieve lines from the source code.
If limit is set, format the limit most recent frames if limit is positive.
Otherwise, format the abs(limit)
oldest frames.
If most_recent_first is True, the order of the formatted frames is reversed, returning the most
recent frame first instead of last.
Similar to the traceback.format_tb() function, except that format() does not include newlines.
These libraries help you with publishing and installing Python software.
While these modules are designed to work in
conjunction with the Python Package Index, they can also be used with a local index server, or without any index server
at all.
This bootstrapping approach reflects the fact that pip is an independent project with its own release
cycle, and the latest available stable version is bundled with maintenance and feature releases of the CPython reference
interpreter.
In most cases, end users of Python shouldn’t need to invoke this module directly (as pip should be bootstrapped by de-
fault), but it may be needed if installing pip was skipped when installing Python (or when creating a virtual environment)
or after explicitly uninstalling pip.
Note: This module does not access the internet.
All of the components needed to bootstrap pip are included as internal
parts of the package.
See also:
installing-index The end user guide for installing Python packages
PEP 453: Explicit bootstrapping of pip in Python installations The original rationale and specification for this
module.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The command line interface is invoked using the interpreter’s -m switch.
By default, pip is installed into the current virtual environment (if one is active) or into the system site packages (if
there is no active virtual environment).
Installs pip relative to the given root directory rather than the root of the currently active virtual
environment (if any) or the default root for the current Python installation.
Installs pip into the user site packages directory rather than globally for the current Python installation
(this option is not permitted inside an active virtual environment).
By default, the scripts pipX and pipX.Y will be installed (where X.Y stands for the version of Python used to invoke
ensurepip).
The scripts installed can be controlled through two additional command line options:
• --altinstall: if an alternate installation is requested, the pipX script will not be installed.
Providing both of the script selection options will trigger an exception.
If root is None, then installation uses the default
install location for the current environment.
By default, the scripts pipX and pipX.Y will be installed (where X.Y stands for the current version of Python).
If altinstall is set, then pipX will not be installed.
If default_pip is set, then pip will be installed in addition to the two regular scripts.
Setting both altinstall and default_pip will trigger ValueError.
Note: The bootstrapping process has side effects on both sys.path and os.environ.
Invoking the command
line interface in a subprocess instead allows these side effects to be avoided.
Note: The bootstrapping process may install additional modules required by pip, but other software should not
assume those dependencies will always be present by default (as the dependencies may be removed in a future
version of pip).
Source code: Lib/venv/
The venv module supports creating lightweight “virtual environments”, each with their own independent set of Python
packages installed in their site directories.
A virtual environment is created on top of an existing Python installa-
tion, known as the virtual environment’s “base” Python, and may optionally be isolated from the packages in the base
environment, so only those explicitly installed in the virtual environment are available.
When used from within a virtual environment, common installation tools such as pip will install Python packages into a
virtual environment without needing to be told to do so explicitly.
A virtual environment is (amongst other things):
• Used to contain a specific Python interpreter and software libraries and binaries which are needed to support a
project (library or application).
These are by default isolated from software in other virtual environments and
Python interpreters and libraries installed in the operating system.
You don’t place any project
code in the environment
• Not considered as movable or copyable – you just recreate the same environment in the target location.
See PEP 405 for more background on Python virtual environments.
Creating and using virtual environments
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
It also creates a bin (or Scripts on Windows) subdirectory containing a
copy/symlink of the Python binary/binaries (as appropriate for the platform or arguments used at environment creation
time).
It also creates an (initially empty) lib/pythonX.Y/site-packages subdirectory (on Windows, this is
Lib\site-packages).
If an existing directory is specified, it will be re-used.
Deprecated since version 3.6: pyvenv was the recommended tool for creating virtual environments for Python 3.3 and
3.4, and is deprecated in Python 3.6.
Changed in version 3.5: The use of venv is now recommended for creating virtual environments.
A directory to create the environment in.
Delete the contents of the environment directory if it
already exists, before environment creation.
PROMPT
Provides an alternative prompt prefix for this
environment.
Once an environment has been created, you may wish to activate it, e.g. by
sourcing an activate script in its bin directory.
Add --upgrade-deps option to upgrade pip + setuptools to the latest on PyPI
Changed in version 3.4: Installs pip by default, added the --without-pip and --copies options
Changed in version 3.4: In earlier versions, if the target directory already existed, an error was raised, unless the --clear
or --upgrade option was provided.
Note: While symlinks are supported on Windows, they are not recommended.
Of particular note is that double-clicking
python.exe in File Explorer will resolve the symlink eagerly and ignore the virtual environment.
Note: On Microsoft Windows, it may be required to enable the Activate.ps1 script by setting the execution policy
for the user.
RemoteSigned -Scope CurrentUser
See About Execution Policies for more information.
The created pyvenv.cfg file also includes the include-system-site-packages key, set to true if venv
is run with the --system-site-packages option, false otherwise.
Unless the --without-pip option is given, ensurepip will be invoked to bootstrap pip into the virtual environ-
ment.
Multiple paths can be given to venv, in which case an identical virtual environment will be created, according to the
given options, at each provided path.
It is sufficient to check sys.prefix !
Scripts on Windows).
This will prepend that directory to your PATH, so that running python will invoke the environment’s Python interpreter
and you can run installed scripts without having to use their full path.
The invocation of the activation script is platform-
specific (<venv> must be replaced by the path to the directory containing the virtual environment):
Platform
Shell
Command to activate virtual environment
POSIX
bash/zsh
$ source <venv>/bin/activate
fish
$ source <venv>/bin/activate.fish
csh/tcsh
$ source <venv>/bin/activate.csh
PowerShell
$ <venv>/bin/Activate.ps1
Windows
cmd.exe
C:\> <venv>\Scripts\activate.bat
PowerShell
PS C:\> <venv>\Scripts\Activate.ps1
New in version 3.4: fish and csh activation scripts.
PowerShell activation scripts installed under POSIX for PowerShell Core support.
You don’t specifically need to activate a virtual environment, as you can just specify the full path to that environment’s
Python interpreter when invoking Python.
Furthermore, all scripts installed in the environment should be runnable without
activating it.
In order to achieve this, scripts installed into virtual environments have a “shebang” line which points to the environment’s
Python interpreter, i.e. #!/<path-to-venv>/bin/python.
This means that the script will run with that inter-
preter regardless of the value of PATH.
On Windows, “shebang” line processing is supported if you have the launcher
installed.
Thus, double-clicking an installed script in a Windows Explorer window should run it with the correct interpreter
without the environment needing to be activated or on the PATH.
When a virtual environment has been activated, the VIRTUAL_ENV environment variable is set to the path of the en-
vironment.
Since explicitly activating a virtual environment is not required to use it, VIRTUAL_ENV cannot be relied
upon to determine whether a virtual environment is being used.
Warning: Because scripts installed in environments should not expect the environment to be activated, their shebang
lines contain the absolute paths to their environment’s interpreters.
Because of this, environments are inherently non-
portable, in the general case.
You should always have a simple means of recreating an environment (for example, if
you have a requirements file requirements.txt, you can invoke pip install -r requirements.txt
using the environment’s pip to install all of the packages needed by the environment).
If for any reason you need to
move the environment to a new location, you should recreate it at the desired location and delete the one at the old
location.
If you move an environment because you moved a parent directory of it, you should recreate the environment
in its new location.
Otherwise, software installed into the environment may not work as expected.
You can deactivate a virtual environment by typing deactivate in your shell.
The exact mechanism is platform-specific
and is an internal implementation detail (typically, a script or shell function will be used).
The EnvBuilder class accepts the following keyword arguments on instantiation:
• system_site_packages – a Boolean value indicating that the system Python site-packages should be
available to the environment (defaults to False).
This uses
ensurepip with the --default-pip option.
If the special string "." is provided, the basename of the current
directory is used as the prompt.
The returned env-builder is an object which has a method, create:
create(env_dir)
Create a virtual environment by specifying the target directory (absolute or relative to the current directory)
which is to contain the virtual environment.
The create method will either create the environment in the
specified directory, or raise an appropriate exception.
The create method of the EnvBuilder class illustrates the hooks available for subclass customization:

Create a virtualized Python environment in a directory.
Each
of
the
methods
ensure_directories(),
create_configuration(),
setup_python(), setup_scripts() and post_setup() can be overridden.
ensure_directories(env_dir)
Creates the environment directory and all necessary subdirectories that don’t already exist, and returns a
context object.
This context object is just a holder for attributes (such as paths) for use by the other methods.
If the EnvBuilder is created with the arg clear=True, contents of the environment directory will be
cleared and then all necessary subdirectories will be recreated.
The returned context object is a types.
The location of the virtual environment.
The name of the virtual environment.
Used for __VENV_NAME__ in activation scripts
(see install_scripts()).
Used for __VENV_PROMPT__ in activation
scripts (see install_scripts()).
The underlying Python executable used by the virtual environment.
This takes into
account the case where a virtual environment is created from another virtual environment.
The include path for the virtual environment.
The purelib path for the virtual environment.
The script path for the virtual environment.
The name of the script path relative to the virtual environment location.
Used for
__VENV_BIN_NAME__ in activation scripts (see install_scripts()).
The name of the Python interpreter in the virtual environment.
Used for
__VENV_PYTHON__ in activation scripts (see install_scripts()).
The name of the Python interpreter, taking into account filesystem redirections.
This can be used to run Python in the virtual environment.
Changed in version 3.11: The venv sysconfig installation scheme is used to construct the paths of the created
directories.
Installs activation scripts appropriate to the platform into the virtual environment.
Upgrades the core venv dependency packages (currently pip) in the environment.
This is done by shelling
out to the pip executable in the environment.
New in version 3.9.
A placeholder method which can be overridden in third party implementations to pre-install packages in the
virtual environment or perform other post-creation steps.
Changed in version 3.7.2: Windows now uses redirector scripts for python[w].exe instead of copying the
actual binaries.
In 3.7.2 only setup_python() does nothing unless running from a build in the source tree.
Changed in version 3.7.3: Windows copies the redirector scripts as part of setup_python() instead of
setup_scripts().
This was not the case in 3.7.2.
When using symlinks, the original executables will be
linked.
In addition, EnvBuilder provides this utility method that can be called from setup_scripts() or
post_setup() in subclasses to assist in installing custom scripts into the virtual environment.
VENV_DIR__ is replaced with the absolute path of the environment directory.
VENV_BIN_NAME__ is replaced with the name of the bin directory (either bin or Scripts).
The directories are allowed to exist (for when an existing environment is being upgraded).
New in version 3.3.
An example of extending EnvBuilder
The following script shows how to extend EnvBuilder by implementing a subclass which installs setuptools and pip
into a created virtual environment:
import os
import os.path
from subprocess import Popen, PIPE
import sys
from threading import Thread
from urllib.parse import urlparse
from urllib.request import urlretrieve
import venv
class ExtendedEnvBuilder(venv.
EnvBuilder):
"""
This builder installs setuptools and pip so that you can pip or
easy_install other packages into the created virtual environment.
If specified, it is called with two
arguments: a string indicating some progress, and a
context indicating where the string is coming from.
The context argument can have one of three values:
'main', indicating that it is called from virtualize()
itself, and 'stdout' and 'stderr', which are obtained
by reading lines from the output streams of a subprocess
which is used to install the app.
If a callable is not specified, default progress
information is output to sys.stderr.
This script is also available for download online.
Source code: Lib/zipapp.py
This module provides tools to manage the creation of zip files containing Python code, which can be executed directly by
the Python interpreter.
The module provides both a Command-Line Interface and a Python API.
When run, the archive will execute the main function from the module myapp in the
archive.
If source is a file, it should be an archive, and
it will be copied to the target archive (or the contents of its shebang line will be displayed if the –info option is specified).
The following options are understood:
-o <output>, --output=<output>
Write the output to a file named output.
If this option is not specified, the output filename will be the same as the
input source, with the extension .pyz added.
If an explicit filename is given, it is used as is (so a .pyz extension
should be included if required).
An output filename must be specified if the source is an archive (and in that case, output must not be the same as
source).
Also, on POSIX, make the archive
executable.
The default is to write no #! line, and not make the file executable.
The mainfn argument should have the form
“pkg.mod:fn”, where “pkg.mod” is a package/module in the archive, and “fn” is a callable in the given module.
The
__main__.py file will execute that callable.
--main cannot be specified when copying an archive.
By default, files are stored uncompressed
in the archive.
In this case, any other options are ignored
and SOURCE must be an archive, not a directory.
Python API
The module defines two convenience functions:
zipapp.create_archive(source, target=None, interpreter=None, main=None, filter=None, compressed=False)
Create an application archive from source.
The source can be any of the following:
• The name of a directory, or a path-like object referring to a directory, in which case a new application archive
will be created from the content of that directory.
The name of an existing application archive file, or a path-like object referring to such a file, in which case the
file is copied to the target (modifying it to reflect the value given for the interpreter argument).
The file name
should include the .pyz extension, if required.
A file object open for reading in bytes mode.
The content of the file should be an application archive, and the
file object is assumed to be positioned at the start of the archive.
If it is the name of a file, or a path-like object, the archive will be written to that file.
If it is an open file object, the archive will be written to that file object, which must be open for writing in
bytes mode.
If the target is omitted (or None), the source must be a directory and the target will be a file with the same
name as the source, with a .pyz extension added.
The interpreter argument specifies the name of the Python interpreter with which the archive will be executed.
It is written as a “shebang” line at the start of the archive.
On POSIX, this will be interpreted by the OS, and
on Windows it will be handled by the Python launcher.
Omitting the interpreter results in no shebang line being
written.
If an interpreter is specified, and the target is a filename, the executable bit of the target file will be set.
The main argument specifies the name of a callable which will be used as the main program for the archive.
It can
only be specified if the source is a directory, and the source does not already contain a __main__.py file.
The
main argument should take the form “pkg.module:callable” and the archive will be run by importing “pkg.module”
and executing the given callable with no arguments.
It is an error to omit main if the source is a directory and does
not contain a __main__.py file, as otherwise the resulting archive would not be executable.
The optional filter argument specifies a callback function that is passed a Path object representing the path to the
file being added (relative to the source directory).
It should return True if the file is to be added.
The optional compressed argument determines whether files are compressed.
If set to True, files in the archive are
compressed with the deflate method; otherwise, files are stored uncompressed.
This argument has no effect when
copying an existing archive.
If a file object is specified for source or target, it is the caller’s responsibility to close it after calling create_archive.
When copying an existing archive, file objects supplied only need read and readline, or write methods.
When creating an archive from a directory, if the target is a file object it will be passed to the zipfile.
ZipFile
class, and must supply the methods needed by that class.
Added the filter and compressed arguments.
If there is no #! line, return None.
The
archive argument can be a filename or a file-like object open for reading in bytes mode.
It is assumed to be at the
start of the archive.
The same can be done using the create_archive() function:
To make the application directly executable on POSIX, specify an interpreter to use.
Note that there is a risk when overwriting a file in place that an error will result in the loss of the original file.
This code does not protect against such errors, but production code should do so.
Specifying the Interpreter
Note that if you specify an interpreter and then distribute your application archive, you need to ensure that the interpreter
used is portable.
The Python launcher for Windows supports most common forms of POSIX #! line, but there are other
issues to consider:
• If you use “/usr/bin/env python” (or other forms of the “python” command, such as “/usr/bin/python”), you need to
consider that your users may have either Python 2 or Python 3 as their default, and write your code to work under
both versions.
If you use an explicit version, for example “/usr/bin/env python3” your application will not work for users who do
not have that version.
Creating Standalone Applications with zipapp
Using the zipapp module, it is possible to create self-contained Python programs, which can be distributed to end users
who only need to have a suitable version of Python installed on their system.
The key to doing this is to bundle all of the
application’s dependencies into the archive, along with the application code.
The steps to create a standalone archive are as follows:
1.
Create your application in a directory as normal, so you have a myapp directory containing a __main__.py file,
and any supporting application code.
See Specifying the Interpreter for details.
It can be shipped to users as a single file.
On Unix, the myapp.pyz file is executable as it stands.
You can rename the file to remove the .pyz extension if you
prefer a “plain” command name.
Caveats
If your application depends on a package that includes a C extension, that package cannot be run from a zip file (this is
an OS limitation, as executable code must be present in the filesystem for the OS loader to load it).
In this case, you
can exclude that dependency from the zipfile, and either require your users to have it installed, or ship it alongside your
zipfile and add code to your __main__.py to include the directory containing the unzipped module in sys.path.
In
this case, you will need to make sure to ship appropriate binaries for your target architecture(s) (and potentially pick the
correct version to add to sys.path at runtime, based on the user’s machine).
The Python Zip Application Archive Format
Python has been able to execute zip files which contain a __main__.py file since version 2.6.
As usual for any Python script, the parent of the script (in this case the zip file) will
be placed on sys.path and thus further modules can be imported from the zip file.
The zip file format allows arbitrary data to be prepended to a zip file.
The zip application format uses this ability to
prepend a standard POSIX “shebang” line to the file (#!/path/to/interpreter).
An optional shebang line, containing the characters b'#!' followed by an interpreter name, and then a new-
line (b'\n') character.
The interpreter name can be anything acceptable to the OS “shebang” processing, or
the Python launcher on Windows.
The interpreter should be encoded in UTF-8 on Windows, and in sys.
Standard zipfile data, as generated by the zipfile module.
The zipfile content must include a file called
__main__.py (which must be in the “root” of the zipfile - i.e., it cannot be in a subdirectory).
The zipfile
data can be compressed or uncompressed.
If an application archive has a shebang line, it may have the executable bit set on POSIX systems, to allow it to be executed
directly.
There is no requirement that the tools in this module are used to create application archives - the module is a convenience,
but archives in the above format created by any means are acceptable to Python.
The modules described in this chapter provide a wide range of services related to the Python interpreter and its interaction
with its environment.
This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly
with the interpreter.
It is always available.
sys.abiflags
On POSIX systems where Python was built with the standard configure script, this contains the ABI flags as
specified by PEP 3149.
Default flags became an empty string (m flag for pymalloc has been removed).
Append the callable hook to the list of active auditing hooks for the current (sub)interpreter.
When an auditing event is raised through the sys.audit() function, each hook will be called in the order it
was added with the event name and the tuple of arguments.
Native hooks added by PySys_AddAuditHook()
are called first, followed by hooks added in the current (sub)interpreter.
Hooks can then log the event, raise an
exception to abort the operation, or terminate the process entirely.
Note that audit hooks are primarily for collecting information about internal or otherwise unobservable actions,
whether by Python or libraries written in Python.
They are not suitable for implementing a “sandbox”.
In partic-
ular, malicious code can trivially disable or bypass hooks added using this function.
At a minimum, any security-
sensitive hooks must be added using the C API PySys_AddAuditHook() before initialising the runtime, and
any modules allowing arbitrary memory modification (such as ctypes) should be completely removed or closely
monitored.
Calling sys.addaudithook() will itself raise an auditing event named sys.addaudithook with no ar-
guments.
If any existing hooks raise an exception derived from RuntimeError, the new hook will not be added
and the exception suppressed.
As a result, callers cannot assume that their hook has been added unless they control
all existing hooks.
See the audit events table for all events raised by CPython, and PEP 578 for the original design discussion.
New in version 3.8.
Otherwise, trace functions will skip the
hook.
If the command was executed using the -c command line option
to the interpreter, argv[0] is set to the string '-c'.
If no script name was passed to the Python interpreter,
argv[0] is the empty string.
To loop over the standard input, or the list of files given on the command line, see the fileinput module.
See also sys.orig_argv.
Note:
On Unix, command line arguments are passed by bytes from OS.
Python decodes them with filesys-
tem encoding and “surrogateescape” error handler.
When you need original bytes, you can get it by [os.
fsencode(arg) for arg in sys.argv].
The number and types of arguments for a given
event are considered a public and stable API and should not be modified between releases.
For example, one auditing event is named os.chdir.
This event has one argument called path that will contain
the requested new working directory.
In general, if an exception is raised, it should not be handled and the process should
be terminated as quickly as possible.
This allows hook implementations to decide how to respond to particular
events: they can merely log the event or abort the operation by raising an exception.
Hooks are added using the sys.addaudithook() or PySys_AddAuditHook() functions.
The native equivalent of this function is PySys_Audit().
Using the native function is preferred when possible.
See the audit events table for all events raised by CPython.
New in version 3.8.
sys.base_exec_prefix
Set during Python startup, before site.py is run, to the same value as exec_prefix.
If not running in a virtual
environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of
prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and
base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment
was created from).
New in version 3.3.
sys.base_prefix
Set during Python startup, before site.py is run, to the same value as prefix.
If not running in a virtual
environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of
prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and
base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment
was created from).
This will have the value 'big' on big-endian (most-significant byte first)
platforms, and 'little' on little-endian (least-significant byte first) platforms.
See also the sys.stdlib_module_names list.
The tracing state is saved, and restored afterwards.
This is intended
to be called from a debugger from a checkpoint, to recursively debug or profile some other code.
Tracing is suspended while calling a tracing function set by settrace() or setprofile() to avoid infinite
recursion.
Clear the internal type cache.
The type cache is used to speed up attribute and method lookups.
Use the function
only to drop unnecessary references during reference leak debugging.
This function should be used for internal and specialized purposes only.
Note that functions in the traceback module can build the call stack given such
a frame.
This is most useful for debugging deadlock: this function does not require the deadlocked threads’ cooperation, and
such threads’ call stacks are frozen for as long as they remain deadlocked.
The frame returned for a non-deadlocked
thread may bear no relationship to that thread’s current activity by the time calling code examines the frame.
This function should be used for internal and specialized purposes only.
Raises an auditing event sys._current_frames with no arguments.
If a thread is not currently handling an exception, it is not included in the result
dictionary.
This is most useful for statistical profiling.
This function should be used for internal and specialized purposes only.
Raises an auditing event sys._current_exceptions with no arguments.
Changed in version 3.12: Each value in the dictionary is now a single exception instance, rather than a 3-tuple as
returned from sys.exc_info().
This hook function is called by built-in breakpoint().
By default, it drops you into the pdb debugger, but it
can be set to any other function so that you can choose which debugger gets used.
The signature of this function is dependent on what it calls.
For example, the default binding (e.g.
pdb.
The built-in breakpoint() function passes its *args and **kws straight through.
Whatever breakpointhooks() returns is returned from breakpoint().
The default implementation first consults the environment variable PYTHONBREAKPOINT.
If that is set to "0"
then this function returns immediately; i.e. it is a no-op.
If the environment variable is not set, or is set to the empty
string, pdb.set_trace() is called.
Otherwise this variable should name a function to run, using Python’s
dotted-import nomenclature, e.g. package.subpackage.module.function.
In this case, package.
This is run, passing in *args and **kws, and whatever function() returns, sys.breakpointhook()
returns to the built-in breakpoint() function.
Note that if anything goes wrong while importing the callable named by PYTHONBREAKPOINT, a
RuntimeWarning is reported and the breakpoint is ignored.
Also note that if sys.breakpointhook() is overridden programmatically, PYTHONBREAKPOINT is not
consulted.
New in version 3.7.
If Python is built in debug mode (configure --with-pydebug option), it also performs some expensive
internal consistency checks.
New in version 3.3.
CPython implementation detail: This function is specific to CPython.
The exact output format is not defined
here, and may change.
Integer specifying the handle of the Python DLL.
If value is not None, this function prints repr(value) to sys.stdout, and saves value in builtins._.
If repr(value) is not encodable to sys.stdout.encoding with sys.stdout.errors error handler
(which is probably 'strict'), encode it to sys.stdout.encoding with 'backslashreplace' error
handler.
sys.displayhook is called on the result of evaluating an expression entered in an interactive Python ses-
sion.
The display of these values can be customized by assigning another one-argument function to sys.
Pseudo-code:

return
# Set '_' to None to avoid recursion
builtins.
This value is initially set to
True or False depending on the -B command line option and the PYTHONDONTWRITEBYTECODE environ-
ment variable, but you can set it yourself to control bytecode file generation.
The named tuple
is provisional and may change in the future.
Availability: Emscripten.
New in version 3.11.
Thus if you use compileall as a pre-build step, you must ensure you run it with the same pycache prefix
(if any) that you will use at runtime.
A relative path is interpreted relative to the current working directory.
This value is initially set based on the value of the -X pycache_prefix=PATH command-line option or the
PYTHONPYCACHEPREFIX environment variable (command-line takes precedence).
This function prints out a given traceback and exception to sys.stderr.
When an exception other than SystemExit is raised and uncaught, the interpreter calls sys.excepthook
with three arguments, the exception class, exception instance, and a traceback object.
In an interactive session this
happens just before control is returned to the prompt; in a Python program this happens just before the program
exits.
The handling of such top-level exceptions can be customized by assigning another three-argument function
to sys.excepthook.
Raise an auditing event sys.excepthook with arguments hook, type, value, traceback when an un-
caught exception occurs.
If no hook has been set, hook may be None.
If any hook raises an exception derived from
RuntimeError the call to the hook will be suppressed.
Otherwise, the audit hook exception will be reported as
unraisable and sys.excepthook will be called.
These objects contain the original values of breakpointhook, displayhook, excepthook, and
unraisablehook at the start of the program.
They are saved so that breakpointhook, displayhook
and excepthook, unraisablehook can be restored in case they happen to get replaced with broken or
alternative objects.
This function, when called while an exception handler is executing (such as an except or except* clause),
returns the exception instance that was caught by this handler.
When exception handlers are nested within one
another, only the exception handled by the innermost handler is accessible.
If no exception handler is executing, this function returns None.
New in version 3.11.
This function returns the old-style representation of the handled exception.
If an exception e is currently handled (so
exception() would return e), exc_info() returns the tuple (type(e), e, e.__traceback__).
That is, a tuple containing the type of the exception (a subclass of BaseException), the exception itself, and a
traceback object which typically encapsulates the call stack at the point where the exception last occurred.
If no exception is being handled anywhere on the stack, this function return a tuple containing three None values.
The type and traceback fields are now derived from the value (the exception
instance), so when an exception is modified while it is being handled, the changes are reflected in the results of
subsequent calls to exc_info().
This can be set at build time with the --exec-prefix argument to
the configure script.
Specifically, all configuration files (e.g. the pyconfig.h header file) are installed
in the directory exec_prefix/lib/pythonX.Y/config, and shared library modules are installed in
exec_prefix/lib/pythonX.Y/lib-dynload, where X.Y is the version number of Python, for example
3.2.
Note: If a virtual environment is in effect, this value will be changed in site.py to point to the virtual environ-
ment.
The value for the Python installation will still be available, via base_exec_prefix.
sys.executable
A string giving the absolute path of the executable binary for the Python interpreter, on systems where this makes
sense.
The optional argument arg can be an integer giving the exit status (defaulting to zero), or another type of object.
If it is an integer, zero is considered “successful termination” and any nonzero value is considered “abnormal ter-
mination” by shells and the like.
Most systems require it to be in the range 0–127, and produce undefined results
otherwise.
Some systems have a convention for assigning specific meanings to specific exit codes, but these are
generally underdeveloped; Unix programs generally use 2 for command line syntax errors and 1 for all other kind
of errors.
If another type of object is passed, None is equivalent to passing zero, and any other object is printed
to stderr and results in an exit code of 1.
In particular, sys.exit("some error message") is a quick
way to exit a program when an error occurs.
Since exit() ultimately “only” raises an exception, it will only exit the process when called from the main thread,
and the exception is not intercepted.
Cleanup actions specified by finally clauses of try statements are honored,
and it is possible to intercept the exit attempt at an outer level.
If an error occurs in the cleanup after the Python interpreter has caught SystemExit
(such as an error flushing buffered data in the standard streams), the exit status is changed to 120.
sys.flags
The named tuple flags exposes the status of command line flags.
The attributes are read only.
New in version 3.2.3: The hash_randomization attribute.
Removed obsolete division_warning attribute.
Changed in version 3.4: Added isolated attribute for -I isolated flag.
Added the dev_mode attribute for the new Python Development Mode and the
utf8_mode attribute for the new -X utf8 flag.
Added warn_default_encoding attribute for -X warn_default_encoding
flag.
Changed in version 3.11: Added the safe_path attribute for -P option.
Added the int_max_str_digits attribute.
sys.float_info
A named tuple holding information about the float type.
It contains low level information about the precision and
internal representation.
The values correspond to the various floating-point constants defined in the standard header
file float.h for the ‘C’ programming language; see section 5.2.4.2.2 of the 1999 ISO/IEC C standard [C99],
‘Characteristics of floating types’, for details.
See also math.ulp().
The maximum number of decimal
digits that can be faithfully repre-
sented in a float; see below.
The minimum representable posi-
tive normalized float.
Use math.ulp(0.0) to get the
smallest positive denormalized rep-
resentable float.
The attribute sys.float_info.dig needs further explanation.
But for strings with more than sys.float_info.dig significant digits, this isn’t always true:
# 16 significant digits is too many!
This is the
usual behaviour in Python 3.1 and later.
Otherwise, float_repr_style has value 'legacy' and repr(x)
behaves in the same way as it did in versions of Python prior to 3.1.
New in version 3.1.
This function
is mainly useful for tracking and debugging memory leaks.
Because of the interpreter’s internal caches, the result
can vary from call to call; you may have to call _clear_type_cache() and gc.collect() to get more
predictable results.
If a Python build or implementation cannot reasonably compute this information, getallocatedblocks()
is allowed to return 0 instead.
New in version 3.4.
sys.getunicodeinternedsize()
Return the number of unicode objects that have been interned.
New in version 3.12.
Availability: Android.
New in version 3.7.
sys.getdefaultencoding()
Return the name of the current default string encoding used by the Unicode implementation.
Symbolic names for the flag values can be
found in the os module (RTLD_xxx constants, e.g. os.RTLD_LAZY).
Availability: Unix.
sys.getfilesystemencoding()
Get the filesystem encoding: the encoding used with the filesystem error handler to convert between Unicode file-
names and bytes filenames.
The filesystem error handler is returned from getfilesystemencodeerrors().
For best compatibility, str should be used for filenames in all cases, although representing filenames as bytes is also
supported.
Functions accepting or returning filenames should support either str or bytes and internally convert to
the system’s preferred representation.
The filesystem encoding and error handler are configured at Python startup by the PyConfig_Read() function:
see filesystem_encoding and filesystem_errors members of PyConfig.
Changed in version 3.2: getfilesystemencoding() result cannot be None anymore.
Changed in version 3.6:
Windows is no longer guaranteed to return 'mbcs'.
See PEP 529 and
_enablelegacywindowsfsencoding() for more information.
Changed in version 3.7: Return 'utf-8' if the Python UTF-8 Mode is enabled.
The filesystem encoding is returned from getfilesystemencoding().
The filesystem encoding and error handler are configured at Python startup by the PyConfig_Read() function:
see filesystem_encoding and filesystem_errors members of PyConfig.
New in version 3.6.
See
also
set_int_max_str_digits().
New in version 3.11.
sys.getrefcount(object)
Return the reference count of the object.
The count returned is generally one higher than you might expect, because
it includes the (temporary) reference as an argument to getrefcount().
Note that the returned value may not actually reflect how many references to the object are actually held.
For
example, some objects are “immortal” and have a very high refcount that does not reflect the actual number of
references.
Consequently, do not rely on the returned value to be accurate, other than a value of 0 or 1.
Changed in version 3.12: Immortal objects have very large refcounts that do not match the actual number of
references to the object.
This limit
prevents infinite recursion from causing an overflow of the C stack and crashing Python.
It can be set by
setrecursionlimit().
The object can be any type of object.
All built-in objects will return correct
results, but this does not have to hold true for third-party extensions as it is implementation specific.
Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of
objects it refers to.
If given, default will be returned if the object does not provide means to retrieve the size.
Otherwise a TypeError
will be raised.
See recursive sizeof recipe for an example of using getsizeof() recursively to find the size of containers and
all their contents.
New in version 3.2.
If optional integer depth is given, return the frame object that many calls
below the top of the stack.
If that is deeper than the call stack, ValueError is raised.
The default for depth is
zero, returning the frame at the top of the call stack.
Raises an auditing event sys._getframe with argument frame.
CPython implementation detail: This function should be used for internal and specialized purposes only.
It is
not guaranteed to exist in all implementations of Python.
If optional integer depth is given, return the module that many
calls below the top of the stack.
If that is deeper than the call stack, or if the module is unidentifiable, None is
returned.
The default for depth is zero, returning the module at the top of the call stack.
Raises an auditing event sys._getframemodulename with argument depth.
CPython implementation detail: This function should be used for internal and specialized purposes only.
It is
not guaranteed to exist in all implementations of Python.
CPython implementation detail: The gettrace() function is intended only for implementing debuggers,
profilers, coverage tools and the like.
Its behavior is part of the implementation platform, rather than part of the
language definition, and thus may not be available in all Python implementations.
The named elements are major, mi-
nor, build, platform, service_pack, service_pack_minor, service_pack_major, suite_mask, product_type and plat-
form_version.
The
components can also be accessed by name, so sys.getwindowsversion()[0] is equivalent to sys.
getwindowsversion().major.
For compatibility with prior versions, only the first 5 elements are retriev-
able by indexing.
The system is a workstation.
The system is a domain controller.
The system is a server, but not a domain controller.
This function wraps the Win32 GetVersionEx() function;
see the Microsoft documentation on
OSVERSIONINFOEX() for more information about these fields.
It is intended for use in logging rather than for feature
detection.
Please use platform module for achieving accurate OS version.
Availability: Windows.
Changed in version 3.2: Changed to a named tuple and added service_pack_minor, service_pack_major, suite_mask,
and product_type.
Changed in version 3.6: Added platform_version
sys.get_asyncgen_hooks()
Returns an asyncgen_hooks object, which is similar to a namedtuple of the form (firstiter,
finalizer), where firstiter and finalizer are expected to be either None or functions which take an asyn-
chronous generator iterator as an argument, and are used to schedule finalization of an asynchronous generator by
an event loop.
New in version 3.6: See PEP 525 for more details.
New in version 3.7.
Use it only for debugging
purposes.
For more details about hashing of numeric
types, see Hashing of numeric types.
Changed in version 3.4: Added algorithm, hash_bits and seed_bits
sys.hexversion
The version number encoded as a single integer.
This is guaranteed to increase with each version, including proper
support for non-production releases.
This is called hexversion since it only really looks meaningful when viewed as the result of passing it to the built-
in hex() function.
The named tuple sys.version_info may be used for a more human-friendly encoding
of the same information.
More details of hexversion can be found at apiabiversion.
The fol-
lowing attributes are required to exist in all Python implementations.
The actual string is defined by the Python implemen-
tation, but it is guaranteed to be lower case.
It represents the version of the
Python implementation.
This has a distinct meaning from the specific version of the Python language to
which the currently running interpreter conforms, which sys.version_info represents.
For example, for
PyPy 1.8 sys.implementation.version might be sys.version_info(1, 8, 0, 'final',
0), whereas sys.version_info would be sys.version_info(2, 7, 2, 'final', 0).
For
CPython they are the same value, since it is the reference implementation.
By convention, it would be a
composite of the implementation’s name and version, like 'cpython-33'.
However, a Python implementation
may use some other value if appropriate.
If cache_tag is set to None, it indicates that module caching should
be disabled.
sys.implementation may contain additional attributes specific to the Python implementation.
These non-
standard attributes must start with an underscore, and are not described here.
Regardless of its contents, sys.
implementation will not change during a run of the interpreter, nor between implementation versions.
See PEP 421 for more information.
New in version 3.3.
Note: The addition of new required attributes must go through the normal PEP process.
See PEP 421 for more
information.
The attributes are read
only.
The number of bits held in each digit.
Python integers are stored internally in base 2**int_info.
New in version 3.1.
Changed in version 3.11: Added default_max_str_digits and str_digits_check_threshold.
This is done after the PYTHONSTARTUP file is read, so that you can set this hook there.
The
site module sets this.
Raises an auditing event cpython.run_interactivehook with the hook object as the argument when the
hook is called on startup.
New in version 3.4.
sys.intern(string)
Enter string in the table of “interned” strings and return the interned string – which is string itself or a copy.
Interning
strings is useful to gain a little performance on dictionary lookup – if the keys in a dictionary are interned, and the
lookup key is interned, the key comparisons (after hashing) can be done by a pointer compare instead of a string
compare.
Normally, the names used in Python programs are automatically interned, and the dictionaries used to
hold module, class or instance attributes have interned keys.
Interned strings are not immortal; you must keep a reference to the return value of intern() around to benefit
from it.
New in version 3.5.
Its intended use is to allow an interactive user to import
a debugger module and engage in post-mortem debugging without having to re-execute the command that caused
the error.
New in version 3.12.
They hold the legacy representation of sys.
It’s usually 2**31 - 1 on a
32-bit platform and 2**63 - 1 on a 64-bit platform.
Changed in version 3.3: Before PEP 393, sys.maxunicode used to be either 0xFFFF or 0x10FFFF, de-
pending on the configuration option that specified whether Unicode characters were stored as UCS-2 or UCS-4.
sys.meta_path
A list of meta path finder objects that have their find_spec() methods called to see if one of the objects can
find the module to be imported.
By default, it holds entries that implement Python’s default import semantics.
The
find_spec() method is called with at least the absolute name of the module being imported.
The method returns a module spec, or None if the module cannot be found.
See also:
importlib.abc.
The abstract base class defining the interface of finder objects on
meta_path.
importlib.machinery.
The concrete class which find_spec() should return instances
of.
Module specs were introduced in Python 3.4, by PEP 451.
Earlier versions of Python
looked for a method called find_module().
This is still called as a fallback if a meta_path entry doesn’t
have a find_spec() method.
This can be manipulated
to force reloading of modules and other tricks.
However, replacing the dictionary will not necessarily work as
expected and deleting essential items from the dictionary may cause Python to fail.
If you want to iterate over this
global dictionary always use sys.modules.copy() or tuple(sys.modules) to avoid exceptions as its
size may change during iteration as a side effect of code or activity in other threads.
See also sys.argv.
New in version 3.10.
sys.path
A list of strings that specifies the search path for modules.
Initialized from the environment variable PYTHONPATH,
plus an installation-dependent default.
By default, as initialized upon program startup, a potentially unsafe path is prepended to sys.path (before the
entries inserted as a result of PYTHONPATH):
• python -m module command line: prepend the current working directory.
If it’s a symbolic link, resolve sym-
bolic links.
To not prepend this potentially unsafe path, use the -P command line option or the PYTHONSAFEPATH environ-
ment variable.
A program is free to modify this list for its own purposes.
Only strings should be added to sys.path; all other
data types are ignored during import.
See also:
• Module site This describes how to use .pth files to extend sys.path.
If a finder can be created, it is to
be returned by the callable, else raise ImportError.
Originally specified in PEP 302.
The keys are paths that have been passed to sys.path_hooks
and the values are the finders that are found.
If a path is a valid file system path but no finder is found on sys.
path_hooks then None is stored.
Originally specified in PEP 302.
For Unix systems, except on Linux and AIX, this is the lowercased OS name as returned by uname -s with the
first part of the version as returned by uname -r appended, e.g. 'sunos5' or 'freebsd8', at the time when
Python was built.
Unless you want to test for a specific system version, it is therefore recommended to use the
following idiom:
if sys.platform.startswith('freebsd'):
# FreeBSD-specific code here...
elif sys.platform.startswith('linux'):
# Linux-specific code here...
elif sys.platform.startswith('aix'):
# AIX-specific code here...
For other systems, the values are:
System
platform value
AIX
'aix'
Emscripten
'emscripten'
Linux
'linux'
WASI
'wasi'
Windows
'win32'
Windows/Cygwin
'cygwin'
macOS
'darwin'
Changed in version 3.3: On Linux, sys.platform doesn’t contain the major version anymore.
It is always
'linux', instead of 'linux2' or 'linux3'.
It is always
'aix', instead of 'aix5' or 'aix7'.
Since older Python versions include the version number, it is recom-
mended to always use the startswith idiom presented above.
See also:
os.name has a coarser granularity.
The platform module provides detailed checks for the system’s identity.
It is used to build the path of standard library and the paths of
installed extension modules.
It is equal to "lib" on most platforms.
On Fedora and SuSE, it is equal to "lib64" on 64-bit platforms which
gives the following sys.path paths (where X.Y is the Python major.minor version):
• /usr/lib64/pythonX.Y/: Standard library (like os.py of the os module)
• /usr/lib64/pythonX.Y/lib-dynload/: C extension modules of the standard library (like the
errno module, the exact filename is platform specific)
• /usr/lib/pythonX.Y/site-packages/ (always use lib, not sys.platlibdir): Third-party
modules
• /usr/lib64/pythonX.Y/site-packages/: C extension modules of third-party packages
New in version 3.9.
This can be set at build time with the --prefix argument to the configure
script.
See Installation paths for derived paths.
Note: If a virtual environment is in effect, this value will be changed in site.py to point to the virtual environ-
ment.
The value for the Python installation will still be available, via base_prefix.
sys.ps1
sys.ps2
Strings specifying the primary and secondary prompt of the interpreter.
These are only defined if the interpreter is
in interactive mode.
Their initial values in this case are 'to either variable, its str() is re-evaluated each time the interpreter prepares to read a new interactive command;
this can be used to implement a dynamic prompt.
Among other things, this will enable a lazy resolving of symbols when importing a module, if called as sys.
setdlopenflags(0).
To share symbols across extension modules, call as sys.setdlopenflags(os.
RTLD_GLOBAL).
Symbolic names for the flag values can be found in the os module (RTLD_xxx constants, e.g.
os.RTLD_LAZY).
Availability: Unix.
sys.set_int_max_str_digits(maxdigits)
Set
the
integer
string
conversion
length
limitation
used
by
this
interpreter.
See
also
get_int_max_str_digits().
New in version 3.11.
See
chapter The Python Profilers for more information on the Python profiler.
The system’s profile function is called
similarly to the system’s trace function (see settrace()), but it is called with different events, for example it
isn’t called for each executed line of code (only on call and return, but the return event is reported even when an
exception has been set).
The function is thread-specific, but there is no way for the profiler to know about context
switches between threads, so it does not make sense to use this in the presence of multiple threads.
Also, its return
value is not used, so it can simply return None.
Error in the profile function will cause itself unset.
Note:
The same tracing mechanism is used for setprofile() as settrace().
To trace calls with
setprofile() inside a tracing function (e.g. in a debugger breakpoint), see call_tracing().
Profile functions should have three arguments: frame, event, and arg.
The events have the following meaning:
'call' A function is called (or some other code block entered).
The profile function is called; arg is None.
'return' A function (or other code block) is about to return.
The profile function is called; arg is the value that
will be returned, or None if the event is caused by an exception being raised.
This may be an extension function or a built-in.
Raises an auditing event sys.setprofile with no arguments.
This limit prevents infinite recursion from causing
an overflow of the C stack and crashing Python.
The highest possible limit is platform-dependent.
A user may need to set the limit higher when they have a program
that requires deep recursion and a platform that supports a higher limit.
This should be done with care, because a
too-high limit can lead to a crash.
If the new limit is too low at the current recursion depth, a RecursionError exception is raised.
A RecursionError exception is now raised if the new limit is too low at the current
recursion depth.
sys.setswitchinterval(interval)
Set the interpreter’s thread switch interval (in seconds).
This floating-point value determines the ideal duration of
the “timeslices” allocated to concurrently running Python threads.
Please note that the actual value can be higher,
especially if long-running internal functions or methods are used.
Also, which thread becomes scheduled at the
end of the interval is the operating system’s decision.
The interpreter doesn’t have its own scheduler.
New in version 3.2.
sys.settrace(tracefunc)
Set the system’s trace function, which allows you to implement a Python source code debugger in Python.
The
function is thread-specific; for a debugger to support multiple threads, it must register a trace function using
settrace() for each thread being debugged or use threading.settrace().
Trace functions should have three arguments: frame, event, and arg.
The trace function is invoked (with event set to 'call') whenever a new local scope is entered; it should return
a reference to a local trace function to be used for the new scope, or None if the scope shouldn’t be traced.
The local trace function should return a reference to itself, or to another function which would then be used as the
local trace function for the scope.
If there is any error occurred in the trace function, it will be unset, just like settrace(None) is called.
Note: Tracing is disabled while calling the trace function (e.g. a function set by settrace()).
For recursive
tracing see call_tracing().
The events have the following meaning:
'call' A function is called (or some other code block entered).
The global trace function is called; arg is None;
the return value specifies the local trace function.
The local
trace function is called; arg is None; the return value specifies the new local trace function.
See Objects/
lnotab_notes.txt for a detailed explanation of how this works.
Per-line events may be disabled for a
frame by setting f_trace_lines to False on that frame.
The local trace function is called; arg is the value
that will be returned, or None if the event is caused by an exception being raised.
The trace function’s return
value is ignored.
The local trace function is called; arg is a tuple (exception,
value, traceback); the return value specifies the new local trace function.
The local trace
function is called; arg is None; the return value specifies the new local trace function.
Per-opcode events are
not emitted by default: they must be explicitly requested by setting f_trace_opcodes to True on the
frame.
Note that as an exception is propagated down the chain of callers, an 'exception' event is generated at each
level.
For more fine-grained usage, it’s possible to set a trace function by assigning frame.f_trace = tracefunc
explicitly, rather than relying on it being set indirectly via the return value from an already installed trace function.
This is also required for activating the trace function on the current frame, which settrace() doesn’t do.
Note
that in order for this to work, a global tracing function must have been installed with settrace() in order to
enable the runtime tracing machinery, but it doesn’t need to be the same tracing function (e.g. it could be a low
overhead tracing function that simply returns None to disable itself immediately on each frame).
For more information on code and frame objects, refer to types.
CPython implementation detail: The settrace() function is intended only for implementing debuggers,
profilers, coverage tools and the like.
Its behavior is part of the implementation platform, rather than part of the
language definition, and thus may not be available in all Python implementations.
Changed in version 3.7: 'opcode' event type added; f_trace_lines and f_trace_opcodes attributes
added to frames
sys.set_asyncgen_hooks(firstiter, finalizer)
Accepts two optional keyword arguments which are callables that accept an asynchronous generator iterator as an
argument.
The firstiter callable will be called when an asynchronous generator is iterated for the first time.
The
finalizer will be called when an asynchronous generator is about to be garbage collected.
Raises an auditing event sys.set_asyncgen_hooks_firstiter with no arguments.
Two auditing events are raised because the underlying API consists of two calls, each of which must raise its own
event.
New in version 3.6: See PEP 525 for more details, and for a reference example of a finalizer method see the
implementation of asyncio.
When enabled, the cr_origin attribute on coroutine
objects will contain a tuple of (filename, line number, function name) tuples describing the traceback where the
coroutine object was created, with the most recent call first.
When disabled, cr_origin will be None.
To enable, pass a depth value greater than zero; this sets the number of frames whose information will be captured.
To disable, pass set depth to zero.
This setting is thread-specific.
New in version 3.7.
Use it only for debugging
purposes.
Activate the stack profiler trampoline backend.
The only supported backend is "perf".
Availability: Linux.
New in version 3.12.
Deactivate the current stack profiler trampoline backend.
If no stack profiler is activated, this function has no effect.
Availability: Linux.
New in version 3.12.
Availability: Linux.
New in version 3.12.
This is equivalent to defining the PYTHONLEGACYWINDOWSFSENCODING environment variable before launch-
ing Python.
See also sys.getfilesystemencoding() and sys.getfilesystemencodeerrors().
Availability: Windows.
New in version 3.6: See PEP 529 for more details.
The interpreter’s own prompts and its error messages go to stderr.
These streams are regular text files like those returned by the open() function.
The encoding and error handling are is initialized from PyConfig.stdio_encoding and PyConfig.
On Windows, UTF-8 is used for the console device.
Non-character devices such as disk files and pipes use the
system locale encoding (i.e. the ANSI codepage).
Non-console character devices such as NUL (i.e. where
isatty() returns True) use the value of the console input and output codepages at startup, respectively
for stdin and stdout/stderr.
This defaults to the system locale encoding if the process is not initially attached
to a console.
The special behaviour of the console can be overridden by setting the environment variable PYTHONLE-
GACYWINDOWSSTDIO before starting Python.
In that case, the console codepages are used as for any
other character device.
Under all platforms, you can override the character encoding by setting the PYTHONIOENCODING en-
vironment variable before starting Python or by using the new -X utf8 command line option and
PYTHONUTF8 environment variable.
However, for the Windows console, this only applies when
PYTHONLEGACYWINDOWSSTDIO is also set.
Otherwise, it is block-buffered like regular text files.
The stderr stream is line-buffered in both cases.
You can make both streams unbuffered by passing the
-u command-line option or setting the PYTHONUNBUFFERED environment variable.
Changed in version 3.9: Non-interactive stderr is now line-buffered instead of fully buffered.
Note: To write or read binary data from/to the standard streams, use the underlying binary buffer object.
For
example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').
These objects contain the original values of stdin, stderr and stdout at the start of the program.
They are
used during finalization, and could be useful to print to the actual standard stream no matter if the sys.std*
object has been redirected.
It can also be used to restore the actual files to known working file objects in case they have been overwritten with
a broken object.
However, the preferred way to do this is to explicitly save the previous stream before replacing it,
and restore the saved object.
Note:
Under some conditions stdin, stdout and stderr as well as the original values __stdin__,
__stdout__ and __stderr__ can be None.
It is usually the case for Windows GUI apps that aren’t connected
to a console and Python apps started with pythonw.
It is the same on all platforms.
Modules which are not available on some platforms and modules disabled at Python
build are also listed.
All module kinds are listed: pure Python, built-in, frozen and extension modules.
Test modules
are excluded.
For packages, only the main package is listed: sub-packages and sub-modules are not listed.
For example, the
email package is listed, but the email.mime sub-package and the email.message sub-module are not
listed.
See also the sys.builtin_module_names list.
New in version 3.10.
It is a string, or None if this information is unknown.
New in version 3.3.
The default is 1000.
When set to 0 or less, all traceback information
is suppressed and only the exception type and value are printed.
Called when an exception has occurred but there is no way for Python to handle it.
For example, when a destructor
raises an exception or during garbage collection (gc.collect()).
The unraisable argument has the following attributes:
• exc_type: Exception type.
The default hook formats err_msg and object as: f'{err_msg}: {object!r}'; use “Exception ig-
nored in” error message if err_msg is None.
sys.unraisablehook() can be overridden to control how unraisable exceptions are handled.
See also:
excepthook() which handles uncaught exceptions.
Warning:
Storing exc_value using a custom hook can create a reference cycle.
It should be cleared
explicitly to break the reference cycle when the exception is no longer needed.
Storing object using a custom hook can resurrect it if it is set to an object which is being finalized.
Avoid
storing object after the custom hook completes to avoid resurrecting objects.
Raise an auditing event sys.unraisablehook with arguments hook, unraisable when an exception that cannot
be handled occurs.
The unraisable object is the same as what will be passed to the hook.
If no hook has been set,
hook may be None.
New in version 3.8.
This string is displayed when the interactive interpreter is started.
Do not extract version
information out of it, rather, use version_info and the functions provided by the platform module.
Programmers may find this useful when debugging version conflicts between
Python and extension modules.
All val-
ues except releaselevel are integers; the release level is 'alpha', 'beta', 'candidate', or 'final'.
The
version_info value corresponding to the Python version 2.0 is (2, 0, 0, 'final', 0).
The com-
ponents can also be accessed by name, so sys.version_info[0] is equivalent to sys.version_info.
Changed in version 3.1: Added named component attributes.
This is an implementation detail of the warnings framework; do not modify this value.
Refer to the warnings
module for more information on the warnings framework.
This is stored as string resource 1000 in the
Python DLL.
The value is normally the major and minor versions of the running Python interpreter.
It is provided
in the sys module for informational purposes; modifying this value has no effect on the registry keys used by
Python.
Availability: Windows.
See sys.
monitoring for details.
Option
names are either mapped to their values, if given explicitly, or to True.
New in version 3.2.
Citations
29.2 sys.monitoring — Execution event monitoring
New in version 3.12.
Note: sys.monitoring is a namespace within the sys module, not an independent module, so there is no need to
import sys.monitoring, simply import sys and then use sys.monitoring.
This namespace provides access to the functions and constants necessary to activate and control event monitoring.
As programs execute, events occur that might be of interest to tools that monitor execution.
The sys.monitoring
namespace provides means to receive callbacks when events of interest occur.
Tool identifiers are used to discourage tools from interfering with
each other and to allow multiple tools to operate at the same time.
Currently tools are completely independent and cannot
be used to monitor each other.
This restriction may be lifted in the future.
Before registering or activating events, a tool should choose an identifier.
Identifiers are integers in the range 0 to 5
inclusive.
Registering and using tools
sys.monitoring.use_tool_id(tool_id: int, name: str, /) → None
Must be called before tool_id can be used.
Raises a ValueError
if tool_id is in use.
However, tools are encouraged to use a unique ID and respect other tools.
BRANCH
A conditional branch is taken (or not).
CALL
A call in Python code (event occurs before the call).
An exception raised from any callable, except for Python functions (event occurs after the exit).
An exception is handled.
INSTRUCTION
A VM instruction is about to be executed.
An unconditional jump in the control flow graph is made.
An instruction is about to be executed that has a different line number from the preceding instruction.
PY_RESUME
Resumption of a Python function (for generator and coroutine functions), except for throw() calls.
sys.monitoring.events.
PY_RETURN
Return from a Python function (occurs immediately before the return, the callee’s frame will be on the stack).
A Python function is resumed by a throw() call.
PY_UNWIND
Exit from a Python function during exception unwinding.
sys.monitoring.events.
PY_YIELD
Yield from a Python function (occurs immediately before the yield, the callee’s frame will be on the stack).
An exception is raised, except those that cause a STOP_ITERATION event.
An artificial StopIteration is raised; see the STOP_ITERATION event.
More events may be added in the future.
These events are attributes of the sys.monitoring.events namespace.
Each event is represented as a power-of-2
integer constant.
To define a set of events, simply bitwise or the individual events together.
Events are divided into three groups:
Local events
Local events are associated with normal execution of the program and happen at clearly defined locations.
All local events
can be disabled.
C_RETURN and C_RAISE events will only
be seen if the corresponding CALL event is being monitored.
Other events
Other events are not necessarily tied to a specific location in the program and cannot be individually disabled.
The other events that can be monitored are:
• PY_THROW
• PY_UNWIND
• RAISE
• EXCEPTION_HANDLED
The STOP_ITERATION event
PEP 380 specifies that a StopIteration exception is raised when returning a value from a generator or coroutine.
However, this is a very inefficient way to return a value, so some Python implementations, notably CPython 3.12+, do not
raise an exception unless it would be visible to other code.
To allow tools to monitor for real exceptions without slowing down generators and coroutines, the STOP_ITERATION
event is provided.
STOP_ITERATION can be locally disabled, unlike RAISE.
Turning events on and off
In order to monitor an event, it must be turned on and a corresponding callback must be registered.
Events can be turned
on or off by setting the events either globally or for a particular code object.
Raises a ValueError if tool_id is not in use.
No events are active by default.
Per code object events
Events can also be controlled on a per code object basis.
Raises a ValueError if tool_id is not in use.
Local events add to global events, but do not mask them.
In other words, all global events will trigger for a code object,
regardless of the local events.
Disabling events
sys.monitoring.
DISABLE
A special value that can be returned from a callback function to disable events for the current code location.
Local events can be disabled for a specific code location by returning sys.monitoring.
DISABLE from a callback
function.
This does not change which events are set, or any other code locations for the same event.
Disabling events for specific locations is very important for high performance monitoring.
For example, a program can
be run under a debugger with no overhead if the debugger disables all monitoring except for a few breakpoints.
DISABLE for all tools.
Callable | None, /) → Callable | None
Registers the callable func for the event with the given tool_id
If another callback was registered for the given tool_id and event, it is unregistered and returned.
Otherwise
register_callback() returns None.
Functions can be unregistered by calling sys.monitoring.register_callback(tool_id, event,
None).
Callback functions can be registered and unregistered at any time.
Registering or unregistering a callback function will generate a sys.audit() event.
MISSING
A special value that is passed to a callback function to indicate that there are no arguments to the call.
When an active event occurs, the registered callback function is called.
For an untaken branch this will be the offset
of the instruction following the branch.
Any
29.3 sysconfig — Provide access to Python’s configuration infor-
mation
New in version 3.2.
Source code: Lib/sysconfig.py
The sysconfig module provides access to Python’s configuration information like the list of installation paths and the
configuration variables relevant for the current platform.
Notice that on Windows, it’s a much smaller set.
sysconfig.get_config_vars(*args)
With no arguments, return a dictionary of all configuration variables relevant for the current platform.
With arguments, return a list of values that result from looking up each argument in the configuration variable
dictionary.
For each argument, if the value is not found, return None.
sysconfig.get_config_var(name)
Return the value of a single variable name.
Equivalent to get_config_vars().get(name).
These schemes
are stored in sysconfig under unique identifiers based on the value returned by os.name.
The schemes are used by
package installers to determine where to copy files to.
Python currently supports nine schemes:
• posix_prefix: scheme for POSIX platforms like Linux or macOS.
This is the default scheme used when Python or
a component is installed.
This scheme defines paths located under
a specific home prefix.
This scheme defines paths located under
the user’s home directory (site.USER_BASE).
This is the default scheme used when Python or a component is installed.
Each scheme is itself composed of a series of paths and each path has a unique identifier.
Python currently uses eight
paths:
• stdlib: directory containing the standard Python library files that are not platform-specific.
This scheme installs
pure Python modules and extension modules in the same location (also known as site.USER_SITE).
This scheme’s
name is derived from the idea of a “home” directory on Unix, since it’s not unusual for a Unix user to make their home
directory have a layout similar to /usr/ or /usr/local/. This scheme can be used by anyone, regardless of the
operating system they are installing for.
Prefix scheme
The “prefix scheme” is useful when you wish to use one Python installation to perform the build/install (i.e., to run the
setup script), but install modules into the third-party module directory of a different Python installation (or something
that looks like a different Python installation).
If this sounds a trifle unusual, it is—that’s why the user and home schemes
come before.
However, there are at least two known cases where the prefix scheme will be useful.
First, consider that many Linux distributions put Python in /usr, rather than the more traditional /usr/local.
This
is entirely appropriate, since in those cases Python is part of “the system” rather than a local add-on.
New in version 3.10: This function was previously named _get_default_scheme() and considered an im-
plementation detail.
Changed in version 3.11: When Python runs from a virtual environment, the venv scheme is returned.
The return value is a scheme name listed in get_scheme_names().
It can be passed to sysconfig functions
that take a scheme argument, such as get_paths().
New in version 3.10.
Changed in version 3.11: When Python runs from a virtual environment and key="prefix", the venv scheme
is returned.
Python implementers and redistributors
may add their preferred schemes to the _INSTALL_SCHEMES module-level global value, and modify this function
to return those scheme names, to e.g. provide different schemes for system and language package managers to use,
so packages installed by either do not mix with those by the other.
End users should not use this function, but get_default_scheme() and get_preferred_scheme()
instead.
New in version 3.10.
sysconfig.get_path_names()
Return a tuple containing all path names currently supported in sysconfig.
For instance the stdlib path for the nt scheme is: {base}/Lib.
get_path() will use the variables returned by get_config_vars() to expand the path.
All variables have
default values for each platform so one may call this function and get the default value.
If scheme is provided, it must be a value from the list returned by get_scheme_names().
Otherwise, the
default scheme for the current platform is used.
If vars is provided, it must be a dictionary of variables that will update the dictionary returned by
get_config_vars().
If expand is set to False, the path will not be expanded using the variables.
If name is not found, raise a KeyError.
See get_path()
for more information.
If scheme is not provided, will use the default scheme for the current platform.
If vars is provided, it must be a dictionary of variables that will update the dictionary used to expand the paths.
If expand is set to false, the paths will not be expanded.
If scheme is not an existing scheme, get_paths() will raise a KeyError.
Similar to '%d.%d' % sys.
version_info[:2].
sysconfig.get_platform()
Return a string that identifies the current platform.
This is used mainly to distinguish platform-specific build directories and platform-specific built distributions.
Typ-
ically includes the OS name and version and the architecture (as supplied by ‘os.uname()’), although the exact
information included depends on the OS; e.g., on Linux, the kernel version isn’t particularly important.
A dictionary containing name/value pairs is returned.
If an optional dictionary is passed in as the second argument,
it is used instead of a new dictionary, and updated with the values read in the file.
Using sysconfig as a script
You can use sysconfig as a script with Python’s -m option:
$ python -m sysconfig
Platform: "macosx-10.4-i386"
Python version: "3.2"
Current installation scheme: "posix_prefix"
Paths:
data = "/usr/local"
include = "/Users/tarek/Dev/svn.python.org/py3k/Include"
platinclude = ".
AC_APPLE_UNIVERSAL_BUILD = "0"
AIX_GENUINE_CPLUSPLUS = "0"
AR = "ar"
ARFLAGS = "rc"

This
call
will
print
in
the
standard
output
the
information
returned
by
get_platform(),
get_python_version(), get_path() and get_config_vars().
See Built-in Functions and Built-in Constants for documentation.
This module is not normally accessed explicitly by most applications, but can be useful in modules that provide objects
with the same name as a built-in value, but in which the built-in of that name is also needed.
Since this
is an implementation detail, it may not be used by alternate implementations of Python.
They are explained in detail below.
If you’re new to Python modules, see the tutorial section tut-modules for an
introduction.
When a Python module or package is imported, __name__ is set to the module’s name.
However, if the module is executed in the top-level code environment, its __name__ is set to the string '__main__'.
What is the “top-level code environment”?
__main__ is the name of the environment where top-level code is run.
It’s “top-level” because it imports all other modules that the program needs.
Sometimes “top-
level code” is called an entry point to the application.
The top-level code environment can be:
• the scope of an interactive prompt:
'__main__'
• the Python module passed to the Python interpreter as a file argument:
$ python helloworld.py
Hello, world!
Explicit is better than implicit.
See also:
For a more detailed look at how __name__ is set in all situations, see the tutorial section tut-modules.
Idiomatic Usage
Some modules contain code that is intended for script use only, like parsing command-line arguments or fetching data
from standard input.
If a module like this was imported from a different module, for example to unit test it, the script
code would unintentionally execute as well.
Code within this block won’t
run unless the module is executed in the top-level environment.
None:
"""A dummy wrapper around print.
This is error-prone
as other functions within the module could be unintentionally using the global variable instead of a local name.
A main
function solves this problem.
When
echo.py is imported, the echo and main functions will be defined, but neither of them will be called, because
__name__ !
Packaging Considerations
main functions are often used to create command-line tools by specifying them as entry points for console scripts.
When
this is done, pip inserts the function call into a template script, where the return value of main is passed into sys.
exit().
Since the call to main is wrapped in sys.exit(), the expectation is that your function will return some value accept-
able as an input to sys.exit(); typically, an integer or None (which is implicitly returned if your function does not
have a return statement).
By proactively following this convention ourselves, our module will have the same behavior when run directly (i.e.
python echo.py) as it will have if we later package it as a console script entry-point in a pip-installable package.
In particular, be careful about returning strings from your main function.
The echo.py example from earlier exemplifies using the sys.exit(main()) convention.
See also:
Python Packaging User Guide contains a collection of tutorials and references on how to distribute and install Python
packages with modern tools.
Most commonly, the __main__.
Consider the following hypothetical package, “band-
class”:
bandclass
├── __init__.py
├── __main__.py
└── student.py
__main__.py will be executed when the package itself is invoked directly from the command line using the -m flag.
For example:
$ python -m bandclass
This command will cause __main__.py to run.
This import style can
be used when referencing modules within a package.
For more details, see intra-package-references in the tut-modules
section of the tutorial.
Instead, those
files are kept short and import functions to execute from other modules.
Those other modules can then be easily unit-tested
and are properly reusable.
This won’t work for __main__.py files in the root directory of a .zip file though.
Hence, for consistency, minimal
__main__.py like the venv one mentioned below are preferred.
See runpy for more details on the -m flag to the interpreter executable.
See zipapp for how to run applications packaged as .zip files.
In this case Python looks for a __main__.py file in
the root directory of the archive.
Regardless of which module a Python program was started with, other modules running within that same program can
import the top-level environment’s scope (namespace) by importing the __main__ module.
This doesn’t import a
__main__.py file but rather whichever module that received the special name '__main__'.
Now, if we started our program, the result would look like this:
$ python start.py
Define the variable `my_name`!
The exit code of the program would be 1, indicating an error.
Why does this work?
In our example this is the start module which runs line by line and imports namely.
That’s an import cycle!
Fortunately, since the partially populated __main__
module is present in sys.modules, Python passes that to namely.
See Special considerations for __main__ in the
import system’s reference for details on how this works.
ValueError: Define the variable `my_name`!
The __main__ scope is used in the implementation of pdb and rlcompleter.
29.5.
For example, one
might want to issue a warning when a program uses an obsolete module.
Python programmers issue warnings by calling the warn() function defined in this module.
Warning messages are normally written to sys.stderr, but their disposition can be changed flexibly, from ignoring
all warnings to turning them into exceptions.
The disposition of warnings can vary based on the warning category, the
text of the warning message, and the source location where it is issued.
Repetitions of a particular warning for the same
source location are typically suppressed.
There are two stages in warning control: first, each time a warning is issued, a determination is made whether a message
should be issued or not; next, if a message is to be issued, it is formatted and printed using a user-settable hook.
The determination whether to issue a warning message is controlled by the warning filter, which is a sequence of matching
rules and actions.
Rules can be added to the filter by calling filterwarnings() and reset to its default state by calling
resetwarnings().
The printing of warning messages is done by calling showwarning(), which may be overridden; the default implemen-
tation of this function formats the message by calling formatwarning(), which is also available for use by custom
implementations.
See also:
logging.captureWarnings() allows you to handle all warnings with the standard logging infrastructure.
This categorization is useful to be able to
filter out groups of warnings.
User code can define additional warning categories by subclassing one of the standard warning categories.
A warning
category must always be a subclass of the Warning class.
It is a subclass of
Exception.
UserWarning
The default category for warn().
DeprecationWarning
Base category for warnings about deprecated features when those warn-
ings are intended for other Python developers (ignored by default, unless
triggered by code in __main__).
SyntaxWarning
Base category for warnings about dubious syntactic features.
RuntimeWarning
Base category for warnings about dubious runtime features.
FutureWarning
Base category for warnings about deprecated features when those warn-
ings are intended for end users of applications that are written in Python.
PendingDeprecationWarning
Base category for warnings about features that will be deprecated in the
future (ignored by default).
ImportWarning
Base category for warnings triggered during the process of importing a
module (ignored by default).
UnicodeWarning
Base category for warnings related to Unicode.
BytesWarning
Base category for warnings related to bytes and bytearray.
ResourceWarning
Base category for warnings related to resource usage (ignored by default).
Changed in version 3.7: Previously DeprecationWarning and FutureWarning were distinguished based on
whether a feature was being removed entirely or changing its behaviour.
They are now distinguished based on their
intended audience and the way they’re handled by the default warnings filters.
The Warnings Filter
The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception).
Conceptually, the warnings filter maintains an ordered list of filter specifications; any specific warning is matched against
each filter specification in the list in turn until a match is found; the filter determines the disposition of the match.
Each
entry is a tuple of the form (action, message, category, module, lineno), where:
• action is one of the following strings:
Value
Disposition
"default" print the first occurrence of matching warnings for each location (module + line number) where
the warning is issued
"error"
turn matching warnings into exceptions
"ignore" never print matching warnings
"always" always print matching warnings
"module" print the first occurrence of matching warnings for each module where the warning is issued
(regardless of line number)
"once"
print only the first occurrence of matching warnings, regardless of location
• message is a string containing a regular expression that the start of the warning message must match, case-
insensitively.
In -W and PYTHONWARNINGS, message is a literal string that the start of the warning message
must contain (case-insensitively), ignoring any whitespace at the start or end of message.
In -W and PYTHONWARNINGS, module is a literal string that the fully qualified module name
must be equal to (case-sensitively), ignoring any whitespace at the start or end of module.
If a warning is reported and doesn’t match any registered filter then the “default” action is applied (hence its name).
The warnings filter is initialized by -W options passed to the Python interpreter command line and the
PYTHONWARNINGS environment variable.
The interpreter saves the arguments for all supplied entries without inter-
pretation in sys.warnoptions; the warnings module parses these when it is first imported (invalid options are
ignored, after printing a message to sys.stderr).
Individual warnings filters are specified as a sequence of fields separated by colons:
action:message:category:module:line
The meaning of each of these fields is as described in The Warnings Filter.
When listing multiple filters on a single line
(as for PYTHONWARNINGS), the individual filters are separated by commas and the filters listed later take precedence
over those listed before them (as they’re applied left-to-right, and the most recently applied filters take precedence over
earlier ones).
Commonly used warning filters apply to either all warnings, warnings in a particular category, or warnings raised by
particular modules or packages.
Some examples:
default
# Show all warnings (even those ignored by default)
ignore
# Ignore all warnings
error
# Convert all warnings to errors
error::ResourceWarning
# Treat ResourceWarning messages as errors
default::DeprecationWarning
# Show DeprecationWarning messages
ignore,default:::mymodule
# Only report warnings triggered by "mymodule"
error:::mymodule
# Convert warnings to errors in "mymodule"
Default Warning Filter
By default, Python installs several warning filters, which can be overridden by the -W command-line option, the
PYTHONWARNINGS environment variable and calls to filterwarnings().
DeprecationWarning is once again shown by default when triggered directly by code in
__main__.
While within the context manager all warnings will simply be ignored.
This allows you to use known-deprecated code
without having to see the warning while not suppressing the warning for other code that might not be aware of its use
of deprecated code.
Note: this can only be guaranteed in a single-threaded application.
If two or more threads use the
catch_warnings context manager at the same time, the behavior is undefined.
With it you can temporarily mutate the
warnings filter to facilitate your testing.
For instance, do the following to capture all raised warnings to check:
import warnings

with warnings.catch_warnings(record=True) as w:
# Cause all warnings to always be triggered.
One can also cause all warnings to be exceptions by using error instead of always.
One thing to be aware of is that
if a warning has already been raised because of a once/default rule, then no matter what filters are set the warning
will not be seen again unless the warnings registry related to the warning has been cleared.
Once the context manager exits, the warnings filter is restored to its state when the context was entered.
This prevents
tests from changing the warnings filter in unexpected ways between tests and leading to indeterminate test results.
The
showwarning() function in the module is also restored to its original value.
Note: this can only be guaranteed in a
single-threaded application.
If two or more threads use the catch_warnings context manager at the same time, the
behavior is undefined.
When testing multiple operations that raise the same kind of warning, it is important to test them in a manner that
confirms each operation is raising a new warning (e.g. set warnings to be raised as exceptions and check the operations
raise exceptions, check that the length of the warning list continues to increase after each operation, or else delete the
previous entries from the warnings list before each new operation).
Updating Code For New Versions of Dependencies
Warning categories that are primarily of interest to Python developers (rather than end users of applications written in
Python) are ignored by default.
Notably, this “ignored by default” list includes DeprecationWarning (for every module except __main__), which
means developers should make sure to test their code with typically ignored warnings made visible in order to receive
timely notifications of future breaking API changes (whether in the standard library or third party packages).
In the ideal case, the code will have a suitable test suite, and the test runner will take care of implicitly enabling all warnings
when running tests (the test runner provided by the unittest module does this).
In less ideal cases, applications can be checked for use of deprecated interfaces by passing -Wd to the Python interpreter
(this is shorthand for -W default) or setting PYTHONWARNINGS=default in the environment.
This enables de-
fault handling for all warnings, including those that are ignored by default.
To change what action is taken for encountered
warnings you can change what argument is passed to -W (e.g. -W error).
The category argument, if given, must be a warn-
ing category class; it defaults to UserWarning.
Alternatively, message can be a Warning instance, in which
case category will be ignored and message.__class__ will be used.
In this case, the message text will be
str(message).
This function raises an exception if the particular warning issued is changed into an error by
the warnings filter.
This makes the warning refer to deprecated_api’s caller, rather than to the source of deprecated_api
itself (since the latter would defeat the purpose of the warning message).
The skip_file_prefixes keyword argument can be used to indicate which stack frames are ignored when counting
stack levels.
This can be useful when you want the warning to always appear at call sites outside of a package when
a constant stacklevel does not fit all call paths or is otherwise challenging to maintain.
If supplied, it must be a tuple
of strings.
When prefixes are supplied, stacklevel is implicitly overridden to be max(2, stacklevel).
This makes the warning refer to both the example.lower.one_way() and package.higher.
another_way() call sites only from calling code living outside of example package.
Changed in version 3.6: Added source parameter.
This is a low-level interface to the functionality of warn(), passing in explicitly the message, cate-
gory, filename and line number, and optionally the module name and the registry (which should be the
__warningregistry__ dictionary of the module).
The module name defaults to the filename with .py
stripped; if no registry is passed, the warning is never suppressed.
Add the source parameter.
The default implementation calls formatwarning(message, category,
filename, lineno, line) and writes the resulting string to file, which defaults to sys.stderr.
You
may replace this function with any callable by assigning to warnings.showwarning.
None)
Format a warning the standard way.
This returns a string which may contain embedded newlines and ends
in a newline.
The entry is inserted at the front by default; if append
is true, it is inserted at the end.
This checks the types of the arguments, compiles the message and module regular
expressions, and inserts them as a tuple in the list of warnings filters.
Entries closer to the front of the list override
entries later in the list, if both match a particular warning.
Omitted arguments default to a value that matches
everything.
The meaning of the function parameters is as for
filterwarnings(), but regular expressions are not needed as the filter inserted always matches any message
in any module as long as the category and line number match.
This discards the effect of all previous calls to filterwarnings(), including that of
the -W command line options and calls to simplefilter().
A context manager that copies and, upon exit, restores the warnings filter and the showwarning() function.
If
the record argument is False (the default) the context manager returns None on entry.
If record is True, a list is
returned that is progressively populated with objects as seen by a custom showwarning() function (which also
suppresses output to sys.stdout).
Each object in the list has attributes with the same names as the arguments
to showwarning().
The module argument takes a module that will be used instead of the module returned when you import warnings
whose filter will be protected.
This argument exists primarily for testing the warnings module itself.
If the action argument is not None, the remaining arguments are passed to simplefilter() as if it were called
immediately on entering the context.
Note:
The catch_warnings manager works by replacing and then later restoring the module’s
showwarning() function and internal list of filter specifications.
It was originally described in PEP 557.
The member variables to use in these generated methods are defined using PEP 526 type annotations.
For example, this
code:
from dataclasses import dataclass

class InventoryItem:
"""Class for keeping track of an item in inventory.
New in version 3.7.
This function is a decorator that is used to add generated special methods to classes, as described below.
The dataclass() decorator examines the class to find fields.
A field is defined as a class variable that has
a type annotation.
With two exceptions described below, nothing in dataclass() examines the type specified
in the variable annotation.
The order of the fields in all of the generated methods is the order in which they appear in the class definition.
The dataclass() decorator will add various “dunder” methods to the class, described below.
If any of the added
methods already exist in the class, the behavior depends on the parameter, as documented below.
The decorator
returns the same class that it is called on; no new class is created.
If dataclass() is used just as a simple decorator with no parameters, it acts as if it has the default values
documented in this signature.
That is, these three uses of dataclass() are equivalent:
29.7.
The parameters to dataclass() are:
• init: If true (the default), a __init__() method will be generated.
If the class already defines __init__(), this parameter is ignored.
The generated repr string
will have the class name and the name and repr of each field, in the order they are defined in the
class.
Fields that are marked as being excluded from the repr are not included.
For example:
InventoryItem(name='widget', unit_price=3.0, quantity_on_hand=10).
If the class already defines __repr__(), this parameter is ignored.
This method compares the class as if it
were a tuple of its fields, in order.
Both instances in the comparison must be of the identical type.
If the class already defines __eq__(), this parameter is ignored.
These compare the class as if it were a tuple of its fields, in order.
Both instances in the
comparison must be of the identical type.
If order is true and eq is false, a ValueError is raised.
If the class already defines any of __lt__(), __le__(), __gt__(), or __ge__(), then TypeError
is raised.
Having a __hash__() implies that instances of the class are immutable.
Mutability is
a complicated property that depends on the programmer’s intent, the existence and behavior of __eq__(),
and the values of the eq and frozen flags in the dataclass() decorator.
By default, dataclass() will not implicitly add a __hash__() method unless it is safe to do so.
Nei-
ther will it add or change an existing explicitly defined __hash__() method.
If __hash__() is not explicitly defined, or if it is set to None, then dataclass() may add an im-
plicit __hash__() method.
Although not recommended, you can force dataclass() to create a
__hash__() method with unsafe_hash=True.
This might be the case if your class is logically im-
mutable but can nonetheless be mutated.
This is a specialized use case and should be considered carefully.
Here are the rules governing implicit creation of a __hash__() method.
Note that you cannot both have
an explicit __hash__() method in your dataclass and set unsafe_hash=True; this will result in a
TypeError.
If eq and frozen are both true, by default dataclass() will generate a __hash__() method for you.
If eq is true and frozen is false, __hash__() will be set to None, marking it unhashable (which it is,
since it is mutable).
If eq is false, __hash__() will be left untouched meaning the __hash__() method
of the superclass will be used (if the superclass is object, this means it will fall back to id-based hashing).
This emulates read-
only frozen instances.
See the discussion below.
If false, or if __match_args__ is already defined in the class, then __match_args__ will not be
generated.
New in version 3.10.
If a field is
marked as keyword-only, then the only effect is that the __init__() parameter generated from a keyword-
only field must be specified with a keyword when __init__() is called.
There is no effect on any other
aspect of dataclasses.
See the parameter glossary entry for details.
Also see the KW_ONLY section.
New in version 3.10.
• slots: If true (the default is False), __slots__ attribute will be generated and new class will be
returned instead of the original one.
If __slots__ is already defined in the class, then TypeError is
raised.
New in version 3.10.
Changed in version 3.11: If a field name is already included in the __slots__ of a base class, it
will not be included in the generated __slots__ to prevent overriding them.
Therefore, do not use
__slots__ to retrieve the field names of a dataclass.
Use fields() instead.
To be able to deter-
mine inherited slots, base class __slots__ may be any iterable, but not an iterator.
It is an error to specify weakref_slot=True without also specifying
slots=True.
New in version 3.11.
This is true whether
this occurs in a single class, or as a result of class inheritance.
There are, however, some dataclass features
that require additional per-field information.
To satisfy this need for additional information, you can replace the
default field value with a call to the provided field() function.
This sentinel is used because None is a valid value for some parameters with a distinct meaning.
No code
should directly use the MISSING value.
The parameters to field() are:
• default: If provided, this will be the default value for this field.
This is needed because the field() call
itself replaces the normal position of the default value.
Among other purposes, this can be used to specify fields with mutable default
values, as discussed below.
It is an error to specify both default and default_factory.
If true, this field is included in the generated __hash__() method.
If None (the default), use the value of compare: this would normally be the expected behavior.
A field
should be considered in the hash if it’s used for comparisons.
Setting this value to anything other than None
is discouraged.
One possible reason to set hash=False but compare=True would be if a field is expensive to compute
a hash value for, that field is needed for equality testing, and there are other fields that contribute to the type’s
hash value.
Even if a field is excluded from the hash, it will still be used for comparisons.
None is treated as an empty dict.
This value is wrapped in
MappingProxyType() to make it read-only, and exposed on the Field object.
It is not used at all by
Data Classes, and is provided as a third-party extension mechanism.
Multiple third-parties can each have
their own key, to use as a namespace in the metadata.
This is used when the generated __init__()
method’s parameters are computed.
New in version 3.10.
If the default value of a field is specified by a call to field(), then the class attribute for this field will be replaced
by the specified default value.
If no default is provided, then the class attribute will be deleted.
The intent
is that after the dataclass() decorator runs, the class attributes will all contain the default values for the fields,
just as if the default value itself were specified.
The class attribute C.z will be 10, the class attribute C.t will be 20, and the class attributes C.x and C.y will
not be set.
Field
Field objects describe each defined field.
These objects are created internally, and are returned by the
fields() module-level method (see below).
Users should never instantiate a Field object directly.
Its docu-
mented attributes are:
• name: The name of the field.
Other attributes may exist, but they are private and must not be inspected or relied on.
dataclasses.fields(class_or_instance)
Returns a tuple of Field objects that define the fields for this dataclass.
Accepts either a dataclass, or an instance
of a dataclass.
Raises TypeError if not passed a dataclass or instance of one.
Does not return pseudo-fields
which are ClassVar or InitVar.
dataclasses.asdict(obj, *, dict_factory=dict)
Converts the dataclass obj to a dict (by using the factory function dict_factory).
Each dataclass is converted
to a dict of its fields, as name: value pairs.
Other objects
are copied with copy.deepcopy().
Each dataclass is con-
verted to a tuple of its field values.
Other objects are copied
with copy.deepcopy().
If just name is supplied, typing.
Any is used for
type.
The values of init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots,
and weakref_slot have the same meaning as they do in dataclass().
If module is defined, the __module__ attribute of the dataclass is set to that value.
By default, it is set to the
module name of the caller.
This function is not strictly required, because any Python mechanism for creating a new class with
__annotations__ can then apply the dataclass() function to convert that class to a dataclass.
This
function is provided as a convenience.
For example:
C = make_dataclass('C',
[('x', int),
'y',
('z', int, field(default=5))],
namespace={'add_one': lambda self: self.x + 1})
Is equivalent to:

class C:
x: int
y: 'typing.
If obj is not a Data
Class, raises TypeError.
If values in changes do not specify fields, raises TypeError.
The newly returned object is created by calling the __init__() method of the dataclass.
This ensures that
__post_init__(), if present, is also called.
It is an error for changes to contain any fields that are defined as having init=False.
A ValueError will
be raised in this case.
Be forewarned about how init=False fields work during a call to replace().
They are not copied from the
source object, but rather are initialized in __post_init__(), if they’re initialized at all.
It is expected that
init=False fields will be rarely and judiciously used.
If they are used, it might be wise to have alternate class
constructors, or perhaps a custom replace() (or similarly named) method which handles instance copying.
If you need to know if a class is an instance of a dataclass (and not a dataclass itself), then add a further check for
not isinstance(obj, type):

dataclasses.
MISSING
A sentinel value signifying a missing default or default_factory.
dataclasses.
A sentinel value used as a type annotation.
Any fields after a pseudo-field with the type of KW_ONLY are marked
as keyword-only fields.
Note that a pseudo-field of type KW_ONLY is otherwise completely ignored.
This includes
the name of such a field.
By convention, a name of _ is used for a KW_ONLY field.
Keyword-only fields signify
__init__() parameters that must be specified as keywords when the class is instantiated.
In a single dataclass, it is an error to specify more than one field whose type is KW_ONLY.
New in version 3.10.
exception dataclasses.
FrozenInstanceError
Raised when an implicitly defined __setattr__() or __delattr__() is called on a dataclass which was
defined with frozen=True.
It is a subclass of AttributeError.
Post-init processing
dataclasses.__post_init__()
When defined on the class, it will be called by the generated __init__(), normally as self.
__post_init__().
However, if any InitVar fields are defined, they will also be passed to
__post_init__() in the order they were defined in the class.
If no __init__() method is generated,
then __post_init__() will not automatically be called.
Among other uses, this allows for initializing field values that depend on one or more other fields.
If the base
class has an __init__() method that has to be called, it is common to call this method in a __post_init__()
method:

class Rectangle:
height: float
width: float

class Square(Rectangle):
side: float

Note, however, that in general the dataclass-generated __init__() methods don’t need to be called, since the derived
dataclass will take care of initializing all fields of any base class that is a dataclass itself.
See the section below on init-only variables for ways to pass parameters to __post_init__().
Also see the warning
about how replace() handles init=False fields.
Class variables
One of the few places where dataclass() actually inspects the type of a field is to determine if a field is a class
variable as defined in PEP 526.
It does this by checking if the type of the field is typing.
ClassVar.
If a field is a
ClassVar, it is excluded from consideration as a field and is ignored by the dataclass mechanisms.
Such ClassVar
pseudo-fields are not returned by the module-level fields() function.
It does
this by seeing if the type of a field is of type dataclasses.
InitVar.
If a field is an InitVar, it is considered
a pseudo-field called an init-only field.
As it is not a true field, it is not returned by the module-level fields() func-
tion.
Init-only fields are added as parameters to the generated __init__() method, and are passed to the optional
__post_init__() method.
They are not otherwise used by dataclasses.
In this case, fields() will return Field objects for i and j, but not for database.
Frozen instances
It is not possible to create truly immutable Python objects.
However, by passing frozen=True to the dataclass()
decorator you can emulate immutability.
These methods will raise a FrozenInstanceError when invoked.
There is a tiny performance penalty when using frozen=True: __init__() cannot use simple assignment to ini-
tialize fields, and must use object.__setattr__().
After all of the base class fields are added, it adds its own fields to the ordered mapping.
All
of the generated methods will use this combined, calculated ordered mapping of fields.
Because the fields are in insertion
order, derived classes override base classes.
An example:

class Base:
x: Any = 15.0
y: int = 0

class C(Base):
z: int = 10
x: int = 15
The final list of fields is, in order, x, y, z.
The final type of x is int, as specified in class C.
After the parameters needed for __init__() are computed, any keyword-only parameters are moved to come after
all regular (non-keyword-only) parameters.
This is a requirement of how keyword-only parameters are implemented in
Python: they must come after non-keyword-only parameters.
In this example, Base.y, Base.w, and D.t are keyword-only fields, and Base.x and D.z are regular fields:

class Base:
x: Any = 15.0
(continues on next page)
29.7.
The generated __init__() method for D will look like:

fields are followed by parameters derived from keyword-only fields.
The relative ordering of keyword-only parameters is maintained in the re-ordered __init__() parameter list.
If a field is excluded from __init__() (using init=False) and the field also specifies default_factory, then
the default factory function will always be called from the generated __init__() function.
This happens because there
is no other way to give the field an initial value.
Mutable default values
Python stores default member variable values in class attributes.
Consider this example, not using dataclasses:
class C:
x = []

o1 = C()
o2 = C()
o1.add(1)
o2.add(2)
assert o1.x == [1, 2]
assert o1.x is o2.x
Note that the two instances of class C share the same class variable x, as expected.
This has the same issue as the original example using class C. That is, two instances of class D that do not specify a value
for x when creating a class instance will share the same copy of x. Because dataclasses just use normal Python class
creation they also share this behavior.
There is no general way for Data Classes to detect this condition.
Instead, the
dataclass() decorator will raise a ValueError if it detects an unhashable default parameter.
The assumption is
that if a value is unhashable, it is mutable.
This is a partial solution, but it does protect against many common errors.
Unhashability is used to approximate mutability.
The value for the field passed to the dataclass’s __init__ method is passed to the descriptor’s __set__ method
rather than overwriting the descriptor object.
If the descriptor re-
turns a value in this case, it will be used as the field’s default.
On the other hand, if the descriptor raises
AttributeError in this situation, no default value will be provided for the field.
For more information see also Context
Manager Types and context-managers.
AbstractContextManager
An abstract base class for classes that implement object.__enter__() and object.__exit__().
A default implementation for object.__enter__() is provided which returns self while object.
See also the definition of Context Manager
Types.
New in version 3.6.
An abstract base class for classes that implement object.__aenter__() and object.__aexit__().
A default implementation for object.__aenter__() is provided which returns self while object.
__aexit__() is an abstract method which by default returns None.
See also the definition of async-context-
managers.
New in version 3.7.
While many objects natively support use in with statements, sometimes a resource needs to be managed that isn’t
a context manager in its own right, and doesn’t implement a close() method for use with contextlib.
This iterator must yield exactly one
value, which will be bound to the targets in the with statement’s as clause, if any.
At the point where the generator yields, the block nested in the with statement is executed.
The generator is then
resumed after the block is exited.
If an unhandled exception occurs in the block, it is reraised inside the generator
at the point where the yield occurred.
Thus, you can use a try…except…finally statement to trap the
error (if any), or ensure that some cleanup takes place.
If an exception is trapped merely in order to log it or to
perform some action (rather than to suppress it entirely), the generator must reraise that exception.
Otherwise the
generator context manager will indicate to the with statement that the exception has been handled, and execution
will resume with the statement immediately following the with statement.
When used as a decorator, a new generator instance is implicitly created on each
function call (this allows the otherwise “one-shot” context managers created by contextmanager() to meet
the requirement that context managers support multiple invocations in order to be used as decorators).
Changed in version 3.2: Use of ContextDecorator.
This function is a decorator that can be used to define a factory function for async with statement
asynchronous context managers, without needing to create a class or separate __aenter__() and
__aexit__() methods.
It must be applied to an asynchronous generator function.
A simple example:
from contextlib import asynccontextmanager

async def get_connection():
conn = await acquire_db_connection()
try:

finally:
await release_db_connection(conn)
async def get_all_users():
async with get_connection() as conn:

29.8.
Context managers defined with asynccontextmanager() can be used either as decorators or with
async with statements:
import time
from contextlib import asynccontextmanager

async def timeit():
now = time.monotonic()
try:

print(f'it took {time.monotonic() - now}s to run')
@timeit()
async def main():
# ... async code ...
When used as a decorator, a new generator instance is implicitly created on each function call.
This
allows the otherwise “one-shot” context managers created by asynccontextmanager() to meet
the requirement that context managers support multiple invocations in order to be used as decorators.
Changed in version 3.10: Async context managers created with asynccontextmanager() can be used as
decorators.
This is basically equivalent to:
from contextlib import contextmanager



finally:
thing.close()
And lets you write code like this:
from contextlib import closing
from urllib.request import urlopen
with closing(urlopen('https://www.python.org')) as page:
for line in page:
print(line)
without needing to explicitly close page.
Even if an error occurs, page.close() will be called when the with
block is exited.
contextlib.aclosing(thing)
Return an async context manager that calls the aclose() method of thing upon completion of the block.
For example:
from contextlib import aclosing
async with aclosing(my_generator()) as values:
async for value in values:
if value == 42:
break
This pattern ensures that the generator’s async exit code is executed in the same context as its iterations (so that
exceptions and context variables work as expected, and the exit code isn’t run after the lifetime of some task it
depends on).
New in version 3.10.
It is intended
to be used as a stand-in for an optional context manager, for example:

# Use suppress to ignore all exceptions.
ClientSession()
else:
(continues on next page)
29.8.
Changed in version 3.10: asynchronous context manager support was added.
As with any other mechanism that completely suppresses exceptions, this context manager should be used only to
cover very specific errors where silently continuing with program execution is known to be the right thing to do.
If the code within the with block raises a BaseExceptionGroup, suppressed exceptions are removed from
the group.
If any exceptions in the group are not suppressed, a group containing them is re-raised.
New in version 3.4.
Changed in version 3.12:
suppress now supports suppressing exceptions raised as part of an
BaseExceptionGroup.
contextlib.redirect_stdout(new_target)
Context manager for temporarily redirecting sys.stdout to another file or file-like object.
This tool adds flexibility to existing functions or classes whose output is hardwired to stdout.
For example, the output of help() normally is sent to sys.stdout.
You can capture that output in a string by
redirecting the output to an io.StringIO object.
To send the output of help() to sys.stderr:
with redirect_stdout(sys.stderr):
help(pow)
Note that the global side effect on sys.stdout means that this context manager is not suitable for use in library
code and most threaded applications.
It also has no effect on the output of subprocesses.
However, it is still a useful
approach for many utility scripts.
This context manager is reentrant.
Similar to redirect_stdout() but redirecting sys.stderr to another file or file-like object.
This context manager is reentrant.
Non parallel-safe context manager to change the current working directory.
As this changes a global state, the
working directory, it is not suitable for use in most threaded or async contexts.
It is also not suitable for most non-
linear code execution, like generators, where the program execution is temporarily relinquished – unless explicitly
desired, you should not yield when this context manager is active.
This is a simple wrapper around chdir(), it changes the current working directory upon entering and restores
the old one on exit.
This context manager is reentrant.
New in version 3.11.
ContextDecorator
A base class that enables a context manager to also be used as a decorator.
Context managers inheriting from ContextDecorator have to implement __enter__ and __exit__ as
normal.
ContextDecorator is used by contextmanager(), so you get this functionality automatically.
Example of ContextDecorator:
from contextlib import ContextDecorator
class mycontext(ContextDecorator):


(continues on next page)
29.8.
It makes it clear that the cm applies to the whole function, rather than just a piece of it (and saving an indentation
level is nice, too).
Existing context managers that already have a base class can be extended by using ContextDecorator as a
mixin class:
from contextlib import ContextDecorator
class mycontext(ContextBaseClass, ContextDecorator):


Note: As the decorated function must be able to be called multiple times, the underlying context manager must
support use in multiple with statements.
If this is not the case, then the original construct with the explicit with
statement inside the function should be used.
AsyncContextDecorator
Similar to ContextDecorator but only for asynchronous functions.
Starting
The bit in the middle
Finishing
New in version 3.10.
class contextlib.
ExitStack
A context manager that is designed to make it easy to programmatically combine other context managers and
cleanup functions, especially those that are optional or otherwise driven by input data.
Each instance maintains a stack of registered callbacks that are called in reverse order when the instance is closed
(either explicitly or implicitly at the end of a with statement).
Note that callbacks are not invoked implicitly when
the context stack instance is garbage collected.
This stack model is used so that context managers that acquire their resources in their __init__ method (such
as file objects) can be handled correctly.
Since registered callbacks are invoked in the reverse order of registration, this ends up behaving as if multiple nested
with statements had been used with the registered set of callbacks.
This even extends to exception handling - if
an inner callback suppresses or replaces an exception, then outer callbacks will be passed arguments based on that
updated state.
This is a relatively low level API that takes care of the details of correctly unwinding the stack of exit callbacks.
It
provides a suitable foundation for higher level context managers that manipulate the exit stack in application specific
ways.
New in version 3.3.
The return value is
the result of the context manager’s own __enter__() method.
These context managers may suppress exceptions just as they normally would if used directly as part of a
with statement.
Raises TypeError instead of AttributeError if cm is not a context manager.
push(exit)
Adds a context manager’s __exit__() method to the callback stack.
As __enter__ is not invoked, this method can be used to cover part of an __enter__() implementation
with a context manager’s own __exit__() method.
If passed an object that is not a context manager, this method assumes it is a callback with the same signature
as a context manager’s __exit__() method and adds it directly to the callback stack.
By returning true values, these callbacks can suppress exceptions the same way context manager
__exit__() methods can.
The passed in object is returned from the function, allowing this method to be used as a function decorator.
Unlike the other methods, callbacks added this way cannot suppress exceptions (as they are never passed the
exception details).
The passed in callback is returned from the function, allowing this method to be used as a function decorator.
No callbacks are invoked by this
operation - instead, they will now be invoked when the new stack is closed (either explicitly or implicitly at
the end of a with statement).
If opening any file fails, all previously opened files will be
# closed automatically.
If all files are opened successfully,
# they will remain open even after the with statement ends.
AsyncExitStack
An asynchronous context manager, similar to ExitStack, that supports combining both synchronous and asyn-
chronous context managers, as well as having coroutines for cleanup logic.
The close() method is not implemented, aclose() must be used instead.
Similar to push() but expects either an asynchronous context manager or a coroutine function.
Similar to callback() but expects a coroutine function.
Similar to close() but properly handles awaitables.
New in version 3.7.
This section describes some examples and recipes for making effective use of the tools provided by contextlib.
Supporting a variable number of context managers
The primary use case for ExitStack is the one given in the class documentation: supporting a variable number of
context managers and other cleanup operations in a single with statement.
The variability may come from the number
of context managers needed being driven by user input (such as opening a user specified collection of files), or from some
of the context managers being optional:
with ExitStack() as stack:
for resource in resources:
stack.enter_context(resource)
if need_special_resource():
special = acquire_special_resource()
stack.callback(release_special_resource, special)
# Perform operations that use the acquired resources
As shown, ExitStack also makes it quite easy to use with statements to manage arbitrary resources that don’t natively
support the context management protocol.
When a
context manager is the only resource management API provided, then ExitStack can make it easier to handle various
situations that can’t be handled directly in a with statement.
As with any try statement based code, this can cause problems for development and review, because the setup code and
the cleanup code can end up being separated by arbitrarily long sections of code.
This allows the intended cleanup up behaviour to be made explicit up front, rather than requiring a separate flag variable.
If the resource cleanup isn’t already neatly bundled into a standalone function, then it is still possible to use the decorator
form of ExitStack.callback() to declare the resource cleanup in advance:
29.8.
Due to the way the decorator protocol works, a callback function declared this way cannot take any parameters.
Instead,
any resources to be released must be accessed as closure variables.
Using a context manager as a function decorator
ContextDecorator makes it possible to use a context manager in both an ordinary with statement and also as a
function decorator.
For example, it is sometimes useful to wrap functions or groups of statements with a logger that can track the time of
entry and time of exit.
Rather than writing both a function decorator and a context manager for the task, inheriting from
ContextDecorator provides both capabilities in a single definition:
from contextlib import ContextDecorator
import logging
logging.basicConfig(level=logging.
If that value is needed, then it is still necessary to use an explicit with statement.
The “with” statement The specification, background, and examples for the Python with statement.
These
single use context managers must be created afresh each time they’re used - attempting to use them a second time will
trigger an exception or otherwise not work correctly.
This common limitation means that it is generally advisable to create context managers directly in the header of the with
statement where they are used (as shown in all of the usage examples above).
Files are an example of effectively single use context managers, since the first with statement will close the file, preventing
any further IO operations using that file object.
RuntimeError: generator didn't yield
Reentrant context managers
More sophisticated context managers may be “reentrant”.
These context managers can not only be used in multiple with
statements, but may also be used inside a with statement that is already using the same context manager.
RLock is an example of a reentrant context manager, as are suppress(), redirect_stdout(),
and chdir().
This is written directly to stdout
This is written to the stream rather than stdout
This is also written to the stream
29.8.
Note also that being reentrant is not the same thing as being thread safe.
Reusable context managers
Distinct from both single use and reentrant context managers are “reusable” context managers (or, to be completely
explicit, “reusable, but not reentrant” context managers, since reentrant context managers are also reusable).
These context
managers support being used multiple times, but will fail (or otherwise not work correctly) if the specific context manager
instance has already been used in a containing with statement.
Lock is an example of a reusable, but not reentrant, context manager (for a reentrant lock, it is necessary
to use threading.
RLock instead).
Lib/abc.py
This module provides the infrastructure for defining abstract base classes (ABCs) in Python, as outlined in PEP 3119; see
the PEP for why this was added to Python.
The collections module has some concrete classes that derive from ABCs; these can, of course, be further derived.
In addition, the collections.abc submodule has some ABCs that can be used to test whether a class or instance
provides a particular interface, for example, if it is hashable or if it is a mapping.
This module provides the metaclass ABCMeta for defining ABCs and a helper class ABC to alternatively define ABCs
through inheritance:
class abc.
ABC
A helper class that has ABCMeta as its metaclass.
With this class, an abstract base class can be created by simply
deriving from ABC avoiding sometimes confusing metaclass usage, for example:
from abc import ABC
class MyABC(ABC):

metaclass usage, as multiple inheritance may lead to metaclass conflicts.
ABCMeta
Metaclass for defining Abstract Base Classes (ABCs).
Use this metaclass to create an ABC.
An ABC can be subclassed directly, and then acts as a mix-in class.
You can
also register unrelated concrete classes (even built-in classes) and unrelated ABCs as “virtual subclasses” – these and
their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function,
but the registering ABC won’t show up in their MRO (Method Resolution Order) nor will method implementations
defined by the registering ABC be callable (not even via super()).1
Classes created with a metaclass of ABCMeta have the following method:
1 C++ programmers should note that Python’s virtual base class concept is not the same as C++’s.
For example:
from abc import ABC
class MyABC(ABC):

assert issubclass(tuple, MyABC)
assert isinstance((), MyABC)
Changed in version 3.3: Returns the registered subclass, to allow usage as a class decorator.
Changed in version 3.4: To detect calls to register(), you can use the get_cache_token() function.
Check whether subclass is considered a subclass of this ABC.
This means that you can customize the behavior
of issubclass further without the need to call register() on every class you want to consider a
subclass of the ABC.
This method should return True, False or NotImplemented.
If it returns True, the subclass is con-
sidered a subclass of this ABC.
If it returns False, the subclass is not considered a subclass of this ABC,
even if it would normally be one.
If it returns NotImplemented, the subclass check is continued with the
usual mechanism.
The ABC MyIterable defines the standard iterable method, __iter__(), as an abstract method.
The im-
plementation given here can still be called from subclasses.
The get_iterator() method is also part of the
MyIterable abstract base class, but it does not have to be overridden in non-abstract derived classes.
The __subclasshook__() class method defined here says that any class that has an __iter__() method in
its __dict__ (or in that of one of its base classes, accessed via the __mro__ list) is considered a MyIterable
too.
Finally, the last line makes Foo a virtual subclass of MyIterable, even though it does not define
an __iter__() method (it uses the old-style iterable protocol, defined in terms of __len__() and
__getitem__()).
Note that this will not make get_iterator available as a method of Foo, so it is provided
separately.
The abc module also provides the following decorator:

A decorator indicating abstract methods.
Using this decorator requires that the class’s metaclass is ABCMeta or is derived from it.
A class that has a metaclass
derived from ABCMeta cannot be instantiated unless all of its abstract methods and properties are overridden.
The
abstract methods can be called using any of the normal ‘super’ call mechanisms.
Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method
or class once it is created, are only supported using the update_abstractmethods() function.
The
abstractmethod() only affects subclasses derived using regular inheritance; “virtual subclasses” registered
with the ABC’s register() method are not affected.
In order to correctly interoperate with the abstract base class machinery, the descriptor must identify itself as
abstract using __isabstractmethod__.
In general, this attribute should be True if any of the methods used
to compose the descriptor are abstract.
Note: Unlike Java abstract methods, these abstract methods may have an implementation.
This implementation
can be called via the super() mechanism from the class that overrides it.
This could be useful as an end-point
for a super-call in a framework that uses cooperative multiple-inheritance.
The abc module also supports the following legacy decorators:

New in version 3.2.
Deprecated since version 3.3: It is now possible to use classmethod with abstractmethod(), making this
decorator redundant.
A subclass of the built-in classmethod(), indicating an abstract classmethod.
Otherwise it is similar to
abstractmethod().
This special case is deprecated, as the classmethod() decorator is now correctly identified as abstract when
applied to an abstract method:
class C(ABC):




New in version 3.2.
Deprecated since version 3.3: It is now possible to use staticmethod with abstractmethod(), making
this decorator redundant.
A subclass of the built-in staticmethod(), indicating an abstract staticmethod.
Otherwise it is similar to
abstractmethod().
A subclass of the built-in property(), indicating an abstract property.
The abc module also provides the following functions:
abc.get_cache_token()
Returns the current abstract base class cache token.
The token is an opaque object (that supports equality testing) identifying the current version of the abstract base
class cache for virtual subclasses.
The token changes with every call to ABCMeta.register() on any ABC.
A function to recalculate an abstract class’s abstraction status.
This function should be called if a class’s abstract
methods have been implemented or changed after it was created.
Usually, this function should be called from within
a class decorator.
Returns cls, to allow usage as a class decorator.
If cls is not an instance of ABCMeta, does nothing.
Note: This function assumes that cls’s superclasses are already updated.
It does not update any subclasses.
New in version 3.10.
Functions thus registered are auto-
matically executed upon normal interpreter termination.
C, B, A.
Note: The functions registered via this module are not called when the program is killed by a signal not handled by
Python, when a Python fatal internal error is detected, or when os._exit() is called.
Note: The effect of registering or unregistering functions from within a cleanup function is undefined.
Changed in version 3.7: When used with C-API subinterpreters, registered functions are local to the interpreter they were
registered in.
Register func as a function to be executed at termination.
Any optional arguments that are to be passed to func
must be passed as arguments to register().
It is possible to register the same function and arguments more
than once.
At normal program termination (for instance, if sys.exit() is called or the main module’s execution completes),
all functions registered are called in last in, first out order.
The assumption is that lower level modules will normally
be imported before higher level modules and thus must be cleaned up later.
If an exception is raised during execution of the exit handlers, a traceback is printed (unless SystemExit is
raised) and the exception information is saved.
After all exit handlers have had a chance to run, the last exception
to be raised is re-raised.
This function returns func, which makes it possible to use it as a decorator.
Warning: Starting new threads or calling os.fork() from a registered function can lead to race condition
between the main Python runtime thread freeing thread states while internal threading routines or the new
process try to use that state.
This can lead to crashes rather than clean shutdown.
Remove func from the list of functions to be run at interpreter shutdown.
If func has been registered more than once, every occurrence of that function
in the atexit call stack will be removed.
Equality comparisons (==) are used internally during unregistration,
so function references do not need to have matching identities.
See also:
Module readline Useful example of atexit to read and write readline history files.
The following simple example demonstrates how a module can initialize a counter from a file when it is imported and
save the counter’s updated value automatically when the program terminates without relying on the application making
an explicit call into this module at termination.
This only works with functions that can be called without arguments.
It exactly mimics
the behavior of the Python interpreter when it prints a stack trace.
This is useful when you want to print stack traces under
program control, such as in a “wrapper” around the interpreter.
The module uses traceback objects — these are objects of type types.
See also:
Module faulthandler Used to dump Python tracebacks explicitly, on a fault, after a timeout, or on a user signal.
Module pdb Interactive source code debugger for Python programs.
The module defines the following functions:
traceback.print_tb(tb, limit=None, file=None)
Print up to limit stack trace entries from traceback object tb (starting from the caller’s frame) if limit is positive.
Otherwise, print the last abs(limit) entries.
If limit is omitted or None, all entries are printed.
If file is omitted
or None, the output goes to sys.stderr; otherwise it should be an open file or file-like object to receive the
output.
Changed in version 3.5: Added negative limit support.
This differs from print_tb()
in the following ways:
• if tb is not None, it prints a header Traceback (most recent call last):
• it prints the exception type and value after the stack trace
• if type(value) is SyntaxError and value has the appropriate format, it prints the line where the syntax
error occurred with a caret indicating the approximate position of the error.
Since Python 3.10, instead of passing value and tb, an exception object can be passed as the first argument.
If value
and tb are provided, the first argument is ignored in order to provide backwards compatibility.
The optional limit argument has the same meaning as for print_tb().
The etype argument is ignored and inferred from the type of value.
The etype parameter has been renamed to exc and is now positional-only.
This is a shorthand for print_exception(sys.exception(), limit, file, chain).
This is a shorthand for print_exception(sys.last_exc, limit, file, chain).
In general it
will work only after an exception has reached an interactive prompt (see sys.last_exc).
Otherwise, print the last
abs(limit) entries.
If limit is omitted or None, all entries are printed.
The optional f argument can be used to
specify an alternate stack frame to start.
The optional file argument has the same meaning as for print_tb().
Changed in version 3.5: Added negative limit support.
It is useful for alternate formatting of stack traces.
The optional limit argument has the same
meaning as for print_tb().
A “pre-processed” stack trace entry is a FrameSummary object containing
attributes filename, lineno, name, and line representing the information that is usually printed for a stack
trace.
The return value has the same format as for
extract_tb().
Each string in the resulting list corresponds to the item with the same
index in the argument list.
Format the exception part of a traceback using an exception value such as given by sys.last_value.
The return
value is a list of strings, each ending in a newline.
The list contains the exception’s message, which is normally a
single string; however, for SyntaxError exceptions, it contains several lines that (when printed) display detailed
information about where the syntax error occurred.
Following the message, the list contains the exception’s notes.
Since Python 3.10, instead of passing value, an exception object can be passed as the first argument.
If value is
provided, the first argument is ignored in order to provide backwards compatibility.
The etype parameter has been renamed to exc and is now positional-only.
The returned list now includes any notes attached to the exception.
traceback.format_exception(exc, /[, value, tb], limit=None, chain=True)
Format a stack trace and the exception information.
The arguments have the same meaning as the corresponding
arguments to print_exception().
The return value is a list of strings, each ending in a newline and some
containing internal newlines.
When these lines are concatenated and printed, exactly the same text is printed as
does print_exception().
The etype argument is ignored and inferred from the type of value.
This function’s behavior and signature were modified to match print_exception().
This is like print_exc(limit) but returns a string instead of printing to a file.
A shorthand for format_list(extract_tb(tb, limit)).
A shorthand for format_list(extract_stack(f, limit)).
New in version 3.4.
traceback.walk_stack(f)
Walk a stack following f.f_back from the given frame, yielding the frame and line number for each frame.
If f
is None, the current stack is used.
This helper is used with StackSummary.extract().
New in version 3.5.
This helper is used with
StackSummary.extract().
New in version 3.5.
The module also defines the following classes:
29.11.
TracebackException objects are created from actual exceptions to capture data for later printing in a lightweight
fashion.
TracebackException(exc_type, exc_value, exc_traceback, *, limit=None,
lookup_lines=True, capture_locals=False, compact=False,
max_group_width=15, max_group_depth=10)
Capture an exception for later rendering.
If compact is true, only data that is required by TracebackException’s format method is saved in
the class attributes.
Note that when locals are captured, they are also shown in the traceback.
The depth refers to the nesting level of the group, and the width refers to the
size of a single exception group’s exceptions array.
The formatted output is truncated when either limit is
exceeded.
Added the compact parameter.
A TracebackException of the original __context__.
If it
is not None is it formatted in the traceback after the exception string.
New in version 3.11.
Can be None if not present.
New in version 3.10.
Can be None if not present.
New in version 3.10.
Note that when locals are captured, they are also shown in the traceback.
New in version 3.11.
If chain is not True, __cause__ and __context__ will not be formatted.
The return value is a generator of strings, each ending in a newline and some containing internal newlines.
Format the exception part of the traceback.
The return value is a generator of strings, each ending in a newline.
The generator emits the exception’s message followed by its notes (if it has any).
The exception message
is normally a single string; however, for SyntaxError exceptions, it consists of several lines that (when
printed) display detailed information about where the syntax error occurred.
The exception’s notes are now included in the output.
StackSummary objects represent a call stack ready for formatting.
StackSummary
29.11.
If limit is supplied, only this many frames are taken from frame_gen.
If lookup_lines is False, the re-
turned FrameSummary objects will not have read their lines in yet, making the cost of creating the
StackSummary cheaper (which may be valuable if it may not actually get formatted).
If capture_locals is
True the local variables in each FrameSummary are captured as object representations.
Exceptions raised from repr() on a local variable (when capture_locals is True)
are no longer propagated to the caller.
classmethod from_list(a_list)
Construct a StackSummary object from a supplied list of FrameSummary objects or old-style list of
tuples.
Each tuple should be a 4-tuple with filename, lineno, name, line as the elements.
Each string in the resulting list corresponds to a single frame from
the stack.
Each string ends in a newline; the strings may contain internal newlines as well, for those items
with source text lines.
For long sequences of the same frame and line, the first few repetitions are shown, followed by a summary
line stating the exact number of further repetitions.
Long sequences of repeated frames are now abbreviated.
This method is called for each
FrameSummary object to be printed by StackSummary.format().
If it returns None, the frame
is omitted from the output.
New in version 3.11.
A FrameSummary object represents a single frame in a traceback.
FrameSummary(filename, lineno, name, lookup_line=True, locals=None, line=None)
Represent a single frame in the traceback or stack that is being formatted or printed.
It may optionally have a
stringified version of the frames locals included in it.
If lookup_line is False, the source code is not looked
up until the FrameSummary has the line attribute accessed (which also happens when casting it to a tuple).
Traceback Examples
This simple example implements a basic read-eval-print loop, similar to (but less useful than) the standard Python inter-
active interpreter loop.
For a more complete implementation of the interpreter loop, refer to the code module.
IndexError: tuple index out of range
*** print_exc:
Traceback (most recent call last):
(continues on next page)
29.11.
Lib/__future__.py
__future__ is a real module, and serves three purposes:
• To avoid confusing existing tools that analyze import statements and expect to find the modules they’re importing.
This is a form of executable documentation, and can be inspected programmatically via importing __future__
and examining its contents.
Each statement in __future__.py is of the form:
FeatureName = _Feature(OptionalRelease, MandatoryRelease,
CompilerFlag)
where, normally, OptionalRelease is less than MandatoryRelease, and both are 5-tuples of the same form as sys.
OptionalRelease records the first release in which the feature was accepted.
In the case of a MandatoryRelease that has not yet occurred, MandatoryRelease predicts the release in which the feature
will become part of the language.
Else MandatoryRelease records when the feature became part of the language; in releases at or after that, modules no
longer need a future statement to use the feature in question, but may continue to use such imports.
MandatoryRelease may also be None, meaning that a planned feature got dropped.
Instances
of
class
_Feature
have
two
corresponding
methods,
getOptionalRelease()
and
getMandatoryRelease().
CompilerFlag is the (bitfield) flag that should be passed in the fourth argument to the built-in function compile() to
enable the feature in dynamically compiled code.
This flag is stored in the compiler_flag attribute on _Feature
instances.
No feature description will ever be deleted from __future__.
Bytes literals in Python 3000
generator_stop
3.5.0b1
3.7
PEP 479: StopIteration handling inside generators
annotations
3.7.0b1
TBD1
PEP 563: Postponed evaluation of annotations
See also:
future How the compiler treats future imports.
It provides the ability to disable the collector, tune
the collection frequency, and set debugging options.
It also provides access to unreachable objects that the collector found
but cannot free.
Since the collector supplements the reference counting already used in Python, you can disable the
collector if you are sure your program does not create reference cycles.
Automatic collection can be disabled by calling
gc.disable().
To debug a leaking program call gc.set_debug(gc.DEBUG_LEAK).
Notice that this includes
gc.
DEBUG_SAVEALL, causing garbage-collected objects to be saved in gc.garbage for inspection.
The gc module provides the following functions:
gc.enable()
Enable automatic garbage collection.
Disable automatic garbage collection.
With no arguments, run a full collection.
The optional argument generation may be an integer specifying which
generation to collect (from 0 to 2).
A ValueError is raised if the generation number is invalid.
The number of
unreachable objects found is returned.
The free lists maintained for a number of built-in types are cleared whenever a full collection or collection of the
highest generation (2) is run.
Not all items in some free lists may be freed due to the particular implementation, in
particular float.
The effect of calling gc.collect() while the interpreter is already performing a collection is undefined.
Debugging information will be written to sys.stderr.
No final decision has been made yet.
If generation is not None, return
only the objects tracked by the collector that are in that generation.
New generation parameter.
Raises an auditing event gc.get_objects with argument generation.
The number
of keys may change in the future, but currently each dictionary will contain the following items:
• collections is the number of times this generation was collected;
• collected is the total number of objects collected inside this generation;
• uncollectable is the total number of objects which were found to be uncollectable (and were therefore
moved to the garbage list) inside this generation.
New in version 3.4.
Setting threshold0 to zero disables collection.
The GC classifies objects into three generations depending on how many collection sweeps they have survived.
New objects are placed in the youngest generation (generation 0).
If an object survives a collection it is moved
into the next older generation.
Since generation 2 is the oldest generation, objects in that generation remain there
after a collection.
In order to decide when to run, the collector keeps track of the number object allocations and
deallocations since the last collection.
When the number of allocations minus the number of deallocations exceeds
threshold0, collection starts.
Initially only generation 0 is examined.
If generation 0 has been examined more than
threshold1 times since generation 1 has been examined, then generation 1 is examined as well.
With the third
generation, things are a bit more complicated, see Collecting the oldest generation for more information.
This function will only locate those containers which
support garbage collection; extension types which do refer to other objects but do not support garbage collection
will not be found.
Note that objects which have already been dereferenced, but which live in cycles and have not yet been collected by
the garbage collector can be listed among the resulting referrers.
To get only currently live objects, call collect()
before calling get_referrers().
Warning: Care must be taken when using objects returned by get_referrers() because some of them
could still be under construction and hence in a temporarily invalid state.
Avoid using get_referrers()
for any purpose other than debugging.
Raises an auditing event gc.get_referrers with argument objs.
The referents returned are those objects visited
by the arguments’ C-level tp_traverse methods (if any), and may not be all objects actually directly reachable.
So, for example, if an integer is directly reachable from an argument,
that integer object may or may not appear in the result list.
Raises an auditing event gc.get_referents with argument objs.
Returns True if the object is currently tracked by the garbage collector, False otherwise.
As a general rule,
instances of atomic types aren’t tracked and instances of non-atomic types (containers, user-defined objects…) are.
Returns True if the given object has been finalized by the garbage collector, False otherwise.
If a process will fork() without exec(), avoiding unnecessary copy-on-write in child processes will maximize
memory sharing and reduce overall memory usage.
This requires both avoiding creation of freed “holes” in memory
pages in the parent process and ensuring that GC collections in child processes won’t touch the gc_refs counter
of long-lived objects originating in the parent process.
To accomplish both, call gc.disable() early in the
parent process, gc.freeze() right before fork(), and gc.enable() early in child processes.
New in version 3.7.
New in version 3.7.
The following variables are provided for read-only access (you can mutate the values but should not rebind them):
gc.garbage
A list of objects which the collector found to be unreachable but could not be freed (uncollectable objects).
Starting
with Python 3.4, this list should be empty most of the time, except when using instances of C extension types with
a non-NULL tp_del slot.
If DEBUG_SAVEALL is set, then all unreachable objects will be added to this list rather than freed.
Changed in version 3.2: If this list is non-empty at interpreter shutdown, a ResourceWarning is emitted, which
is silent by default.
If DEBUG_UNCOLLECTABLE is set, in addition all uncollectable objects are printed.
Changed in version 3.4: Following PEP 442, objects with a __del__() method don’t end up in gc.garbage
anymore.
The callbacks will be
called with two arguments, phase and info.
The following keys are currently defined:
“generation”: The oldest generation being collected.
Applications can add their own callbacks to this list.
The primary use cases are:
Gathering statistics about garbage collection, such as how often various generations are collected, and
how long the collection takes.
Allowing applications to identify and clear their own uncollectable types when they appear in garbage.
New in version 3.3.
Print statistics during collection.
This information can be useful when tuning the collection frequency.
These objects will be added to the garbage list.
Changed in version 3.2: Also print the contents of the garbage list at interpreter shutdown, if it isn’t empty.
gc.
When set, all unreachable objects found will be appended to garbage rather than being freed.
This can be useful
for debugging a leaking program.
The debugging flags necessary for the collector to print information about a leaking program (equal to
DEBUG_COLLECTABLE | DEBUG_UNCOLLECTABLE | DEBUG_SAVEALL).
For example, it can help you examine the contents of a
class, retrieve the source code of a method, extract and format the argument list for a function, or get all the information
you need to display a detailed traceback.
There are four main kinds of services provided by this module: type checking, getting source code, inspecting classes and
functions, and examining the interpreter stack.
The getmembers() function retrieves the members of an object such as a class or module.
The functions whose names
begin with “is” are mainly provided as convenient choices for the second argument to getmembers().
See sys.set_coroutine_origin_tracking_dep
builtin
__doc__
documentation string
__name__
original name of this function or method
__qualname__
qualified name
__self__
instance to which a method is bound, or None
29.14.
The __name__ attribute of generators is now set from the function name, instead of the code name, and it can now be
modified.
Add cr_origin attribute to coroutines.
Changed in version 3.10: Add __builtins__ attribute to functions.
If the optional predicate
argument—which will be called with the value object of each member—is supplied, only members for which
the predicate returns a true value are included.
Note:
getmembers() will only return class attributes defined in the metaclass when the argument is a class
and those attributes have been listed in the metaclass’ custom __dir__().
Optionally, only return members that satisfy a
given predicate.
Note:
getmembers_static() may not be able to retrieve all members that getmembers can fetch (like
dynamically created attributes) and may find members that getmembers can’t (like descriptors that raise Attribu-
teError).
It can also return descriptor objects instead of instance members in some cases.
New in version 3.11.
The file
extension is checked against all of the entries in importlib.machinery.all_suffixes().
If it matches,
the final path component is returned with the extension removed.
Otherwise, None is returned.
Note that this function only returns a meaningful name for actual Python modules - paths that potentially refer to
Python packages will still return None.
Changed in version 3.3: The function is based directly on importlib.
Return True if the object is a bound method written in Python.
Return True if the object is a coroutine function (a function defined with an async def syntax), a functools.
New in version 3.5.
Functions wrapped in functools.partial() now return True if the wrapped
function is a coroutine function.
Changed in version 3.12: Sync functions marked with markcoroutinefunction() now return True.
inspect.markcoroutinefunction(func)
Decorator to mark a callable as a coroutine function if it would not otherwise be detected by
iscoroutinefunction().
This may be of use for sync functions that return a coroutine, if the function is passed to an API that requires
iscoroutinefunction().
When possible, using an async def function is preferred.
New in version 3.12.
inspect.iscoroutine(object)
Return True if the object is a coroutine created by an async def function.
Return True if the object can be used in await expression.
Functions wrapped in functools.partial() now return True if the wrapped
function is a asynchronous generator function.
29.14.
Return True if the object is a code.
Return True if the type of object is a MethodWrapperType.
Return True if the object is a method descriptor, but not if ismethod(), isclass(), isfunction() or
isbuiltin() are true.
This, for example, is true of int.__add__.
An object passing this test has a __get__() method but not a
__set__() method, but beyond that the set of attributes varies.
A __name__ attribute is usually sensible, and
__doc__ often is.
Methods implemented via descriptors that also pass one of the other tests return False from the
ismethoddescriptor() test, simply because the other tests promise more – you can, e.g., count on hav-
ing the __func__ attribute (etc) when an object passes ismethod().
Data descriptors have a __set__ or a __delete__ method.
Examples are properties (defined in Python), get-
sets, and members.
The latter two are defined in C and there are more specific tests available for those types, which
is robust across Python implementations.
Typically, data descriptors will also have __name__ and __doc__
attributes (properties, getsets, and members have both of these attributes), but this is not guaranteed.
inspect.isgetsetdescriptor(object)
Return True if the object is a getset descriptor.
CPython implementation detail: getsets are attributes defined in extension modules via PyGetSetDef struc-
tures.
For Python implementations without such types, this method will always return False.
inspect.ismemberdescriptor(object)
Return True if the object is a member descriptor.
For Python implementations without such types, this method will always return
False.
Get the documentation string for an object, cleaned up with cleandoc().
If the documentation string for an
object is not provided and the object is a class, a method, a property or a descriptor, retrieve the documentation
string from the inheritance hierarchy.
Return None if the documentation string is invalid or missing.
Changed in version 3.5: Documentation strings are now inherited if not overridden.
If the object’s source code is
unavailable, return None.
This could happen if the object has been defined in C or the interactive shell.
This will fail with a TypeError if
the object is a built-in module, class, or function.
Return None if the module cannot be determined.
inspect.getsourcefile(object)
Return the name of the Python source file in which an object was defined or None if no way can be identified to
get the source.
This will fail with a TypeError if the object is a built-in module, class, or function.
The argument may be a module, class, method,
function, traceback, frame, or code object.
The source code is returned as a list of the lines corresponding to the
object and the line number indicates where in the original source file the first line of code was found.
An OSError
is raised if the source code cannot be retrieved.
A TypeError is raised if the object is a built-in module, class,
or function.
Changed in version 3.3: OSError is raised instead of IOError, now an alias of the former.
The argument may be a module, class, method, function, traceback,
frame, or code object.
The source code is returned as a single string.
An OSError is raised if the source code
cannot be retrieved.
A TypeError is raised if the object is a built-in module, class, or function.
Changed in version 3.3: OSError is raised instead of IOError, now an alias of the former.
inspect.cleandoc(doc)
Clean up indentation from docstrings that are indented to line up with blocks of code.
All leading whitespace is removed from the first line.
Any leading whitespace that can be uniformly removed from
the second line onwards is removed.
Empty lines at the beginning and end are subsequently removed.
Also, all tabs
are expanded to spaces.
The Signature object represents the call signature of a callable object and its return annotation.
To retrieve a Signature
object, use the signature() function.
If the passed object has a __signature__ attribute, this function returns it without further computations.
The global, locals, and eval_str parameters are passed
into inspect.get_annotations() when resolving the annotations; see the documentation for inspect.
Also, if the annotations are stringized, and eval_str is not false, the eval() call(s) to un-stringize the anno-
tations could potentially raise any kind of exception.
A slash(/) in the signature of a function denotes that the parameters prior to it are positional-only.
For more info,
see the FAQ entry on positional-only parameters.
New in version 3.5: follow_wrapped parameter.
New in version 3.10: globals, locals, and eval_str parameters.
Note: Some callables may not be introspectable in certain implementations of Python.
For example, in CPython,
some built-in functions defined in C provide no metadata about their arguments.
A Signature object represents the call signature of a function and its return annotation.
For each parameter accepted
by the function it stores a Parameter object in its parameters collection.
The optional return_annotation argument, can be an arbitrary Python object, is the “return” annotation of the
callable.
Signature objects are immutable.
Use Signature.replace() to make a modified copy.
Changed in version 3.5: Signature objects are picklable and hashable.
Parameters appear in
strict definition order, including keyword-only parameters.
Python only explicitly guaranteed that it preserved the declaration order of keyword-
only parameters as of version 3.7, although in practice this order had always been preserved in Python 3.
return_annotation
The “return” annotation for the callable.
If the callable has no “return” annotation, this attribute is set to
Signature.empty.
bind(*args, **kwargs)
Create a mapping from positional and keyword arguments to parameters.
Returns BoundArguments if
*args and **kwargs match the signature, or raises a TypeError.
It is possible to pass
different parameters and/or return_annotation to override the corresponding properties of the
base signature.
To remove return_annotation from the copied Signature, pass in Signature.empty.


"(a, b) -> 'new return anno'"
classmethod from_callable(obj, *, follow_wrapped=True, globalns=None, localns=None)
Return a Signature (or its subclass) object for a given callable obj.
Its behavior is otherwise identical to that of signature().
New in version 3.5.
New in version 3.10: globalns and localns parameters.
Parameter(name, kind, *, default=Parameter.empty, annotation=Parameter.empty)
Parameter objects are immutable.
Instead of modifying a Parameter object, you can use Parameter.
replace() to create a modified copy.
Parameter objects are picklable and hashable.
The name must be a valid Python identifier.
These parameter names are exposed by this module as names like implicit0.
If the parameter has no default value, this attribute is set to
Parameter.empty.
If the parameter has no annotation, this attribute is set to Parameter.
empty.
The possible values are accessible via
Parameter (like Parameter.KEYWORD_ONLY), and support comparison and ordering, in the follow-
ing order:
Name
Meaning
POSITIONAL_ONLY
Value must be supplied as a positional argument.
Positional only
parameters are those which appear before a / entry (if present) in a
Python function definition.
VAR_POSITIONAL
A tuple of positional arguments that aren’t bound to any other
parameter.
This corresponds to a *args parameter in a Python
function definition.
KEYWORD_ONLY
Value must be supplied as a keyword argument.
Keyword only
parameters are those which appear after a * or *args entry in a
Python function definition.
VAR_KEYWORD
A dict of keyword arguments that aren’t bound to any other parameter.
This corresponds to a **kwargs parameter in a Python function
definition.
Parameter: c
kind.description
Describes a enum value of Parameter.kind.
New in version 3.8.
To override a Parameter
attribute, pass the corresponding argument.
In Python 3.3 Parameter objects were allowed to have name set to None if their kind
was set to POSITIONAL_ONLY.
This is no longer permitted.
BoundArguments
Result of a Signature.bind() or Signature.bind_partial() call.
Holds the mapping of arguments
to the function’s parameters.
Contains only explicitly bound arguments.
Changes in arguments will reflect in args and kwargs.
Should be used in conjunction with Signature.parameters for any argument processing purposes.
Note:
Arguments for which Signature.bind() or Signature.bind_partial() relied on a
default value are skipped.
However, if needed, use BoundArguments.apply_defaults() to add
them.
Changed in version 3.9: arguments is now of type dict.
Formerly, it was of type collections.
OrderedDict.
Dynamically computed from the arguments attribute.
kwargs
A dict of keyword arguments values.
Dynamically computed from the arguments attribute.
For variable-positional arguments (*args) the default is an empty tuple.
The detailed specification, implementation details and examples.
Where a nested list appears, it contains classes
derived from the class whose entry immediately precedes the list.
Each entry is a 2-tuple containing a class and
a tuple of its base classes.
If the unique argument is true, exactly one entry appears in the returned structure for
each class in the given list.
Otherwise, classes using multiple inheritance and their descendants will appear multiple
times.
inspect.getfullargspec(func)
Get the names and default values of a Python function’s parameters.
A named tuple is returned:
FullArgSpec(args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults,
annotations)
args is a list of the positional parameter names.
The special
key "return" is used to report the function return value annotation (if any).
Note that signature() and Signature Object provide the recommended API for callable introspection, and
support additional behaviours (like positional-only arguments) that are sometimes encountered in extension module
APIs.
This function is retained primarily for use in code that needs to maintain compatibility with the Python 2
inspect module API.
This method was previously documented as deprecated in favour of signature()
in Python 3.5, but that decision has been reversed in order to restore a clearly supported standard interface for
single-source Python 2/3 code migrating away from the legacy getargspec() API.
Get information about arguments passed into a particular frame.
A named tuple ArgInfo(args, varargs,
keywords, locals) is returned.
The format* arguments are
the corresponding optional formatting functions that are called to turn names and values into strings.
Note: This function was inadvertently marked as deprecated in Python 3.5.
inspect.getmro(cls)
Return a tuple of class cls’s base classes, including cls, in method resolution order.
No class appears more than
once in this tuple.
Note that the method resolution order depends on cls’s type.
Unless a very peculiar user-defined
metatype is in use, cls will be the first element of the tuple.
For bound methods, bind also the first argument (typically named self) to the associated instance.
A dict
is returned, mapping the argument names (including the names of the * and ** arguments, if any) to their values
from args and kwds.
In case of invoking func incorrectly, i.e. whenever func(*args, **kwds) would raise
an exception because of incompatible signature, an exception of the same type and the same or similar message is
raised.
For example:


True
True
(continues on next page)
29.14.
Traceback (most recent call last):

TypeError: f() missing 1 required positional argument: 'a'
New in version 3.2.
Deprecated since version 3.5: Use Signature.bind() and Signature.bind_partial() instead.
inspect.getclosurevars(func)
Get the mapping of external name references in a Python function or method func to their current values.
A
named tuple ClosureVars(nonlocals, globals, builtins, unbound) is returned.
TypeError is raised if func is not a Python function or method.
New in version 3.3.
inspect.unwrap(func, *, stop=None)
Get the object wrapped by func.
It follows the chain of __wrapped__ attributes returning the last object in the
chain.
If the callback never returns a true value, the last object
in the chain is returned as usual.
ValueError is raised if a cycle is encountered.
New in version 3.4.
inspect.get_annotations(obj, *, globals=None, locals=None, eval_str=False)
Compute the annotations dict for an object.
Passing in an object of any other type raises TypeError.
Returns a dict.
If eval_str is true, values of type str will be un-stringized using eval().
If obj doesn’t have an annotations dict, returns an empty dict.
If a class doesn’t have its own annotations dict, returns an empty
dict.
All accesses to object members and dict values are done using getattr() and dict.get() for safety.
If eval_str is true, eval() is called on values of type str.
If eval_str is false (the default), values of type str are unchanged.
If obj is a module, globals defaults to obj.__dict__.
If obj is a class, globals defaults to sys.modules[obj.__module__].__dict__ and locals
defaults to the obj class namespace.
Calling get_annotations is best practice for accessing the annotations dict of any object.
See annotations-
howto for more information on annotations best practices.
New in version 3.10.
The interpreter stack
Some of the following functions return FrameInfo objects.
For backwards compatibility these objects allow tuple-like
operations on all attributes except positions.
This behavior is considered deprecated and may be removed in the
future.
FrameInfo
frame
The frame object that the record corresponds to.
Positions object containing the start line number, end line number, start column offset, and end
column offset associated with the instruction being executed by the frame this record corresponds to.
Return a named tuple instead of a tuple.
Changed in version 3.11: FrameInfo is now a class instance (that is backwards compatible with the previous
named tuple).
Traceback
filename
The file name associated with the code being executed by the frame this traceback corresponds to.
29.14.
Positions object containing the start line number, end line number, start column offset, and end
column offset associated with the instruction being executed by the frame this traceback corresponds to.
Changed in version 3.11: Traceback is now a class instance (that is backwards compatible with the previous
named tuple).
Note:
Keeping references to frame objects, as found in the first element of the frame records these functions return,
can cause your program to create reference cycles.
Once a reference cycle has been created, the lifespan of all objects
which can be accessed from the objects which form the cycle can become much longer even if Python’s optional cycle
detector is enabled.
If such cycles must be created, it is important to ensure they are explicitly broken to avoid the delayed
destruction of objects and increased memory consumption which occurs.
Though the cycle detector will catch these, destruction of the frames (and local variables) can be made deterministic by
removing the cycle in a finally clause.
This is also important if the cycle detector was disabled when Python was
compiled or using gc.disable().
For example:

try:
# do something with the frame
finally:
del frame
If you want to keep the frame around (for example to print a traceback later), you can also break reference cycles by using
the frame.clear() method.
The optional context argument supported by most of these functions specifies the number of lines of context to return,
which are centered around the current line.
A Traceback object is returned.
A Traceback object is returned instead of a named tuple.
These frames represent the calls that lead to
the creation of frame.
The first entry in the returned list represents frame; the last entry represents the outermost
call on frame’s stack.
A list of named tuples FrameInfo(frame, filename, lineno, function,
code_context, index) is returned.
Changed in version 3.11: A list of FrameInfo objects is returned.
These frames represent calls made
as a consequence of frame.
The first entry in the list represents traceback; the last entry represents where the
exception was raised.
A list of named tuples FrameInfo(frame, filename, lineno, function,
code_context, index) is returned.
Changed in version 3.11: A list of FrameInfo objects is returned.
CPython implementation detail: This function relies on Python stack frame support in the interpreter, which
isn’t guaranteed to exist in all implementations of Python.
Return a list of FrameInfo objects for the caller’s stack.
The first entry in the returned list represents the caller;
the last entry represents the outermost call on the stack.
A list of named tuples FrameInfo(frame, filename, lineno, function,
code_context, index) is returned.
Changed in version 3.11: A list of FrameInfo objects is returned.
The first entry in the list represents the caller; the last entry represents where
the exception was raised.
A list of named tuples FrameInfo(frame, filename, lineno, function,
code_context, index) is returned.
Changed in version 3.11: A list of FrameInfo objects is returned.
For
cases
where
you
want
passive
introspection,
like
documentation
tools,
this
can
be
inconvenient.
Note: this function may not be able to retrieve all attributes that getattr can fetch (like dynamically created attributes)
and may find attributes that getattr can’t (like descriptors that raise AttributeError).
It can also return descriptors
objects instead of instance members.
If the instance __dict__ is shadowed by another member (for example a property) then this function will be
unable to find instance members.
New in version 3.2.
The descriptor object is returned instead of the underlying attribute.
You can handle these with code like the following.
When implementing coroutine schedulers and for other advanced uses of generators, it is useful to determine
whether a generator is currently executing, is waiting to start or resume or execution, or has already terminated.
Possible states are:
• GEN_CREATED: Waiting to start execution.
New in version 3.2.
inspect.getcoroutinestate(coroutine)
Get current state of a coroutine object.
The function is intended to be used with coroutine objects created by async
def functions, but will accept any coroutine-like object that has cr_running and cr_frame attributes.
Possible states are:
• CORO_CREATED: Waiting to start execution.
CORO_RUNNING: Currently being executed by the interpreter.
Execution has completed.
The function is intended to be used with asynchronous iter-
ator objects created by async def functions which use the yield statement, but will accept any asynchronous
generator-like object that has ag_running and ag_frame attributes.
Possible states are:
• AGEN_CREATED: Waiting to start execution.
AGEN_CLOSED: Execution has completed.
New in version 3.12.
The current internal state of the generator can also be queried.
This is mostly useful for testing purposes, to ensure that
internal state is being updated as expected:
inspect.getgeneratorlocals(generator)
Get the mapping of live local variables in generator to their current values.
A dictionary is returned that maps from
variable names to values.
This is the equivalent of calling locals() in the body of the generator, and all the
same caveats apply.
If generator is a generator with no currently associated frame, then an empty dictionary is returned.
TypeError
is raised if generator is not a Python generator object.
CPython implementation detail: This function relies on the generator exposing a Python stack frame for intro-
spection, which isn’t guaranteed to be the case in all implementations of Python.
In such cases, this function will
always return an empty dictionary.
This function is analogous to getgeneratorlocals(), but works for coroutine objects created by async
def functions.
This function is analogous to getgeneratorlocals(), but works for asynchronous generator objects created
by async def functions which use the yield statement.
New in version 3.12.
CO_OPTIMIZED
The code object is optimized, using fast locals.
inspect.
If set, a new dict will be created for the frame’s f_locals when the code object is executed.
The code object has a variable positional parameter (*args-like).
The code object has a variable keyword parameter (**kwargs-like).
CO_NESTED
The flag is set when the code object is a nested function.
The flag is set when the code object is a generator function, i.e. a generator object is returned when the code object
is executed.
The flag is set when the code object is a coroutine function.
When the code object is executed it returns a coroutine
object.
See PEP 492 for more details.
New in version 3.5.
inspect.
The flag is used to transform generators into generator-based coroutines.
Generator objects with this flag can be
used in await expression, and can yield from coroutine objects.
See PEP 492 for more details.
New in version 3.5.
The flag is set when the code object is an asynchronous generator function.
When the code object is executed it
returns an asynchronous generator object.
See PEP 525 for more details.
New in version 3.6.
The flags are specific to CPython, and may not be defined in other Python implementations.
Furthermore, the
flags are an implementation detail, and can be removed or deprecated in future Python releases.
It’s recommended to use
public APIs from the inspect module for any introspection needs.
BufferFlags
This is an enum.
IntFlag that represents the flags that can be passed to the __buffer__() method of objects
implementing the buffer protocol.
The meaning of the flags is explained at buffer-request-types.
By default, accepts the name of a module and prints the source of that module.
A class or function within the module can
be printed instead by appended a colon and the qualified name of the target object.
The automatic import can be suppressed using the
interpreter’s -S option.
Importing this module will append site-specific paths to the module search path and add a few builtins, unless -S was
used.
In that case, this module can be safely imported with no automatic modifications to the module search path or
additions to the builtins.
To explicitly trigger the usual site-specific additions, call the main() function.
Importing the module used to trigger paths manipulation even when using -S.
It starts by constructing up to four directories from a head and a tail part.
For the head part, it uses sys.prefix
and sys.exec_prefix; empty heads are skipped.
For the tail part, it uses the empty string and then lib/
site-packages (on Windows) or lib/pythonX.Y/site-packages (on Unix and macOS).
For each of the
distinct head-tail combinations, it sees if it refers to an existing directory, and if so, adds it to sys.path and also inspects
the newly added path for configuration files.
Changed in version 3.5: Support for the “site-python” directory has been removed.
If a file named “pyvenv.cfg” exists one directory above sys.executable, sys.prefix and sys.exec_prefix are set to that direc-
tory and it is also checked for site-packages (sys.base_prefix and sys.base_exec_prefix will always be the “real” prefixes of
the Python installation).
If “pyvenv.cfg” (a bootstrap configuration file) contains the key “include-system-site-packages”
set to anything other than “true” (case-insensitive), the system-level prefixes will not be searched for site-packages; oth-
erwise they will.
A path configuration file is a file whose name has the form name.pth and exists in one of the four directories mentioned
above; its contents are additional items (one per line) to be added to sys.path.
Non-existing items are never added to
sys.path, and no check is made that the item refers to a directory rather than a file.
No item is added to sys.path
more than once.
Blank lines and lines beginning with # are skipped.
Lines starting with import (followed by space or
tab) are executed.
Note:
An executable line in a .pth file is run at every Python startup, regardless of whether a particular module is
actually going to be used.
Its impact should thus be kept to a minimum.
The primary intended purpose of executable
lines is to make the corresponding module(s) importable (load 3rd-party import hooks, adjust PATH etc).
Any other
initialization is supposed to be done upon a module’s actual import, if and when it happens.
Limiting a code chunk to a
single line is a deliberate measure to discourage putting anything more complex here.
For example, suppose sys.prefix and sys.exec_prefix are set to /usr/local.
The Python X.Y li-
brary is then installed in /usr/local/lib/pythonX.Y.
Assume foo.pth contains the following:
# foo package configuration
foo
bar
bletch
and bar.pth contains:
# bar package configuration
bar
Then the following version-specific directories are added to sys.path, in this order:
/usr/local/lib/pythonX.Y/site-packages/bar
/usr/local/lib/pythonX.Y/site-packages/foo
Note that bletch is omitted because it doesn’t exist; the bar directory precedes the foo directory because bar.pth
comes alphabetically before foo.pth; and spam is omitted because it is not mentioned in either path configuration file.
It is typically created by a system administrator in the site-packages direc-
tory.
If this import fails with an ImportError or its subclass exception, and the exception’s name attribute equals to
'sitecustomize', it is silently ignored.
If Python is started without output streams available, as with pythonw.
Any
other exception causes a silent and perhaps mysterious failure of the process.
This file is intended to be created in the user site-packages directory
(see below), which is part of sys.path unless disabled by -s.
If this import fails with an ImportError or its
subclass exception, and the exception’s name attribute equals to 'usercustomize', it is silently ignored.
Note that for some non-Unix systems, sys.prefix and sys.exec_prefix are empty, and the path manipulations
are skipped; however the import of sitecustomize and usercustomize is still attempted.
To disable it, delete (or override) the sys.__interactivehook__
attribute in your sitecustomize or usercustomize module or your PYTHONSTARTUP file.
Changed in version 3.4: Activation of rlcompleter and history was made automatic.
PREFIXES
A list of prefixes for site-packages directories.
site.
ENABLE_USER_SITE
Flag showing the status of the user site-packages directory.
True means that it is enabled and was added to sys.
path.
False means that it was disabled by user request (with -s or PYTHONNOUSERSITE).
None means it
was disabled for security reasons (mismatch between user or group id and effective id) or by an administrator.
site.USER_SITE
Path to the user site-packages for the running Python.
Can be None if getusersitepackages()
hasn’t been called yet.
Default value is ~/.local/lib/pythonX.Y/site-packages for UNIX and
non-framework macOS builds, ~/Library/Python/X.Y/lib/python/site-packages for macOS
framework builds, and %APPDATA%\Python\PythonXY\site-packages on Windows.
This directory is
a site directory, which means that .pth files in it will be processed.
site.
USER_BASE
Path to the base directory for the user site-packages.
Can be None if getuserbase() hasn’t been called yet.
Default value is ~/.local for UNIX and macOS non-framework builds, ~/Library/Python/X.Y for ma-
cOS framework builds, and %APPDATA%\Python for Windows.
This value is used to compute the installation di-
rectories for scripts, data files, Python modules, etc. for the user installation scheme.
See also PYTHONUSERBASE.
This function is called automatically when
this module is imported, unless the Python interpreter was started with the -S flag.
Changed in version 3.3: This function used to be called unconditionally.
Typically used in sitecustomize or usercustomize
(see above).
29.15.
New in version 3.2.
If it is not initialized yet, this function will also set it,
respecting PYTHONUSERBASE.
New in version 3.2.
If it is not initialized yet, this function
will also set it, respecting USER_BASE.
To determine if the user-specific site-packages was added to sys.path
ENABLE_USER_SITE should be used.
New in version 3.2.
If both options are given, user base and user site will be printed (always in this order), separated by os.pathsep.
If any option is given, the script will exit with one of these values: 0 if the user site-packages directory is enabled, 1 if it
was disabled by the user, 2 if it is disabled for security reasons or by an administrator, and a value greater than 2 if there
is an error.
The modules described in this chapter allow writing interfaces similar to Python’s interactive interpreter.
If you want a
Python interpreter that supports some special feature in addition to the Python language, you should look at the code
module.
The full list of modules described in this chapter is:
30.1 code — Interpreter base classes
Source code: Lib/code.py
The code module provides facilities to implement read-eval-print loops in Python.
Two classes and convenience functions
are included which can be used to build applications which provide an interactive interpreter prompt.
This class deals with parsing and interpreter state (the user’s namespace); it does not deal with input buffering or
prompting or input file naming (the filename is always passed in explicitly).
InteractiveConsole(locals=None, filename=’<console>’)
Closely
emulate
the
behavior
of
the
interactive
Python
interpreter.
This
class
builds
on
InteractiveInterpreter and adds prompting using the familiar sys.ps1 and sys.ps2, and
input buffering.
This creates a new instance of InteractiveConsole
and sets readfunc to be used as the InteractiveConsole.raw_input() method, if provided.
If local
is provided, it is passed to the InteractiveConsole constructor for use as the default namespace for the
interpreter loop.
The interact() method of the instance is then run with banner and exitmsg passed as the
banner and exit message to use, if provided.
The console object is discarded after use.
Changed in version 3.6: Added exitmsg parameter.
This function is useful for programs that want to emulate Python’s interpreter main loop (a.k.a.
The tricky part is to determine when the user has entered an incomplete command that can be completed
by entering more text (as opposed to a complete command or a syntax error).
This function almost always makes
the same decision as the real interpreter main loop.
Compile and run some source in the interpreter.
Arguments are the same as for compile_command(); the
default for filename is '<input>', and for symbol is 'single'.
The
input
is
incorrect;
compile_command()
raised
an
exception
(SyntaxError
or
OverflowError).
A syntax traceback will be printed by calling the showsyntaxerror()
method.
The input is incomplete, and more input is required;
compile_command() returned None.
runsource() returns True.
The input is complete; compile_command() returned a code object.
The code is executed by calling the
runcode() (which also handles run-time exceptions, except for SystemExit).
The return value can be used to decide whether to use sys.ps1 or sys.ps2 to prompt the next line.
InteractiveInterpreter.runcode(code)
Execute a code object.
When an exception occurs, showtraceback() is called to display a traceback.
All
exceptions are caught except SystemExit, which is allowed to propagate.
A note about KeyboardInterrupt: this exception may occur elsewhere in this code, and may not always be
caught.
The caller should be prepared to deal with it.
InteractiveInterpreter.showsyntaxerror(filename=None)
Display the syntax error that just occurred.
This does not display a stack trace because there isn’t one for syntax
errors.
If filename is given, it is stuffed into the exception instead of the default filename provided by Python’s
parser, because it always uses '<string>' when reading from a string.
The output is written by the write()
method.
InteractiveInterpreter.showtraceback()
Display the exception that just occurred.
We remove the first stack item because it is within the interpreter object
implementation.
The output is written by the write() method.
The full chained traceback is displayed instead of just the primary traceback.
InteractiveInterpreter.write(data)
Write a string to the standard error stream (sys.stderr).
Derived classes should override this to provide the
appropriate output handling as needed.
InteractiveConsole.interact(banner=None, exitmsg=None)
Closely emulate the interactive Python console.
The optional banner argument specify the banner to print before
the first interaction; by default it prints a banner similar to the one printed by the standard Python interpreter,
followed by the class name of the console object in parentheses (so as not to confuse this with the real interpreter
– since it’s so close!).
The optional exitmsg argument specifies an exit message printed when exiting.
Pass the empty string to suppress
the exit message.
If exitmsg is not given or None, a default message is printed.
Changed in version 3.4: To suppress printing any banner, pass an empty string.
Print an exit message when exiting.
InteractiveConsole.push(line)
Push a line of source text to the interpreter.
The line should not have a trailing newline; it may have internal newlines.
The line is appended to a buffer and the interpreter’s runsource() method is called with the concatenated
contents of the buffer as source.
If this indicates that the command was executed or invalid, the buffer is reset;
otherwise, the command is incomplete, and the buffer is left as it was after the line was appended.
The return value is
True if more input is required, False if the line was dealt with in some way (this is the same as runsource()).
InteractiveConsole.resetbuffer()
Remove any unhandled source text from the input buffer.
InteractiveConsole.raw_input(prompt=”)
Write a prompt and read a line.
The returned line does not include the trailing newline.
When the user enters
the EOF key sequence, EOFError is raised.
The base implementation reads from sys.stdin; a subclass may
replace this with a different implementation.
As a result, you probably don’t want to use the module directly; if you want to include such a loop in your
program you probably want to use the code module instead.
There are two parts to this job:
1.
Being able to tell if a line of input completes a Python statement: in short, telling whether to print ‘next.
Remembering which future statements the user has entered, so subsequent input can be compiled with these in
effect.
The codeop module provides a way of doing each of these things, and a way of doing them both.
Tries to compile source, which should be a string of Python code and return a code object if source is valid Python
code.
In that case, the filename attribute of the code object will be filename, which defaults to '<input>'.
Returns None if source is not valid Python code, but is a prefix of valid Python code.
If there is a problem with source, an exception will be raised.
SyntaxError is raised if there is invalid Python
syntax, and OverflowError or ValueError if there is an invalid literal.
The symbol argument determines whether source is compiled as a statement ('single', the default), as a sequence
of statement ('exec') or as an expression ('eval').
Any other value will cause ValueError to be raised.
Note: It is possible (but not likely) that the parser stops parsing with a successful outcome before reaching the end
of the source; in this case, trailing symbols may be ignored instead of causing an error.
This will be fixed once the API for the parser is
better.
The modules described in this chapter provide new ways to import other Python modules and hooks for customizing the
import process.
The full list of modules described in this chapter is:
31.1 zipimport — Import modules from Zip archives
Source code: Lib/zipimport.py
This module adds the ability to import Python modules (*.py, *.pyc) and packages from ZIP-format archives.
It is
usually not needed to use the zipimport module explicitly; it is automatically used by the built-in import mechanism
for sys.path items that are paths to ZIP archives.
Typically, sys.path is a list of directory names as strings.
This module also allows an item of sys.path to be a string
naming a ZIP file archive.
The ZIP archive can contain a subdirectory structure to support package imports, and a path
within the archive can be specified to only import from a subdirectory.
For example, the path example.zip/lib/
would only import from the lib/ subdirectory within the archive.
ZIP import of
dynamic modules (.pyd, .so) is disallowed.
Changed in version 3.8: Previously, ZIP archives with an archive comment were not supported.
See also:
PKZIP Application Note Documentation on the ZIP file format by Phil Katz, the creator of the format and algorithms
used.
PEP 273 - Import Modules from Zip Archives Written by James C. Ahlstrom, who also provided an implementation.
The implementation of the import machinery Package providing the relevant protocols for all im-
porters to implement.
This module defines an exception:
exception zipimport.
ZipImportError
Exception raised by zipimporter objects.
It’s a subclass of ImportError, so it can be caught as ImportError,
too.
For example, an archivepath of foo/bar.zip/lib will look for modules in the lib directory inside the ZIP
file foo/bar.zip (provided that it exists).
ZipImportError is raised if archivepath doesn’t point to a valid ZIP archive.
Changed in version 3.12: Methods find_loader() and find_module(), deprecated in 3.10 are now re-
moved.
Use find_spec() instead.
create_module(spec)
Implementation of importlib.abc.Loader.create_module() that returns None to explicitly re-
quest the default semantics.
Implementation of importlib.abc.Loader.exec_module().
An implementation of importlib.abc.PathEntryFinder.find_spec().
New in version 3.10.
get_code(fullname)
Return the code object for the specified module.
Raise OSError if the file wasn’t found.
Changed in version 3.3: IOError used to be raised, it is now an alias of OSError.
Raise
ZipImportError if the module couldn’t be imported.
New in version 3.1.
get_source(fullname)
Return the source code for the specified module.
Load the module specified by fullname.
Returns
the imported module on success, raises ZipImportError on failure.
New in version 3.10.
This is the empty string for zipimporter objects
which point to the root of the ZIP file.
The archive and prefix attributes, when combined with a slash, equal the original archivepath argument given
to the zipimporter constructor.
A namedtuple that holds a brief summary of a module’s info.
New in version 3.6.
pkgutil.extend_path(path, name)
Extend the search path for the modules which comprise a package.
For each directory on sys.path that has a subdirectory that matches the package name, add the subdirectory to
the package’s __path__.
This is useful if one wants to distribute different parts of a single logical package as
multiple directories.
It also looks for *.pkg files beginning where * matches the name argument.
This feature is similar to *.pth files
(see the site module for more information), except that it doesn’t special-case lines starting with import.
A
*.pkg file is trusted at face value: apart from checking for duplicates, all entries found in a *.pkg file are added
to the path, regardless of whether they exist on the filesystem.
If the input path is not a list (as is the case for frozen packages) it is returned unchanged.
The input path is not
modified; an extended copy is returned.
Items are only appended to the copy at the end.
It is assumed that sys.path is a sequence.
Items of sys.path that are not strings referring to existing direc-
tories are ignored.
Unicode items on sys.path that cause errors when used as filenames may cause this function
to raise an exception (in line with os.path.isdir() behavior).
Retrieve a module loader for the given fullname.
This is a backwards compatibility wrapper around importlib.util.find_spec() that converts most
failures to ImportError and only returns the loader rather than the full importlib.machinery.
ModuleSpec.
Changed in version 3.4: Updated to be based on PEP 451
Deprecated since version 3.12, will be removed in version 3.14: Use importlib.util.find_spec() in-
stead.
pkgutil.get_importer(path_item)
Retrieve a finder for the given path_item.
The returned finder is cached in sys.path_importer_cache if it was newly created by a path hook.
The cache (or part of it) can be cleared manually if a rescan of sys.path_hooks is necessary.
If the module or package is accessible via the normal import mechanism, a wrapper around the relevant part of
that machinery is returned.
Returns None if the module cannot be found or imported.
If the named module is not
already imported, its containing package (if any) is imported, in order to establish the package __path__.
Changed in version 3.4: Updated to be based on PEP 451
Deprecated since version 3.12, will be removed in version 3.14: Use importlib.util.find_spec() in-
stead.
If fullname contains a '.', the finders will be for the package containing fullname, otherwise they will be all
registered top level finders (i.e. those on both sys.meta_path and sys.path_hooks).
If the named module is in a package, that package is imported as a side effect of invoking this function.
Yields ModuleInfo for all submodules on path, or, if path is None, all top-level modules on sys.path.
Note: Only works for a finder which defines an iter_modules() method.
This interface is non-standard, so
the module also provides implementations for importlib.machinery.
FileFinder and zipimport.
If no onerror function is supplied, ImportErrors are
caught and ignored, while all other exceptions are propagated, terminating the search.
Note: Only works for a finder which defines an iter_modules() method.
This interface is non-standard, so
the module also provides implementations for importlib.machinery.
FileFinder and zipimport.
This is a wrapper for the loader get_data API.
The package argument should be the name of a package, in
standard module format (foo.bar).
The resource argument should be in the form of a relative filename, using /
as the path separator.
The parent directory name .. is not allowed, and nor is a rooted name (starting with a /).
The function returns a binary string that is the contents of the specified resource.
For packages located in the filesystem, which have already been imported, this is the rough equivalent of:
31.2.
If the package cannot be located or loaded, or it uses a loader which does not support get_data, then None is
returned.
In particular, the loader for namespace packages does not support get_data.
pkgutil.resolve_name(name)
Resolve a name to an object.
This functionality is used in numerous places in the standard library (see bpo-12915) - and equivalent functionality
is also in widely used third-party packages such as setuptools, Django and Pyramid.
It is expected that name will be a string in one of the following formats, where W is shorthand for a valid Python
identifier and dot stands for a literal period in these pseudo-regexes:
• W(.W)*
• W(.W)*:(W(.W)*)?
The first form is intended for backward compatibility only.
It assumes that some part of the dotted name is a
package, and the rest is an object somewhere within that package, possibly nested inside other objects.
Because the
place where the package stops and the object hierarchy starts can’t be inferred by inspection, repeated attempts to
import must be done with this form.
In the second form, the caller makes the division point clear through the provision of a single colon: the dotted
name to the left of the colon is a package to be imported, and the dotted name to the right is the object hierarchy
within that package.
Only one import is needed in this form.
If it ends with the colon, then a module object is
returned.
ValueError – if name isn’t in a recognised format.
ImportError – if an import failed when it shouldn’t have.
AttributeError – If a failure occurred when traversing the object hierarchy within the imported package to
get to the desired object.
New in version 3.9.
Record that the package named pkg_name can be found in the specified path.
Allows specifying that the module named oldname is in fact the package named newname.
This class provides run_script() and report() methods to determine the set of modules imported by a
script.
See Example usage of ModuleFinder.
Sample output (may vary depending on the architecture):
Loaded modules:
_types:
copyreg:
_inverted_registry,_slotnames,__all__
re._compiler:
isstring,_sre,_optimize_unicode
_sre:
(continues on next page)
31.3.
Modules not imported:
guido.python.ham
baconhameggs
31.4 runpy — Locating and executing Python modules
Source code: Lib/runpy.py
The runpy module is used to locate and run Python modules without importing them first.
Its main use is to imple-
ment the -m command line switch that allows scripts to be located using the Python module namespace rather than the
filesystem.
Note that this is not a sandbox module - all code is executed in the current process, and any side effects (such as cached
imports of other modules) will remain in place after the functions have returned.
Furthermore, any functions and classes defined by the executed code are not guaranteed to work correctly after a runpy
function has returned.
If that limitation is not acceptable for a given use case, importlib is likely to be a more suitable
choice than this module.
The runpy module provides two functions:
runpy.run_module(mod_name, init_globals=None, run_name=None, alter_sys=False)
Execute the code of the specified module and return the resulting module globals dictionary.
The module’s code
is first located using the standard import mechanism (refer to PEP 302 for details) and then executed in a fresh
module namespace.
The mod_name argument should be an absolute module name.
The optional dictionary argument init_globals may be used to pre-populate the module’s globals dictionary before
the code is executed.
The supplied dictionary will not be modified.
If any of the special global variables below are
defined in the supplied dictionary, those definitions are overridden by run_module().
Both sys.argv[0] and sys.modules[__name__] are restored to their original values before
the function returns.
Note that this manipulation of sys is not thread-safe.
Other threads may see the partially initialised module, as
well as the altered list of arguments.
It is recommended that the sys module be left alone when invoking this
function from threaded code.
See also:
The -m option offering equivalent functionality from the command line.
Changed in version 3.4: Updated to take advantage of the module spec feature added by PEP 451.
This allows
__cached__ to be set correctly for modules run this way, as well as ensuring the real module name is always
accessible as __spec__.name.
See ModuleSpec for alternatives.
As with a
script name supplied to the CPython command line, the supplied path may refer to a Python source file, a compiled
bytecode file or a valid sys.path entry containing a __main__ module (e.g. a zipfile containing a top-level
__main__.py file).
For a simple script, the specified code is simply executed in a fresh module namespace.
For a valid sys.path
entry (typically a zipfile or directory), the entry is first added to the beginning of sys.path.
The function then
looks for and executes a __main__ module using the updated path.
Note that there is no special protection against
invoking an existing __main__ entry located elsewhere on sys.path if there is no such module at the specified
location.
The optional dictionary argument init_globals may be used to pre-populate the module’s globals dictionary before
the code is executed.
The supplied dictionary will not be modified.
If any of the special global variables below are
defined in the supplied dictionary, those definitions are overridden by run_path().
If the supplied path directly references a script file (whether as source or as precompiled byte code), then
__file__ will be set to the supplied path, and __spec__, __cached__, __loader__ and __package__
will all be set to None.
If the supplied path is a reference to a valid sys.path entry, then __spec__ will be set appropriately for the im-
ported __main__ module (that is, __spec__.name will always be __main__).
A number of alterations are also made to the sys module.
Firstly, sys.path may be altered as described above.
All modifications to items in sys are reverted before
the function returns.
Note that, unlike run_module(), the alterations made to sys are not optional in this function as these adjust-
ments are essential to allowing the execution of sys.path entries.
As the thread-safety limitations still apply,
31.4.
See also:
using-on-interface-options for equivalent functionality on the command line (python path/to/script).
New in version 3.2.
Changed in version 3.4: Updated to take advantage of the module spec feature added by PEP 451.
This allows
__cached__ to be set correctly in the case where __main__ is imported from a valid sys.path entry rather
than being executed directly.
See also:
PEP 338 – Executing modules as scripts PEP written and implemented by Nick Coghlan.
PEP 366 – Main module explicit relative imports PEP written and implemented by Nick Coghlan.
PEP 451 – A ModuleSpec Type for the Import System PEP written and implemented by Eric Snow
using-on-general - CPython command line details
The importlib.import_module() function
31.5 importlib — The implementation of import
New in version 3.1.
Source code: Lib/importlib/__init__.py
31.5.1 Introduction
The purpose of the importlib package is three-fold.
One is to provide the implementation of the import statement (and thus, by extension, the __import__() function)
in Python source code.
This provides an implementation of import which is portable to any Python interpreter.
This
also provides an implementation which is easier to comprehend than one implemented in a programming language other
than Python.
Two, the components to implement import are exposed in this package, making it easier for users to create their own
custom objects (known generically as an importer) to participate in the import process.
See also:
import The language reference for the import statement.
Packages specification Original specification of packages.
Some semantics have changed since the writing of this doc-
ument (e.g. redirecting based on None in sys.modules).
The __import__() function The import statement is syntactic sugar for this function.
An implementation of the built-in __import__() function.
Note: Programmatic importing of modules should use import_module() instead of this function.
importlib.import_module(name, package=None)
Import a module.
The name argument specifies what module to import in absolute or relative terms (e.g. either
pkg.mod or ..mod).
If the name is specified in relative terms, then the package argument must be set to the name
of the package which is to act as the anchor for resolving the package name (e.g. import_module('..mod',
'pkg.subpkg') will import pkg.mod).
The import_module() function acts as a simplifying wrapper around importlib.__import__().
This
means all semantics of the function are derived from importlib.__import__().
The most important dif-
ference between these two functions is that import_module() returns the specified package or module (e.g.
pkg.mod), while __import__() returns the top-level package or module (e.g. pkg).
If you are dynamically importing a module that was created since the interpreter began execution (e.g., created a
Python source file), you may need to call invalidate_caches() in order for the new module to be noticed
by the import system.
Changed in version 3.3: Parent packages are automatically imported.
If
a
finder
implements
invalidate_caches() then it will be called to perform the invalidation.
This function should be
called if any modules are created/installed while your program is running to guarantee all finders will notice the
new module’s existence.
New in version 3.3.
Namespace packages created/installed in a different sys.path location after the same
namespace was already imported are noticed.
Reload a previously imported module.
The argument must be a module object, so it must have been successfully
imported before.
This is useful if you have edited the module source file using an external editor and want to try
out the new version without leaving the Python interpreter.
The return value is the module object (which can be
different if re-importing causes a different object to be placed in sys.modules).
When reload() is executed:
• Python module’s code is recompiled and the module-level code re-executed, defining a new set of objects
which are bound to names in the module’s dictionary by reusing the loader which originally loaded the module.
The init function of extension modules is not called a second time.
As with all other objects in Python the old objects are only reclaimed after their reference counts drop to zero.
The names in the module namespace are updated to point to any new or changed objects.
There are a number of other caveats:
When a module is reloaded, its dictionary (containing the module’s global variables) is retained.
Redefinitions of
names will override the old definitions, so this is generally not a problem.
If the new version of a module does
not define a name that was defined by the old version, the old definition remains.
This feature can be used to the
module’s advantage if it maintains a global table or cache of objects — with a try statement it can test for the
table’s presence and skip its initialization if desired:
try:
cache
except NameError:
cache = {}
It is generally not very useful to reload built-in or dynamically loaded modules.
Reloading sys, __main__,
builtins and other key modules is not recommended.
In many cases extension modules are not designed to be
initialized more than once, and may fail in arbitrary ways when reloaded.
If a module imports objects from another module using from … import …, calling reload() for the other
module does not redefine the objects imported from it — one way around this is to re-execute the from statement,
another is to use import and qualified names (module.name) instead.
If a module instantiates instances of a class, reloading the module that defines the class does not affect the method
definitions of the instances — they continue to use the old class definition.
The same is true for derived classes.
New in version 3.4.
Changed in version 3.7:
ModuleNotFoundError is raised when the module being reloaded lacks a
ModuleSpec.
Some subclasses of the
core abstract base classes are also provided to help in implementing the core ABCs.
New in version 3.3.
Changed in version 3.10: No longer a subclass of Finder.
If this is a top-level import, path will be
None.
Otherwise, this is a search for a subpackage or module and path will be the value of __path__ from
the parent package.
If a spec cannot be found, None is returned.
When passed in, target is a module
object that the finder may use to make a more educated guess about what spec to return.
New in version 3.4.
Used
by importlib.invalidate_caches() when invalidating the caches of all finders on sys.
meta_path.
Returns None when called instead of NotImplemented.
An abstract base class representing a path entry finder.
Though it bears some similarities to MetaPathFinder,
PathEntryFinder is meant for use only within the path-based import subsystem provided by importlib.
machinery.
PathFinder.
New in version 3.3.
An abstract method for finding a spec for the specified module.
The finder will search for the module
only within the path entry to which it is assigned.
If a spec cannot be found, None is returned.
When
passed in, target is a module object that the finder may use to make a more educated guess about what
spec to return.
New in version 3.4.
Used by
importlib.machinery.
PathFinder.invalidate_caches() when invalidating the caches of
all cached finders.
Loader
An abstract base class for a loader.
See PEP 302 for the exact definition for a loader.
ResourceReader.
A method that returns the module object to use when importing a module.
This method may return None,
indicating that default module creation semantics should take place.
New in version 3.4.
This method is no longer optional when exec_module() is defined.
An abstract method that executes the module in its own namespace when a module is imported or reloaded.
The module should already be initialized when exec_module() is called.
When this method exists,
create_module() must be defined.
New in version 3.4.
Changed in version 3.6: create_module() must also be defined.
A legacy method for loading a module.
If the module cannot be loaded, ImportError is raised, otherwise
the loaded module is returned.
If the requested module already exists in sys.modules, that module should be used and reloaded.
Oth-
erwise the loader should create a new module and insert it into sys.modules before any loading begins,
to prevent recursion from the import.
If the loader inserted a module and the load fails, it must be removed
by the loader from sys.modules; modules already in sys.modules before the loader began execution
should be left alone.
The module’s fully qualified name.
It is '__main__' for an executed module.
The location the loader used to load the module.
For example, for modules loaded from a
.py file this is the filename.
It is not set on all modules (e.g. built-in modules).
The filename of a compiled version of the module’s code.
It is not set on all modules
(e.g. built-in modules).
The list of locations where the package’s submodules will be found.
Most of the time this
is a single directory.
The import system passes this attribute to __import__() and to finders in
the same way as sys.path but just for the package.
It is not set on non-package modules so it can
be used as an indicator that the module is a package.
The fully qualified name of the package the module is in (or the empty string for a
top-level module).
If the module is a package then this is the same as __name__.
The loader used to load the module.
When exec_module() is available then backwards-compatible functionality is provided.
Raise ImportError when called instead of NotImplementedError.
Func-
tionality provided when exec_module() is available.
The recommended API for loading a module is exec_module() (and
create_module()).
Loaders should implement it instead of load_module().
ResourceLoader
An abstract base class for a loader which implements the optional PEP 302 protocol for loading arbitrary resources
from the storage back-end.
This ABC is deprecated in favour of supporting resource loading through
importlib.resources.abc.
ResourceReader.
An abstract method to return the bytes for the data located at path.
Loaders that have a file-like storage
back-end that allows storing arbitrary data can implement this abstract method to give direct access to the
data stored.
OSError is to be raised if the path cannot be found.
The path is expected to be constructed
using a module’s __file__ attribute or an item from a package’s __path__.
Raises OSError instead of NotImplementedError.
InspectLoader
An abstract base class for a loader which implements the optional PEP 302 protocol for loaders that inspect
modules.
get_code(fullname)
Return the code object for a module, or None if the module does not have a code object (as would be the case,
for example, for a built-in module).
Raise an ImportError if loader cannot find the requested module.
Note:
While the method has a default implementation, it is suggested that it be overridden if possible for
performance.
No longer abstract and a concrete implementation is provided.
An abstract method to return the source of a module.
It is returned as a text string using universal newlines,
translating all recognized line separators into '\n' characters.
Returns None if no source is available (e.g.
a built-in module).
Raises ImportError if the loader cannot find the module specified.
Raises ImportError instead of NotImplementedError.
An optional method to return a true value if the module is a package, a false value otherwise.
ImportError
is raised if the loader cannot find the module.
Raises ImportError instead of NotImplementedError.
The data argument can be whatever the compile() function supports (i.e. string or bytes).
The path
argument should be the “path” to where the source code originated from, which can be an abstract concept
(e.g. location in a zip file).
With the subsequent code object one can execute it in a module by running exec(code, module.
__dict__).
New in version 3.4.
Changed in version 3.5: Made the method static.
New in version 3.4.
Deprecated since version 3.4: use exec_module() instead.
ExecutionLoader
An abstract base class which inherits from InspectLoader that, when implemented, helps a module to be
executed as a script.
The ABC represents an optional PEP 302 protocol.
An abstract method that is to return the value of __file__ for the specified module.
If no path is available,
ImportError is raised.
If source code is available, then the method should return the path to the source file, regardless of whether a
bytecode was used to load the module.
Raises ImportError instead of NotImplementedError.
An abstract base class which inherits from ResourceLoader and ExecutionLoader, providing concrete
implementations of ResourceLoader.get_data() and ExecutionLoader.get_filename().
The fullname argument is a fully resolved name of the module the loader is to handle.
The path argument is the
path to the file for the module.
New in version 3.3.
Calls super’s load_module().
Deprecated since version 3.4: Use Loader.exec_module() instead.
Reads path as a binary file and returns the bytes from it.
SourceLoader
An abstract base class for implementing source (and optionally bytecode) file loading.
The class inherits from both
ResourceLoader and ExecutionLoader, requiring the implementation of:
• ResourceLoader.get_data()
• ExecutionLoader.get_filename() Should only return the path to the source file; sourceless load-
ing is not supported.
The abstract methods defined by this class are to add optional bytecode file support.
Not implementing these
optional methods (or causing them to raise NotImplementedError) causes the loader to only work with
source code.
Implementing the methods allows the loader to work with source and bytecode files; it does not allow
for sourceless loading where only bytecode is provided.
Bytecode files are an optimization to speed up loading by
removing the parsing step of Python’s compiler, and so no bytecode-specific API is exposed.
Optional abstract method which returns a dict containing metadata about the specified path.
Supported
dictionary keys are:
• 'mtime' (mandatory): an integer or floating-point number representing the modification time of the
source code;
• 'size' (optional): the size in bytes of the source code.
Any other keys in the dictionary are ignored, to allow for future extensions.
If the path cannot be handled,
OSError is raised.
New in version 3.3.
Optional abstract method which returns the modification time for the specified path.
This method is deprecated in favour of path_stats().
You don’t have
to implement it, but it is still available for compatibility purposes.
Raise OSError instead of NotImplementedError.
Optional abstract method which writes the specified bytes to a file path.
Any intermediate directories which
do not exist are to be created automatically.
When writing to the path fails because the path is read-only (errno.
EACCES/PermissionError), do
not propagate the exception.
No longer raises NotImplementedError when called.
Concrete implementation of InspectLoader.get_code().
New in version 3.4.
Deprecated since version 3.4: Use exec_module() instead.
Concrete implementation of InspectLoader.get_source().
A module is determined to be a pack-
age if its file path (as provided by ExecutionLoader.get_filename()) is a file named __init__
when the file extension is removed and the module name itself does not end in __init__.
ResourceReader
Superseded by TraversableResources
An abstract base class to provide the ability to read resources.
From the perspective of this ABC, a resource is a binary artifact that is shipped within a package.
Typically this
is something like a data file that lives next to the __init__.py file of the package.
The purpose of this class is
31.5.
For any of methods of this class, a resource argument is expected to be a path-like object which represents concep-
tually just a file name.
This means that no subdirectory paths should be included in the resource argument.
This is
because the location of the package the reader is for, acts as the “directory”.
Hence the metaphor for directories
and file names is packages and resources, respectively.
This is also why instances of this class are expected to
directly correlate to a specific package (instead of potentially representing multiple packages or a module).
Loaders
that
wish
to
support
resource
reading
are
expected
to
provide
a
method
called
get_resource_reader(fullname) which returns an object implementing this ABC’s interface.
If
the module specified by fullname is not a package, this method should return None.
An object compatible with
this ABC should only be returned when the specified module is a package.
New in version 3.7.
Deprecated since version 3.12, will be removed in version 3.14:
Use importlib.resources.abc.
TraversableResources instead.
If the resource cannot be found, FileNotFoundError is raised.
If the resource does not concretely exist on the file system, raise FileNotFoundError.
Returns True if the named name is considered a resource.
FileNotFoundError is raised if name does
not exist.
Do note that it is not required that all names
returned by the iterator be actual resources, e.g. it is acceptable to return names for which is_resource()
would be false.
Allowing non-resource names to be returned is to allow for situations where how a package and its resources
are stored are known a priori and the non-resource names would be useful.
For instance, returning subdirec-
tory names is allowed so that when it is known that the package and resources are stored on the file system
then those subdirectory names can be used directly.
The abstract method returns an iterable of no items.
Traversable
An object with a subset of pathlib.
Path methods suitable for traversing directories and opening files.
For a representation of the object on the file-system, use importlib.resources.as_file().
New in version 3.9.
Deprecated since version 3.12, will be removed in version 3.14:
Use importlib.resources.abc.
Traversable instead.
The base name of this object without any parent references.
Return Traversable child in self.
Return a handle suitable for reading (same as pathlib.
Path.open).
When opening as text, accepts encoding parameters such as those accepted by io.
TextIOWrapper.
read_bytes()
Read contents of self as bytes.
TraversableResources
An abstract base class for resource readers capable of serving the importlib.resources.files() in-
terface.
Subclasses importlib.resources.abc.
ResourceReader and provides concrete implementa-
tions of the importlib.resources.abc.
ResourceReader’s abstract methods.
Therefore, any loader
supplying importlib.abc.
TraversableResources also supplies ResourceReader.
Loaders that wish to support resource reading are expected to implement this interface.
New in version 3.9.
Deprecated since version 3.12, will be removed in version 3.14:
Use importlib.resources.abc.
TraversableResources instead.
Traversable object for the loaded package.
A list of strings representing the recognized file suffixes for source modules.
New in version 3.3.
New in version 3.3.
Deprecated since version 3.5: Use BYTECODE_SUFFIXES instead.
A list of strings representing the file suffixes for optimized bytecode modules.
New in version 3.3.
A list of strings representing the recognized file suffixes for bytecode modules (including the leading dot).
New in version 3.3.
A list of strings representing the recognized file suffixes for extension modules.
New in version 3.3.
This is a helper for code which simply needs to know if a filesystem path potentially refers to a module
without needing any details on the kind of module (for example, inspect.getmodulename()).
New in version 3.3.
BuiltinImporter
An importer for built-in modules.
All known built-in modules are listed in sys.builtin_module_names.
This
class
implements
the
importlib.abc.
MetaPathFinder
and
importlib.abc.
InspectLoader ABCs.
Only class methods are defined by this class to alleviate the need for instantiation.
As part of PEP 489,
the builtin importer now implements Loader.
FrozenImporter
An importer for frozen modules.
This class implements the importlib.abc.
MetaPathFinder and
importlib.abc.
InspectLoader ABCs.
Only class methods are defined by this class to alleviate the need for instantiation.
Changed in version 3.4: Gained create_module() and exec_module() methods.
WindowsRegistryFinder
Finder for modules declared in the Windows registry.
This class implements the importlib.abc.
ABC.
Only class methods are defined by this class to alleviate the need for instantiation.
New in version 3.3.
Deprecated since version 3.6: Use site configuration instead.
Future versions of Python may not enable this
finder by default.
PathFinder
A Finder for sys.path and package __path__ attributes.
This class implements the importlib.abc.
ABC.
Only class methods are defined by this class to alleviate the need for instantiation.
For each path entry that is searched, sys.path_importer_cache is checked.
If a non-false
object is found then it is used as the path entry finder to look for the module being searched for.
If no entry
is found in sys.path_importer_cache, then sys.path_hooks is searched for a finder for the
path entry and, if found, is stored in sys.path_importer_cache along with being queried about the
module.
If no finder is ever found then None is both stored in the cache and returned.
New in version 3.4.
If the current working directory – represented by an empty string – is no longer valid
then None is returned but no value is cached in sys.path_importer_cache.
PathEntryFinder.invalidate_caches()
on
all
finders
stored in sys.path_importer_cache that define the method.
Otherwise entries in sys.
Changed in version 3.7: Entries of None in sys.path_importer_cache are deleted.
Calls objects in sys.path_hooks with the current working directory for '' (i.e. the
empty string).
A concrete implementation of importlib.abc.PathEntryFinder which caches results from the file sys-
tem.
The path argument is the directory for which the finder is in charge of searching.
The loader_details argument is a variable number of 2-item tuples each containing a loader and a sequence of file
suffixes the loader recognizes.
The loaders are expected to be callables which accept two arguments of the module’s
name and the path to the file found.
The finder will cache the directory contents as necessary, making stat calls for each module search to verify the cache
is not outdated.
Because cache staleness relies upon the granularity of the operating system’s state information of
the file system, there is a potential race condition of searching for a module, creating a new file, and then searching
for the module the new file represents.
If the operations happen fast enough to fit within the granularity of stat calls,
then the module search will fail.
To prevent this from happening, when you create a module dynamically, make
sure to call importlib.invalidate_caches().
New in version 3.3.
New in version 3.4.
A class method which returns a closure for use on sys.path_hooks.
An instance of FileFinder is
returned by the closure using the path argument given to the closure directly and loader_details indirectly.
If the argument to the closure is not an existing directory, ImportError is raised.
A concrete implementation of importlib.abc.
SourceLoader by subclassing importlib.abc.
FileLoader and providing some concrete implementations of other methods.
New in version 3.3.
Deprecated since version 3.6: Use importlib.abc.Loader.exec_module() instead.
A concrete implementation of importlib.abc.
FileLoader which can import bytecode files (i.e. no source
code files exist).
Please note that direct use of bytecode files (and thus not source code files) inhibits your modules from being usable
by all Python implementations or new versions of Python which change the bytecode format.
New in version 3.3.
Returns the code object for name created from path.
Returns None as bytecode files have no source when this loader is used.
Deprecated since version 3.6: Use importlib.abc.Loader.exec_module() instead.
A concrete implementation of importlib.abc.
ExecutionLoader for extension modules.
The fullname argument specifies the name of the module the loader is to support.
The path argument is the path
to the extension module’s file.
Note that, by default, importing an extension module will fail in subinterpreters if it doesn’t implement multi-phase
init (see PEP 489), even if it would otherwise import successfully.
New in version 3.3.
Changed in version 3.12: Multi-phase init is now required for use in subinterpreters.
New in version 3.5.
exec_module(module)
Initializes the given module object in accordance with PEP 489.
New in version 3.5.
Returns None as extension modules do not have source code.
New in version 3.4.
NamespaceLoader(name, path, path_finder):
A concrete implementation of importlib.abc.
InspectLoader for namespace packages.
This is an alias
for a private class and is only made public for introspecting the __loader__ attribute on namespace packages:
True
True
New in version 3.11.
ModuleSpec(name, loader, *, origin=None, loader_state=None,
is_package=None)
31.5.
A specification for a module’s import-system-related state.
This is typically exposed as the module’s __spec__
attribute.
In the descriptions below, the names in parentheses give the corresponding attribute available directly on
the module object, e.g. module.__spec__.origin == module.__file__.
Note, however, that while
the values are usually equivalent, they can differ since there is no synchronization between the two objects.
For
example, it is possible to update the module’s __file__ at runtime and this will not be automatically reflected in
the module’s __spec__.origin, and vice versa.
The module’s fully qualified name.
The finder should always set this attribute to a non-empty string.
The loader used to load the module.
The location the loader should use to load the module.
For example, for modules loaded from a .py file this is the
filename.
The finder should always set this attribute to a meaningful value for the loader to use.
In the uncommon
case that there is not one (like for namespace packages), it should be set to None.
The list of locations where the package’s submodules will be found.
Most of the time this is a single directory.
The finder should set this attribute to a list, even an empty one, to indicate to the import system that the module is
a package.
It should be set to None for non-package modules.
It is set automatically later to a special object for
namespace packages.
The filename of a compiled version of the module’s code.
The finder should always set this attribute but it may be
None for modules that do not need compiled code stored.
If the module is a package then this is the same as name.
This value impacts how origin
is interpreted and how the module’s __file__ is populated.
The bytes which represent the bytecode version number.
If you need help with loading/writing bytecode then
consider importlib.abc.
SourceLoader.
New in version 3.4.
For example, if
path is /foo/bar/baz.py the return value would be /foo/bar/__pycache__/baz.cpython-32.
pyc for Python 3.2.
The cpython-32 string comes from the current magic tag (see get_tag(); if sys.
The optimization parameter is used to specify the optimization level of the bytecode file.
An empty string represents
no optimization, so /foo/bar/baz.py with an optimization of '' will result in a bytecode path of /foo/
bar/__pycache__/baz.cpython-32.pyc.
None causes the interpreter’s optimization level to be used.
Any other value’s string representation is used, so /foo/bar/baz.py with an optimization of 2 will lead to the
bytecode path of /foo/bar/__pycache__/baz.cpython-32.opt-2.pyc.
The string representation
of optimization can only be alphanumeric, else ValueError is raised.
The debug_override parameter is deprecated and can be used to override the system’s value for __debug__.
A
True value is the equivalent of setting optimization to the empty string.
A False value is the same as setting
optimization to 1.
If both debug_override an optimization are not None then TypeError is raised.
New in version 3.4.
The optimization parameter was added and the debug_override parameter was deprecated.
Given the path to a PEP 3147 file name, return the associated source code file path.
For example, if path is /foo/
bar/__pycache__/baz.cpython-32.pyc the returned path would be /foo/bar/baz.py.
If sys.
implementation.cache_tag is not defined, NotImplementedError is raised.
New in version 3.4.
Decode the given bytes representing source code and return it as a string with universal newlines (as required by
importlib.abc.InspectLoader.get_source()).
New in version 3.4.
If name has no leading dots, then name is simply returned.
This allows for usage such as importlib.util.
resolve_name('sys', __spec__.parent) without doing a check to see if the package argument is
needed.
ImportError is also raised if a relative name would escape its containing package (e.g. requesting
..bacon from within the spam package).
New in version 3.3.
Changed in version 3.9: To improve consistency with import statements, raise ImportError instead of
ValueError for invalid relative import attempts.
If the module is in sys.modules,
then sys.modules[name].__spec__ is returned (unless the spec would be None or is not set, in which
case ValueError is raised).
Otherwise a search using sys.meta_path is done.
None is returned if no spec
is found.
If name is for a submodule (contains a dot), the parent module is automatically imported.
New in version 3.4.
Raises ModuleNotFoundError instead of AttributeError if package is in fact
not a package (i.e. lacks a __path__ attribute).
If spec.loader.create_module does not return None, then any pre-existing attributes will not be reset.
Also, no AttributeError will be raised if triggered while accessing spec or setting an attribute on the module.
This function is preferred over using types.
ModuleType to create a new module as spec is used to set as many
import-controlled attributes on the module as possible.
New in version 3.5.
A factory function for creating a ModuleSpec instance based on a loader.
The parameters have the same
meaning as they do for ModuleSpec.
The function uses available loader APIs, such as InspectLoader.
is_package(), to fill in any missing information on the spec.
A factory function for creating a ModuleSpec instance based on the path to a file.
Missing information will be
filled in on the spec by making use of loader APIs and by the implication that the module will be file-based.
New in version 3.4.
Accepts a path-like object.
importlib.util.source_hash(source_bytes)
Return the hash of source_bytes as bytes.
A hash-based .pyc file embeds the source_hash() of the corre-
sponding source file’s contents in its header.
New in version 3.7.
By default the check
is enabled and will fail when a single-phase init module is imported in a subinterpreter.
There’s
is a pretty good chance this is not what you were looking for.
You can get the same effect as this function by implementing the basic interface of multi-phase init (PEP 489) and
lying about support for multiple interpreters (or per-interpreter GIL).
Warning: Using this function to disable the check can lead to unexpected behavior and even crashes.
It should
only be used during extension module development.
New in version 3.12.
A class which postpones the execution of the loader of a module until the module has an attribute accessed.
This class only works with loaders that define exec_module() as control over what module type is used for
the module is required.
Finally, modules which
substitute the object placed into sys.modules will not work as there is no way to properly replace the module
references throughout the interpreter safely; ValueError is raised if such a substitution is detected.
Note: For projects where startup time is critical, this class allows for potentially minimizing the cost of loading
a module if it is never used.
For projects where startup time is not essential then use of this class is heavily
discouraged due to error messages created during loading being postponed and thus occurring out of context.
New in version 3.5.
Changed
in
version
3.6:
Began
calling
create_module(),
removing
the
compatibility
warn-
ing
for
importlib.machinery.
A class method which returns a callable that creates a lazy loader.
This is meant to be used in situations where
the loader is passed by class instead of by instance.
SourceFileLoader
lazy_loader = importlib.util.
LazyLoader.factory(loader)
finder = importlib.machinery.
Examples
Importing programmatically
To programmatically import a module, use importlib.import_module().
Note that if name is a submodule (contains a dot), importlib.util.find_spec() will import the parent module.
Importing a source file directly
To import a Python source file directly, use the following recipe:
import importlib.util
import sys
# For illustrative purposes.
False
Setting up an importer
For deep customizations of import, you typically want to implement an importer.
This means managing both the finder
and loader side of things.
For finders there are two flavours to choose from depending on your needs: a meta path finder
or a path entry finder.
The former is what you would put on sys.meta_path while the latter is what you create using
a path entry hook on sys.path_hooks which works with sys.path entries to potentially create a finder.
This
example will show you how to register your own importers so that import will use them (for creating an importer for
yourself, read the documentation for the appropriate classes defined within this package):
import importlib.machinery
import sys
# For illustrative purposes only.
SpamMetaPathFinder = importlib.machinery.
PathFinder
SpamPathEntryFinder = importlib.machinery.
FileFinder
loader_details = (importlib.machinery.SourceFileLoader,
importlib.machinery.SOURCE_SUFFIXES)
# Setting up a meta path finder.
Approximating importlib.import_module()
Import itself is implemented in Python code, making it possible to expose most of the import machinery through importlib.
Lib/importlib/resources/__init__.py
New in version 3.7.
This module leverages Python’s import system to provide access to resources within packages.
The resources may be contained
directly in a package, within a subdirectory contained in that package, or adjacent to modules outside a package.
Resources
may be text or binary.
As a result, Python module sources (.py) of a package and compilation artifacts (pycache) are
technically de-facto resources of that package.
In practice, however, resources are primarily those non-Python artifacts
exposed specifically by the package author.
Resources can be opened or read in either binary or text mode.
Resources are roughly akin to files inside directories, though it’s important to keep in mind that this is just a metaphor.
Resources and packages do not have to exist as physical files and directories on the file system: for example, a package
and its resources can be imported from a zip file using zipimport.
Note:
This module provides functionality similar to pkg_resources Basic Resource Access without the performance
overhead of that package.
This makes reading resources included in packages easier, with more stable and consistent
semantics.
The standalone backport of this module provides more information on using importlib.resources and migrating from
pkg_resources to importlib.resources.
Loaders that wish to support resource reading should implement a get_resource_reader(fullname) method
as specified by importlib.resources.abc.
ResourceReader.
Anchor
Represents an anchor for resources, either a module object or a module name as a string.
A Traversable may contain other containers (think subdirectories).
If the anchor is a package, resources are resolved from that package.
If a module,
resources are resolved adjacent to that module (in the same package or the package root).
If the anchor is omitted,
the caller’s module is used.
New in version 3.9.
Changed in version 3.12: “package” parameter was renamed to “anchor”.
Consider passing the anchor positionally or using importlib_resources >=
5.10 for a compatible interface on older Pythons.
Given a Traversable object representing a file or directory, typically from importlib.resources.
files(), return a context manager for use in a with statement.
The context manager provides a pathlib.
Path object.
Exiting the context manager cleans up any temporary file or directory created when the resource was extracted from
e.g. a zip file.
Use as_file when the Traversable methods (read_text, etc) are insufficient and an actual file or directory
on the file system is required.
New in version 3.9.
Changed in version 3.12: Added support for traversable representing a directory.
The
main drawback of these functions is that they do not support directories: they assume all resources are located directly
within a package.
importlib.resources.
Whenever a function accepts a Package argument, you can pass in either a module object
or a module name as a string.
You can only pass module objects whose __spec__.
The Package type is defined as Union[str, ModuleType].
Deprecated since version 3.12.
importlib.resources.
Resource
For resource arguments of the functions below, you can pass in the name of a resource as a string or a path-like
object.
The Resource type is defined as Union[str, os.
PathLike].
This function returns a typing.
BinaryIO instance, a binary I/O stream open for
reading.
Deprecated since version 3.11: Calls to this function can be replaced by:
31.6.
By default, the resource is opened for reading as UTF-8.
package is either a name or a module object which conforms to the Package requirements.
This function returns a typing.
O stream open for reading.
Deprecated since version 3.11: Calls to this function can be replaced by:
files(package).joinpath(resource).open('r', encoding=encoding)
importlib.resources.read_binary(package, resource)
Read and return the contents of the resource within package as bytes.
This function returns the contents of the resource as bytes.
Deprecated since version 3.11: Calls to this function can be replaced by:
files(package).joinpath(resource).read_bytes()
importlib.resources.read_text(package, resource, encoding=’utf-8’, errors=’strict’)
Read and return the contents of resource within package as a str.
By default, the contents are read as strict UTF-8.
This function returns
the contents of the resource as str.
Deprecated since version 3.11: Calls to this function can be replaced by:
files(package).joinpath(resource).read_text(encoding=encoding)
importlib.resources.path(package, resource)
Return the path to the resource as an actual file system path.
This function returns a context manager for use in a
with statement.
The context manager provides a pathlib.
Path object.
Exiting the context manager cleans up any temporary file created when the resource needs to be extracted from e.g.
a zip file.
Deprecated since version 3.11: Calls to this function can be replaced using as_file():
as_file(files(package).joinpath(resource))
importlib.resources.is_resource(package, name)
Return True if there is a resource named name in the package, otherwise False.
This function does not con-
sider directories to be resources.
The iterable returns str resources (e.g. files) and
non-resources (e.g. directories).
The iterable does not recurse into subdirectories.
ResourceReader
Superseded by TraversableResources
An abstract base class to provide the ability to read resources.
From the perspective of this ABC, a resource is a binary artifact that is shipped within a package.
Typically this
is something like a data file that lives next to the __init__.py file of the package.
The purpose of this class is
to help abstract out the accessing of such data files so that it does not matter if the package and its data file(s) are
stored in a e.g. zip file versus on the file system.
For any of methods of this class, a resource argument is expected to be a path-like object which represents concep-
tually just a file name.
This means that no subdirectory paths should be included in the resource argument.
This is
because the location of the package the reader is for, acts as the “directory”.
Hence the metaphor for directories
and file names is packages and resources, respectively.
This is also why instances of this class are expected to
directly correlate to a specific package (instead of potentially representing multiple packages or a module).
Loaders
that
wish
to
support
resource
reading
are
expected
to
provide
a
method
called
get_resource_reader(fullname) which returns an object implementing this ABC’s interface.
If
the module specified by fullname is not a package, this method should return None.
An object compatible with
this ABC should only be returned when the specified module is a package.
Deprecated since version 3.12, will be removed in version 3.14:
Use importlib.resources.abc.
TraversableResources instead.
If the resource cannot be found, FileNotFoundError is raised.
If the resource does not concretely exist on the file system, raise FileNotFoundError.
FileNotFoundError is raised if name does
not exist.
Do note that it is not required that all names
returned by the iterator be actual resources, e.g. it is acceptable to return names for which is_resource()
would be false.
Allowing non-resource names to be returned is to allow for situations where how a package and its resources
are stored are known a priori and the non-resource names would be useful.
For instance, returning subdirec-
tory names is allowed so that when it is known that the package and resources are stored on the file system
then those subdirectory names can be used directly.
The abstract method returns an iterable of no items.
Traversable
An object with a subset of pathlib.
Path methods suitable for traversing directories and opening files.
For a representation of the object on the file-system, use importlib.resources.as_file().
The base name of this object without any parent references.
Return Traversable child in self.
Return a handle suitable for reading (same as pathlib.
Path.open).
When opening as text, accepts encoding parameters such as those accepted by io.
TextIOWrapper.
read_bytes()
Read contents of self as bytes.
TraversableResources
An abstract base class for resource readers capable of serving the importlib.resources.files() inter-
face.
Subclasses ResourceReader and provides concrete implementations of the ResourceReader’s ab-
stract methods.
Therefore, any loader supplying TraversableResources also supplies ResourceReader.
Loaders that wish to support resource reading are expected to implement this interface.
Source code: Lib/importlib/metadata/__init__.py
importlib.metadata is a library that provides access to the metadata of an installed Distribution Package, such as
its entry points or its top-level names (Import Packages, modules, if any).
Built in part on Python’s import system, this
library intends to replace similar functionality in the entry point API and metadata API of pkg_resources.
Along with
importlib.resources, this package can eliminate the need to use the older and less efficient pkg_resources
package.
Specifically, it works with distributions with discoverable dist-info or egg-info
directories, and metadata defined by the Core metadata specifications.
Important: These are not necessarily equivalent to or correspond 1:1 with the top-level import package names that can
be imported inside Python code.
One distribution package can contain multiple import packages (and single modules),
and one top-level import package may map to multiple distribution packages if it is a namespace package.
You can use
package_distributions() to get a mapping between them.
By default, distribution metadata can live on the file system or in zip archives on sys.path.
Through an extension
mechanism, the metadata can live almost anywhere.
The documentation for importlib_metadata, which supplies a
backport of importlib.metadata.
This includes an API reference for this module’s classes and functions,
as well as a migration guide for existing users of pkg_resources.
You can also get a collection of entry points selectable by properties of the EntryPoint (typically ‘group’ or ‘name’), such as
console_scripts, distutils.commands and others.
Each group contains a collection of EntryPoint objects.
You can also get a distribution’s version number, list its constituent files, and get a list of the distribution’s Distribution
requirements.
The entry_points() function returns a collection of entry points.
Entry points are represented by EntryPoint
instances; each EntryPoint has a .name, .group, and .value attributes and a .load() method to resolve the
value.
There are also .module, .attr, and .extras attributes for getting the components of the .value attribute.
Read the setuptools docs for more information on entry points, their definition, and usage.
Compatibility Note
The “selectable” entry points were introduced in importlib_metadata 3.6 and Python 3.10.
Prior to those changes,
entry_points accepted no parameters and always returned a dictionary of entry points, keyed by group.
With
importlib_metadata 5.0 and Python 3.12, entry_points always returns an EntryPoints object.
See back-
ports.entry_points_selectable for compatibility options.
The keys of the returned data structure, a PackageMetadata, name the metadata keywords, and the values are returned
unparsed from the distribution metadata:
'>=2.7, !
The Description is now included in the metadata when presented through the payload.
Line continuation characters have been removed.
The json attribute was added.
Distribution versions
The version() function is the quickest way to get a Distribution Package’s version number, as a string:
'0.32.3'
31.8.
You can also get the full set of files contained within a distribution.
The files() function takes a Distribution Package
name and returns all of the files installed by this distribution.
Each file object returned is a PackagePath, a pathlib.
PurePath derived object with additional dist, size, and hash properties as indicated by the metadata.
For example:
PackagePath('wheel/util.py')

<importlib.metadata._hooks.
In the case where the metadata file listing files (RECORD or SOURCES.txt) is missing, files() will return None.
The caller may wish to wrap calls to files() in always_iterable or otherwise guard against this condition if the target
distribution is not known to have the metadata present.
Distribution requirements
To get the full set of requirements for a Distribution Package, use the requires() function:
["pytest (>=3.0.0) ; extra == 'test'", "pytest-cov ; extra == 'test'"]
Mapping import to distribution packages
A convenience method to resolve the Distribution Package name (or names, in the case of a namespace package) that
provide each importable top-level Python module or Import Package:
{'importlib_metadata': ['importlib-metadata'], 'yaml': ['PyYAML'], 'jaraco': ['jaraco.
�→classes', 'jaraco.functools'], ...}
Some editable installs, do not supply top-level names, and thus this function is not reliable with such installs.
A Distribution is an abstract object that represents the metadata for a Python Distribution
Package.
Thus, an alternative way to get the version number is through the Distribution instance:
'0.32.3'
There are all kinds of additional metadata available on the Distribution instance:
'>=2.7, !
See the Core metadata specifications for additional details.
This metadata finder search defaults to sys.path, but varies slightly in how it interprets those values from how
other import machinery does.
In particular:
• importlib.metadata does not honor bytes objects on sys.path.
Path objects on sys.path even though such
values will be ignored for imports.
Extending the search algorithm
Because Distribution Package metadata is not available through sys.path searches, or package loaders directly,
the metadata for a distribution is found through import system finders.
To find a distribution package’s metadata,
importlib.metadata queries the list of meta path finders on sys.meta_path.
By default importlib.metadata installs a finder for distribution packages found on the file system.
This finder
doesn’t actually find any distributions, but it can find their metadata.
The abstract class importlib.abc.
MetaPathFinder defines the interface expected of finders by Python’s import
system.
Context()):
"""Return an iterable of all Distribution instances capable of
loading the metadata for packages for the indicated ``context``.
Context object provides .path and .name properties indicating the path to search
and name to match and may supply other relevant context.
What this means in practice is that to support finding distribution package metadata in locations other than the file system,
subclass Distribution and implement the abstract methods.
Then from a custom finder, return instances of this
derived Distribution in the find_distributions() method.
The initialization of the sys.path module search path
A module search path is initialized when Python starts.
This module search path may be accessed at sys.path.
The first entry in the module search path is the directory that contains the input script, if there is one.
Otherwise, the first
entry is the current directory, which is the case when executing the interactive shell, a -c command, or -m module.
The PYTHONPATH environment variable is often used to add directories to the search path.
If this environment variable
is found then the contents are added to the module search path.
Note: PYTHONPATH will affect all installed Python versions/environments.
Be wary of setting this in your shell profile
or global environment variables.
The site module offers more nuanced techniques as mentioned below.
The next items added are the directories containing standard Python modules as well as any extension modules that these
modules depend on.
Extension modules are .pyd files on Windows and .so files on other platforms.
The directory
with the platform-independent Python modules is called prefix.
The directory with the extension modules is called
exec_prefix.
The PYTHONHOME environment variable may be used to set the prefix and exec_prefix locations.
Otherwise
these directories are found by using the Python executable as a starting point and then looking for various ‘landmark’ files
and directories.
Note that any symbolic links are followed so the real Python executable location is used as the search
starting point.
The Python executable location is called home.
Once
home
is
determined,
the
prefix
directory
is
found
by
first
looking
for
pythonmajorversionminorversion.zip (python311.zip).
On Windows the zip archive is searched for
in home and on Unix the archive is expected to be in lib.
Note that the expected zip archive location is added to
the module search path even if the archive does not exist.
If no archive was found, Python on Windows will continue
the search for prefix by looking for Lib\os.py.
Python on Unix will look for lib/pythonmajorversion.
On Windows prefix and exec_prefix are the same,
however on other platforms lib/pythonmajorversion.minorversion/lib-dynload (lib/python3.
On some platforms lib may be
lib64 or another value, see sys.platlibdir and PYTHONPLATLIBDIR.
Once found, prefix and exec_prefix are available at sys.prefix and sys.exec_prefix respectively.
Finally, the site module is processed and site-packages directories are added to the module search path.
A
common way to customize the search path is to create sitecustomize or usercustomize modules as described
in the site module documentation.
Note: Certain command line options may further affect path calculations.
Virtual environments
If Python is run in a virtual environment (as described at tut-venv) then prefix and exec_prefix are specific to the
virtual environment.
If home is an absolute path and PYTHONHOME is not set, this path is used instead of the path to the main executable
when deducing prefix and exec_prefix.
The shared library path is always known on Windows, however it may not
be available on other platforms.
In the ._pth file specify one line for each path to add to sys.path.
The file based on
the shared library name overrides the one based on the executable, which allows paths to be restricted for any program
loading the runtime if desired.
When the file exists, all registry and environment variables are ignored, isolated mode is enabled, and site is not imported
unless one line in the file specifies import site.
Blank paths and lines starting with # are ignored.
Each path may
be absolute or relative to the location of the file.
Import statements other than to site are not permitted, and arbitrary
code cannot be specified.
Note that .pth files (without leading underscore) will be processed normally by the site module when import site
has been specified.
The path specific details are described at init-path-config.
Alternatively the older
Py_SetPath() can be used to bypass the initialization of the module search path.
See also:
• windows_finding_modules for detailed Windows notes.
Importing Modules
CHAPTER
THIRTYTWO
PYTHON LANGUAGE SERVICES
Python provides a number of modules to assist in working with the Python language.
These modules support tokenizing,
parsing, syntax analysis, bytecode disassembly, and various other facilities.
These modules include:
32.1 ast — Abstract Syntax Trees
Source code: Lib/ast.py
The ast module helps Python applications to process trees of the Python abstract syntax grammar.
The abstract syntax
itself might change with each Python release; this module helps to find out programmatically what the current grammar
looks like.
An abstract syntax tree can be generated by passing ast.
PyCF_ONLY_AST as a flag to the compile() built-in
function, or using the parse() helper provided in this module.
The result will be a tree of objects whose classes all
inherit from ast.
AST.
An abstract syntax tree can be compiled into a Python code object using the built-in compile()
function.
BitAnd | FloorDiv
unaryop = Invert | Not | UAdd | USub
cmpop = Eq | NotEq | Lt | LtE | Gt | GtE | Is | IsNot | In | NotIn
comprehension = (expr target, expr iter, expr* ifs, int is_async)
excepthandler = ExceptHandler(expr?
AST
This is the base of all AST node classes.
The actual node classes are derived from the Parser/Python.asdl
file, which is reproduced above.
They are defined in the _ast C module and re-exported in ast.
There is one class defined for each left-hand side symbol in the abstract grammar (for example, ast.stmt or
ast.expr).
In addition, there is one class defined for each constructor on the right-hand side; these classes inherit
from the classes for the left-hand side trees.
For example, ast.
BinOp inherits from ast.expr.
For production
rules with alternatives (aka “sums”), the left-hand side class is abstract: only instances of specific constructor nodes
are ever created.
Each instance of a concrete class has one attribute for each child node, of the type as defined in the grammar.
For example, ast.
BinOp instances have an attribute left of type ast.expr.
If these attributes are marked as optional in the grammar (using a question mark), the value might be None.
If the attributes can have zero-or-more values (marked with an asterisk), the values are represented as Python
lists.
The lineno and end_lineno are the first and last line numbers of
source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are
the corresponding UTF-8 byte offsets of the first and last tokens that generated the node.
The UTF-8 offset
is recorded because the parser uses UTF-8 internally.
Note that the end positions are not required by the compiler and are therefore optional.
The end offset
is after the last symbol, for example one can get the source segment of a one-line expression node using
source_line[node.col_offset : node.end_col_offset].
The constructor of a class ast.
If there are positional arguments, there must be as many as there are items in T._fields; they will be
assigned as attributes of these names.
If there are keyword arguments, they will set the attributes of the same names to the given values.
For example, to create and populate an ast.
UnaryOp node, you could use
node = ast.
UnaryOp()
node.op = ast.
USub()
node.operand = ast.
Constant()
node.operand.value = 5
node.operand.lineno = 0
node.operand.col_offset = 0
node.lineno = 0
node.col_offset = 0
or the more compact
node = ast.UnaryOp(ast.
USub(), ast.
Constant(5, lineno=0, col_offset=0),
lineno=0, col_offset=0)
Changed in version 3.8: Class ast.
Constant is now used for all constants.
Old classes ast.
Num, ast.
Str, ast.
Bytes, ast.
NameConstant and ast.
Ellipsis are still available, but they will be removed in future Python releases.
In the meantime, instantiating them
will return an instance of a different class.
Old classes ast.
Index and ast.
ExtSlice are still available, but they will be removed
in future Python releases.
In the meantime, instantiating them will return an instance of a different class.
Note: The descriptions of the specific node classes displayed here were initially adapted from the fantastic Green Tree
Snakes project and all its contributors.
A Python module, as with file input.
Node type generated by ast.parse() in the default "exec" mode.
A single Python expression input.
Node type generated by ast.parse() when mode is "eval".
Expression(
body=Constant(value=123))
class ast.
A single interactive input, like in tut-interac.
Node type generated by ast.parse() when mode is "single".
Interactive(
body=[
Assign(
targets=[
Name(id='x', ctx=Store())],
value=Constant(value=1)),
Assign(
targets=[
Name(id='y', ctx=Store())],
value=Constant(value=2))])
class ast.
A representation of an old-style type comments for functions, as Python versions prior to 3.5 didn’t support PEP
484 annotations.
Node type generated by ast.parse() when mode is "func_type".
Such type comments would look like this:

# type: (int, int) -> int

argtypes is a list of expression nodes.
New in version 3.8.
Literals
class ast.
A constant value.
The value attribute of the Constant literal contains the Python object it represents.
The
values represented can be simple types such as a number, string or None, but also immutable container types
(tuples and frozensets) if all of their elements are constant.
Expression(
body=Constant(value=123))
class ast.
Node representing a single formatting field in an f-string.
If the string contains a single formatting field and nothing
else the node can be isolated otherwise it appears in JoinedStr.
Both conversion and format_spec can be set at the same time.
An f-string, comprising a series of FormattedValue and Constant nodes.
List(elts, ctx)
class ast.
Tuple(elts, ctx)
A list or tuple.
A set.
A dictionary.
When doing dictionary unpacking using dictionary literals the expression to be expanded goes in the values list,
with a None at the corresponding position in keys.
Variables
class ast.
A variable name.
Load
class ast.
Store
class ast.
Del
Variable references can be used to load the value of a variable, to assign a new value to it, or to delete it.
Variable
references are given a context to distinguish these cases.
Module(
body=[
Expr(
value=Name(id='a', ctx=Load()))],
type_ignores=[])
Module(
body=[
Assign(
targets=[
Name(id='a', ctx=Store())],
value=Constant(value=1))],
type_ignores=[])
Module(
body=[
Delete(
targets=[
Name(id='a', ctx=Del())])],
type_ignores=[])
class ast.
A *var variable reference.
This type must be used when
building a Call node with *args.
Module(
body=[
Assign(
targets=[
Tuple(
elts=[
Name(id='a', ctx=Store()),
Starred(
value=Name(id='b', ctx=Store()),
(continues on next page)
32.1.
When an expression, such as a function call, appears as a statement by itself with its return value not used or stored,
it is wrapped in this container. value holds one of the other nodes in this section, a Constant, a Name, a
Lambda, a Yield or YieldFrom node.
Module(
body=[
Expr(
value=UnaryOp(
op=USub(),
operand=Name(id='a', ctx=Load())))],
type_ignores=[])
class ast.
UnaryOp(op, operand)
A unary operation.
UAdd
class ast.
USub
class ast.
Not
class ast.
Invert
Unary operator tokens.
Not is the not keyword, Invert is the ~ operator.
Expression(
body=UnaryOp(
op=Not(),
operand=Name(id='x', ctx=Load())))
class ast.
A binary operation (like addition or division).
Expression(
body=BinOp(
left=Name(id='x', ctx=Load()),
op=Add(),
right=Name(id='y', ctx=Load())))
class ast.
Add
class ast.
Sub
class ast.
Mult
class ast.
Div
class ast.
FloorDiv
class ast.
Pow
class ast.
LShift
class ast.
RShift
class ast.
BitOr
class ast.
BitXor
class ast.
BitAnd
class ast.
MatMult
Binary operator tokens.
BoolOp(op, values)
A boolean operation, ‘or’ or ‘and’.
Consecutive operations with
the same operator, such as a or b or c, are collapsed into one node with several values.
This doesn’t include not, which is a UnaryOp.
Expression(
body=BoolOp(
op=Or(),
values=[
Name(id='x', ctx=Load()),
Name(id='y', ctx=Load())]))
class ast.
And
class ast.
Or
Boolean operator tokens.
A comparison of two or more values.
Expression(
body=Compare(
left=Constant(value=1),
ops=[
LtE(),
Lt()],
comparators=[
Name(id='a', ctx=Load()),
Constant(value=10)]))
class ast.
Eq
class ast.
NotEq
class ast.
Lt
class ast.
LtE
class ast.
Gt
class ast.
GtE
class ast.
Is
class ast.
IsNot
class ast.
In
32.1.
NotIn
Comparison operator tokens.
A function call.
Of the arguments:
• args holds a list of the arguments passed by position.
When creating a Call node, args and keywords are required, but they can be empty lists.
A keyword argument to a function call or class definition.
An expression such as a if b else c. Each field holds a single node, so in the following example, all three
are Name nodes.
Expression(
body=IfExp(
test=Name(id='b', ctx=Load()),
body=Name(id='a', ctx=Load()),
orelse=Name(id='c', ctx=Load())))
class ast.
Attribute(value, attr, ctx)
Attribute access, e.g. d.keys.
Expression(
body=Attribute(
value=Name(id='snake', ctx=Load()),
attr='colour',
ctx=Load()))
class ast.
NamedExpr(target, value)
A named expression.
This AST node is produced by the assignment expressions operator (also known as the walrus
operator).
New in version 3.8.
Subscripting
class ast.
A subscript, such as l[1]. value is the subscripted object (usually sequence or mapping).
It can be a Tuple and contain a Slice. ctx is Load, Store or Del according to the action
performed with the subscript.
Expression(
body=Subscript(
value=Name(id='l', ctx=Load()),
slice=Tuple(
elts=[
Slice(
lower=Constant(value=1),
upper=Constant(value=2)),
Constant(value=3)],
ctx=Load()),
ctx=Load()))
class ast.
Slice(lower, upper, step)
Regular slicing (on the form lower:upper or lower:upper:step).
Comprehensions
class ast.
ListComp(elt, generators)
class ast.
SetComp(elt, generators)
class ast.
GeneratorExp(elt, generators)
class ast.
DictComp(key, value, generators)
List and set comprehensions, generator expressions, and dictionary comprehensions.
The value is an
integer (0 or 1).
An assignment.
Multiple nodes in targets represents assigning the same value to each.
Unpacking is represented by putting a
Tuple or List within targets.
Module(
body=[
Assign(
targets=[
Name(id='a', ctx=Store()),
Name(id='b', ctx=Store())],
value=Constant(value=1))],
type_ignores=[])
Module(
body=[
Assign(
targets=[
Tuple(
elts=[
Name(id='a', ctx=Store()),
Name(id='b', ctx=Store())],
ctx=Store())],
value=Name(id='c', ctx=Load()))],
type_ignores=[])
class ast.
An assignment with a type annotation.
AugAssign(target, op, value)
Augmented assignment, such as a += 1.
In the following example, target is a Name node for x (with the
Store context), op is Add, and value is a Constant with value for 1.
The target attribute cannot be of class Tuple or List, unlike the targets of Assign.
Module(
body=[
AugAssign(
target=Name(id='x', ctx=Store()),
op=Add(),
value=Constant(value=2))],
type_ignores=[])
class ast.
A raise statement.
An assertion.
Module(
body=[
Delete(
targets=[
Name(id='x', ctx=Del()),
Name(id='y', ctx=Del()),
Name(id='z', ctx=Del())])],
type_ignores=[])
class ast.
Pass
A pass statement.
Module(
body=[
Pass()],
type_ignores=[])
class ast.
A type alias created through the type statement.
Module(
body=[
TypeAlias(
name=Name(id='Alias', ctx=Store()),
type_params=[],
value=Name(id='int', ctx=Load()))],
type_ignores=[])
New in version 3.12.
An import statement.
Module(
body=[
Import(
names=[
alias(name='x'),
alias(name='y'),
alias(name='z')])],
type_ignores=[])
class ast.
Both parameters are raw strings of the names.
Module(
body=[
ImportFrom(
module='foo.bar',
names=[
alias(name='a', asname='b'),
alias(name='c')],
level=2)],
type_ignores=[])
32.1.
An if statement.
Those in orelse are executed if the loop finishes normally, rather than via a break
statement.
A while loop. test holds the condition, such as a Compare node.
Module(
body=[
While(
test=Name(id='x', ctx=Load()),
body=[
Expr(
value=Constant(value=Ellipsis))],
orelse=[
Expr(
value=Constant(value=Ellipsis))])],
type_ignores=[])
class ast.
Break
class ast.
Continue
The break and continue statements.
Try(body, handlers, orelse, finalbody)
try blocks.
Module(
body=[
Try(
body=[
Expr(
value=Constant(value=Ellipsis))],
handlers=[
ExceptHandler(
type=Name(id='Exception', ctx=Load()),
body=[
Expr(
value=Constant(value=Ellipsis))]),
ExceptHandler(
type=Name(id='OtherException', ctx=Load()),
name='e',
body=[
Expr(
value=Constant(value=Ellipsis))])],
orelse=[
Expr(
value=Constant(value=Ellipsis))],
finalbody=[
Expr(
value=Constant(value=Ellipsis))])],
type_ignores=[])
class ast.
TryStar(body, handlers, orelse, finalbody)
try blocks which are followed by except* clauses.
A single except clause.
Module(
body=[
Try(
body=[
Expr(
value=BinOp(
left=Name(id='a', ctx=Load()),
op=Add(),
right=Constant(value=1)))],
handlers=[
ExceptHandler(
type=Name(id='TypeError', ctx=Load()),
body=[
Pass()])],
orelse=[],
finalbody=[])],
type_ignores=[])
class ast.With(items, body, type_comment)
A with block.
Pattern matching
class ast.
Match(subject, cases)
A match statement.
New in version 3.10.
A single case pattern in a match statement.
Note that the AST nodes produced for patterns differ from those produced for expressions, even when they
share the same syntax.
The guard attribute contains an expression that will be evaluated if the pattern matches the subject.
A match literal or value pattern that compares by equality.
Permitted value nodes
are restricted as described in the match statement documentation.
A match literal pattern that compares by identity.
This pattern succeeds if the match subject is the given constant.
32.1.
Module(
body=[
Match(
subject=Name(id='x', ctx=Load()),
cases=[
match_case(
pattern=MatchSingleton(value=None),
body=[
Expr(
value=Constant(value=Ellipsis))])])],
type_ignores=[])
New in version 3.10.
class ast.
A match sequence pattern.
Matches a variable length sequence if one of the subpatterns is a MatchStar node, otherwise
matches a fixed length sequence.
Module(
body=[
Match(
subject=Name(id='x', ctx=Load()),
cases=[
match_case(
pattern=MatchSequence(
patterns=[
MatchValue(
value=Constant(value=1)),
MatchValue(
value=Constant(value=2))]),
body=[
Expr(
value=Constant(value=Ellipsis))])])],
type_ignores=[])
New in version 3.10.
class ast.
Matches the rest of the sequence in a variable length match sequence pattern.
If name is not None, a list containing
the remaining sequence elements is bound to that name if the overall sequence pattern is successful.
A match mapping pattern.
Permitted key expressions are restricted as described in the match statement documentation.
This pattern succeeds if the subject is a mapping, all evaluated key expressions are present in the mapping, and the
value corresponding to each key matches the corresponding subpattern.
If rest is not None, a dict containing the
remaining mapping elements is bound to that name if the overall mapping pattern is successful.
Module(
body=[
Match(
subject=Name(id='x', ctx=Load()),
cases=[
match_case(
pattern=MatchMapping(
keys=[
Constant(value=1),
Constant(value=2)],
patterns=[
MatchAs(),
(continues on next page)
32.1.
A match class pattern.
This pattern succeeds if the subject is an instance of the nominated class, all positional patterns match the corre-
sponding class-defined attributes, and any specified keyword attributes match their corresponding pattern.
Note: classes may define a property that returns self in order to match a pattern node against the instance being
matched.
A match “as-pattern”, capture pattern or wildcard pattern.
If the pattern is None, the node represents a capture pattern (i.e a bare name) and will
always succeed.
The name attribute contains the name that will be bound if the pattern is successful.
If name is None, pattern
must also be None and the node represents the wildcard pattern.
An or-pattern matches each of its subpatterns in turn to the subject, until one succeeds.
The
or-pattern is then deemed to succeed.
If none of the subpatterns succeed the or-pattern fails.
The patterns
attribute contains a list of match pattern nodes that will be matched against the subject.
Module(
body=[
Match(
subject=Name(id='x', ctx=Load()),
cases=[
match_case(
pattern=MatchOr(
patterns=[
MatchSequence(
patterns=[
MatchAs(name='x')]),
MatchAs(name='y')]),
body=[
Expr(
value=Constant(value=Ellipsis))])])],
type_ignores=[])
New in version 3.10.
Type parameters
Type parameters can exist on classes, functions, and type aliases.
TypeVar(name, bound)
A typing.
TypeVar. name is the name of the type variable.
If
bound is a Tuple, it represents constraints; otherwise it represents the bound.
A typing.
ParamSpec.
A typing.
TypeVarTuple. name is the name of the type variable tuple.
Module(
body=[
TypeAlias(
name=Name(id='Alias', ctx=Store()),
type_params=[
TypeVarTuple(name='Ts')],
value=Subscript(
value=Name(id='tuple', ctx=Load()),
slice=Tuple(
elts=[
Starred(
value=Name(id='Ts', ctx=Load()),
ctx=Load())],
ctx=Load()),
ctx=Load()))],
type_ignores=[])
New in version 3.12.
Function and class definitions
class ast.
A function definition.
Lambda(args, body)
lambda is a minimal function definition that can be used inside an expression.
Unlike FunctionDef, body
holds a single node.
The arguments for a function.
• posonlyargs, args and kwonlyargs are lists of arg nodes.
If one is None, the corresponding
argument is required.
If there are fewer defaults,
they correspond to the last n arguments.
A single argument in a list.
A return statement.
Module(
body=[
Return(
value=Constant(value=4))],
type_ignores=[])
class ast.
Yield(value)
class ast.
A yield or yield from expression.
Because these are expressions, they must be wrapped in a Expr node if
the value sent back is not used.
Module(
body=[
Expr(
value=Yield(
value=Name(id='x', ctx=Load())))],
type_ignores=[])
Module(
body=[
Expr(
value=YieldFrom(
value=Name(id='x', ctx=Load())))],
type_ignores=[])
class ast.Global(names)
class ast.Nonlocal(names)
global and nonlocal statements.
A class definition.
Other keywords will be passed to the
metaclass, as per PEP-3115.
Async and await
class ast.
An async def function definition.
Has the same fields as FunctionDef.
Changed in version 3.12: Added type_params.
An await expression.
AsyncFor(target, iter, body, orelse, type_comment)
class ast.
AsyncWith(items, body, type_comment)
async for loops and async with context managers.
They have the same fields as For and With, respec-
tively.
Only valid in the body of an AsyncFunctionDef.
Note: When a string is parsed by ast.parse(), operator nodes (subclasses of ast.operator, ast.unaryop,
ast.cmpop, ast.boolop and ast.expr_context) on the returned tree will be singletons.
Changes to one will
be reflected in all other occurrences of the same value (e.g. ast.
Add).
Equivalent to compile(source, filename, mode, ast.
PyCF_ONLY_AST).
If type_comments=True is given, the parser is modified to check and return type comments as specified
by PEP 484 and PEP 526.
This is equivalent to adding ast.
PyCF_TYPE_COMMENTS to the flags passed to
compile().
This will report syntax errors for misplaced type comments.
Without this flag, type comments will
be ignored, and the type_comment field on selected AST nodes will always be None.
In addition, the locations
of # type: ignore comments will be returned as the type_ignores attribute of Module (otherwise it is
always an empty list).
In addition, if mode is 'func_type', the input syntax is modified to correspond to PEP 484 “signature type
comments”, e.g. (str, int) -> List[str].
Also, setting feature_version to a tuple (major, minor) will attempt to parse using that Python ver-
sion’s grammar.
Currently major must equal to 3.
For example, setting feature_version=(3, 4) will
allow the use of async and await as variable names.
The lowest supported version is (3, 4); the highest is
sys.version_info[0:2].
If source contains a null character (’0’), ValueError is raised.
Warning: Note that successfully parsing source code into an AST object doesn’t guarantee that the source code
provided is valid Python code that can be executed as the compilation step can raise further SyntaxError
exceptions.
For instance, the source return 42 generates a valid AST node for a return statement, but it
cannot be compiled alone (it needs to be inside a function node).
In particular, ast.parse() won’t do any scoping checks, which the compilation step does.
Warning:
It is possible to crash the Python interpreter with a sufficiently large/complex string due to stack
depth limitations in Python’s AST compiler.
Changed in version 3.8: Added type_comments, mode='func_type' and feature_version.
ast.unparse(ast_obj)
Unparse an ast.
AST object and generate a string with code that would produce an equivalent ast.
AST object
if parsed back with ast.parse().
Warning:
The produced code string will not necessarily be equal to the original code that generated the
ast.
AST object (without any compiler optimizations, such as constant tuples/frozensets).
Warning: Trying to unparse a highly complex expression would result with RecursionError.
The string or node
provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts,
sets, booleans, None and Ellipsis.
This can be used for evaluating strings containing Python values without the need to parse the values oneself.
It is
not capable of evaluating arbitrarily complex expressions, for example involving operators or indexing.
This function had been documented as “safe” in the past without defining what that meant.
That was misleading.
This is specifically designed not to execute Python code, unlike the more general eval().
There is no names-
pace, no name lookups, or ability to call out.
But it is not free from attack: A relatively small input can lead to
memory exhaustion or to C stack exhaustion, crashing the process.
There is also the possibility for excessive CPU
consumption denial of service on some inputs.
Calling it on untrusted data is thus not recommended.
Warning: It is possible to crash the Python interpreter due to stack depth limitations in Python’s AST compiler.
It can raise ValueError, TypeError, SyntaxError, MemoryError and RecursionError de-
pending on the malformed input.
Changed in version 3.2: Now allows bytes and set literals.
Changed in version 3.9: Now supports creating empty sets with 'set()'.
For string inputs, leading spaces and tabs are now stripped.
If clean is true, clean up the docstring’s indentation with
inspect.cleandoc().
Changed in version 3.5: AsyncFunctionDef is now supported.
If some location information (lineno,
end_lineno, col_offset, or end_col_offset) is missing, return None.
If padded is True, the first line of a multi-line statement will be padded with spaces to match its original position.
When you compile a node tree with compile(), the compiler expects lineno and col_offset attributes
for every node that supports them.
This is rather tedious to fill in for generated nodes, so this helper adds these
attributes recursively where not already set, by setting them to the values of the parent node.
Increment the line number and end line number of each node in the tree starting at node by n.
This is useful to
“move code” to a different location in a file.
This
is useful if you only want to modify nodes in place and don’t care about the context.
NodeVisitor
A node visitor base class that walks the abstract syntax tree and calls a visitor function for every node found.
This
function may return a value which is forwarded by the visit() method.
This class is meant to be subclassed, with the subclass adding visitor methods.
visit(node)
Visit a node.
This visitor calls visit() on all children of the node.
Note that child nodes of nodes that have a custom visitor method won’t be visited unless the visitor calls
generic_visit() or visits them itself.
Don’t use the NodeVisitor if you want to apply changes to nodes during traversal.
For this a special visitor
exists (NodeTransformer) that allows modifications.
Deprecated
since
version
3.8:
Methods
visit_Num(),
visit_Str(),
visit_Bytes(),
visit_NameConstant() and visit_Ellipsis() are deprecated now and will not be called in
future Python versions.
Add the visit_Constant() method to handle all constant nodes.
NodeTransformer
A NodeVisitor subclass that walks the abstract syntax tree and allows modification of nodes.
The NodeTransformer will walk the AST and use the return value of the visitor methods to replace or remove
the old node.
If the return value of the visitor method is None, the node will be removed from its location, otherwise
it is replaced with the return value.
The return value may be the original node in which case no replacement takes
place.
Keep in mind that if the node you’re operating on has child nodes you must either transform the child nodes yourself
or call the generic_visit() method for the node first.
Usually you use the transformer like this:
node = YourTransformer().visit(node)
ast.dump(node, annotate_fields=True, include_attributes=False, *, indent=None)
Return a formatted dump of the tree in node.
This is mainly useful for debugging purposes.
If annotate_fields
is true (by default), the returned string will show the names and the values for fields.
If annotate_fields is false,
the result string will be more compact by omitting unambiguous field names.
Attributes such as line numbers and
column offsets are not dumped by default.
If this is wanted, include_attributes can be set to true.
If indent is a non-negative integer or string, then the tree will be pretty-printed with that indent level.
An indent
level of 0, negative, or "" will only insert newlines.
None (the default) selects the single line representation.
Using
a positive integer indent indents that many spaces per level.
If indent is a string (such as "\t"), that string is used
to indent each level.
Added the indent option.
PyCF_ALLOW_TOP_LEVEL_AWAIT
Enables support for top-level await, async for, async with and async comprehensions.
New in version 3.8.
PyCF_ONLY_AST
Generates and returns an abstract syntax tree instead of returning a compiled code object.
PyCF_TYPE_COMMENTS
Enables support for PEP 484 and PEP 526 style type comments (# type: <type>, # type: ignore
<stuff>).
New in version 3.8.
The ast module can be executed as a script from the command line.
If infile is specified its contents are parsed to AST and dumped to stdout.
Otherwise, the content is read from stdin.
See also:
Green Tree Snakes, an external documentation resource, has good details on working with Python ASTs.
ASTTokens annotates Python ASTs with the positions of tokens and text in the source code that generated them.
This is
helpful for tools that make source code transformations.
LibCST parses code as a Concrete Syntax Tree that looks like an ast tree and keeps all formatting details.
It’s useful for
building automated refactoring (codemod) applications and linters.
Parso is a Python parser that supports error recovery and round-trip parsing for different Python versions (in multiple
Python versions).
Parso is also able to list multiple syntax errors in your python file.
Lib/symtable.py
Symbol tables are generated by the compiler from AST just before bytecode is generated.
The symbol table is responsible
for calculating the scope of every identifier in the code.
SymbolTable
A namespace table for a block.
The constructor is not public.
Possible values are 'class', 'module', 'function',
'annotation', 'TypeVar bound', 'type alias', and 'type parameter'.
The latter four
refer to different flavors of annotation scopes.
This is the name of the class if the table is for a class, the name of the function
if the table is for a function, or 'top' if the table is global (get_type() returns 'module').
For
type parameter scopes (which are used for generic classes, functions, and type aliases), it is the name of the
underlying class, function, or type alias.
For type alias scopes, it is the name of the type alias.
For TypeVar
bound scopes, it is the name of the TypeVar.
These can be obtained with get_children().
See the documentation of view objects.
lookup(name)
Lookup name in the table and return a Symbol instance.
Function
A namespace for a function or method.
This class inherits SymbolTable.
Class
A namespace of a class.
This class inherits SymbolTable.
Symbol
An entry in a SymbolTable corresponding to an identifier in the source.
The constructor is not public.
New in version 3.6.
If the name is used as the target of a function or class statement, this will be true.
For example:
True
Note that a single name can be bound to multiple objects.
If the result is True, the name may also be bound
to other objects, like an int or list, that does not introduce a new namespace.
If more than one or no namespace is bound to this name, a
ValueError is raised.
Refer to the file Grammar/Tokens in the Python distribution for the definitions of the names in the context of the
language grammar.
The specific numeric values which the names map to may change between Python versions.
The module also provides a mapping from numeric codes to names and some functions.
The functions mirror definitions
in the Python C header files.
ISTERMINAL(x)
Return True for terminal token values.
token.
ISNONTERMINAL(x)
Return True for non-terminal token values.
token.
ISEOF(x)
Return True if x is the marker indicating the end of input.
The token constants are:
token.
ENDMARKER
token.
NAME
token.
NUMBER
token.
STRING
token.
NEWLINE
token.
INDENT
token.
DEDENT
token.
LPAR
Token value for "(".
token.
RPAR
Token value for ")".
token.
LSQB
Token value for "[".
token.
RSQB
Token value for "]".
token.
COLON
Token value for ":".
token.
COMMA
Token value for ",".
32.3.
SEMI
Token value for ";".
token.
PLUS
Token value for "+".
MINUS
Token value for "-".
token.
STAR
Token value for "*".
token.
SLASH
Token value for "/".
token.
VBAR
Token value for "|".
AMPER
Token value for "&".
token.
LESS
Token value for "<".
token.
GREATER
Token value for ">".
token.
EQUAL
Token value for "=".
token.
DOT
Token value for ".".
PERCENT
Token value for "%".
token.
LBRACE
Token value for "{".
token.
RBRACE
Token value for "}".
token.
EQEQUAL
Token value for "==".
token.
NOTEQUAL
Token value for "!=".
token.
LESSEQUAL
Token value for "<=".
token.
GREATEREQUAL
Token value for ">=".
token.
TILDE
Token value for "~".
token.
CIRCUMFLEX
Token value for "^".
LEFTSHIFT
Token value for "<<".
token.
RIGHTSHIFT
Token value for ">>".
token.
DOUBLESTAR
Token value for "**".
token.
PLUSEQUAL
Token value for "+=".
token.
MINEQUAL
Token value for "-=".
token.
STAREQUAL
Token value for "*=".
token.
SLASHEQUAL
Token value for "/=".
PERCENTEQUAL
Token value for "%=".
token.
AMPEREQUAL
Token value for "&=".
token.
VBAREQUAL
Token value for "|=".
token.
CIRCUMFLEXEQUAL
Token value for "^=".
token.
LEFTSHIFTEQUAL
Token value for "<<=".
token.
RIGHTSHIFTEQUAL
Token value for ">>=".
token.
DOUBLESTAREQUAL
Token value for "**=".
token.
DOUBLESLASH
Token value for "//".
token.
DOUBLESLASHEQUAL
Token value for "//=".
token.
AT
Token value for "@".
token.
ATEQUAL
Token value for "@=".
token.
RARROW
Token value for "->".
token.
ELLIPSIS
Token value for "...".
32.3.
COLONEQUAL
Token value for ":=".
token.
EXCLAMATION
Token value for "!".
token.
OP
token.
AWAIT
token.
ASYNC
token.
TYPE_IGNORE
token.
TYPE_COMMENT
token.
SOFT_KEYWORD
token.
FSTRING_START
token.
FSTRING_MIDDLE
token.
FSTRING_END
token.
COMMENT
token.
NL
token.
ERRORTOKEN
token.
N_TOKENS
token.
The following token type values aren’t used by the C tokenizer but are needed for the tokenize module.
COMMENT
Token value used to indicate a comment.
token.
NL
Token value used to indicate a non-terminating newline.
The NEWLINE token indicates the end of a logical line
of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.
ENCODING
Token value that indicates the encoding used to decode the source bytes into text.
The first token returned by
tokenize.tokenize() will always be an ENCODING token.
TYPE_COMMENT
Token value indicating that a type comment was recognized.
Such tokens are only produced when ast.parse()
is invoked with type_comments=True.
Changed in version 3.5: Added AWAIT and ASYNC tokens.
Changed in version 3.7: Added COMMENT, NL and ENCODING tokens.
Changed in version 3.7: Removed AWAIT and ASYNC tokens.
Changed in version 3.8: Added TYPE_COMMENT, TYPE_IGNORE, COLONEQUAL.
Added AWAIT and ASYNC tokens
back (they’re needed to support parsing older Python versions for ast.parse() with feature_version set to 6
or lower).
Return True if s is a Python soft keyword.
New in version 3.9.
New in version 3.9.
The scanner in this
module returns comments as tokens as well, making it useful for implementing “pretty-printers”, including colorizers for
on-screen displays.
To simplify token stream handling, all operator and delimiter tokens and Ellipsis are returned using the generic OP
token type.
The exact type can be determined by checking the exact_type property on the named tuple returned from
tokenize.tokenize().
Warning:
Note that the functions in this module are only designed to parse syntactically valid Python code (code
that does not raise when parsed using ast.parse()).
The behavior of the functions in this module is undefined
when providing invalid Python code and it can change at any point.
Tokenizing Input
The primary entry point is a generator:
tokenize.tokenize(readline)
The tokenize() generator requires one argument, readline, which must be a callable object which provides the
same interface as the io.IOBase.readline() method of file objects.
Each call to the function should return
one line of input as bytes.
The generator produces 5-tuples with these members: the token type; the token string; a 2-tuple (srow, scol)
of ints specifying the row and column where the token begins in the source; a 2-tuple (erow, ecol) of ints
specifying the row and column where the token ends in the source; and the line on which the token was found.
The
line passed (the last tuple item) is the physical line.
The 5 tuple is returned as a named tuple with the field names:
type string start end line.
The returned named tuple has an additional property named exact_type that contains the exact operator type
for OP tokens.
For all other token types exact_type equals the named tuple type field.
Changed in version 3.1: Added support for named tuples.
Changed in version 3.3: Added support for exact_type.
tokenize() determines the source encoding of the file by looking for a UTF-8 BOM or encoding cookie,
according to PEP 263.
tokenize.generate_tokens(readline)
Tokenize a source reading unicode strings instead of bytes.
Like tokenize(), the readline argument is a callable returning a single line of input.
However,
generate_tokens() expects readline to return a str object rather than bytes.
The result is an iterator yielding named tuples, exactly like tokenize().
It does not yield an ENCODING token.
All constants from the token module are also exported from tokenize.
Another function is provided to reverse the tokenization process.
This is useful for creating tools that tokenize a script,
modify the token stream, and write back the modified script.
Converts tokens back into Python source code.
The iterable must return sequences with at least two elements, the
token type and the token string.
Any additional sequence elements are ignored.
The reconstructed script is returned as a single string.
The result is guaranteed to tokenize back to match the input
so that the conversion is lossless and round-trips are assured.
The guarantee applies only to the token type and
token string as the spacing between tokens (column positions) may change.
It returns bytes, encoded using the ENCODING token, which is the first token sequence output by tokenize().
If there is no encoding token in the input, it returns a str instead.
The detect_encoding() function is used to detect the encoding that should be used to decode a Python
source file.
It requires one argument, readline, in the same way as the tokenize() generator.
It detects the encoding from the presence of a UTF-8 BOM or an encoding cookie as specified in PEP 263.
If
both a BOM and a cookie are present, but disagree, a SyntaxError will be raised.
Note that if the BOM is
found, 'utf-8-sig' will be returned as an encoding.
If no encoding is specified, then the default of 'utf-8' will be returned.
Use open() to open Python source files: it uses detect_encoding() to detect the file encoding.
tokenize.open(filename)
Open a file in read only mode using the encoding detected by detect_encoding().
TokenError
Raised when either a docstring or expression that may be split over several lines is not completed anywhere in the
file, for example:
"""Beginning of
docstring
or:
[1,
2,

32.5.2 Command-Line Usage
New in version 3.3.
The tokenize module can be executed as a script from the command line.
It is as simple as:
python -m tokenize [-e] [filename.py]
The following options are accepted:
-h, --help
show this help message and exit
-e, --exact
display token names using the exact type
If filename.py is specified its contents are tokenized to stdout.
Otherwise, tokenization is performed on stdin.
Known cases are "e-007" (Windows) and "e-07" (not Windows).
Since
we're only showing 12 digits, and the 13th isn't close to 5, the
rest of the output should be platform-independent.
Or reading bytes directly with tokenize():
import tokenize
with open('hello.py', 'rb') as f:
tokens = tokenize.tokenize(f.readline)
for token in tokens:
print(token)
32.6 tabnanny — Detection of ambiguous indentation
Source code: Lib/tabnanny.py
For the time being this module is intended to be called as a script.
However it is possible to import it into an IDE and use
the function check() described below.
Note:
The API provided by this module is likely to change in future releases; such changes may not be backward
compatible.
If file_or_dir is an ordinary Python source file, it is checked for whitespace
related problems.
The diagnostic messages are written to standard output using the print() function.
This is incremented by the -v option if called as a script.
This is set to
true by the -q option if called as a script.
NannyNag
Raised by process_tokens() if detecting an ambiguous indent.
This function is used by check() to process tokens generated by the tokenize module.
See also:
Module tokenize Lexical scanner for Python source code.
The information is sufficient to implement a module browser.
The information is extracted from the Python
source code rather than by importing the module, so this module is safe to use with untrusted code.
This restriction
makes it impossible to use this module with modules not implemented in Python, including all standard and optional
extension modules.
If possible, descriptors for imported
base classes are included.
Parameter module is a string with the name of the module to read; it may be the name
of a module within a package.
If given, path is a sequence of directory paths prepended to sys.path, which is
used to locate the module source code.
This function is the original interface and is only kept for back compatibility.
It returns a filtered version of the
following.
The returned dictionary maps module-level function and class names
to their descriptors.
Nested objects are entered into the children dictionary of their parent.
As with readmodule,
module names the module to be read and path is prepended to sys.path.
Descriptors for nested definitions.
They are accessed through the new children attribute.
Each has a
new parent attribute.
The descriptors returned by these functions are instances of Function and Class classes.
Users are not expected to create
instances of these classes.
Function Objects
Class Function instances describe functions defined by def statements.
They have the following attributes:
Function.file
Name of the file in which the function is defined.
Function.module
The name of the module defining the function described.
Function.name
The name of the function.
The line number in the file where the definition starts.
Function.parent
For top-level functions, None.
For nested functions, the parent.
New in version 3.7.
A dictionary mapping names to descriptors for nested functions and classes.
New in version 3.7.
Function.is_async
True for functions that are defined with the async prefix, False otherwise.
New in version 3.10.
They have the same attributes as Functions and two
more.
Class.file
Name of the file in which the class is defined.
Class.module
The name of the module defining the class described.
Class.name
The name of the class.
The line number in the file where the definition starts.
Class.parent
For top-level classes, None.
For nested classes, the parent.
New in version 3.7.
A dictionary mapping names to descriptors for nested functions and classes.
New in version 3.7.
Classes which
are named as superclasses but which are not discoverable by readmodule_ex() are listed as a string with the
class name instead of as Class objects.
Class.methods
A dictionary mapping method names to line numbers.
This can be derived from the newer children dictionary, but
remains for back-compatibility.
Lib/py_compile.py
The py_compile module provides a function to generate a byte-code file from a source file, and another function used
when the module source file is invoked as a script.
Though not often needed, this function can be useful when installing modules for shared use, especially if some of the
users may not have permission to write the byte-code cache files in the directory containing the source code.
PyCompileError
Exception raised when an error occurs while attempting to compile the file.
TIMESTAMP, quiet=0)
Compile a source file to byte-code and write out the byte-code cache file.
The source code is loaded from the
file named file.
The byte-code is written to cfile, which defaults to the PEP 3147/PEP 488 path, ending in
.pyc.
For example, if file is /foo/bar/baz.py cfile will default to /foo/bar/__pycache__/baz.
cpython-32.pyc for Python 3.2.
If dfile is specified, it is used instead of file as the name of the source file
from which source lines are obtained for display in exception tracebacks.
If doraise is true, a PyCompileError
is raised when an error is encountered while compiling file.
If doraise is false (the default), an error string is written
to sys.stderr, but no exception is raised.
This function returns the path to byte-compiled file, i.e. whatever
cfile value was used.
The doraise and quiet arguments determine how errors are handled while compiling file.
If quiet is 0 or 1, and
doraise is false, the default behaviour is enabled: an error string is written to sys.stderr, and the function
returns None instead of a path.
If doraise is true, a PyCompileError is raised instead.
However if quiet is 2,
no message is written, and doraise has no effect.
If the path that cfile becomes (either explicitly specified or computed) is a symlink or non-regular file,
FileExistsError will be raised.
This is to act as a warning that import will turn those paths into regular
files if it is allowed to write byte-compiled files to those paths.
This is a side-effect of import using file renaming
to place the final byte-compiled file into place to prevent concurrent file writing issues.
The default of -1
selects the optimization level of the current interpreter.
The default is PycInvalidationMode.CHECKED_HASH if
the SOURCE_DATE_EPOCH environment variable is set, otherwise the default is PycInvalidationMode.
TIMESTAMP.
Changed in version 3.2: Changed default value of cfile to be PEP 3147-compliant.
Previous default was file + 'c'
('o' if optimization was enabled).
This means file
creation/writing semantics now match what importlib does, e.g. permissions, write-and-move semantics, etc.
Also added the caveat that FileExistsError is raised if cfile is a symlink or non-regular file.
The invalidation_mode parameter was added as specified in PEP 552.
If
the
SOURCE_DATE_EPOCH
environment
variable
is
set,
invalidation_mode
will
be
forced
to
PycInvalidationMode.CHECKED_HASH.
The SOURCE_DATE_EPOCH environment variable no longer overrides the value of the
invalidation_mode argument, and determines its default value instead.
The quiet parameter was added.
PycInvalidationMode
A enumeration of possible methods the interpreter can use to determine whether a bytecode file is up to date with
a source file.
The .pyc file indicates the desired invalidation mode in its header.
See pyc-invalidation for more
information on how Python invalidates .pyc files at runtime.
New in version 3.7.
The .pyc file includes the timestamp and size of the source file, which Python will compare against the
metadata of the source file at runtime to determine if the .pyc file needs to be regenerated.
CHECKED_HASH
The .pyc file includes a hash of the source file content, which Python will compare against the source at
runtime to determine if the .pyc file needs to be regenerated.
UNCHECKED_HASH
Like CHECKED_HASH, the .pyc file includes a hash of the source file content.
However, Python will at
runtime assume the .pyc file is up to date and not validate the .pyc against the source file at all.
This option is useful when the .pycs are kept up to date by some system external to Python like a build
system.
The files named in filenames are compiled and the
resulting bytecode is cached in the normal manner.
This program does not search a directory structure to locate source
files; it only compiles files named explicitly.
The exit status is nonzero if one of the files could not be compiled.
If - is the only parameter, the list of files is taken from standard input.
Changed in version 3.2: Added support for -.
Changed in version 3.10: Added support for -q.
See also:
Module compileall Utilities to compile all Python source files in a directory tree.
32.8.
These functions compile Python source
files in a directory tree.
This module can be used to create the cached byte-code files at library installation time, which
makes them available for use even by users who don’t have write permission to the library directories.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Positional arguments are files to compile or directories that contain source files, traversed recursively.
If no argu-
ment is given, behave as if the command line was -l <directories from sys.path>.
If passed once, error messages will still be printed.
If passed twice (-qq),
all output is suppressed.
This will appear in compilation time tracebacks, and
is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the
source file does not exist at the time the byte-code file is executed.
Cannot be combined with -d.
-x regex
regex is used to search the full path to each file considered for compilation, and if the regex produces a match, the
file is skipped.
If list is -,
read lines from stdin.
-b
Write the byte-code files to their legacy locations and names, which may overwrite byte-code files created by another
version of Python.
If this is given, then -l option will not be taken into
account.
N
Use N workers to compile the files within the given directory.
If 0 is used, then the result of os.cpu_count()
will be used.
The timestamp value, means that
.pyc files with the source timestamp and size embedded will be generated.
The checked-hash and
unchecked-hash values cause hash-based pycs to be generated.
Hash-based pycs embed a hash of the source
file contents rather than a timestamp.
See pyc-invalidation for more information on how Python validates bytecode
cache files at runtime.
The default is timestamp if the SOURCE_DATE_EPOCH environment variable is not
set, and checked-hash if the SOURCE_DATE_EPOCH environment variable is set.
May be used multiple times to compile for multiple levels at a time (for
example, compileall -o 1 -o 2).
Changed in version 3.2: Added the -i, -b and -h options.
Added the -j, -r, and -qq options.
Changed in version 3.7: Added the --invalidation-mode option.
Added the -s, -p, -e and --hardlink-dupes options.
Raised the default recursion limit
from 10 to sys.getrecursionlimit().
Added the possibility to specify the -o option multiple times.
Similarly, the compile() function respects the sys.pycache_prefix setting.
The generated bytecode cache will
only be useful if compile() is run with the same sys.pycache_prefix (if any) that will be used at runtime.
Return a true value if
all the files compiled successfully, and a false value otherwise.
The
maxlevels
parameter
is
used
to
limit
the
depth
of
the
recursion;
it
defaults
to
sys.
getrecursionlimit().
If ddir is given, it is prepended to the path to each file being compiled for use in compilation time tracebacks, and
is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the
source file does not exist at the time the byte-code file is executed.
If force is true, modules are re-compiled even if the timestamps are up to date.
If rx is given, its search method is called on the complete path to each file considered for compilation, and if it
returns a true value, the file is skipped.
This can be used to exclude files matching a regular expression, given as a
re.Pattern object.
If quiet is False or 0 (the default), the filenames and other information are printed to standard out.
Set to 1, only
errors are printed.
Set to 2, all output is suppressed.
If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files
created by another version of Python.
The default is to write files to their PEP 3147 locations and names, which
allows byte-code files from multiple versions of Python to coexist.
It is passed to the built-in compile() function.
Accepts
also a sequence of optimization levels which lead to multiple compilations of one .py file in one call.
The argument workers specifies how many workers are used to compile files in parallel.
The default is to not
use multiple workers.
If the platform can’t use multiple workers and workers argument is given, then sequential
compilation will be used as a fallback.
If workers is 0, the number of cores in the system is used.
If workers is
lower than 0, a ValueError will be raised.
PycInvalidationMode enum and controls
how the generated pycs are invalidated at runtime.
The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above.
They may be specified as str or os.
PathLike.
If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links
to consolidate duplicate files.
Changed in version 3.2: Added the legacy and optimize parameter.
Added the workers parameter.
Changed in version 3.5: quiet parameter was changed to a multilevel value.
The legacy parameter only writes out .pyc files, not .pyo files no matter what the value
of optimize is.
Accepts a path-like object.
Changed in version 3.7: The invalidation_mode parameter was added.
Changed in version 3.7.2: The invalidation_mode parameter’s default value is updated to None.
Changed in version 3.8: Setting workers to 0 now chooses the optimal number of cores.
Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments.
Default value of
maxlevels was changed from 10 to sys.getrecursionlimit()
compileall.compile_file(fullname, ddir=None, force=False, rx=None, quiet=0, legacy=False, optimize=- 1,
invalidation_mode=None, *, stripdir=None, prependdir=None, limit_sl_dest=None,
hardlink_dupes=False)
Compile the file with path fullname.
Return a true value if the file compiled successfully, and a false value otherwise.
If ddir is given, it is prepended to the path to the file being compiled for use in compilation time tracebacks, and
is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the
source file does not exist at the time the byte-code file is executed.
If rx is given, its search method is passed the full path name to the file being compiled, and if it returns a true
value, the file is not compiled and True is returned.
This can be used to exclude files matching a regular expression,
given as a re.Pattern object.
If quiet is False or 0 (the default), the filenames and other information are printed to standard out.
Set to 1, only
errors are printed.
Set to 2, all output is suppressed.
If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files
created by another version of Python.
The default is to write files to their PEP 3147 locations and names, which
allows byte-code files from multiple versions of Python to coexist.
It is passed to the built-in compile() function.
Accepts
also a sequence of optimization levels which lead to multiple compilations of one .py file in one call.
invalidation_mode should be a member of the py_compile.
PycInvalidationMode enum and controls
how the generated pycs are invalidated at runtime.
The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above.
They may be specified as str or os.
PathLike.
If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links
to consolidate duplicate files.
New in version 3.2.
Changed in version 3.5: quiet parameter was changed to a multilevel value.
The legacy parameter only writes out .pyc files, not .pyo files no matter what the value
of optimize is.
Changed in version 3.7: The invalidation_mode parameter was added.
Changed in version 3.7.2: The invalidation_mode parameter’s default value is updated to None.
Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments.
Return a true value if all the files compiled successfully,
and a false value otherwise.
If skip_curdir is true (the default), the current directory is not included in the search.
All other parameters are
passed to the compile_dir() function.
Added the legacy and optimize parameter.
Changed in version 3.5: quiet parameter was changed to a multilevel value.
The legacy parameter only writes out .pyc files, not .pyo files no matter what the value
of optimize is.
Changed in version 3.7: The invalidation_mode parameter was added.
Changed in version 3.7.2: The invalidation_mode parameter’s default value is updated to None.
To force a recompile of all the .py files in the Lib/ subdirectory and all its subdirectories:
import compileall
compileall.compile_dir('Lib/', force=True)
# Perform same compilation, excluding files in .svn directories.
Path objects can also be used.
See also:
Module py_compile Byte-compile a single source file.
The CPython bytecode which this
module takes as an input is defined in the file Include/opcode.h and used by the compiler and the interpreter.
CPython implementation detail: Bytecode is an implementation detail of the CPython interpreter.
No guarantees are
made that bytecode will not be added, removed, or changed between versions of Python.
Use of this module should not
be considered to work across Python VMs or Python releases.
Use 2 bytes for each instruction.
Previously the number of bytes varied by instruction.
The argument of jump, exception handling and loop instructions is now the instruction offset
rather than the byte offset.
Changed in version 3.11: Some instructions are accompanied by one or more inline cache entries, which take the form
of CACHE instructions.
These instructions are hidden by default, but can be shown by passing show_caches=True
to any dis utility.
Furthermore, the interpreter now adapts the bytecode to specialize it for different runtime conditions.
The adaptive bytecode can be shown by passing adaptive=True.
The argument of a jump is the offset of the target instruction relative to the instruction that
appears immediately after the jump instruction’s CACHE entries.
As a consequence, the presence of the CACHE instructions is transparent for forward jumps but needs to be taken into
account when reasoning about backward jumps.
Example: Given the function myfunc():

the following command can be used to display the disassembly of myfunc():

0 RESUME


2 LOAD_GLOBAL
1 (NULL + len)
12 LOAD_FAST
0 (alist)
14 CALL

22 RETURN_VALUE
(The “2” is a line number).
If infile is specified, its disassembled code will be written to stdout.
Otherwise, disassembly is performed on compiled
source code recieved from stdin.
The bytecode analysis API allows pieces of Python code to be wrapped in a Bytecode object that provides easy access
to details of the compiled code.
Analyse the bytecode corresponding to a function, generator, asynchronous generator, coroutine, method, string of
source code, or a code object (as returned by compile()).
This is a convenience wrapper around many of the functions listed below, most notably get_instructions(),
as iterating over a Bytecode instance yields the bytecode operations as Instruction instances.
If first_line is not None, it indicates the line number that should be reported for the first source line in the dis-
assembled code.
Otherwise, the source line information (if any) is taken directly from the disassembled code
object.
If current_offset is not None, it refers to an instruction offset in the disassembled code.
Setting this means dis()
will display a “current instruction” marker against the specified opcode.
If show_caches is True, dis() will display inline cache entries used by the interpreter to specialize the bytecode.
If adaptive is True, dis() will display specialized bytecode that may be different from the original bytecode.
This can now handle coroutine and asynchronous generator objects.
Changed in version 3.11: Added the show_caches and adaptive parameters.
32.10.
They
can be useful if only a single operation is being performed, so the intermediate analysis object isn’t useful:
dis.code_info(x)
Return a formatted multi-line string with detailed code object information for the supplied function, generator,
asynchronous generator, coroutine, method, source code string or code object.
Note that the exact contents of code info strings are highly implementation dependent and they may change arbi-
trarily across Python VMs or Python releases.
New in version 3.2.
This can now handle coroutine and asynchronous generator objects.
This is a convenient shorthand for print(code_info(x), file=file), intended for interactive explo-
ration at the interpreter prompt.
New in version 3.2.
Changed in version 3.4: Added file parameter.
For a module,
it disassembles all functions.
For a class, it disassembles all methods (including class and static methods).
For a
code object or sequence of raw bytecode, it prints one line per bytecode instruction.
It also recursively disassembles
nested code objects.
These can include generator expressions, nested functions, the bodies of nested classes, and the
code objects used for annotation scopes.
Strings are first compiled to code objects with the compile() built-in
function before being disassembled.
If no object is provided, this function disassembles the last traceback.
The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise.
The maximal depth of recursion is limited by depth unless it is None.
If show_caches is True, this function will display inline cache entries used by the interpreter to specialize the
bytecode.
If adaptive is True, this function will display specialized bytecode that may be different from the original bytecode.
Changed in version 3.4: Added file parameter.
Changed in version 3.7: Implemented recursive disassembling and added depth parameter.
This can now handle coroutine and asynchronous generator objects.
Changed in version 3.11: Added the show_caches and adaptive parameters.
The instruction
causing the exception is indicated.
The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise.
Changed in version 3.4: Added file parameter.
Disassemble a code object, indicating the last instruction if lasti was provided.
The output is divided in the following
columns:
1.
The parameter interpretation recognizes local and global variable names, constant values, branch targets, and com-
pare operators.
The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise.
Changed in version 3.4: Added file parameter.
Changed in version 3.11: Added the show_caches and adaptive parameters.
The iterator generates a series of Instruction named tuples giving the details of each operation in the supplied
code.
If first_line is not None, it indicates the line number that should be reported for the first source line in the dis-
assembled code.
Otherwise, the source line information (if any) is taken directly from the disassembled code
object.
The show_caches and adaptive parameters work as they do in dis().
New in version 3.4.
This generator function uses the co_lines method of the code object code to find the offsets which are starts of
lines in the source code.
They are generated as (offset, lineno) pairs.
Changed in version 3.6: Line numbers can be decreasing.
Before, they were always increasing.
The PEP 626 co_lines method is used instead of the co_firstlineno and
co_lnotab attributes of the code object.
32.10.
If the code has a jump target and jump is True, stack_effect() will return the stack effect of jumping.
If
jump is False, it will return the stack effect of not jumping.
And if jump is None (default), it will return the
maximal stack effect of both cases.
New in version 3.4.
Changed in version 3.8: Added jump parameter.
Instruction
Details for a bytecode operation
opcode
numeric code for operation, corresponding to the opcode values listed below and the bytecode values in the
Opcode collections.
Positions object holding the start and end locations that are covered by this instruction.
New in version 3.4.
Changed in version 3.11: Field positions is added.
The Python compiler currently generates the following bytecode instructions.
General instructions
In the following, We will refer to the interpreter stack as STACK and describe operations on it as if it was a Python list.
The top of the stack corresponds to STACK[-1] in this language.
NOP
Do nothing code.
Removes the top-of-stack item:
STACK.pop()
END_FOR
Removes the top two values from the stack.
Equivalent to POP_TOP; POP_TOP.
Used to clean up at the end of
loops, hence the name.
New in version 3.12.
END_SEND
Implements del STACK[-2].
Used to clean up when a generator exits.
New in version 3.11.
SWAP(i)
Swap the top of the stack with the i-th element:
STACK[-i], STACK[-1] = stack[-1], STACK[-i]
New in version 3.11.
CACHE
Rather than being an actual instruction, this opcode is used to mark extra space for the interpreter to cache use-
ful data directly in the bytecode itself.
It is automatically hidden by all dis utilities, but can be viewed with
show_caches=True.
Logically, this space is part of the preceding instruction.
Many opcodes expect to be followed by an exact number
of caches, and will instruct the interpreter to skip over them at runtime.
Populated caches can look like arbitrary instructions, so great care should be taken when reading or modifying raw,
adaptive bytecode containing quickened data.
New in version 3.11.
32.10.
UNARY_NOT
Implements STACK[-1] = not STACK[-1].
UNARY_INVERT
Implements STACK[-1] = ~STACK[-1].
GET_ITER
Implements STACK[-1] = iter(STACK[-1]).
GET_YIELD_FROM_ITER
If STACK[-1] is a generator iterator or coroutine object it is left as is.
Otherwise, implements STACK[-1] =
iter(STACK[-1]).
New in version 3.5.
Binary and in-place operations
Binary operations remove the top two items from the stack (STACK[-1] and STACK[-2]).
They perform the opera-
tion, then put the result back on the stack.
Coroutine opcodes
GET_AWAITABLE(where)
Implements STACK[-1] = get_awaitable(STACK[-1]), where get_awaitable(o) returns o if
o is a coroutine object or a generator object with the CO_ITERABLE_COROUTINE flag, or resolves o.
__await__.
If the where operand is nonzero, it indicates where the instruction occurs:
• 1: After a call to __aenter__
• 2: After a call to __aexit__
New in version 3.5.
Changed in version 3.11: Previously, this instruction did not have an oparg.
GET_AITER
Implements STACK[-1] = STACK[-1].__aiter__().
New in version 3.5.
Changed in version 3.7: Returning awaitable objects from __aiter__ is no longer supported.
See
GET_AWAITABLE for details about get_awaitable.
New in version 3.5.
END_ASYNC_FOR
Terminates an async for loop.
Handles an exception raised when awaiting a next item.
The stack contains the
async iterable in STACK[-2] and the raised exception in STACK[-1].
Both are popped.
If the exception is not
StopAsyncIteration, it is re-raised.
New in version 3.8.
Changed in version 3.11: Exception representation on the stack now consist of one, not three, items.
32.10.
If STACK[-1]
is an instance of StopIteration, pop three values from the stack and push its value member.
Otherwise,
re-raise STACK[-1].
New in version 3.12.
BEFORE_ASYNC_WITH
Resolves __aenter__ and __aexit__ from STACK[-1].
New in version 3.5.
Implements:
item = STACK.pop()
set.add(STACK[-i], item)
Used to implement set comprehensions.
Implements:
item = STACK.pop()
list.append(STACK[-i], item)
Used to implement list comprehensions.
MAP_ADD(i)
Implements:
value = STACK.pop()
key = STACK.pop()
dict.__setitem__(STACK[-i], key, value)
Used to implement dict comprehensions.
New in version 3.1.
Map value is STACK[-1] and map key is STACK[-2].
Before, those were reversed.
For all of the SET_ADD, LIST_APPEND and MAP_ADD instructions, while the added value or key/value pair is popped
off, the container object remains on the stack so that it is available for further iterations of the loop.
RETURN_VALUE
Returns with STACK[-1] to the caller of the function.
Returns with co_consts[consti] to the caller of the function.
Changed in version 3.11: oparg set to be the stack depth.
Changed in version 3.12: oparg set to be the exception block depth, for efficient closing of generators.
SETUP_ANNOTATIONS
Checks whether __annotations__ is defined in locals(), if not it is set up to an empty dict.
This opcode
is only emitted if a class or module body contains variable annotations statically.
New in version 3.6.
POP_EXCEPT
Pops a value from the stack, which is used to restore the exception state.
Changed in version 3.11: Exception representation on the stack now consist of one, not three, items.
RERAISE
Re-raises the exception currently on top of the stack.
If oparg is non-zero, pops an additional value from the stack
which is used to set f_lasti of the current frame.
New in version 3.9.
Changed in version 3.11: Exception representation on the stack now consist of one, not three, items.
PUSH_EXC_INFO
Pops a value from the stack.
Pushes the current exception to the top of the stack.
Pushes the value originally popped
back to the stack.
Used in exception handlers.
New in version 3.11.
CHECK_EXC_MATCH
Performs exception matching for except.
Tests whether the STACK[-2] is an exception matching
STACK[-1].
Pops STACK[-1] and pushes the boolean result of the test.
New in version 3.11.
CHECK_EG_MATCH
Performs exception matching for except*.
Applies split(STACK[-1]) on the exception group representing
STACK[-2].
In case of a match, pops two items from the stack and pushes the non-matching subgroup (None in case of full
match) followed by the matching subgroup.
When there is no match, pops one item (the match type) and pushes
None.
New in version 3.11.
WITH_EXCEPT_START
Calls the function in position 4 on the stack with arguments (type, val, tb) representing the exception at the top of
the stack.
Used to implement the call context_manager.__exit__(*exc_info()) when an exception
has occurred in a with statement.
New in version 3.9.
The __exit__ function is in position 4 of the stack rather than 7.
Exception represen-
tation on the stack now consist of one, not three, items.
LOAD_ASSERTION_ERROR
Pushes AssertionError onto the stack.
Used by the assert statement.
New in version 3.9.
It is later called to construct a class.
BEFORE_WITH
This opcode performs several operations before a with block starts.
First, it loads __exit__() from the context
manager and pushes it onto the stack for later use by WITH_EXCEPT_START.
Then, __enter__() is called.
Finally, the result of calling the __enter__() method is pushed onto the stack.
New in version 3.11.
GET_LEN
Perform STACK.append(len(STACK[-1])).
New in version 3.10.
MATCH_MAPPING
If STACK[-1] is an instance of collections.abc.
Mapping (or, more technically:
if it has the
Py_TPFLAGS_MAPPING flag set in its tp_flags), push True onto the stack.
Otherwise, push False.
New in version 3.10.
MATCH_SEQUENCE
If
STACK[-1]
is
an
instance
of
collections.abc.
Sequence
and
is
not
an
instance
of
str/bytes/bytearray (or, more technically:
if it has the Py_TPFLAGS_SEQUENCE flag set in its
tp_flags), push True onto the stack.
Otherwise, push False.
New in version 3.10.
MATCH_KEYS
STACK[-1] is a tuple of mapping keys, and STACK[-2] is the match subject.
If STACK[-2] contains all of
the keys in STACK[-1], push a tuple containing the corresponding values.
Otherwise, push None.
New in version 3.10.
Changed in version 3.11: Previously, this instruction also pushed a boolean value indicating success (True) or
failure (False).
Implements name = STACK.pop().
The compiler tries to use STORE_FAST or STORE_GLOBAL if possible.
Implements del name, where namei is the index into co_names attribute of the code object.
UNPACK_SEQUENCE(count)
Unpacks STACK[-1] into count individual values, which are put onto the stack right-to-left.
Implements assignment with a starred target: Unpacks an iterable in STACK[-1] into individual values, where
the total number of values can be smaller than the number of items in the iterable: one of the new values will be a
list of all leftover items.
The number of values before and after the list value is limited to 255.
The number of values before the list value is encoded in the argument of the opcode.
The number of values after
the list if any is encoded using an EXTENDED_ARG.
The extracted values are put onto the stack right-to-left, i.e. a, *b, c = d will be stored after execution as
STACK.extend((a, b, c)).
Implements:
obj = STACK.pop()
del obj.name
where namei is the index of name into co_names of the code object.
STORE_GLOBAL(namei)
Works as STORE_NAME, but stores the name as a global.
DELETE_GLOBAL(namei)
Works as DELETE_NAME, but deletes a global name.
Pushes co_consts[consti] onto the stack.
LOAD_NAME(namei)
Pushes the value associated with co_names[namei] onto the stack.
The name is looked up within the locals,
then the globals, then the builtins.
LOAD_LOCALS
Pushes a reference to the locals dictionary onto the stack.
This is used to prepare namespace dictionaries for
LOAD_FROM_DICT_OR_DEREF and LOAD_FROM_DICT_OR_GLOBALS.
New in version 3.12.
LOAD_FROM_DICT_OR_GLOBALS(i)
Pops a mapping off the stack and looks up the value for co_names[namei].
If the name is not found there,
looks it up in the globals and then the builtins, similar to LOAD_GLOBAL.
This is used for loading global variables
in annotation scopes within class bodies.
Pushes a new dictionary object onto the stack.
Pops 2 * count items so that the dictionary holds count entries:
{..., STACK[-4]: STACK[-3], STACK[-2]: STACK[-1]}.
The dictionary is created from stack items instead of creating an empty dictionary pre-sized
to hold count items.
The version of BUILD_MAP specialized for constant keys.
Pops the top element on the stack which contains a
tuple of keys, then starting from STACK[-2], pops count values to form values in the built dictionary.
Concatenates count strings from the stack and pushes the resulting string onto the stack.
Implements:
seq = STACK.pop()
list.extend(STACK[-i], seq)
Used to build lists.
Implements:
seq = STACK.pop()
set.update(STACK[-i], seq)
Used to build sets.
New in version 3.9.
Implements:
map = STACK.pop()
dict.update(STACK[-i], map)
Used to build dicts.
New in version 3.9.
If the low bit of namei
is not set,
this replaces STACK[-1]
with getattr(STACK[-1],
co_names[namei>>1]).
If the low bit of namei is set, this will attempt to load a method named co_names[namei>>1] from the
STACK[-1] object.
STACK[-1] is popped.
This bytecode distinguishes two cases: if STACK[-1] has a
method with the correct name, the bytecode pushes the unbound method and STACK[-1].
STACK[-1] will be
used as the first argument (self) by CALL when calling the unbound method.
Otherwise, NULL and the object
returned by the attribute lookup are pushed.
This opcode implements super(), both in its zero-argument and two-argument forms (e.g.
super().
It pops three values from the stack (from top of stack down): - self: the first argument to the current method -
cls: the class within which the current method was defined - the global super
With respect to its argument, it works similarly to LOAD_ATTR, except that namei is shifted left by 2 bits instead
of 1.
The low bit of namei signals to attempt a method load, as with LOAD_ATTR, which results in pushing None and
the loaded method.
When it is unset a single value is pushed to the stack.
The second-low bit of namei, if set, means that this was a two-argument call to super() (unset means zero-
argument).
New in version 3.12.
COMPARE_OP(opname)
Performs a Boolean operation.
The operation name can be found in cmp_op[opname].
IS_OP(invert)
Performs is comparison, or is not if invert is 1.
Performs in comparison, or not in if invert is 1.
Imports the module co_names[namei].
STACK[-1] and STACK[-2] are popped and provide the fromlist
and level arguments of __import__().
The module object is pushed onto the stack.
The current namespace is
not affected: for a proper import statement, a subsequent STORE_FAST instruction modifies the namespace.
IMPORT_FROM(namei)
Loads the attribute co_names[namei] from the module found in STACK[-1].
Increments bytecode counter by delta.
Decrements bytecode counter by delta.
Checks for interrupts.
Decrements bytecode counter by delta.
Does not check for interrupts.
New in version 3.11.
32.10.
If STACK[-1] is true, increments the bytecode counter by delta.
STACK[-1] is popped.
The oparg is now a relative delta rather than an absolute target.
This opcode is a pseudo-
instruction, replaced in final bytecode by the directed versions (forward/backward).
If STACK[-1] is false, increments the bytecode counter by delta.
STACK[-1] is popped.
The oparg is now a relative delta rather than an absolute target.
This opcode is a pseudo-
instruction, replaced in final bytecode by the directed versions (forward/backward).
If STACK[-1] is not None, increments the bytecode counter by delta.
STACK[-1] is popped.
This opcode is a pseudo-instruction, replaced in final bytecode by the directed versions (forward/backward).
New in version 3.11.
If STACK[-1] is None, increments the bytecode counter by delta.
STACK[-1] is popped.
This opcode is a pseudo-instruction, replaced in final bytecode by the directed versions (forward/backward).
New in version 3.11.
STACK[-1] is an iterator.
Call its __next__() method.
If this yields a new value, push it on the stack (leaving
the iterator below it).
If the iterator indicates it is exhausted then the byte code counter is incremented by delta.
Changed in version 3.12: Up until 3.11 the iterator was popped when it was exhausted.
LOAD_GLOBAL(namei)
Loads the global named co_names[namei>>1] onto the stack.
If the low bit of namei is set, then a NULL is pushed to the stack before the global
variable.
Pushes a reference to the local co_varnames[var_num] onto the stack.
Changed in version 3.12: This opcode is now only used in situations where the local variable is guaranteed to be
initialized.
Pushes a reference to the local co_varnames[var_num] onto the stack, raising an UnboundLocalError
if the local variable has not been initialized.
New in version 3.12.
LOAD_FAST_AND_CLEAR(var_num)
Pushes a reference to the local co_varnames[var_num] onto the stack (or pushes NULL onto the stack if the
local variable has not been initialized) and sets co_varnames[var_num] to NULL.
New in version 3.11.
Pushes a reference to the cell contained in slot i of the “fast locals” storage.
The name of the variable is
co_fastlocalnames[i].
Note that LOAD_CLOSURE is effectively an alias for LOAD_FAST.
It exists to keep bytecode a little more readable.
Changed in version 3.11: i is no longer offset by the length of co_varnames.
Loads the cell contained in slot i of the “fast locals” storage.
Pushes a reference to the object the cell contains on
the stack.
Pops a mapping off the stack and looks up the name associated with slot i of the “fast locals” storage in this
mapping.
If the name is not found there, loads it from the cell contained in slot i, similar to LOAD_DEREF.
This
is used for loading free variables in class bodies (which previously used LOAD_CLASSDEREF) and in annotation
scopes within class bodies.
New in version 3.12.
STORE_DEREF(i)
Stores STACK.pop() into the cell contained in slot i of the “fast locals” storage.
Empties the cell contained in slot i of the “fast locals” storage.
Used by the del statement.
New in version 3.2.
Copies the n free variables from the closure into the frame.
Removes the need for special code on the caller’s side
when calling closures.
New in version 3.11.
RAISE_VARARGS(argc)
Raises an exception using one of the 3 forms of the raise statement, depending on the value of argc:
• 0: raise (re-raise previous exception)
• 1: raise STACK[-1] (raise exception instance or type at STACK[-1])
• 2: raise STACK[-2] from STACK[-1] (raise exception instance or type at STACK[-2] with
__cause__ set to STACK[-1])
32.10.
The remaining positional arguments
• The named arguments
argc is the total of the positional and named arguments, excluding self when a NULL is not present.
CALL pops all arguments and the callable object off the stack, calls the callable object with those arguments, and
pushes the return value returned by the callable object.
New in version 3.11.
CALL_FUNCTION_EX(flags)
Calls a callable object with variable set of positional and keyword arguments.
If the lowest bit of flags is set, the
top of the stack contains a mapping object containing additional keyword arguments.
Before the callable is called,
the mapping object and iterable object are each “unpacked” and their contents passed in as keyword and positional
arguments respectively.
New in version 3.6.
PUSH_NULL
Pushes a NULL to the stack.
Used in the call sequence to match the NULL pushed by LOAD_METHOD for non-
method calls.
Prefixes CALL.
Stores a reference to co_consts[consti] into an internal variable for use by CALL.
New in version 3.11.
MAKE_FUNCTION(flags)
Pushes a new function object on the stack.
BUILD_SLICE(argc)
Pushes a slice object on the stack.
See the slice() built-in function for more information.
Prefixes any opcode which has an argument too big to fit into the default one byte.
For each opcode, at most three prefixal EXTENDED_ARG are allowed,
forming an argument from two-byte to four-byte.
FORMAT_VALUE(flags)
Used for implementing formatted literal strings (f-strings).
Pops an optional fmt_spec from the stack, then a re-
quired value.
Formatting is performed using PyObject_Format().
The result is pushed on the stack.
New in version 3.6.
STACK[-1] is a tuple of keyword attribute names, STACK[-2] is the class being matched against, and
STACK[-3] is the match subject.
Pop STACK[-1], STACK[-2], and STACK[-3].
If STACK[-3] is an instance of STACK[-2] and has
the positional and keyword attributes required by count and STACK[-1], push a tuple of extracted attributes.
Otherwise, push None.
New in version 3.10.
Changed in version 3.11: Previously, this instruction also pushed a boolean value indicating success (True) or
failure (False).
A no-op.
Performs internal tracing, debugging and optimization checks.
The start of a function, which is neither a generator, coroutine nor an async generator
32.10.
Used as first opcode of in code object
for the above mentioned callables.
Clear the current frame and return the newly created generator.
New in version 3.11.
SEND(delta)
Equivalent to STACK[-1] = STACK[-2].send(STACK[-1]).
Used in yield from and await state-
ments.
If the call raises StopIteration, pop the top value from the stack, push the exception’s value attribute, and
increment the bytecode counter by delta.
New in version 3.11.
HAVE_ARGUMENT
This is not really an opcode.
It identifies the dividing line between opcodes in the range [0,255] which don’t use
their argument and those that do (< HAVE_ARGUMENT and >= HAVE_ARGUMENT, respectively).
If your application uses pseudo instructions, use the hasarg collection instead.
Now every instruction has an argument, but opcodes < HAVE_ARGUMENT ignore it.
Before, only opcodes >= HAVE_ARGUMENT had an argument.
Changed in version 3.12: Pseudo instructions were added to the dis module, and for them it is not true that
comparison with HAVE_ARGUMENT indicates whether they use their arg.
CALL_INTRINSIC_1
Calls an intrinsic function with one argument.
Passes STACK[-1] as the argument and sets STACK[-1] to the
result.
Used to implement functionality that is not performance critical.
The operand determines which intrinsic function is called:
Operand
Description
INTRINSIC_1_INVALIDNot valid
INTRINSIC_PRINT
Prints the argument to standard out.
Used in the REPL.
INTRINSIC_IMPORT_STAR
Performs import * for the named module.
INTRINSIC_STOPITERATION_ERROR
Extracts the return value from a StopIteration exception.
INTRINSIC_ASYNC_GEN_WRAP
Wraps an aync generator value
INTRINSIC_UNARY_POSITIVE
Performs the unary + operation
INTRINSIC_LIST_TO_TUPLE
Converts a list to a tuple
INTRINSIC_TYPEVAR Creates a typing.
TypeVar
INTRINSIC_PARAMSPECCreates a typing.
ParamSpec
INTRINSIC_TYPEVARTUPLE
Creates a typing.
TypeVarTuple
INTRINSIC_SUBSCRIPT_GENERIC
Returns typing.
Generic subscripted with the argument
INTRINSIC_TYPEALIASCreates a typing.
TypeAliasType; used in the type statement.
The argu-
ment is a tuple of the type alias’s name, type parameters, and value.
The operand determines which intrinsic function is called:
Operand
Description
INTRINSIC_2_INVALID
Not valid
INTRINSIC_PREP_RERAISE_STAR
Calculates
the
ExceptionGroup
to
raise
from
a
try-except*.
INTRINSIC_TYPEVAR_WITH_BOUND
Creates a typing.
TypeVar with a bound.
INTRINSIC_TYPEVAR_WITH_CONSTRAINTS
Creates a typing.
TypeVar with constraints.
INTRINSIC_SET_FUNCTION_TYPE_PARAMS
Sets the __type_params__ attribute of a function.
New in version 3.12.
Pseudo-instructions
These opcodes do not appear in Python bytecode.
They are used by the compiler but are replaced by real opcodes or
removed before bytecode is generated.
SETUP_FINALLY(target)
Set up an exception handler for the following code block.
If an exception occurs, the value stack level is restored
to its current state and control is transferred to the exception handler at target.
Like SETUP_FINALLY, but in case of an exception also pushes the last instruction (lasti) to the stack so that
RERAISE can restore it.
If an exception occurs, the value stack level and the last instruction on the frame are
restored to their current state, and control is transferred to the exception handler at target.
SETUP_WITH(target)
Like SETUP_CLEANUP, but in case of an exception one more item is popped from the stack before control is
transferred to the exception handler at target.
This variant is used in with and async with constructs, which push the return value of the context manager’s
__enter__() or __aenter__() to the stack.
POP_BLOCK
Marks the end of the code block associated with the last SETUP_FINALLY, SETUP_CLEANUP or
SETUP_WITH.
JUMP
JUMP_NO_INTERRUPT
Undirected relative jump instructions which are replaced by their directed (forward/backward) counterparts by the
assembler.
LOAD_METHOD
Optimized unbound method lookup.
Emitted as a LOAD_ATTR opcode with a flag set in the arg.
32.10.
The collections now contain pseudo instructions and instrumented instructions as well.
These
are opcodes with values >= MIN_PSEUDO_OPCODE and >= MIN_INSTRUMENTED_OPCODE.
New in version 3.12.
dis.hasconst
Sequence of bytecodes that access a constant.
It does not include
references to global or builtin scopes.
dis.hasname
Sequence of bytecodes that access an attribute by name.
The contents of this module are useful
for Python core developers who are working on the pickle; ordinary users of the pickle module probably won’t find
the pickletools module relevant.
When invoked from the command line, python -m pickletools will disassemble the contents of one or more
pickle files.
Note that if you want to see the Python object stored in the pickle rather than the details of pickle format, you
may want to use -m pickle instead.
However, when the pickle file that you want to examine comes from an untrusted
source, -m pickletools is a safer option because it does not execute pickle bytecode.
For example, with a tuple (1, 2) pickled in file x.pickle:
$ python -m pickle x.pickle
(1, 2)
$ python -m pickletools x.pickle
0: \x80 PROTO

2: K
BININT1

4: K
BININT1

6: \x86 TUPLE2
7: q
BINPUT

9: .
STOP
highest protocol among opcodes = 2
Command line options
-a, --annotate
Annotate each line with a short opcode description.
The number of blanks by which to indent a new MARK level.
When multiple objects are disassembled, preserve memo between disassemblies.
When more than one pickle file are specified, print given preamble before each disassembly.
32.11.
Successive levels, indicated
by MARK opcodes in the stream, are indented by indentlevel spaces.
If a nonzero value is given to annotate, each
opcode in the output is annotated with a short description.
The value of annotate is used as a hint for the column
where annotation should start.
The annotate argument.
pickletools.genops(pickle)
Provides an iterator over all of the opcodes in a pickle, returning a sequence of (opcode, arg, pos) triples.
opcode is an instance of an OpcodeInfo class; arg is the decoded value, as a Python object, of the opcode’s
argument; pos is the position at which this opcode is located.
Returns a new equivalent pickle string after eliminating unused PUT opcodes.
The optimized pickle is shorter,
takes less transmission time, requires less storage space, and unpickles more efficiently.
This chapter describes modules that are only available on MS Windows platforms.
Some higher-level modules use these
functions to build the Windows implementations of their services.
For example, the getpass module uses this in the
implementation of the getpass() function.
Further documentation on these functions can be found in the Platform API documentation.
The module implements both the normal and wide char variants of the console I/O api.
The normal API deals only with
ASCII characters and is of limited use for internationalized applications.
The wide char API should be used where ever
possible.
Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.
Lock part of a file based on file descriptor fd from the C runtime.
Raises OSError on failure.
The locked region
of the file extends from the current file position for nbytes bytes, and may continue beyond the end of the file.
Multiple regions in a file may be locked at the same time, but may
not overlap.
Adjacent regions are not merged; they must be unlocked individually.
Raises an auditing event msvcrt.locking with arguments fd, mode, nbytes.
LK_LOCK
msvcrt.
LK_RLCK
Locks the specified bytes.
If the bytes cannot be locked, the program immediately tries again after 1 second.
If,
after 10 attempts, the bytes cannot be locked, OSError is raised.
LK_NBLCK
msvcrt.
LK_NBRLCK
Locks the specified bytes.
If the bytes cannot be locked, OSError is raised.
LK_UNLCK
Unlocks the specified bytes, which must have been previously locked.
To set it to text mode, flags should be os.O_TEXT;
for binary, it should be os.
O_BINARY.
msvcrt.open_osfhandle(handle, flags)
Create a C runtime file descriptor from the file handle handle.
The flags parameter should be a bitwise OR of
os.
O_APPEND, os.O_RDONLY, and os.O_TEXT.
The returned file descriptor may be used as a parameter to
os.fdopen() to create a file object.
Raises an auditing event msvcrt.open_osfhandle with arguments handle, flags.
Raises OSError if fd is not recognized.
Raises an auditing event msvcrt.get_osfhandle with argument fd.
Console I/O
msvcrt.kbhit()
Return True if a keypress is waiting to be read.
msvcrt.getch()
Read a keypress and return the resulting character as a byte string.
Nothing is echoed to the console.
This call will
block if a keypress is not already available, but will not wait for Enter to be pressed.
If the pressed key was a
special function key, this will return '\000' or '\xe0'; the next call will return the keycode.
The Control-C
keypress cannot be read with this function.
Print the byte string char to the console without buffering.
Force the malloc() heap to clean itself up and return unused blocks to the operating system.
On failure, this
raises OSError.
Instead of using an integer as the registry handle, a handle
object is used to ensure that the handles are closed correctly, even if the programmer neglects to explicitly close them.
Changed in version 3.3: Several functions in this module used to raise a WindowsError, which is now an alias of
OSError.
Closes a previously opened registry key.
The hkey argument specifies a previously opened key.
If hkey is not closed using this method (or via hkey.
Close()), it is closed when the hkey object is
destroyed by Python.
ConnectRegistry(computer_name, key)
Establishes a connection to a predefined registry handle on another computer, and returns a handle object.
If None, the local
computer is used.
key is the predefined handle to connect to.
The return value is the handle of the opened key.
If the function fails, an OSError exception is raised.
Raises an auditing event winreg.
ConnectRegistry with arguments computer_name, key.
Changed in version 3.3: See above.
winreg.
CreateKey(key, sub_key)
Creates or opens the specified key, returning a handle object.
If key is one of the predefined keys, sub_key may be None.
In that case, the handle returned is the same key handle
passed in to the function.
If the key already exists, this function opens the existing key.
The return value is the handle of the opened key.
If the function fails, an OSError exception is raised.
Raises an auditing event winreg.
CreateKey with arguments key, sub_key, access.
Raises an auditing event winreg.
OpenKey/result with argument key.
CreateKeyEx(key, sub_key, reserved=0, access=KEY_WRITE)
Creates or opens the specified key, returning a handle object.
The default is zero.
Default is
KEY_WRITE.
See Access Rights for other allowed values.
If key is one of the predefined keys, sub_key may be None.
In that case, the handle returned is the same key handle
passed in to the function.
If the key already exists, this function opens the existing key.
The return value is the handle of the opened key.
If the function fails, an OSError exception is raised.
Raises an auditing event winreg.
CreateKey with arguments key, sub_key, access.
Raises an auditing event winreg.
OpenKey/result with argument key.
New in version 3.2.
Changed in version 3.3: See above.
winreg.
DeleteKey(key, sub_key)
Deletes the specified key.
This value must not be None,
and the key may not have subkeys.
This method can not delete keys with subkeys.
If the method succeeds, the entire key, including all of its values, is removed.
If the method fails, an OSError
exception is raised.
Raises an auditing event winreg.
DeleteKey with arguments key, sub_key, access.
Changed in version 3.3: See above.
winreg.
DeleteKeyEx(key, sub_key, access=KEY_WOW64_64KEY, reserved=0)
Deletes the specified key.
This value must not be None,
and the key may not have subkeys.
The default is zero.
Default
is KEY_WOW64_64KEY.
On 32-bit Windows, the WOW64 constants are ignored.
See Access Rights for other
allowed values.
This method can not delete keys with subkeys.
If the method succeeds, the entire key, including all of its values, is removed.
If the method fails, an OSError
exception is raised.
DeleteKey with arguments key, sub_key, access.
New in version 3.2.
Changed in version 3.3: See above.
winreg.
Removes a named value from a registry key.
Raises an auditing event winreg.
DeleteValue with arguments key, value.
winreg.
EnumKey(key, index)
Enumerates subkeys of an open registry key, returning a string.
The function retrieves the name of one subkey each time it is called.
It is typically called repeatedly until an
OSError exception is raised, indicating, no more values are available.
Raises an auditing event winreg.
EnumKey with arguments key, index.
Changed in version 3.3: See above.
winreg.
EnumValue(key, index)
Enumerates values of an open registry key, returning a tuple.
The function retrieves the name of one subkey each time it is called.
It is typically called repeatedly, until an
OSError exception is raised, indicating no more values.
Raises an auditing event winreg.
EnumValue with arguments key, index.
Changed in version 3.3: See above.
winreg.
Expands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ:
'C:\\Windows'
Raises an auditing event winreg.
ExpandEnvironmentStrings with argument str.
winreg.
FlushKey(key)
Writes all the attributes of a key to the registry.
It is not necessary to call FlushKey() to change a key.
Registry changes are flushed to disk by the registry
using its lazy flusher.
Registry changes are also flushed to disk at system shutdown.
Unlike CloseKey(), the
FlushKey() method returns only when all the data has been written to the registry.
An application should only
call FlushKey() if it requires absolute certainty that registry changes are on disk.
Note: If you don’t know whether a FlushKey() call is required, it probably isn’t.
LoadKey(key, sub_key, file_name)
Creates a subkey under the specified key and stores registration information from a specified file into that subkey.
This file must have been created with the SaveKey()
function.
Under the file allocation table (FAT) file system, the filename may not have an extension.
A call to LoadKey() fails if the calling process does not have the SE_RESTORE_PRIVILEGE privilege.
Note
that privileges are different from permissions – see the RegLoadKey documentation for more details.
If key is a handle returned by ConnectRegistry(), then the path specified in file_name is relative to the
remote computer.
Raises an auditing event winreg.
LoadKey with arguments key, sub_key, file_name.
winreg.
OpenKey(key, sub_key, reserved=0, access=KEY_READ)
winreg.OpenKeyEx(key, sub_key, reserved=0, access=KEY_READ)
Opens the specified key, returning a handle object.
The default is zero.
Default is
KEY_READ.
See Access Rights for other allowed values.
The result is a new handle to the specified key.
If the function fails, OSError is raised.
Raises an auditing event winreg.
OpenKey with arguments key, sub_key, access.
Raises an auditing event winreg.
OpenKey/result with argument key.
Changed in version 3.2: Allow the use of named arguments.
Changed in version 3.3: See above.
winreg.
Returns information about a key, as a tuple.
In-
dex
Meaning

An integer giving the number of sub keys this key has.

An integer giving the number of values this key has.

An integer giving when the key was last modified (if available) as 100’s of nanoseconds since Jan 1,
1601.
Raises an auditing event winreg.
QueryInfoKey with argument key.
winreg.
QueryValue(key, sub_key)
Retrieves the unnamed value for a key, as a string.
If this parameter is None
or empty, the function retrieves the value set by the SetValue() method for the key identified by key.
Values in the registry have name, type, and data components.
This method retrieves the data for a key’s first value
that has a NULL name.
But the underlying API call doesn’t return the type, so always use QueryValueEx() if
possible.
Raises an auditing event winreg.
QueryValue with arguments key, sub_key, value_name.
winreg.
QueryValueEx(key, value_name)
Retrieves the type and data for a specified value name associated with an open registry key.
The result is a tuple of 2 items:
Index
Meaning

The value of the registry item.
Raises an auditing event winreg.
QueryValue with arguments key, sub_key, value_name.
winreg.
Saves the specified key, and all its subkeys to the specified file.
This file cannot already exist.
If this filename includes an
extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method.
If key represents a key on a remote computer, the path described by file_name is relative to the remote computer.
The caller of this method must possess the SeBackupPrivilege security privilege.
Note that privileges are different
than permissions – see the Conflicts Between User Rights and Permissions documentation for more details.
This function passes NULL for security_attributes to the API.
Raises an auditing event winreg.
SaveKey with arguments key, file_name.
winreg.
SetValue(key, sub_key, type, value)
Associates a value with a specified key.
Currently this must be REG_SZ, meaning only strings are
supported.
Use the SetValueEx() function for support for other data types.
If the key specified by the sub_key parameter does not exist, the SetValue function creates it.
Value lengths are limited by available memory.
Long values (more than 2048 bytes) should be stored as files with
the filenames stored in the configuration registry.
This helps the registry perform efficiently.
The key identified by the key parameter must have been opened with KEY_SET_VALUE access.
Raises an auditing event winreg.
SetValue with arguments key, sub_key, type, value.
winreg.
Stores data in the value field of an open registry key.
See Value Types for the available types.
This method can also set additional value and type information for the specified key.
The key identified by the key
parameter must have been opened with KEY_SET_VALUE access.
To open the key, use the CreateKey() or OpenKey() methods.
Value lengths are limited by available memory.
Long values (more than 2048 bytes) should be stored as files with
the filenames stored in the configuration registry.
This helps the registry perform efficiently.
Raises an auditing event winreg.
SetValue with arguments key, sub_key, type, value.
winreg.DisableReflectionKey(key)
Disables registry reflection for 32-bit processes running on a 64-bit operating system.
Will generally raise NotImplementedError if executed on a 32-bit operating system.
If the key is not on the reflection list, the function succeeds but has no effect.
Disabling reflection for a key does
not affect reflection of any subkeys.
Raises an auditing event winreg.
Restores registry reflection for the specified disabled key.
key is an already open key, or one of the predefined HKEY_* constants.
Will generally raise NotImplementedError if executed on a 32-bit operating system.
Restoring reflection for a key does not affect reflection of any subkeys.
Raises an auditing event winreg.
EnableReflectionKey with argument key.
winreg.
Determines the reflection state for the specified key.
Raises an auditing event winreg.
QueryReflectionKey with argument key.
Constants
winreg.
HKEY_CLASSES_ROOT
Registry entries subordinate to this key define types (or classes) of documents and the properties associated with
those types.
Shell and COM applications use the information stored under this key.
winreg.
HKEY_CURRENT_USER
Registry entries subordinate to this key define the preferences of the current user.
These preferences include the
settings of environment variables, data about program groups, colors, printers, network connections, and application
preferences.
winreg.
HKEY_LOCAL_MACHINE
Registry entries subordinate to this key define the physical state of the computer, including data about the bus type,
system memory, and installed hardware and software.
winreg.
HKEY_USERS
Registry entries subordinate to this key define the default user configuration for new users on the local computer
and the user configuration for the current user.
HKEY_PERFORMANCE_DATA
Registry entries subordinate to this key allow you to access performance data.
The data is not actually stored in the
registry; the registry functions cause the system to collect the data from its source.
winreg.
HKEY_CURRENT_CONFIG
Contains information about the current hardware profile of the local computer system.
winreg.
This key is not used in versions of Windows after 98.
Access Rights
For more information, see Registry Key Security and Access.
winreg.
KEY_ALL_ACCESS
Combines
the
STANDARD_RIGHTS_REQUIRED,
KEY_QUERY_VALUE,
KEY_SET_VALUE,
KEY_CREATE_SUB_KEY, KEY_ENUMERATE_SUB_KEYS, KEY_NOTIFY, and KEY_CREATE_LINK
access rights.
winreg.
KEY_WRITE
Combines the STANDARD_RIGHTS_WRITE, KEY_SET_VALUE, and KEY_CREATE_SUB_KEY access
rights.
winreg.
KEY_READ
Combines the STANDARD_RIGHTS_READ, KEY_QUERY_VALUE, KEY_ENUMERATE_SUB_KEYS, and
KEY_NOTIFY values.
KEY_EXECUTE
Equivalent to KEY_READ.
winreg.
KEY_QUERY_VALUE
Required to query the values of a registry key.
winreg.
KEY_SET_VALUE
Required to create, delete, or set a registry value.
winreg.
KEY_CREATE_SUB_KEY
Required to create a subkey of a registry key.
winreg.
KEY_ENUMERATE_SUB_KEYS
Required to enumerate the subkeys of a registry key.
winreg.
KEY_NOTIFY
Required to request change notifications for a registry key or for subkeys of a registry key.
winreg.
KEY_CREATE_LINK
Reserved for system use.
KEY_WOW64_64KEY
Indicates that an application on 64-bit Windows should operate on the 64-bit registry view.
On 32-bit Windows,
this constant is ignored.
winreg.
KEY_WOW64_32KEY
Indicates that an application on 64-bit Windows should operate on the 32-bit registry view.
On 32-bit Windows,
this constant is ignored.
Value Types
For more information, see Registry Value Types.
winreg.
REG_BINARY
Binary data in any form.
REG_DWORD
32-bit number.
winreg.
REG_DWORD_LITTLE_ENDIAN
A 32-bit number in little-endian format.
Equivalent to REG_DWORD.
winreg.
A 32-bit number in big-endian format.
winreg.
REG_EXPAND_SZ
Null-terminated string containing references to environment variables (%PATH%).
REG_LINK
A Unicode symbolic link.
REG_MULTI_SZ
A sequence of null-terminated strings, terminated by two null characters.
REG_NONE
No defined value type.
winreg.
REG_QWORD
A 64-bit number.
New in version 3.6.
winreg.
REG_QWORD_LITTLE_ENDIAN
A 64-bit number in little-endian format.
Equivalent to REG_QWORD.
New in version 3.6.
winreg.
REG_RESOURCE_LIST
A device-driver resource list.
winreg.
REG_FULL_RESOURCE_DESCRIPTOR
A hardware setting.
winreg.
REG_RESOURCE_REQUIREMENTS_LIST
A hardware resource list.
REG_SZ
A null-terminated string.
To guarantee cleanup,
you can call either the Close() method on the object, or the CloseKey() function.
All registry functions in this module return one of these objects.
All registry functions in this module which accept a handle object also accept an integer, however, use of the handle object
is encouraged.
Yes if the handle is currently valid (has not been closed or detached).
The object also support comparison semantics, so handle objects will compare true if they both reference the same
underlying Windows handle value.
Handle objects can be converted to an integer (e.g., using the built-in int() function), in which case the underlying
Windows handle value is returned.
You can also use the Detach() method to return the integer handle, and also
disconnect the Windows handle from the handle object.
PyHKEY.Close()
Closes the underlying Windows handle.
If the handle is already closed, no error is raised.
The result is an integer that holds the value of the handle before it is detached.
If the handle is already detached or
closed, this will return zero.
After calling this function, the handle is effectively invalidated, but the handle is not closed.
You would call this
function when you need the underlying Win32 handle to exist beyond the lifetime of the handle object.
Raises an auditing event winreg.
The HKEY object implements __enter__() and __exit__() and thus supports the context protocol for the
with statement:
with OpenKey(HKEY_LOCAL_MACHINE, "foo") as key:

# work with key
will automatically close key when control leaves the with block.
It in-
cludes functions and several constants.
Beep the PC’s speaker.
The frequency parameter specifies frequency, in hertz, of the sound, and must be in the
range 37 through 32,767.
The duration parameter specifies the number of milliseconds the sound should last.
If
the system is not able to beep the speaker, RuntimeError is raised.
PlaySound(sound, flags)
Call the underlying PlaySound() function from the Platform API.
The sound parameter may be a filename,
a system sound alias, audio data as a bytes-like object, or None.
Its interpretation depends on the value of flags,
which can be a bitwise ORed combination of the constants described below.
If the sound parameter is None, any
currently playing waveform sound is stopped.
If the system indicates an error, RuntimeError is raised.
MessageBeep(type=MB_OK)
Call the underlying MessageBeep() function from the Platform API.
This plays a sound as specified in
the registry.
The type argument specifies which sound to play; possible values are -1, MB_ICONASTERISK,
MB_ICONEXCLAMATION, MB_ICONHAND, MB_ICONQUESTION, and MB_OK, all described below.
The value
-1 produces a “simple beep”; this is the final fallback if a sound cannot be played otherwise.
If the system indicates
an error, RuntimeError is raised.
SND_FILENAME
The sound parameter is the name of a WAV file.
Do not use with SND_ALIAS.
winsound.
SND_ALIAS
The sound parameter is a sound association name from the registry.
If the registry contains no such name, play
the system default sound unless SND_NODEFAULT is also specified.
If no default sound is registered, raise
RuntimeError.
Do not use with SND_FILENAME.
For example:
import winsound
# Play Windows exit sound.
winsound.
PlaySound("SystemExit", winsound.
SND_ALIAS)
# Probably play Windows default sound, if any is registered (because
# "*" probably isn't the registered name of any sound).
PlaySound("*", winsound.
SND_ALIAS)
winsound.
SND_LOOP
Play the sound repeatedly.
The SND_ASYNC flag must also be used to avoid blocking.
Cannot be used with
SND_MEMORY.
winsound.
SND_MEMORY
The sound parameter to PlaySound() is a memory image of a WAV file, as a bytes-like object.
Note: This module does not support playing from a memory image asynchronously, so a combination of this flag
and SND_ASYNC will raise RuntimeError.
winsound.
SND_PURGE
Stop playing all instances of the specified sound.
Note: This flag is not supported on modern Windows platforms.
SND_ASYNC
Return immediately, allowing sounds to play asynchronously.
SND_NODEFAULT
If the specified sound cannot be found, do not play the system default sound.
winsound.
SND_NOSTOP
Do not interrupt sounds currently playing.
SND_NOWAIT
Return immediately if the sound driver is busy.
Note: This flag is not supported on modern Windows platforms.
MB_ICONASTERISK
Play the SystemDefault sound.
winsound.
MB_ICONEXCLAMATION
Play the SystemExclamation sound.
33.3.
MB_ICONHAND
Play the SystemHand sound.
winsound.
MB_ICONQUESTION
Play the SystemQuestion sound.
winsound.
MB_OK
Play the SystemDefault sound.
The modules described in this chapter provide interfaces to features that are unique to the Unix operating system, or in
some cases to some or many variants of it.
Here’s an overview:
34.1 posix — The most common POSIX system calls
This module provides access to operating system functionality that is standardized by the C Standard and the POSIX
standard (a thinly disguised Unix interface).
Availability: Unix.
Do not import this module directly.
Instead, import the module os, which provides a portable version of this interface.
On Unix, the os module provides a superset of the posix interface.
On non-Unix operating systems the posix module
is not available, but a subset is always available through the os interface.
Once os is imported, there is no performance
penalty in using it instead of posix.
In addition, os provides some additional functionality, such as automatically calling
putenv() when an entry in os.environ is changed.
Errors are reported as exceptions; the usual exceptions are given for type errors, while errors reported by the system calls
raise OSError.
Large File Support
Several operating systems (including AIX and Solaris) provide support for files that are larger than 2 GiB from a C
programming model where int and long are 32-bit values.
This is typically accomplished by defining the relevant size
and offset types as 64-bit values.
Such files are sometimes referred to as large files.
Large file support is enabled in Python when the size of an off_t is larger than a long and the long long is at least
as large as an off_t.
It may be necessary to configure and compile Python with certain compiler flags to enable this
mode.
In addition to many functions described in the os module documentation, posix defines the following data item:
posix.environ
A dictionary representing the string environment at the time the interpreter was started.
Keys and values are bytes
on Unix and str on Windows.
For example, environ[b'HOME'] (environ['HOME'] on Windows) is the
pathname of your home directory, equivalent to getenv("HOME") in C.
Modifying this dictionary does not affect the string environment passed on by execv(), popen() or
system(); if you need to change the environment, pass environ to execve() or add variable assignments
and export statements to the command string for system() or popen().
Changed in version 3.2: On Unix, keys and values are bytes.
Note:
The os module provides an alternate implementation of environ which updates the environment on
modification.
Note also that updating os.environ will render this dictionary obsolete.
Use of the os module
version of this is recommended over direct access to the posix module.
It is available on all Unix versions.
Availability: Unix, not Emscripten, not WASI.
Password database entries are reported as a tuple-like object, whose attributes correspond to the members of the passwd
structure (Attribute field below, see <pwd.h>):
Index
Attribute
Meaning

pw_name
Login name

pw_passwd
Optional encrypted password

pw_uid
Numerical user ID

pw_gid
Numerical group ID

pw_gecos
User name or comment field

pw_dir
User home directory

pw_shell
User command interpreter
The uid and gid items are integers, all others are strings.
KeyError is raised if the entry asked for cannot be found.
Note: In traditional Unix the field pw_passwd usually contains a password encrypted with a DES derived algorithm (see
module crypt).
However most modern unices use a so-called shadow password system.
On those unices the pw_passwd
field only contains an asterisk ('*') or the letter 'x' where the encrypted password is stored in a file /etc/shadow
which is not world readable.
Whether the pw_passwd field contains anything useful is system-dependent.
If available, the
spwd module should be used where access to the encrypted password is required.
See also:
Module grp An interface to the group database, similar to this.
Module spwd An interface to the shadow password database, similar to this.
It is available on all Unix versions.
Availability: Unix, not Emscripten, not WASI.
Group database entries are reported as a tuple-like object, whose attributes correspond to the members of the group
structure (Attribute field below, see <grp.h>):
Index
Attribute
Meaning

gr_name
the name of the group

gr_passwd
the (encrypted) group password; often empty

gr_gid
the numerical group ID

gr_mem
all the group member’s user names
The gid is an integer, name and password are strings, and the member list is a list of strings.
Check both databases to get
complete membership information.
It defines the following items:
grp.getgrgid(id)
Return the group database entry for the given numeric group ID.
KeyError is raised if the entry asked for cannot
be found.
TypeError is raised for non-integer arguments like floats or strings.
KeyError is raised if the entry asked for cannot be
found.
See also:
Module pwd An interface to the user database, similar to this.
Module spwd An interface to the shadow password database, similar to this.
Unix manual page.
It is only available for those Unix versions that support POSIX termios style tty I/O
control configured during installation.
Availability: Unix.
All functions in this module take a file descriptor fd as their first argument.
This can be an integer file descriptor, such as
returned by sys.stdin.fileno(), or a file object, such as sys.stdin itself.
This module also defines all the constants needed to work with the functions provided here; these have the same name as
their counterparts in C. Please refer to your system documentation for more information on using these terminal control
interfaces.
The interpre-
tation of the flags and the speeds as well as the indexing in the cc array must be done using the symbolic constants
defined in the termios module.
The when argument determines when the attributes are changed:
termios.
TCSANOW
Change attributes immediately.
termios.
TCSADRAIN
Change attributes after transmitting all queued output.
termios.
TCSAFLUSH
Change attributes after transmitting all queued output and discarding all queued input.
A zero duration sends a break for 0.25–0.5 seconds; a nonzero duration has a
system dependent meaning.
The queue selector specifies which queue: TCIFLUSH for the input
queue, TCOFLUSH for the output queue, or TCIOFLUSH for both queues.
The action argument can be TCOOFF to suspend output,
TCOON to restart output, TCIOFF to suspend input, or TCION to restart input.
termios.tcgetwinsize(fd)
Return a tuple (ws_row, ws_col) containing the tty window size for file descriptor fd.
Requires termios.
TIOCGWINSZ or termios.
TIOCGSIZE.
Requires at least one of the pairs (termios.
TIOCGWINSZ,
termios.
TIOCSWINSZ); (termios.
TIOCGSIZE, termios.
TIOCSSIZE) to be defined.
New in version 3.11.
See also:
Module tty Convenience functions for common terminal control operations.
Note the technique using a separate
tcgetattr() call and a try … finally statement to ensure that the old tty attributes are restored exactly no
matter what happens:

fd = sys.stdin.fileno()
old = termios.tcgetattr(fd)
new = termios.tcgetattr(fd)
new[3] = new[3] & ~termios.
ECHO
# lflags
try:
termios.tcsetattr(fd, termios.
TCSADRAIN, new)
passwd = input(prompt)
finally:
termios.tcsetattr(fd, termios.
TCSADRAIN, old)

34.5 tty — Terminal control functions
Source code: Lib/tty.py
The tty module defines functions for putting the tty into cbreak and raw modes.
Availability: Unix.
Because it requires the termios module, it will work only on Unix.
The tty module defines the following functions:
tty.cfmakeraw(mode)
Convert the tty attribute list mode, which is a list like the one returned by termios.tcgetattr(), to that of
a tty in raw mode.
New in version 3.12.
tty.cfmakecbreak(mode)
Convert the tty attribute list mode, which is a list like the one returned by termios.tcgetattr(), to that of
a tty in cbreak mode.
New in version 3.12.
TCSAFLUSH)
Change the mode of the file descriptor fd to raw.
If when is omitted, it defaults to termios.
TCSAFLUSH, and
is passed to termios.tcsetattr().
The return value of termios.tcgetattr() is saved before setting
fd to raw mode; this value is returned.
Changed in version 3.12: The return value is now the original tty attributes, instead of None.
tty.setcbreak(fd, when=termios.
TCSAFLUSH)
Change the mode of file descriptor fd to cbreak.
If when is omitted, it defaults to termios.
TCSAFLUSH, and
is passed to termios.tcsetattr().
The return value of termios.tcgetattr() is saved before setting
fd to cbreak mode; this value is returned.
Changed in version 3.12: The return value is now the original tty attributes, instead of None.
See also:
Module termios Low-level terminal control interface.
Availability: Unix.
Pseudo-terminal handling is highly platform dependent.
The pty module defines the following functions:
pty.fork()
Fork.
Connect the child’s controlling terminal to a pseudo-terminal.
Return value is (pid, fd).
Note that the
child gets pid 0, and the fd is invalid.
The parent’s return value is the pid of the child, and fd is a file descriptor
connected to the child’s controlling terminal (and also to the child’s standard input and output).
Return a pair of file descriptors (master, slave), for the master and the slave end, respectively.
This is often used
to baffle programs which insist on reading from the controlling terminal.
It is expected that the process spawned
behind the pty will eventually terminate, and when it does spawn will return.
A loop copies STDIN of the current process to the child and data received from the child to STDOUT of the current
process.
It is not signaled to the child if STDIN of the current process closes down.
The functions master_read and stdin_read are passed a file descriptor which they should read from, and they should
always return a byte string.
In order to force spawn to return before the child process exits an empty byte array
should be returned to signal end of file.
The default implementation for both functions will read and return up to 1024 bytes each time the function is
called.
The master_read callback is passed the pseudoterminal’s master file descriptor to read output from the
child process, and stdin_read is passed file descriptor 0, to read from the parent process’s standard input.
If stdin_read signals EOF the controlling terminal can no longer communicate
with the parent process OR the child process.
Unless the child process will quit without any input, spawn will then
loop forever.
If master_read signals EOF the same behavior results (on linux at least).
Return the exit status value from os.waitpid() on the child process.
Raises an auditing event pty.spawn with argument argv.
Changed in version 3.4: spawn() now returns the status value from os.waitpid() on the child process.
Example
The following program acts like the Unix command script(1), using a pseudo-terminal to record all input and output
of a terminal session in a “typescript”.
It is an interface to the fcntl() and ioctl()
Unix routines.
Unix manual pages.
Availability: Unix, not Emscripten, not WASI.
All functions in this module take a file descriptor fd as their first argument.
This can be an integer file descriptor, such
as returned by sys.stdin.fileno(), or an io.
IOBase object, such as sys.stdin itself, which provides a
fileno() that returns a genuine file descriptor.
Changed in version 3.3: Operations in this module used to raise an IOError where they now raise an OSError.
The fcntl module now contains F_ADD_SEALS, F_GET_SEALS, and F_SEAL_* constants
for sealing of os.memfd_create() file descriptors.
On macOS, the fcntl module exposes the F_GETPATH constant, which obtains the path of
a file from a file descriptor.
On Linux(>=3.15), the fcntl module exposes the F_OFD_GETLK, F_OFD_SETLK and
F_OFD_SETLKW constants, which are used when working with open file description locks.
Changed in version 3.10: On Linux >= 2.6.11, the fcntl module exposes the F_GETPIPE_SZ and F_SETPIPE_SZ
constants, which allow to check and modify a pipe’s size respectively.
Changed in version 3.11: On FreeBSD, the fcntl module exposes the F_DUP2FD and F_DUP2FD_CLOEXEC constants,
which allow to duplicate a file descriptor, the latter setting FD_CLOEXEC flag in addition.
Changed in version 3.12: On Linux >= 4.5, the fcntl module exposes the FICLONE and FICLONERANGE constants,
which allow to share some data of one file with another file by reflinking on some filesystems (e.g., btrfs, OCFS2, and
XFS).
This behavior is commonly referred to as “copy-on-write”.
The module defines the following functions:
fcntl.fcntl(fd, cmd, arg=0)
Perform the operation cmd on file descriptor fd (file objects providing a fileno() method are accepted as well).
The values used for cmd are operating system dependent, and are available as constants in the fcntl module,
using the same names as used in the relevant C header files.
The argument arg can either be an integer value,
or a bytes object.
With an integer value, the return value of this function is the integer return value of the C
fcntl() call.
When the argument is bytes it represents a binary structure, e.g. created by struct.pack().
The binary data is copied to a buffer whose address is passed to the C fcntl() call.
The return value after a
successful call is the contents of the buffer, converted to a bytes object.
The length of the returned object will be
the same as the length of the arg argument.
This is limited to 1024 bytes.
If the information returned in the buffer
by the operating system is larger than 1024 bytes, this is most likely to result in a segmentation violation or a more
subtle data corruption.
If the fcntl() fails, an OSError is raised.
Raises an auditing event fcntl.fcntl with arguments fd, cmd, arg.
This function is identical to the fcntl() function, except that the argument handling is even more complicated.
The request parameter is limited to values that can fit in 32-bits.
Additional constants of interest for use as the
request argument can be found in the termios module, under the same names as used in the relevant C header
files.
The parameter arg can be one of an integer, an object supporting the read-only buffer interface (like bytes) or
an object supporting the read-write buffer interface (like bytearray).
If a mutable buffer is passed, then the behaviour is determined by the value of the mutate_flag parameter.
If it is false, the buffer’s mutability is ignored and behaviour is as for a read-only buffer, except that the 1024 byte
limit mentioned above is avoided – so long as the buffer you pass is at least as long as what the operating system
wants to put there, things should work.
If mutate_flag is true (the default), then the buffer is (in effect) passed to the underlying ioctl() system call,
the latter’s return code is passed back to the calling Python, and the buffer’s new contents reflect the action of the
ioctl().
This is a slight simplification, because if the supplied buffer is less than 1024 bytes long it is first copied
into a static buffer 1024 bytes long which is then passed to ioctl() and copied back into the supplied buffer.
If the ioctl() fails, an OSError exception is raised.
See the Unix manual flock(2) for details.
If the flock() fails, an OSError exception is raised.
Raises an auditing event fcntl.flock with arguments fd, operation.
This is essentially a wrapper around the fcntl() locking calls.
If LOCK_NB is used and the lock cannot be acquired, an OSError will be raised and the exception
will have an errno attribute set to EACCES or EAGAIN (depending on the operating system; for portability, check
for both values).
On at least some systems, LOCK_EX can only be used if the file descriptor refers to a file opened
for writing.
The default for start is 0, which means to start at the beginning of the file.
The default for len is 0 which means to
lock to the end of the file.
The default for whence is also 0.
Raises an auditing event fcntl.lockf with arguments fd, cmd, len, start, whence.
F_SETFL, os.O_NDELAY)
lockdata = struct.pack('hhllhh', fcntl.
F_WRLCK, 0, 0, 0, 0, 0)
rv = fcntl.fcntl(f, fcntl.
F_SETLKW, lockdata)
Note that in the first example the return value variable rv will hold an integer value; in the second example it will hold a
bytes object.
The structure lay-out for the lockdata variable is system dependent — therefore using the flock() call
may be better.
Module os If the locking flags O_SHLOCK and O_EXLOCK are present in the os module (on BSD only), the os.
open() function provides an alternative to the lockf() and flock() functions.
34.8 resource — Resource usage information
This module provides basic mechanisms for measuring and controlling system resources utilized by a program.
Availability: Unix, not Emscripten, not WASI.
Symbolic constants are used to specify particular system resources and to request usage information about either the
current process or its children.
An OSError is raised on syscall failure.
Changed in version 3.3: Following PEP 3151, this class was made an alias of OSError.
Each resource is controlled by a
pair of limits: a soft limit and a hard limit.
The soft limit is the current limit, and may be lowered or raised by a process
over time.
The soft limit can never exceed the hard limit.
The hard limit can be lowered to any value greater than the soft
limit, but not raised.
The specific resources that can be limited are system dependent.
They are described in the getrlimit(2) man page.
The resources listed below are supported when the underlying operating system supports them; resources which cannot
be checked or controlled by the operating system are not defined in this module for those platforms.
Raises ValueError if an
invalid resource is specified, or error if the underlying system call fails unexpectedly.
The limits argument must be a tuple (soft, hard) of two integers
describing the new limits.
A value of RLIM_INFINITY can be used to request a limit that is unlimited.
Raises ValueError if an invalid resource is specified, if the new soft limit exceeds the hard limit, or if a process
tries to raise its hard limit.
Specifying a limit of RLIM_INFINITY when the hard or system limit for that resource
is not unlimited will result in a ValueError.
A process with the effective UID of super-user can request any valid
limit value, including unlimited, but ValueError will still be raised if the requested limit exceeds the system
imposed limit.
VxWorks only supports setting RLIMIT_NOFILE.
Raises an auditing event resource.setrlimit with arguments resource, limits.
Combines setrlimit() and getrlimit() in one function and supports to get and set the resources limits
of an arbitrary process.
If pid is 0, then the call applies to the current process.
When limits is not given the function returns the resource limit of the process pid.
When limits is given the resource
limit of the process is set and the former resource limit is returned.
Raises ProcessLookupError when pid can’t be found and PermissionError when the user doesn’t have
CAP_SYS_RESOURCE for the process.
Raises an auditing event resource.prlimit with arguments pid, resource, limits.
Availability: Linux >= 2.6.36 with glibc >= 2.13.
New in version 3.4.
These symbols define resources whose consumption can be controlled using the setrlimit() and getrlimit()
functions described below.
The values of these symbols are exactly the constants used by C programs.
The Unix man page for getrlimit(2) lists the available resources.
Note that not all systems use the same symbol
or same value to denote the same resource.
This module does not attempt to mask platform differences — symbols not
defined for a platform will not be available from this module on that platform.
RLIMIT_CORE
The maximum size (in bytes) of a core file that the current process can create.
This may result in the creation of a
partial core file if a larger core would be required to contain the entire process image.
resource.
The maximum amount of processor time (in seconds) that a process can use.
If this limit is exceeded, a SIGXCPU
signal is sent to the process.
RLIMIT_FSIZE
The maximum size of a file which the process may create.
resource.
RLIMIT_DATA
The maximum size (in bytes) of the process’s heap.
The maximum size (in bytes) of the call stack for the current process.
This only affects the stack of the main thread
in a multi-threaded process.
resource.
The maximum resident set size that should be made available to the process.
The maximum number of processes the current process may create.
resource.
The maximum number of open file descriptors for the current process.
The maximum address space which may be locked in memory.
resource.
RLIMIT_VMEM
The largest area of mapped memory which the process may occupy.
The maximum area (in bytes) of address space which may be taken by the process.
resource.
The number of bytes that can be allocated for POSIX message queues.
Availability: Linux >= 2.6.8.
New in version 3.4.
The ceiling for the process’s nice level (calculated as 20 - rlim_cur).
Availability: Linux >= 2.6.12.
New in version 3.4.
The ceiling of the real-time priority.
Availability: Linux >= 2.6.12.
New in version 3.4.
The time limit (in microseconds) on CPU time that a process can spend under real-time scheduling without making
a blocking syscall.
Availability: Linux >= 2.6.25.
New in version 3.4.
RLIMIT_SIGPENDING
The number of signals which the process may queue.
Availability: Linux >= 2.6.8.
The maximum size (in bytes) of socket buffer usage for this user.
This limits the amount of network memory, and
hence the amount of mbufs, that this user may hold at any time.
Availability: FreeBSD.
New in version 3.4.
resource.
RLIMIT_SWAP
The maximum size (in bytes) of the swap space that may be reserved or used by all of this user id’s processes.
This
limit is enforced only if bit 1 of the vm.overcommit sysctl is set.
Please see tuning(7) for a complete description
of this sysctl.
Availability: FreeBSD.
New in version 3.4.
RLIMIT_NPTS
The maximum number of pseudo-terminals created by this user id.
Availability: FreeBSD.
New in version 3.4.
The maximum number of kqueues this user id is allowed to create.
Availability: FreeBSD >= 11.
New in version 3.10.
This function returns an object that describes the resources consumed by either the current process or its children,
as specified by the who parameter.
The who parameter should be specified using one of the RUSAGE_* constants
described below.
The fields of the return value each describe how a particular system resource has been used, e.g. amount of time
spent running is user mode or number of times the process was swapped out of main memory.
Some values are
dependent on the clock tick internal, e.g. the amount of memory the process is using.
For backward compatibility, the return value is also accessible as a tuple of 16 elements.
The fields ru_utime and ru_stime of the return value are floating point values representing the amount of time
spent executing in user mode and the amount of time spent executing in system mode, respectively.
The remaining
values are integers.
Consult the getrusage(2) man page for detailed information about these values.
A brief
summary is presented here:
Index
Field
Resource

ru_utime
time in user mode (float seconds)

ru_stime
time in system mode (float seconds)

ru_maxrss
maximum resident set size

ru_ixrss
shared memory size

ru_idrss
unshared memory size

ru_isrss
unshared stack size

ru_minflt
page faults not requiring I/O

ru_majflt
page faults requiring I/O

ru_nswap
number of swap outs

ru_inblock
block input operations

ru_oublock
block output operations

ru_msgsnd
messages sent

ru_msgrcv
messages received

ru_nsignals
signals received

ru_nvcsw
voluntary context switches

ru_nivcsw
involuntary context switches
This function will raise a ValueError if an invalid who parameter is specified.
It may also raise error exception
in unusual circumstances.
The following RUSAGE_* symbols are passed to the getrusage() function to specify which processes information
should be provided for.
resource.
RUSAGE_SELF
Pass to getrusage() to request resources consumed by the calling process, which is the sum of resources used
by all threads in the process.
Pass to getrusage() to request resources consumed by both the current process and child processes.
May not
be available on all systems.
Pass to getrusage() to request resources consumed by the current thread.
May not be available on all systems.
Refer to the Unix manual pages for a detailed
description of the syslog facility.
Availability: Unix, not Emscripten, not WASI.
This module wraps the system syslog family of routines.
A pure Python library that can speak to a syslog server is
available in the logging.handlers module as SysLogHandler.
The module defines the following functions:
syslog.syslog(message)
syslog.syslog(priority, message)
Send the string message to the system logger.
A trailing newline is added if necessary.
Each message is tagged with
a priority composed of a facility and a level.
The optional priority argument, which defaults to LOG_INFO, deter-
mines the message priority.
If the facility is not encoded in priority using logical-or (LOG_INFO | LOG_USER),
the value given in the openlog() call is used.
If openlog() has not been called prior to the call to syslog(), openlog() will be called with no arguments.
Raises an auditing event syslog.syslog with arguments priority, message.
Changed in version 3.2: In previous versions, openlog() would not be called automatically if it wasn’t called
prior to the call to syslog(), deferring to the syslog implementation to call openlog().
Changed in version 3.12: This function is restricted in subinterpreters.
Otherwise it will raise RuntimeError.
Logging options of subsequent syslog() calls can be set by calling openlog().
The optional ident keyword argument is a string which is prepended to every message, and defaults to sys.
The optional logoption keyword argument (default is 0) is a bit
field – see below for possible values to combine.
The optional facility keyword argument (default is LOG_USER)
sets the default facility for messages which do not have a facility explicitly encoded.
Raises an auditing event syslog.openlog with arguments ident, logoption, facility.
Changed in version 3.2: In previous versions, keyword arguments were not allowed, and ident was required.
Changed in version 3.12: This function is restricted in subinterpreters.
This may only be called in the main interpreter.
It will
raise RuntimeError if called in a subinterpreter.
syslog.closelog()
Reset the syslog module values and call the system library closelog().
This causes the module to behave as it does when initially imported.
For example, openlog() will be called on
the first syslog() call (if openlog() hasn’t already been called), and ident and other openlog() parameters
are reset to defaults.
Raises an auditing event syslog.closelog with no arguments.
Changed in version 3.12: This function is restricted in subinterpreters.
This may only be called in the main interpreter.
It will
raise RuntimeError if called in a subinterpreter.
34.9.
Calls to syslog() with a priority level
not set in maskpri are ignored.
The default is to log all priorities.
The function LOG_MASK(pri) calculates the
mask for the individual priority pri.
The function LOG_UPTO(pri) calculates the mask for all priorities up to
and including pri.
Raises an auditing event syslog.setlogmask with argument maskpri.
The module defines the following constants:
Priority levels (high to low): LOG_EMERG,
LOG_ALERT,
LOG_CRIT,
LOG_ERR,
LOG_WARNING,
LOG_NOTICE, LOG_INFO, LOG_DEBUG.
Facilities: LOG_KERN,
LOG_USER,
LOG_MAIL,
LOG_DAEMON,
LOG_AUTH,
LOG_LPR,
LOG_NEWS,
LOG_UUCP, LOG_CRON, LOG_SYSLOG, LOG_LOCAL0 to LOG_LOCAL7, and, if defined in <syslog.h>,
LOG_AUTHPRIV.
Log options: LOG_PID,
LOG_CONS,
LOG_NDELAY,
and,
if
defined
in
<syslog.h>,
LOG_ODELAY,
LOG_NOWAIT, and LOG_PERROR.
LOG_PID, facility=syslog.
The following modules have a command-line interface.
The modules described in this chapter are deprecated and only kept for backwards compatibility.
They have been super-
seded by other modules.
This module provides support for reading and writing AIFF and AIFF-C files.
AIFF is Audio Interchange File Format,
a format for storing digital audio samples in a file.
AIFF-C is a newer version of the format that includes the ability to
compress the audio data.
Audio files have a number of parameters that describe the audio data.
The sampling rate or frame rate is the number of
times per second the sound is sampled.
The number of channels indicate if the audio is mono, stereo, or quadro.
Each
frame consists of one sample per channel.
The sample size is the size in bytes of each sample.
Thus a frame consists of
nchannels * samplesize bytes, and a second’s worth of audio consists of nchannels * samplesize *
framerate bytes.
For example, CD quality audio has a sample size of two bytes (16 bits), uses two channels (stereo) and has a frame rate of
44,100 frames/second.
This gives a frame size of 4 bytes (2*2), and a second’s worth occupies 2*2*44100 bytes (176,400
bytes).
Module aifc defines the following function:
aifc.open(file, mode=None)
Open an AIFF or AIFF-C file and return an object instance with methods that are described below.
The argument
file is either a string naming a file or a file object.
If omitted, file.mode is used if it exists,
otherwise 'rb' is used.
When used for writing, the file object should be seekable, unless you know ahead of time
how many samples you are going to write in total and use writeframesraw() and setnframes().
The
open() function may be used in a with statement.
When the with block completes, the close() method is
called.
Changed in version 3.4: Support for the with statement was added.
Objects returned by open() when a file is opened for reading have the following methods:
aifc.getnchannels()
Return the number of audio channels (1 for mono, 2 for stereo).
For AIFF files, the
returned value is b'NONE'.
aifc.getcompname()
Return a bytes array convertible to a human-readable description of the type of compression used in the audio file.
For AIFF files, the returned value is b'not compressed'.
A marker consists of a tuple of three elements.
The first is the mark ID
(an integer), the second is the mark position in frames from the beginning of the data (an integer), the third is the
name of the mark (a string).
The returned data is a string containing for each frame
the uncompressed samples of all channels.
The next readframes() will start from the beginning.
aifc.setpos(pos)
Seek to the specified frame number.
After calling this method, the object can no longer be used.
Objects returned by open() when a file is opened for writing have all the above methods, except for readframes()
and setpos().
In addition the following methods exist.
The get*() methods can only be called after the correspond-
ing set*() methods have been called.
Before the first writeframes() or writeframesraw(), all parameters
except for the number of frames must be filled in.
The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff' in
which case the default is an AIFF file.
The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff'
in which case the default is an AIFF file.
Specify the number of frames that are to be written to the audio file.
If this parameter is not set, or not set correctly,
the file needs to support seeking.
If not specified, the audio data will not be compressed.
In AIFF files, compression is
not possible.
The name parameter should be a human-readable description of the compression type as a bytes array,
the type parameter should be a bytes array of length 4.
Currently the following compression types are supported:
b'NONE', b'ULAW', b'ALAW', b'G722'.
aifc.setparams(nchannels, sampwidth, framerate, comptype, compname)
Set all the above parameters at once.
The argument is a tuple consisting of the various parameters.
This means that
it is possible to use the result of a getparams() call as argument to setparams().
aifc.setmark(id, pos, name)
Add a mark with the given id (larger than 0), and the given name at the given position.
This method can be called
at any time before close().
Useful in combination with setmark().
aifc.writeframes(data)
Write data to the output file.
This method can only be called after the audio file parameters have been set.
Changed in version 3.4: Any bytes-like object is now accepted.
Like writeframes(), except that the header of the audio file is not updated.
Changed in version 3.4: Any bytes-like object is now accepted.
The header of the file is updated to reflect the actual size of the audio data.
After calling this
method, the object can no longer be used.
The audioop module contains some useful operations on sound fragments.
It operates on sound fragments consisting of
signed integer samples 8, 16, 24 or 32 bits wide, stored in bytes-like objects.
All scalar items are integers, unless specified
otherwise.
Changed in version 3.4: Support for 24-bit samples was added.
All functions now accept any bytes-like object.
String
input now results in an immediate error.
This module provides support for a-LAW, u-LAW and Intel/DVI ADPCM encodings.
A few of the more complicated operations only take 16-bit samples, otherwise the sample size (in bytes) is always a
parameter of the operation.
Return a fragment which is the addition of the two samples passed as parameters.
Both fragments should have the same length.
Samples are truncated in case of overflow.
Decode an Intel/DVI ADPCM coded fragment to a linear fragment.
See the description of lin2adpcm() for
details on ADPCM coding.
Return a tuple (sample, newstate) where the sample has the width specified
in width.
No filtering is done, so the usefulness of this
routine is questionable.
audioop.bias(fragment, width, bias)
Return a fragment that is the original fragment with a bias added to each sample.
Samples wrap around in case of
overflow.
Converts big-endian samples to little-
endian and vice versa.
New in version 3.4.
audioop.cross(fragment, width)
Return the number of zero crossings in the fragment passed as an argument.
audioop.findfactor(fragment, reference)
Return a factor F such that rms(add(fragment, mul(reference, -F))) is minimal, i.e., return the
factor with which you should multiply reference to make it match as well as possible to fragment.
The fragments
should both contain 2-byte samples.
The time taken by this routine is proportional to len(fragment).
This
is (conceptually) done by taking slices out of fragment, using findfactor() to compute the best match, and
minimizing the result.
The fragments should both contain 2-byte samples.
Return a tuple (offset, factor)
where offset is the (integer) offset into fragment where the optimal match started and factor is the (floating-point)
factor as per findfactor().
The fragments should both contain 2-byte samples.
The routine takes time proportional to len(fragment).
Convert samples to 4 bit Intel/DVI ADPCM encoding.
ADPCM coding is an adaptive coding scheme, whereby
each 4 bit number is the difference between one sample and the next, divided by a (varying) step.
The Intel/DVI
ADPCM algorithm has been selected for use by the IMA, so it may well become a standard.
The coder returns a tuple (adpcmfrag, newstate), and the
newstate should be passed to the next call of lin2adpcm().
In the initial call, None can be passed as the state.
It is used by the Sun
audio hardware, among others.
Convert samples between 1-, 2-, 3- and 4-byte formats.
Note:
In some audio formats, such as .WAV files, 16, 24 and 32 bit samples are signed, but 8 bit samples are
unsigned.
The same, in reverse, has to be applied when converting from 8 to 16, 24 or 32 bit width samples.
It is used by the Sun
audio hardware, among others.
Return the maximum peak-peak value in the sound fragment.
audioop.minmax(fragment, width)
Return a tuple consisting of the minimum and maximum values of all samples in the sound fragment.
audioop.mul(fragment, width, factor)
Return a fragment that has all samples in the original fragment multiplied by the floating-point value factor.
Samples
are truncated in case of overflow.
The converter returns a tuple (newfragment,
newstate), and newstate should be passed to the next call of ratecv().
The initial call should pass None as
the state.
The weightA and weightB arguments are parameters for a simple digital filter and default to 1 and 0 respectively.
This is a measure of the power in an audio signal.
The left channel is multiplied by lfactor and the right channel by
rfactor before adding the two channels to give a mono signal.
Each pair of samples in the stereo fragment are computed from
the mono sample, whereby left channel samples are multiplied by lfactor and right channel samples by rfactor.
Convert sound fragments in u-LAW encoding to linearly encoded sound fragments.
Note that operations such as mul() or max() make no distinction between mono and stereo fragments, i.e. all samples
are treated equal.
If this is a problem the stereo fragment should be split into two mono fragments first and recombined
later.
If you use the ADPCM coder to build network packets and you want your protocol to be stateless (i.e. to be able to
tolerate packet loss) you should not only transmit the data but also the state.
Note that you should send the initial state
(the one you passed to lin2adpcm()) along to the decoder, not the final state (as returned by the coder).
If you want
to use struct.
Struct to store the state in binary you can code the first element (the predicted value) in 16 bits and
the second (the delta index) in 8.
The ADPCM coders have never been tried against other ADPCM coders, only against themselves.
It could well be that I
misinterpreted the standards in which case they will not be interoperable with the respective standards.
The find*() routines might look a bit funny at first sight.
They are primarily meant to do echo cancellation.
The cgi module is deprecated (see PEP 594 for details
and alternatives).
The FieldStorage class can typically be replaced with urllib.parse.parse_qsl() for GET and HEAD re-
quests, and the email.message module or multipart for POST and PUT.
Most utility functions have replacements.
Support module for Common Gateway Interface (CGI) scripts.
This module defines a number of utilities for use by CGI scripts written in Python.
The global variable maxlen can be set to an integer indicating the maximum size of a POST request.
POST requests
larger than this size will result in a ValueError being raised during parsing.
The default value of this variable is 0,
meaning the request size is unlimited.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Most often, CGI scripts live in the server’s special cgi-bin directory.
The HTTP server places all sorts of information
about the request (such as the client’s hostname, the requested URL, the query string, and lots of other goodies) in the
script’s shell environment, executes the script, and sends the script’s output back to the client.
The script’s input is connected to the client too, and sometimes the form data is read this way; at other times the form data
is passed via the “query string” part of the URL.
This module is intended to take care of the different cases and provide a
simpler interface to the Python script.
It also provides a number of utilities that help in debugging scripts, and the latest
addition is support for file uploads from a form (if your browser supports it).
The output of a CGI script should consist of two sections, separated by a blank line.
The first section contains a number
of headers, telling the client what kind of data is following.
Here’s Python code that prints a simple piece of HTML:
36.3.
This is my first CGI script</H1>")
print("Hello, world!")
36.3.2 Using the cgi module
Begin by writing import cgi.
This activates a special exception handler that will display detailed reports in the web browser if any errors occur.
It’s very helpful to use this feature during script development.
The reports produced by cgitb provide information that
can save you a lot of time in tracking down bugs.
You can always remove the cgitb line later when you have tested your
script and are confident that it works correctly.
To get at submitted form data, use the FieldStorage class.
If the form contains non-ASCII characters, use the
encoding keyword parameter set to the value of the encoding defined for the document.
It is usually contained in the
META tag in the HEAD section of the HTML document or by the Content-Type header.
This reads the form contents
from the standard input or the environment (depending on the value of various environment variables set according to the
CGI standard).
Since it may consume standard input, it should be instantiated only once.
The FieldStorage instance can be indexed like a Python dictionary.
It allows membership testing with the in oper-
ator, and also supports the standard dictionary method keys() and the built-in function len().
Form fields containing
empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional
keep_blank_values keyword parameter when creating the FieldStorage instance.
For instance, the following code (which assumes that the Content-Type header and blank line have already been
printed) checks that the fields name and addr are both set to a non-empty string:
form = cgi.
FieldStorage()
if "name" not in form or "addr" not in form:
print("<H1>Error</H1>")
print("Please fill in the name and addr fields.")

print("<p>addr:", form["addr"].value)
...further form processing here...
Here
the
fields,
accessed
through
form[key],
are
themselves
instances
of
FieldStorage
(or
MiniFieldStorage, depending on the form encoding).
The value attribute of the instance yields the string value
of the field.
The getvalue() method returns this string value directly; it also accepts an optional second argument as
a default to return if the requested key is not present.
If the submitted form data contains more than one field with the same name, the object retrieved by form[key] is not
a FieldStorage or MiniFieldStorage instance but a list of such instances.
Similarly, in this situation, form.
If a field represents an uploaded file, accessing the value via the value attribute or the getvalue() method reads the
entire file in memory as bytes.
This may not be what you want.
You can test for an uploaded file by testing either the
filename attribute or the file attribute.
If an error is encountered when obtaining the contents of an uploaded file (for example, when the user interrupts the form
submission by clicking on a Back or Cancel button) the done attribute of the object for the field will be set to the value
-1.
The file upload draft standard entertains the possibility of uploading multiple files from one field (using a recursive
multipart/* encoding).
When this occurs, the item will be a dictionary-like FieldStorage item.
This can
be determined by testing its type attribute, which should be multipart/form-data (or perhaps another MIME
type matching multipart/*).
In this case, it can be iterated over recursively just like the top-level form object.
When a form is submitted in the “old” format (as the query string or as a single data part of type application/
x-www-form-urlencoded), the items will actually be instances of the class MiniFieldStorage.
In this case,
the list, file, and filename attributes are always None.
A form submitted via POST that also has a query string will contain both FieldStorage and MiniFieldStorage
items.
Changed in version 3.4: The file attribute is automatically closed upon the garbage collection of the creating
FieldStorage instance.
Changed in version 3.5: Added support for the context management protocol to the FieldStorage class.
This section describes a higher
level interface which was added to this class to allow one to do it in a more readable and intuitive way.
The interface doesn’t
make the techniques described in previous sections obsolete — they are still useful to process file uploads efficiently, for
example.
The interface consists of two simple methods.
Using the methods you can process form data in a generic way, without
the need to worry whether only one or more values were posted under one name.
In the previous section, you learned to write following code anytime you expected a user to post more than one value
under one name:
36.3.
The user is requesting more than one item.
In most situations, however, there’s only one form control with a particular name in a form and then you expect and need
only one value associated with this name.
For
example, if a curious user appends another user=foo pair to the query string, then the script would crash, because in
this situation the getvalue("user") method call returns a list instead of a string.
Calling the upper() method on
a list is not valid (since lists do not have a method of this name) and results in an AttributeError exception.
Therefore, the appropriate way to read form data values was to always use the code which checks whether the obtained
value is a single value or a list of values.
That’s annoying and leads to less readable scripts.
A more convenient approach is to use the methods getfirst() and getlist() provided by this higher level inter-
face.
This method always returns only one value associated with form field name.
The method returns only the first
value in case that more values were posted under such name.
If no such form field or value exists
then the method returns the value specified by the optional parameter default.
This parameter defaults to None if
not specified.
This method always returns a list of values associated with form field name.
The method returns an empty list if
no such form field or value exists for name.
It returns a list consisting of one item if only one such value exists.
Using these methods you can write nice compact code:
import cgi
form = cgi.
This way it's safe.
These are useful if you want more control, or if you want to employ some of the algorithms implemented in this module
in other circumstances.
The keep_blank_values,
strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged.
This function, like the rest of the cgi module, is
deprecated.
It can be replaced by calling urllib.parse.parse_qs() directly on the desired query string
(except for multipart/form-data input, which can be handled as described for parse_multipart()).
Parse input of type multipart/form-data (for file uploads).
Arguments are fp for the input file, pdict for a
dictionary containing other parameters in the Content-Type header, and encoding, the request encoding.
Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of
values for that field.
For non-file fields, the value is a list of strings.
This is easy to use but not much good if you are expecting megabytes to be uploaded — in that case, use the
FieldStorage class instead which is much more flexible.
Added the encoding and errors parameters.
For non-file fields, the value is now a list of
strings, not bytes.
Added the separator parameter.
This function, like the rest of the cgi mod-
ule, is deprecated.
It can be replaced with the functionality in the email package (e.g. email.message.
EmailMessage/email.message.
Message) which implements the same MIME RFCs, or with the multi-
part PyPI project.
Parse a MIME header (such as Content-Type) into a main value and a dictionary of parameters.
This function, like the rest of the cgi module, is
deprecated.
It can be replaced with the functionality in the email package, which implements the same MIME
RFCs.
For example, with email.message.
EmailMessage:
from email.message import EmailMessage
msg = EmailMessage()
msg['content-type'] = 'application/json; charset="utf8"'
main, params = msg.get_content_type(), msg['content-type'].params
cgi.test()
Robust test CGI script, usable as main program.
Writes minimal HTTP headers and formats all information pro-
vided to the script in HTML format.
Format a form in HTML.
There’s one important rule: if you invoke an external program (via os.system(), os.popen() or other functions
with similar functionality), make very sure you don’t pass arbitrary strings received from the client to the shell.
This is a
well-known security hole whereby clever hackers anywhere on the web can exploit a gullible CGI script to invoke arbitrary
shell commands.
Even parts of the URL or field names cannot be trusted, since the request doesn’t have to come from
your form!
To be on the safe side, if you must pass a string gotten from a form to a shell command, you should make sure the string
contains only alphanumeric characters, dashes, underscores, and periods.
Installing your CGI script on a Unix system
Read the documentation for your HTTP server and check with your local system administrator to find the directory where
CGI scripts should be installed; usually this is in a directory cgi-bin in the server tree.
Make sure that your script is readable and executable by “others”; the Unix file mode should be 0o755 octal (use chmod
0755 filename).
Make sure that the first line of the script contains #!
Make sure that any files your script needs to read or write are readable or writable, respectively, by “others” — their mode
should be 0o644 for readable and 0o666 for writable.
This is because, for security reasons, the HTTP server executes
your script as user “nobody”, without any special privileges.
It can only read (write, execute) files that everybody can
read (write, execute).
The current directory at execution time is also different (it is usually the server’s cgi-bin directory)
and the set of environment variables is also different from what you get when you log in.
In particular, don’t count on
the shell’s search path for executables (PATH) or the Python module search path (PYTHONPATH) to be set to anything
interesting.
If you need to load modules from a directory which is not on Python’s default module search path, you can change the
path in your script, before importing other modules.
Instructions for non-Unix systems will vary; check your HTTP server’s documentation (it will usually have a section on
CGI scripts).
There’s one reason why you should still test your
script from the command line: if it contains a syntax error, the Python interpreter won’t execute it at all, and the HTTP
server will most likely send a cryptic error to the client.
Assuming your script has no syntax errors, yet it does not work, you have no choice but to read the next section.
If you wonder whether you have understood the installation procedure correctly, try installing a
copy of this module file (cgi.py) as a CGI script.
When invoked as a script, the file will dump its environment and the
contents of the form in HTML format.
Give it the right mode etc., and send it a request.
If it’s installed in the standard
cgi-bin directory, it should be possible to send it a request by entering a URL into your browser of the form:
http://yourhostname/cgi-bin/cgi.py?name=Joe+Blow&addr=At+Home
If this gives an error of type 404, the server cannot find the script – perhaps you need to install it in a different directory.
If it gives another error, there’s an installation problem that you should fix before trying to go any further.
If you get a
nicely formatted listing of the environment and form content (in this example, the fields should be listed as “addr” with
value “At Home” and “name” with value “Joe Blow”), the cgi.py script has been installed correctly.
If you follow the
same procedure for your own script, you should now be able to debug it.
This should produce the same results as those gotten from installing the cgi.py file itself.
When an ordinary Python script raises an unhandled exception (for whatever reason: of a typo in a module name, a file
that can’t be opened, etc.), the Python interpreter prints a nice traceback and exits.
While the Python interpreter will still
do this when your CGI script raises an exception, most likely the traceback will end up in one of the HTTP server’s log
files, or be discarded altogether.
Fortunately, once you have managed to get your script to execute some code, you can easily send tracebacks to the web
browser using the cgitb module.
If you haven’t done so already, just add the lines:
import cgitb
cgitb.enable()
to the top of your script.
Then try running it again; when a problem occurs, you should see a detailed report that will
likely make apparent the cause of the crash.
This relies on the Python interpreter to print the traceback.
The content type of the output is set to plain text, which
disables all HTML processing.
If your script works, the raw HTML will be displayed by your client.
If it raises an
36.3.
Because no HTML
interpretation is going on, the traceback will be readable.
This means that it is not
possible to display a progress report on the client’s display while the script is running.
If your script does not have any syntax errors, try adding import cgitb; cgitb.enable() to the top of
the script.
Usually, this means using absolute path names
— PATH is usually not set to a very useful value in a CGI script.
When reading or writing external files, make sure they can be read or written by the userid under which your CGI
script will be running: this is typically the userid under which the web server is running, or some explicitly specified
userid for a web server’s suexec feature.
This doesn’t work on most systems, and is a security liability as well.
The cgitb module is deprecated (see PEP 594 for
details).
The cgitb module provides a special exception handler for Python scripts.
It was originally
designed to display extensive traceback information in HTML for CGI scripts.
After this module is activated, if an uncaught exception occurs, a detailed, formatted report
will be displayed.
The report includes a traceback showing excerpts of the source code for each level, as well as the values
of the arguments and local variables to currently running functions, to help you debug the problem.
Optionally, you can
save this information to a file instead of sending it to the browser.
The options to the enable() function control whether the report is displayed in the browser and whether the report is
logged to a file for later analysis.
This function causes the cgitb module to take over the interpreter’s default handling for exceptions by setting the
value of sys.excepthook.
The optional argument display defaults to 1 and can be set to 0 to suppress sending the traceback to the browser.
If
the argument logdir is present, the traceback reports are written to files.
The value of logdir should be a directory
where these files will be placed.
If the optional argument format is "html",
the output is formatted as HTML.
Any other value forces plain text output.
The default value is "html".
This function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()),
formatting its traceback as text and returning the result as a string.
This function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()),
formatting its traceback as HTML and returning the result as a string.
This function handles an exception using the default settings (that is, show a report in the browser, but don’t log to
a file).
This can be used when you’ve caught an exception and want to report it using cgitb.
The optional info
argument should be a 3-tuple containing an exception type, exception value, and traceback object, exactly like the
tuple returned by sys.exc_info().
If the info argument is not supplied, the current exception is obtained from
sys.exc_info().
This format is used in at least the Audio
Interchange File Format (AIFF/AIFF-C) and the Real Media File Format (RMFF).
The WAVE audio file format is closely
related and can also be read using this module.
A chunk has the following structure:
Offset
Length
Contents


Chunk ID


Size of chunk in big-endian byte order, not including the header

n
Data bytes, where n is the size given in the preceding field
8 + n
0 or 1
Pad byte needed if n is odd and chunk alignment is used
The ID is a 4-byte string which identifies the type of chunk.
The size field (a 32-bit value, encoded using big-endian byte order) gives the size of the chunk data, not including the
8-byte header.
Usually an IFF-type file consists of one or more chunks.
The proposed usage of the Chunk class defined here is to
instantiate an instance at the start of each chunk and read from the instance until it reaches the end, after which a new
instance can be instantiated.
At the end of the file, creating a new instance will fail with an EOFError exception.
Chunk(file, align=True, bigendian=True, inclheader=False)
Class which represents a chunk.
The file argument is expected to be a file-like object.
An instance of this class
is specifically allowed.
The only method that is needed is read().
If the methods seek() and tell() are
present and don’t raise an exception, they are also used.
If these methods are present and raise an exception, they
1 “EA IFF 85” Standard for Interchange Format Files, Jerry Morrison, Electronic Arts, January 1985.
If the optional argument align is true, chunks are assumed to be aligned
on 2-byte boundaries.
If align is false, no alignment is assumed.
The default value is true.
If the optional argument
bigendian is false, the chunk size is assumed to be in little-endian order.
This is needed for WAVE audio files.
The
default value is true.
If the optional argument inclheader is true, the size given in the chunk header includes the
size of the header.
The default value is false.
A Chunk object supports the following methods:
getname()
Returns the name (ID) of the chunk.
This is the first 4 bytes of the chunk.
Close and skip to the end of the chunk.
This does not close the underlying file.
The remaining methods will raise OSError if called after the close() method has been called.
Before Python
3.3, they used to raise IOError, now an alias of OSError.
The whence argument is optional and defaults to 0 (absolute file positioning);
other values are 1 (seek relative to the current position) and 2 (seek relative to the file’s end).
If the underlying file does not allow seek, only forward seeks are allowed.
If the size argument is negative or omitted, read all data until the end of the chunk.
An empty bytes object is
returned when the end of the chunk is encountered immediately.
All further calls to read() for the chunk will return b''.
If you are not
interested in the contents of the chunk, this method should be called so that the file points to the start of the
next chunk.
The crypt module is deprecated (see PEP 594 for
details and alternatives).
The hashlib module is a potential replacement for certain use cases.
The passlib package can
replace all use cases of this module.
This module implements an interface to the crypt(3) routine, which is a one-way hash function based upon a modified
DES algorithm; see the Unix man page for further details.
Possible uses include storing hashed passwords so you can
check passwords without storing the actual password, or attempting to crack Unix passwords with a dictionary.
Notice that the behavior of this module depends on the actual implementation of the crypt(3) routine in the running
system.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Hashing Methods
New in version 3.3.
The crypt module defines the list of hashing methods (not all methods are available on all platforms):
crypt.
METHOD_SHA512
A Modular Crypt Format method with 16 character salt and 86 character hash based on the SHA-512 hash function.
This is the strongest method.
crypt.
METHOD_SHA256
Another Modular Crypt Format method with 16 character salt and 43 character hash based on the SHA-256 hash
function.
METHOD_BLOWFISH
Another Modular Crypt Format method with 22 character salt and 31 character hash based on the Blowfish cipher.
New in version 3.7.
crypt.
METHOD_MD5
Another Modular Crypt Format method with 8 character salt and 22 character hash based on the MD5 hash function.
crypt.
The traditional method with a 2 character salt and 13 characters of hash.
This is the weakest method.
METHOD_* objects.
This list is sorted from strongest
to weakest.
The optional salt is either
a string as returned from mksalt(), one of the crypt.
METHOD_* values (though not all may be available on
all platforms), or a full encrypted password including salt, as returned by this function.
If salt is not provided, the
strongest method available in methods will be used.
Checking a password is usually done by passing the plain-text password as word and the full results of a previous
crypt() call, which should be the same as the results of this call.
The characters in salt must be in the set [./a-zA-Z0-9], with the
exception of Modular Crypt Format which prefixes a $digit$.
36.6.
Since a few crypt(3) extensions allow different values, with different sizes in the salt, it is recommended to use
the full crypted password as salt when checking for a password.
Changed in version 3.3: Accept crypt.
METHOD_* values in addition to strings for salt.
If no method is given, the strongest method available in
methods is used.
The return value is a string suitable for passing as the salt argument to crypt().
For METHOD_SHA256 and METHOD_SHA512 it must be an integer between 1000 and 999_999_999, the
default is 5000.
For METHOD_BLOWFISH it must be a power of two between 16 (24) and 2_147_483_648
(231), the default is 4096 (212).
New in version 3.3.
Added the rounds parameter.
Examples
A simple example illustrating typical use (a constant-time comparison operation is needed to limit exposure to timing
attacks.
The imghdr module is deprecated (see PEP 594 for
details and alternatives).
The imghdr module determines the type of image contained in a file or byte stream.
The imghdr module defines the following function:
imghdr.what(file, h=None)
Test the image data contained in the file named file and return a string describing the image type.
If h is provided,
the file argument is ignored and h is assumed to contain the byte stream to test.
Accepts a path-like object.
The exr and webp formats were added.
You can extend the list of file types imghdr can recognize by appending to this variable:
imghdr.tests
A list of functions performing the individual tests.
Each function takes two arguments: the byte-stream and an open
file-like object.
When what() is called with a byte-stream, the file-like object will be None.
The test function should return a string describing the image type if the test succeeded, or None if it failed.
Example:
'gif'
36.7.
The mailcap module is deprecated (see PEP 594 for
details).
The mimetypes module provides an alternative.
Mailcap files are used to configure how MIME-aware applications such as mail readers and web browsers react to files with
different MIME types.
For example, a mailcap file
might contain a line like video/mpeg; xmpeg %s.
Then, if the user encounters an email message or web document
with the MIME type video/mpeg, %s will be replaced by a filename (usually one belonging to a temporary file) and
the xmpeg program can be automatically started to view the file.
The mailcap format is documented in RFC 1524, “A User Agent Configuration Mechanism For Multimedia Mail Format
Information”, but is not an internet standard.
However, mailcap files are supported on most Unix systems.
If no matching MIME type
can be found, (None, None) is returned.
Other possible values
might be ‘compose’ and ‘edit’, if you wanted to create a new body of the given MIME type or alter the existing
body data.
See RFC 1524 for a complete list of these fields.
Each entry in the list must
be a string containing the parameter name, an equals sign ('='), and the parameter’s value.
Mailcap entries can
contain named parameters like %{foo}, which will be replaced by the value of the parameter named ‘foo’.
For ex-
ample, if the command line showpartial %{id} %{number} %{total} was in a mailcap file, and plist
was set to ['id=1', 'number=2', 'total=3'], the resulting command line would be 'showpartial
1 2 3'.
In a mailcap file, the “test” field can optionally be specified to test some external condition (such as the machine
architecture, or the window system in use) to determine whether or not the mailcap line applies.
Changed in version 3.11: To prevent security issues with shell metacharacters (symbols that have special effects in
a shell command line), findmatch will refuse to inject ASCII characters other than alphanumerics and @+=:,
./-_ into the returned command line.
If a disallowed character appears in filename, findmatch will always return (None, None) as if no entry was
found.
If such a character appears elsewhere (a value in plist or in MIMEtype), findmatch will ignore all mailcap
entries which use that value.
A warning will be raised in either case.
This dictionary must be passed to the
findmatch() function.
An entry is stored as a list of dictionaries, but it shouldn’t be necessary to know the
details of this representation.
The information is derived from all of the mailcap files found on the system.
Settings in the user’s mailcap file
$HOME/.mailcap will override settings in the system mailcap files /etc/mailcap, /usr/etc/mailcap,
and /usr/local/etc/mailcap.
The msilib module is deprecated (see PEP 594 for
details).
The msilib supports the creation of Microsoft Installer (.msi) files.
Because these files often contain an embed-
ded “cabinet” file (.cab), it also exposes an API to create CAB files.
This package aims to provide complete access to all tables in an .msi file, therefore, it is a fairly low-level API.
One
primary application of this package is the creation of Python installer package itself (although that currently uses a different
version of msilib).
The package contents can be roughly split into four parts: low-level CAB routines, low-level MSI routines, higher-level
MSI routines, and standard table structures.
FCICreate(cabname, files)
Create a new CAB file named cabname.
The files are added to the CAB file in the order they appear in the list.
All files are added into a single CAB file,
using the MSZIP compression algorithm.
Callbacks to Python for the various steps of MSI creation are currently not exposed.
msilib.
UuidCreate()
Return the string representation of a new unique identifier.
This wraps the Windows API functions
UuidCreate() and UuidToString().
msilib.
OpenDatabase(path, persist)
Return a new database object by calling MsiOpenDatabase.
path is the file name of the MSI file; persist
can be one of the constants MSIDBOPEN_CREATEDIRECT, MSIDBOPEN_CREATE, MSIDBOPEN_DIRECT,
MSIDBOPEN_READONLY, or MSIDBOPEN_TRANSACT, and may include the flag MSIDBOPEN_PATCHFILE.
See the Microsoft documentation for the meaning of these flags; depending on the flags, an existing database is
opened, or a new one created.
Return a new record object by calling MSICreateRecord().
The database will contain just the schema and the validation records when this function returns.
The table argument must be one of the predefined tables in the MSI schema, e.g. 'Feature', 'File',
'Component', 'Dialog', 'Control', etc.
records should be a list of tuples, each one containing all fields of a record according to the schema of the table.
For optional fields, None can be passed.
Field values can be ints, strings, or instances of the Binary class.
Binary(filename)
Represents entries in the Binary table; inserting such an object using add_data() reads the file named filename
into the table.
This is typically used to install the sequence tables.
Database Objects
Database.
OpenView(sql)
Return a view object, by calling MSIDatabaseOpenView().
Database.
Commit()
Commit the changes pending in the current transaction, by calling MSIDatabaseCommit().
Database.GetSummaryInformation(count)
Return a new summary information object, by calling MsiGetSummaryInformation().
Database.
Close()
Close the database object, through MsiCloseHandle().
New in version 3.7.
View Objects
View.
Execute(params)
Execute the SQL query of the view, through MSIViewExecute().
If params is not None, it is a record
describing actual values of the parameter tokens in the query.
View.GetColumnInfo(kind)
Return a record describing the columns of the view, through calling MsiViewGetColumnInfo().
Fetch()
Return a result record of the query, through calling MsiViewFetch().
View.
Modify(kind, data)
Modify
the
view,
by
calling
MsiViewModify().
kind
can
be
one
of
MSIMODIFY_SEEK,
MSIMODIFY_REFRESH,
MSIMODIFY_INSERT,
MSIMODIFY_UPDATE,
MSIMODIFY_ASSIGN,
MSIMODIFY_REPLACE,
MSIMODIFY_MERGE,
MSIMODIFY_DELETE,
MSIMODIFY_INSERT_TEMPORARY,
MSIMODIFY_VALIDATE,
MSIMODIFY_VALIDATE_NEW,
MSIMODIFY_VALIDATE_FIELD, or MSIMODIFY_VALIDATE_DELETE.
data must be a record describing the new data.
View.
Close()
Close the view, through MsiViewClose().
See also:
MsiViewExecute MSIViewGetColumnInfo MsiViewFetch MsiViewModify MsiViewClose
36.9.3 Summary Information Objects
SummaryInformation.
GetProperty(field)
Return a property of the summary, through MsiSummaryInfoGetProperty().
SummaryInformation.
GetPropertyCount()
Return the number of summary properties, through MsiSummaryInfoGetPropertyCount().
SummaryInformation.
SetProperty(field, value)
Set a property through MsiSummaryInfoSetProperty().
Possible value types are integer and string.
SummaryInformation.
Persist()
Write the modified properties to the summary information stream, using MsiSummaryInfoPersist().
MsiSummaryInfoPer-
sist
36.9.
Record Objects
Record.
GetFieldCount()
Return the number of fields of the record, through MsiRecordGetFieldCount().
Record.
GetInteger(field)
Return the value of field as an integer where possible.
Record.
GetString(field)
Return the value of field as a string where possible.
Record.
SetString(field, value)
Set field to value through MsiRecordSetString().
Record.
SetStream(field, value)
Set field to the contents of the file named value, through MsiRecordSetStream().
Record.
SetInteger(field, value)
Set field to value through MsiRecordSetInteger().
Both field and value must be an integer.
Record.
ClearData()
Set all fields of the record to 0, through MsiRecordClearData().
Errors
All wrappers around MSI functions raise MSIError; the string inside the exception will contain more detail.
The class CAB represents a CAB file.
During MSI construction, files will be added simultaneously to the Files
table, and to a CAB file.
Then, when all files have been added, the CAB file can be written, then added to the MSI
file.
If there is already a file named
logical, a new file name is created.
Return the index of the file in the CAB file, and the new name of the file inside the CAB file.
Directory(database, cab, basedir, physical, logical, default[, componentflags])
Create a new directory in the Directory table.
There is a current component at each point in time for the directory,
which is either explicitly created through start_component(), or implicitly when files are added for the first
time.
Files are added into the current component, and into the cab file.
To create a directory, a base directory
object needs to be specified (can be None), the path to the physical directory, and a logical directory name.
If
no component name is given, the directory name is used.
If no feature is given, the current feature is used.
If no flags are given, the directory’s default flags are used.
If no keyfile is given, the KeyPath is left null in the
Component table.
By
default, the file name in the source and the file table will be identical.
If the src file is specified, it is interpreted
relative to the current directory.
Optionally, a version and a language can be specified for the entry in the File
table.
Individual files can be excluded
in the exclude list.
See also:
Directory Table File Table Component Table FeatureComponents Table
36.9.8 Features
class msilib.
Feature(db, id, title, desc, display, level=1, parent=None, directory=None, attributes=0)
Add a new record to the Feature table, using the values id, parent.id, title, desc, display, level, directory, and
attributes.
The resulting feature object can be passed to the start_component() method of Directory.
New components are automatically added to the default
feature, unless a feature is explicitly specified.
See also:
Feature Table
36.9.
However, no standard user interface is
provided.
Base class of the dialog controls.
RadioButtonGroup(dlg, name, property)
Create a radio button control named name.
If
value is None, it defaults to name.
Dialog(db, name, x, y, w, h, attr, title, first, default, cancel)
Return a new Dialog object.
An entry in the Dialog table is made, with the specified coordinates, dialog
attributes, title, name of the first, default, and cancel controls.
An entry in the Control table is made with the specified parameters.
This is a generic method; for specific types, specialized methods are provided.
Currently, these definitions are based
on MSI version 2.0.
This is the standard MSI schema for MSI 2.0, with the tables variable providing a list of table definitions, and
_Validation_records providing the data for MSI validation.
The nis module is deprecated (see PEP 594 for details).
The nis module gives a thin wrapper around the NIS library, useful for central administration of several hosts.
Because NIS exists only on Unix systems, this module is only available for Unix.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
The nis module defines the following functions:
nis.match(key, mapname, domain=default_domain)
Return the match for key in map mapname, or raise an error (nis.error) if there is none.
Both should be strings,
key is 8-bit clean.
Return value is an arbitrary array of bytes (may contain NULL and other joys).
Note that mapname is first checked if it is an alias to another name.
The domain argument allows overriding the NIS domain used for the lookup.
If unspecified, lookup is in the default
NIS domain.
nis.cat(mapname, domain=default_domain)
Return a dictionary mapping key to value such that match(key, mapname)==value.
Note that both keys
and values of the dictionary are arbitrary arrays of bytes.
Note that mapname is first checked if it is an alias to another name.
The domain argument allows overriding the NIS domain used for the lookup.
If unspecified, lookup is in the default
NIS domain.
The domain argument allows overriding the NIS domain used for the lookup.
If unspecified, lookup is in the default
NIS domain.
36.10.
The nis module defines the following exception:
exception nis.error
An error raised when a NIS function returns an error code.
The nntplib module is deprecated (see PEP 594 for details).
This module defines the class NNTP which implements the client side of the Network News Transfer Protocol.
It can be
used to implement a news reader or poster, or automated news processors.
It is compatible with RFC 3977 as well as the
older RFC 977 and RFC 2980.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Here are two small examples of how it can be used.
Hello fellow committers!
The module itself defines the following classes:
class nntplib.
NNTP(host, port=119, user=None, password=None, readermode=None, usenetrc=False[, timeout
])
Return a new NNTP object, representing a connection to the NNTP server running on host host, listening at port
port.
An optional timeout can be specified for the socket connection.
If the optional user and password are provided,
or if suitable credentials are present in /.netrc and the optional flag usenetrc is true, the AUTHINFO USER and
AUTHINFO PASS commands are used to identify and authenticate the user to the server.
If the optional flag
readermode is true, then a mode reader command is sent before authentication is performed.
Reader mode
is sometimes necessary if you are connecting to an NNTP server on the local machine and intend to call reader-
specific commands, such as group.
If you get unexpected NNTPPermanentErrors, you might need to set
readermode.
All commands will raise an auditing event nntplib.putline with arguments self and line, where line
is the bytes about to be sent to the remote host.
Changed in version 3.2: usenetrc is now False by default.
Changed in version 3.3: Support for the with statement was added.
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
NNTP_SSL(host, port=563, user=None, password=None, ssl_context=None, readermode=None,
usenetrc=False[, timeout])
Return a new NNTP_SSL object, representing an encrypted connection to the NNTP server running on host host,
listening at port port.
NNTP_SSL objects have the same methods as NNTP objects.
If port is omitted, port 563
(NNTPS) is used.
Please read Security considerations
for best practices.
All other parameters behave the same as for NNTP.
Note that SSL-on-563 is discouraged per RFC 4642, in favor of STARTTLS as described below.
However, some
servers only support the former.
Raises an auditing event nntplib.connect with arguments self, host, port.
All commands will raise an auditing event nntplib.putline with arguments self and line, where line
is the bytes about to be sent to the remote host.
New in version 3.2.
Changed in version 3.4: The class now supports hostname check with ssl.
SSLContext.check_hostname
and Server Name Indication (see ssl.HAS_SNI).
If the timeout parameter is set to be zero, it will raise a ValueError to prevent the
creation of a non-blocking socket.
NNTPError
Derived from the standard exception Exception, this is the base class for all exceptions raised by the nntplib
module.
Instances of this class have the following attribute:
response
The response of the server if available, as a str object.
NNTPReplyError
Exception raised when an unexpected reply is received from the server.
NNTPTemporaryError
Exception raised when a response code in the range 400–499 is received.
exception nntplib.
NNTPPermanentError
Exception raised when a response code in the range 500–599 is received.
exception nntplib.
NNTPProtocolError
Exception raised when a reply is received from the server that does not begin with a digit in the range 1–5.
exception nntplib.
NNTPDataError
Exception raised when there is some error in the response data.
Attributes
NNTP.nntp_version
An integer representing the version of the NNTP protocol supported by the server.
In practice, this should be 2
for servers advertising RFC 3977 compliance and 1 for others.
New in version 3.2.
NNTP.nntp_implementation
A string describing the software name and version of the NNTP server, or None if not advertised by the server.
New in version 3.2.
Methods
The response that is returned as the first item in the return tuple of almost all methods is the server’s response: a string
beginning with a three-digit code.
If the server’s response indicates an error, the method raises one of the above exceptions.
Many of the following methods take an optional keyword-only argument file.
When the file argument is supplied, it must
be either a file object opened for binary writing, or the name of an on-disk file to be written to.
The method will then
write any data returned by the server (except for the response line and the terminating dot) to the file; any list of lines,
tuples or objects that the method normally returns will be empty.
Changed in version 3.2: Many of the following methods have been reworked and fixed, which makes them incompatible
with their 3.1 counterparts.
NNTP.quit()
Send a QUIT command and close the connection.
Once this method has been called, no other methods of the
NNTP object should be called.
NNTP.getcapabilities()
Return the RFC 3977 capabilities advertised by the server, as a dict instance mapping capability names to (pos-
sibly empty) lists of values.
On legacy servers which don’t understand the CAPABILITIES command, an empty
dictionary is returned instead.
True
New in version 3.2.
If user and password are None and usenetrc is
true, credentials from ~/.netrc will be used if possible.
Unless intentionally delayed, login is normally performed during the NNTP object initialization and separately
calling this function is unnecessary.
To force authentication to be delayed, you must not set user or password when
creating the object, and must set usenetrc to False.
New in version 3.2.
NNTP.starttls(context=None)
Send a STARTTLS command.
This will enable encryption on the NNTP connection.
The context argument is
optional and should be a ssl.
SSLContext object.
Please read Security considerations for best practices.
Note that this may not be done after authentication information has been transmitted, and authentication occurs by
default if possible during a NNTP object initialization.
See NNTP.login() for information on suppressing this
behavior.
New in version 3.2.
Changed
in
version
3.4:
The
method
now
supports
hostname
check
with
ssl.
SSLContext.
NNTP.newgroups(date, *, file=None)
Send a NEWGROUPS command.
The date argument should be a datetime.date or datetime.datetime
object.
Return a pair (response, groups) where groups is a list representing the groups that are new since
the given date.
If file is supplied, though, then groups will be empty.

GroupInfo(group='gmane.network.tor.devel', last='4', first='1', flag='m')
NNTP.newnews(group, date, *, file=None)
Send a NEWNEWS command.
Here, group is a group name or '*', and date has the same meaning as for
newgroups().
Return a pair (response, articles) where articles is a list of message ids.
This command is frequently disabled by NNTP server administrators.
NNTP.list(group_pattern=None, *, file=None)
Send a LIST or LIST ACTIVE command.
Return a pair (response, list) where list is a list of tuples
representing all the groups available from this NNTP server, optionally matching the pattern string group_pattern.
36.11.
Each tuple has the form (group, last, first, flag), where group is a group name, last and first are
the last and first article numbers, and flag usually takes one of these values:
• y: Local postings and articles from peers are allowed.
The group is moderated and all postings must be approved.
If flag has another value, then the status of the newsgroup should be considered unknown.
This command can return very large results, especially if group_pattern is not specified.
It is best to cache the
results offline unless you really need to refresh them.
Changed in version 3.2: group_pattern was added.
NNTP.descriptions(grouppattern)
Send a LIST NEWSGROUPS command, where grouppattern is a wildmat string as specified in RFC 3977 (it’s
essentially the same as DOS or UNIX shell wildcard strings).
Return a pair (response, descriptions),
where descriptions is a dictionary mapping group names to textual descriptions.
If no group matches, return an empty string.
This elides the response code from the server.
If the response code is needed, use descriptions().
NNTP.group(name)
Send a GROUP command, where name is the group name.
The group is selected as the current group, if it exists.
Return a tuple (response, count, first, last, name) where count is the (estimated) number of
articles in the group, first is the first article number in the group, last is the last article number in the group, and
name is the group name.
NNTP.over(message_spec, *, file=None)
Send an OVER command, or an XOVER command on legacy servers.
Return a pair (response, overviews).
Each overview is a dictionary with the same number of items, but
this number depends on the server.
These items are either message headers (the key is then the lower-cased header
name) or metadata items (the key is then the metadata name prepended with ":").
The value of each item is either a string, or None if not present.
It is advisable to use the decode_header() function on header values when they may contain non-ASCII
characters:

['xref', 'from', ':lines', ':bytes', 'references', 'date', 'message-id', 'subject
�→']
'=?UTF-8?B?Ik1hcnRpbiB2LiBMw7Z3aXMi?= <martin@v.loewis.de>'
'"Martin v. Löwis" <martin@v.loewis.de>'
New in version 3.2.
NNTP.help(*, file=None)
Send a HELP command.
Return a pair (response, list) where list is a list of help strings.
NNTP.stat(message_spec=None)
Send a STAT command, where message_spec is either a message id (enclosed in '<' and '>') or an article number
in the current group.
If message_spec is omitted or None, the current article in the current group is considered.
NNTP.next()
Send a NEXT command.
Return as for stat().
NNTP.last()
Send a LAST command.
Return as for stat().
NNTP.article(message_spec=None, *, file=None)
Send an ARTICLE command, where message_spec has the same meaning as for stat().
Return a tuple
(response, info) where info is a namedtuple with three attributes number, message_id and lines (in
that order).
The lines returned (or written to file) will only contain the
message headers, not the body.
NNTP.body(message_spec=None, *, file=None)
Same as article(), but sends a BODY command.
The lines returned (or written to file) will only contain the
message body, not the headers.
NNTP.post(data)
Post an article using the POST command.
The data argument is either a file object opened for binary reading, or
any iterable of bytes objects (representing raw lines of the article to be posted).
It should represent a well-formed
news article, including the required headers.
The post() method automatically escapes lines beginning with .
and appends the termination line.
If the method succeeds, the server’s response is returned.
If the server refuses posting, a NNTPReplyError is
raised.
NNTP.ihave(message_id, data)
Send an IHAVE command. message_id is the id of the message to send to the server (enclosed in '<' and '>').
The data parameter and the return value are the same as for post().
NNTP.date()
Return a pair (response, date).
NNTP.slave()
Send a SLAVE command.
Set the instance’s debugging level.
This controls the amount of debugging output printed.
The default, 0, produces
no debugging output.
A value of 1 produces a moderate amount of debugging output, generally a single line per
request or response.
A value of 2 or higher produces the maximum amount of debugging output, logging each line
sent and received on the connection (including message text).
The following are optional NNTP extensions defined in RFC 2980.
Some of them have been superseded by newer
commands in RFC 3977.
NNTP.xhdr(hdr, str, *, file=None)
Send an XHDR command.
The hdr argument is a header keyword, e.g. 'subject'.
The str argument should
have the form 'first-last' where first and last are the first and last article numbers to search.
Return a pair
(response, list), where list is a list of pairs (id, text), where id is an article number (as a string) and
text is the text of the requested header for that article.
If the file parameter is supplied, then the output of the XHDR
command is stored in a file.
If file is a string, then the method will open a file with that name, write to it then close
it.
If file is a file object, then it will start calling write() on it to store the lines of the command output.
If file is
supplied, then the returned list is an empty list.
NNTP.xover(start, end, *, file=None)
Send an XOVER command.
The return
value is the same of for over().
The module also defines the following utility function:
nntplib.decode_header(header_str)
Decode a header value, un-escaping any escaped non-ASCII characters.
The
unescaped value is returned.
Using this function is recommended to display some headers in a human readable
form:
'Some subject'
'Débuter en Python'
'Re: problème de matrice'
36.12 optparse — Parser for command line options
Source code: Lib/optparse.py
Deprecated since version 3.2: The optparse module is deprecated and will not be developed further; development will
continue with the argparse module.
With these few lines of code, users of your script can now do the “usual thing” on the command-line, for example:
<yourscript> --file=outfile -q
As it parses the command line, optparse sets attributes of the options object returned by parse_args() based
on user-supplied command-line values.
When parse_args() returns from parsing this command line, options.
Thus, the following command lines are all equivalent to the above example:
36.12.
To that end, it supports only the most common command-line syntax and semantics conventionally used
under Unix.
If you are unfamiliar with these conventions, read this section to acquaint yourself with them.
Terminology
argument a string entered on the command-line, and passed by the shell to execl() or execv().
In Python, ar-
guments are elements of sys.argv[1:] (sys.argv[0] is the name of the program being executed).
Unix
shells also use the term “word”.
It is occasionally desirable to substitute an argument list other than sys.argv[1:], so you should read “argu-
ment” as “an element of sys.argv[1:], or of some other list provided as a substitute for sys.argv[1:]”.
There are many
different syntaxes for options; the traditional Unix syntax is a hyphen (“-”) followed by a single letter, e.g. -x
or -F.
Also, traditional Unix syntax allows multiple options to be merged into a single argument, e.g. -x -F is
equivalent to -xF.
The GNU project introduced -- followed by a series of hyphen-separated words, e.g. --file
or --dry-run.
These are the only two option syntaxes provided by optparse.
These option syntaxes are not supported by optparse, and they never will be.
This is deliberate: the first three
are non-standard on any environment, and the last only makes sense if you’re exclusively targeting Windows or
certain legacy platforms (e.g. VMS, MS-DOS).
With optparse, option arguments may either be in a separate argument from
their option:
-f foo
--file foo
or included in the same argument:
-ffoo
--file=foo
Typically, a given option either takes an argument or it doesn’t.
Lots of people want an “optional option arguments”
feature, meaning that some options will take an argument if they see it, and won’t if they don’t.
This is somewhat
controversial, because it makes parsing ambiguous: if -a takes an optional argument and -b is another option
entirely, how do we interpret -ab?
Because of this ambiguity, optparse does not support this feature.
For example, consider this hypothetical command-line:
prog -v --report report.txt foo bar
-v and --report are both options.
Assuming that --report takes one argument, report.txt is an option
argument.
What are options for?
Options are used to provide extra information to tune or customize the execution of a program.
In case it wasn’t clear,
options are usually optional.
A program should be able to run just fine with no options whatsoever.
Can it run without any options at all and still make sense?
Lots of people want their programs to have “required options”.
Think about it.
If it’s required, then it’s not optional!
If
there is a piece of information that your program absolutely requires in order to run successfully, that’s what positional
arguments are for.
As an example of good command-line interface design, consider the humble cp utility, for copying files.
It doesn’t make
much sense to try to copy files without supplying a destination and at least one source.
Hence, cp fails if you run it with
no arguments.
DEST-DIR
You can get pretty far with just that.
Most cp implementations provide a bunch of options to tweak exactly how the files
are copied: you can preserve mode and modification time, avoid following symlinks, ask before clobbering existing files,
etc.
But none of this distracts from the core mission of cp, which is to copy either one file to another, or several files to
another directory.
36.12.
What are positional arguments for?
Positional arguments are for those pieces of information that your program absolutely, positively requires to run.
A good user interface should have as few absolute requirements as possible.
If your program requires 17 distinct pieces
of information in order to run successfully, it doesn’t much matter how you get that information from the user—most
people will give up and walk away before they successfully run the program.
This applies whether the user interface is a
command-line, a configuration file, or a GUI: if you make that many demands on your users, most of them will simply
give up.
In short, try to minimize the amount of information that users are absolutely required to supply—use sensible defaults
whenever possible.
Of course, you also want to make your programs reasonably flexible.
That’s what options are for.
Again, it doesn’t matter if they are entries in a config file, widgets in the “Preferences” dialog of a GUI, or command-line
options—the more options you implement, the more flexible your program is, and the more complicated its implemen-
tation becomes.
Too much flexibility has drawbacks as well, of course; too many options can overwhelm users and make
your code much harder to maintain.
This section covers the
code patterns that are common to any optparse-based program.
Then you can start defining options.
Each option has one or more option strings, such as -f or --file, and several option attributes that tell optparse
what to expect and what to do when it encounters that option on the command line.
You’re free to define as many short option strings and as many long option strings as you like (including zero), as long as
there is at least one option string overall.
The option strings passed to OptionParser.add_option() are effectively labels for the option defined by that call.
For brevity, we will frequently refer to encountering an option on the command line; in reality, optparse encounters
option strings and looks up options from them.
Of these, action is the most fundamental.
Understanding option actions
Actions tell optparse what to do when it encounters an option on the command line.
There is a fixed set of actions
hard-coded into optparse; adding new actions is an advanced topic covered in section Extending optparse.
Most actions
tell optparse to store a value in some variable—for example, take a string from the command line and store it in an
attribute of options.
If you don’t specify an option action, optparse defaults to store.
The store action
The most common option action is store, which tells optparse to take the next argument (or the remainder of the
current argument), ensure that it is of the correct type, and store it to your chosen destination.
When optparse sees the option string -f, it consumes the next argument, foo.txt, and stores it in options.
filename.
So, after this call to parse_args(), options.filename is "foo.txt".
Some other option types supported by optparse are int and float.
Note that this option has no long option string, which is perfectly acceptable.
Also, there’s no explicit action, since the
default is store.
Let’s parse another fake command-line.
This time, we’ll jam the option argument right up against the option: since -n42
(one argument) is equivalent to -n 42 (two arguments), the code
(options, args) = parser.parse_args(["-n42"])
print(options.num)
will print 42.
If you don’t specify a type, optparse assumes string.
If you don’t supply a destination, optparse figures out a sensible default from the option strings: if the first long option
string is --foo-bar, then the default destination is foo_bar.
If there are no long option strings, optparse looks
at the first short option string: the default destination for -f is f.
optparse also includes the built-in complex type.
Adding types is covered in section Extending optparse.
Here we have two different options with the same destination, which is perfectly OK.
When optparse encounters -v on the command line, it sets options.verbose to True; when it encounters -q,
options.verbose is set to False.
Other actions
Some other actions supported by optparse are:
"store_const" store a constant value, pre-set via Option.const
"append" append this option’s argument to a list
"count" increment a counter by one
"callback" call a specified function
These are covered in section Reference Guide, and section Option Callbacks.
Default values
All of the above examples involve setting some variable (the “destination”) when certain command-line options are seen.
What happens if those options are never seen?
Since we didn’t supply any defaults, they are all set to None.
This is
usually fine, but sometimes you want more control.
First, consider the verbose/quiet example.
Since default values apply to the destination rather than to any particular option, and these two options happen to have the
same destination, this is exactly equivalent:
parser.add_option("-v", action="store_true", dest="verbose")
parser.add_option("-q", action="store_false", dest="verbose", default=True)
Consider this:
parser.add_option("-v", action="store_true", dest="verbose", default=False)
parser.add_option("-q", action="store_false", dest="verbose", default=True)
Again, the default value for verbose will be True: the last default value supplied for any particular destination is the
one that counts.
As before, the last value specified for a given option destination is the one that counts.
For clarity, try to use one method
or the other of setting default values, not both.
Generating help
optparse’s ability to generate help and usage text automatically is useful for creating user-friendly command-line
interfaces.
All you have to do is supply a help value for each option, and optionally a short usage message for your
whole program.
There’s a lot going on here to help optparse generate the best possible help message:
• the script defines its own usage message:
usage = "usage: %prog [options] arg1 arg2"
optparse expands %prog in the usage string to the name of the current program, i.e.
os.path.
The expanded string is then printed before the detailed option help.
If you don’t supply a usage string, optparse uses a bland but sensible default:
"Usage: %prog
[options]", which is fine if your script doesn’t take any positional arguments.
By default, optparse converts the destination variable name to uppercase and uses that for
the meta-variable.
FILE and the informal
semantic description “write output to FILE”.
This is a simple but effective way to make your help text a lot clearer
and more useful for end users.
If an option has no default value (or the default value is None), %default expands
to none.
Grouping Options
When dealing with many options, it is convenient to group these options for better help output.
An OptionParser
can contain several option groups, each of which can contain several options.
An option group is obtained using the class OptionGroup:
class optparse.
OptionGroup(parser, title, description=None)
where
• parser is the OptionParser instance the group will be inserted in to
• title is the group title
• description, optional, is a long description of the group
OptionGroup inherits from OptionContainer (like OptionParser) and so the add_option() method can
be used to add an option to the group.
Once all the options are declared, using the OptionParser method add_option_group() the group is added to
the previously defined parser.
Continuing with the parser defined in the previous section, adding an OptionGroup to a parser is easy:
group = OptionGroup(parser, "Dangerous Options",
"Caution: use these options at your own risk.
It is believed that some
of them bite.
It is believed that some
of them bite.
Debug Options:
-d, --debug
Print debug information
-s, --sql
Print all SQL statements executed
-e
Print every action done
Another interesting method, in particular when working programmatically with option groups is:
OptionParser.get_option_group(opt_str)
Return the OptionGroup to which the short or long option string opt_str (e.g. '-o' or '--option') belongs.
If there’s no such OptionGroup, return None.
36.12.
Apart from that, version can contain anything you like.
When you supply
it, optparse automatically adds a --version option to your parser.
If it encounters this option on the command
line, it expands your version string (by replacing %prog), prints it to stdout, and exits.
For example, if your script is called /usr/bin/foo:
$ /usr/bin/foo --version
foo 1.0
The following two methods can be used to print and get the version string:
OptionParser.print_version(file=None)
Print the version message for the current program (self.version) to file (default stdout).
As with
print_usage(), any occurrence of %prog in self.version is replaced with the name of the current
program.
Does nothing if self.version is empty or undefined.
OptionParser.get_version()
Same as print_version() but returns the version string instead of printing it.
How optparse handles errors
There are two broad classes of errors that optparse has to worry about: programmer errors and user errors.
Pro-
grammer errors are usually erroneous calls to OptionParser.add_option(), e.g. invalid option strings, un-
known option attributes, missing option attributes, etc.
These are dealt with in the usual way: raise an exception (either
optparse.
OptionError or TypeError) and let the program crash.
Handling user errors is much more important, since they are guaranteed to happen no matter how stable your code is.
If optparse’s default error-handling behaviour does not suit your needs, you’ll need to subclass OptionParser and
override its exit() and/or error() methods.
The OptionParser constructor has no required arguments, but a number of optional keyword arguments.
You should
always pass them as keyword arguments, i.e. do not rely on the order in which the arguments are declared.
The usage summary to print when your program is run incorrectly
or with a help option.
When optparse prints the usage string, it expands %prog to os.path.
To suppress a usage
message, pass the special value optparse.
The options in
option_list are added after any options in standard_option_list (a class attribute that may be
36.12.
Deprecated; use add_option()
after creating the parser instead.
Option) Class
to
use
when
adding
options
to
the
parser
in
add_option().
version (default: None) A version string to print when the user supplies a version option.
If you supply
a true value for version, optparse automatically adds a version option with the single option string
--version.
The substring %prog is expanded the same as for usage.
A paragraph of text giving a brief overview of your program.
An instance of optparse.
HelpFormatter that will
be used for printing help text.
If true, optparse will add a help option (with option strings -h and
--help) to the parser.
A paragraph of help text to print after the option help.
Populating the parser
There are several ways to populate the parser with options.
The preferred way is by using OptionParser.
A future version of optparse may split Option into several classes, and make_option() will pick the right class to
instantiate.
You can
specify any number of short or long option strings, but you must specify at least one overall option string.
The canonical way to create an Option instance is with the add_option() method of OptionParser.
The keyword arguments define attributes of the new Option object.
The most important option attribute is action,
and it largely determines which other attributes are relevant or required.
If you pass irrelevant option attributes, or
fail to pass required ones, optparse raises an OptionError exception explaining your mistake.
An option’s action determines what optparse does when it encounters this option on the command-line.
The
standard option actions hard-coded into optparse are:
"store" store this option’s argument (default)
"store_const" store a constant value, pre-set via Option.const
"store_true" store True
"store_false" store False
"append" append this option’s argument to a list
"append_const" append a constant value to a list, pre-set via Option.const
"count" increment a counter by one
"callback" call a specified function
"help" print a usage message including all options and the documentation for them
(If you don’t supply an action, the default is "store".
As you can see, most actions involve storing or updating a value somewhere.
Values.
class optparse.
Values
An object holding parsed argument names and values as attributes.
Normally created by calling when calling
OptionParser.parse_args(), and can be overridden by a custom subclass passed to the values argument
of OptionParser.parse_args() (as described in Parsing arguments).
Option arguments (and various other values) are stored as attributes of this object, according to the dest (destination)
option attribute.
For example, when you call
parser.parse_args()
one of the first things optparse does is create the options object:
36.12.
If one of the options in this parser is defined with
parser.add_option("-f", "--file", action="store", type="string", dest="filename")
and the command-line being parsed includes any of the following:
-ffoo
-f foo
--file=foo
--file foo
then optparse, on seeing this option, will do the equivalent of
options.filename = "foo"
The type and dest option attributes are almost as important as action, but action is the only one that makes sense
for all options.
Option attributes
class optparse.
Option
A single command line argument, with various attributes passed by keyword to the constructor.
Normally created
with OptionParser.add_option() rather than directly, and can be overridden by a custom class via the
option_class argument to OptionParser.
The following option attributes may be passed as keyword arguments to OptionParser.add_option().
Determines optparse’s behaviour when this option is seen on the command line; the available options are doc-
umented here.
The argument type expected by this option (e.g., "string" or "int"); the available option types are documented
here.
If the option’s action implies writing or modifying a value somewhere, this tells optparse where to write it:
dest names an attribute of the options object that optparse builds as it parses the command line.
The value to use for this option’s destination if the option is not seen on the command line.
See also
OptionParser.set_defaults().
How many arguments of type type should be consumed when this option is seen.
If > 1, optparse will store
a tuple of values to dest.
Option.choices
For options of type "choice", the list of strings the user may choose from.
For options with action "callback", the callable to call when this option is seen.
See section Option Callbacks
for detail on the arguments passed to the callable.
Option.callback_args
Option.callback_kwargs
Additional positional and keyword arguments to pass to callback after the four standard callback arguments.
Option.help
Help text to print for this option when listing all available options after the user supplies a help option (such as
--help).
If no help text is supplied, the option will be listed without help text.
To hide this option, use the special
value optparse.
SUPPRESS_HELP.
Option.metavar
(default: derived from option strings)
Stand-in for the option argument(s) to use when printing help text.
See section Tutorial for an example.
Standard option actions
The various option actions all have slightly different requirements and effects.
Most actions have several relevant option
attributes which you may specify to guide optparse’s behaviour; a few have required attributes, which you must specify
for any option using that action.
If nargs > 1, multiple arguments will be consumed from the command line; all will be converted according to
type and stored to dest as a tuple.
See the Standard option types section.
If choices is supplied (a list or tuple of strings), the type defaults to "choice".
If type is not supplied, it defaults to "string".
If dest is not supplied, optparse derives a destination from the first long option string (e.g., --foo-bar
implies foo_bar).
If there are no long option strings, optparse derives a destination from the first short option
string (e.g., -f implies f).
Example:
parser.add_option("-f")
parser.add_option("-p", type="float", nargs=3, dest="point")
As it parses the command line
-f foo.txt -p 1 -3.5 4 -fbar.txt
optparse will set
options.f = "foo.txt"
options.point = (1.0, -3.5, 4.0)
options.f = "bar.txt"
36.12.
The value const is stored in dest.
If --noisy is seen, optparse will set
options.verbose = 2
• "store_true" [relevant: dest]
A special case of "store_const" that stores True to dest.
The option must be followed by an argument, which is appended to the list in dest.
If no default value for dest is
supplied, an empty list is automatically created when optparse first encounters this option on the command-line.
If nargs > 1, multiple arguments are consumed, and a tuple of length nargs is appended to dest.
The defaults for type and dest are the same as for the "store" action.
The append action calls the append method on the current value of the option.
This means that any default
value specified must have an append method.
If no default value is supplied, dest is set to zero before being incremented
the first time.
See section Option Callbacks for more detail.
The help message is constructed
from the usage string passed to OptionParser’s constructor and the help string passed to every option.
If no help string is supplied for an option, it will still be listed in the help message.
To omit an option entirely,
use the special value optparse.
SUPPRESS_HELP.
If optparse sees either -h or --help on the command line, it will print something like the following help
message to stdout (assuming sys.argv[0] is "foo.py"):
36.12.
The version number is actually formatted
and printed by the print_version() method of OptionParser.
Generally only relevant if the version
argument is supplied to the OptionParser constructor.
As with help options, you will rarely create version
options, since optparse automatically adds them when needed.
Standard option types
optparse has five built-in option types: "string", "int", "choice", "float" and "complex".
If you need
to add new option types, see section Extending optparse.
Arguments to string options are not checked or converted in any way: the text on the command line is stored in the
destination (or passed to the callback) as-is.
Integer arguments (type "int") are parsed as follows:
• if the number starts with 0x, it is parsed as a hexadecimal number
• if the number starts with 0, it is parsed as an octal number
• if the number starts with 0b, it is parsed as a binary number
• otherwise, the number is parsed as a decimal number
The conversion is done by calling int() with the appropriate base (2, 8, 10, or 16).
If this fails, so will optparse,
although with a more useful error message.
The choices option attribute (a sequence of strings) defines
the set of allowed option arguments.
Parsing arguments
The whole point of creating and populating an OptionParser is to call its parse_args() method.
OptionParser.parse_args(args=None, values=None)
Parse the command-line options found in args.
Values instance created by
optparse
args the leftover positional arguments after all options have been processed
The most common usage is to supply neither keyword argument.
If you supply values, it will be modified with
repeated setattr() calls (roughly one for every option argument stored to an option destination) and returned by
parse_args().
If parse_args() encounters any errors in the argument list, it calls the OptionParser’s error() method with an
appropriate end-user error message.
This ultimately terminates your process with an exit status of 2 (the traditional Unix
exit status for command-line errors).
The default behavior of the option parser can be customized slightly, and you can also poke around your option parser
and see what’s there.
OptionParser provides several methods to help you out:
OptionParser.disable_interspersed_args()
Set parsing to stop on the first non-option.
This restores traditional Unix syntax, where
option parsing stops with the first non-option argument.
Use this if you have a command processor which runs another command which has options of its own and you want
to make sure these options don’t get confused.
For example, each command might have a different set of options.
OptionParser.enable_interspersed_args()
Set parsing to not stop on the first non-option, allowing interspersing switches with command arguments.
This is
the default behavior.
OptionParser.get_option(opt_str)
Returns the Option instance with the option string opt_str, or None if no options have that option string.
OptionParser.has_option(opt_str)
Return True if the OptionParser has an option with option string opt_str (e.g., -q or --verbose).
OptionParser.remove_option(opt_str)
If the OptionParser has an option corresponding to opt_str, that option is removed.
If that option provided any
other option strings, all of those option strings become invalid.
If opt_str does not occur in any option belonging to
this OptionParser, raises ValueError.
Every time you add an option, optparse checks for conflicts with existing options.
If it finds any, it invokes the current
conflict-handling mechanism.
At this point, optparse detects that a previously added option is already using the -n option string.
Since
conflict_handler is "resolve", it resolves the situation by removing -n from the earlier option’s list of op-
tion strings.
Now --dry-run is the only way for the user to activate that option.
It’s possible to whittle away the option strings for a previously added option until there are none left, and the user has
no way of invoking that option from the command-line.
In that case, optparse removes that option completely, so it
doesn’t show up in help text or anywhere else.
This should not be a problem for Python’s garbage collector, but you
may wish to break the cyclic references explicitly by calling destroy() on your OptionParser once you are done with it.
This is particularly useful in long-running applications where large object graphs are reachable from your OptionParser.
Other methods
OptionParser supports several other public methods:
OptionParser.set_usage(usage)
Set the usage string according to the rules described above for the usage constructor keyword argument.
OptionParser.print_usage(file=None)
Print the usage message for the current program (self.usage) to file (default stdout).
Any occurrence of the
string %prog in self.usage is replaced with the name of the current program.
Does nothing if self.usage
is empty or not defined.
OptionParser.get_usage()
Same as print_usage() but returns the usage string instead of printing it.
Using set_defaults() is the preferred way to set
default values for options, since multiple options can share the same destination.
For example, if several “mode”
options all set the same destination, any one of them can set the default, and the last one wins:
parser.add_option("--advanced", action="store_const",
dest="mode", const="advanced",
default="novice")
# overridden below
parser.add_option("--novice", action="store_const",
dest="mode", const="novice",
default="advanced")
# overrides above setting
To avoid this confusion, use set_defaults():
parser.set_defaults(mode="advanced")
parser.add_option("--advanced", action="store_const",
dest="mode", const="advanced")
parser.add_option("--novice", action="store_const",
dest="mode", const="novice")
36.12.4 Option Callbacks
When optparse’s built-in actions and types aren’t quite enough for your needs, you have two choices: extend
optparse or define a callback option.
Extending optparse is more general, but overkill for a lot of simple cases.
Quite often a simple callback is all you need.
There are two steps to defining a callback option:
• define the option itself using the "callback" action
• write the callback; this is a function (or method) that takes at least four arguments, as described below
36.12.
Apart
from action, the only option attribute you must specify is callback, the function to call:
parser.add_option("-c", action="callback", callback=my_callback)
callback is a function (or other callable object), so you must have already defined my_callback() when you create
this callback option.
In this simple case, optparse doesn’t even know if -c takes any arguments, which usually means
that the option takes no arguments—the mere presence of -c on the command-line is all it needs to know.
In some
circumstances, though, you might want your callback to consume an arbitrary number of command-line arguments.
This
is where writing callbacks gets tricky; it’s covered later in this section.
The four arguments to a callback are described below.
There are several other option attributes that you can supply when you define a callback option:
type has its usual meaning: as with the "store" or "append" actions, it instructs optparse to consume one
argument and convert it to type.
Rather than storing the converted value(s) anywhere, though, optparse passes
it to your callback function.
It then passes a tuple of converted values to your callback.
If type for this option is None (no argument
expected), then value will be None.
If nargs > 1, value will be a tuple of values of the appropriate type.
Feel free to modify parser.largs, e.g. by adding more arguments to it.
Feel free to modify parser.rargs, e.g. by
consuming more arguments.
OptionValues).
This lets callbacks use the same mechanism as the rest of optparse for storing option values; you don’t
need to mess around with globals or closures.
You can also access or modify the value(s) of any options
already encountered on the command-line.
Raising errors in a callback
The callback function should raise OptionValueError if there are any problems with the option or its argument(s).
Your message
should be clear, concise, accurate, and mention the option at fault.
Otherwise, the user will have a hard time figuring out
what they did wrong.
Of course, you could do that with the "store_true" action.
Callback example 2: check option order
Here’s a slightly more interesting example: record the fact that -a is seen, but blow up if it comes after -b in the
command-line.
If you want to re-use this callback for several similar options (set a flag, but blow up if -b has already been seen), it needs
a bit of work: the error message and the flag that it sets must be generalized.
Callback example 4: check arbitrary condition
Of course, you could put any condition in there—you’re not limited to checking the values of already-defined options.
Specifying
that a callback option takes arguments is similar to defining a "store" or "append" option: if you define type, then
the option takes one argument that must be convertible to that type; if you further define nargs, then the option takes
nargs arguments.
Here’s an example that just emulates the standard "store" action:


parser.add_option("--foo",
action="callback", callback=store_value,
type="int", nargs=3, dest="foo")
Note that optparse takes care of consuming 3 arguments and converting them to integers for you; all you have to do
is store them.
Things get hairy when you want an option to take a variable number of arguments.
For this case, you must write a
callback, as optparse doesn’t provide any built-in capabilities for it.
And you have to deal with certain intricacies
of conventional Unix command-line parsing that optparse normally handles for you.
In particular, callbacks should
implement the conventional rules for bare -- and - arguments:
• either -- or - can be option arguments
• bare -- (if not the argument to some option): halt command-line processing and discard the --
• bare - (if not the argument to some option): halt command-line processing but keep the - (append it to parser.
largs)
If you want an option that takes a variable number of arguments, there are several subtle, tricky issues to worry about.
The
exact implementation you choose will be based on which trade-offs you’re willing to make for your application (which is
why optparse doesn’t support this sort of thing directly).
This class has a couple of
attributes that define optparse’s types: TYPES and TYPE_CHECKER.
Option.
A tuple of type names; in your subclass, simply define a new tuple TYPES that builds on the standard one.
Option.TYPE_CHECKER
A dictionary mapping type names to type-checking functions.
A type-checking function has the following signature:
def check_mytype(option, opt, value)
where option is an Option instance, opt is an option string (e.g., -f), and value is the string from the
command line that must be checked and converted to your desired type.
The value returned by a type-checking function will wind up in the
OptionValues instance returned by OptionParser.parse_args(), or be passed to a callback as the value
parameter.
Your
type-checking
function
should
raise
OptionValueError
if
it
encounters
any
problems.
OptionValueError takes a single string argument, which is passed as-is to OptionParser’s error()
method, which in turn prepends the program name and the string "error:" and prints everything to stderr
before terminating the process.
Here’s a silly example that demonstrates adding a "complex" option type to parse Python-style complex numbers on
the command line.
That’s it!
These options require a type attribute to the Option constructor.
These are overlapping sets: some default “store” actions are "store", "store_const", "append", and "count",
while the default “typed” actions are "store", "append", and "callback".
When you add an action, you need to categorize it by listing it in at least one of the following class attributes of Option
(all are lists of strings):
Option.
All actions must be listed in ACTIONS.
Option.
STORE_ACTIONS
“store” actions are additionally listed here.
Option.
TYPED_ACTIONS
“typed” actions are additionally listed here.
Option.
ALWAYS_TYPED_ACTIONS
Actions that always take a type (i.e. whose options always take a value) are additionally listed here.
The only effect
of this is that optparse assigns the default type, "string", to options with no explicit type whose action is
listed in ALWAYS_TYPED_ACTIONS.
In order to actually implement your new action, you must override Option’s take_action() method and add a case
that recognizes your action.
For example, let’s add an "extend" action.
This is similar to the standard "append" action, but instead of taking a
single value from the command-line and appending it to an existing list, "extend" will take multiple values in a single
comma-delimited string, and extend an existing list with them.
That is, if --names is an "extend" option of type
"string", the command line
--names=foo,bar --names blah --names ding,dong
would result in a list
["foo", "bar", "blah", "ding", "dong"]
Again we define a subclass of Option:
36.12.
Values class, which provides the very useful ensure_value()
method.
If the attr attribute of values doesn’t exist or is None, then ensure_value() first sets it to value, and then
returns value.
This is very handy for actions like "extend", "append", and "count", all of which accu-
mulate data in a variable and expect that variable to be of a certain type (a list for the first two, an integer for the
latter).
Using ensure_value() means that scripts using your action don’t have to worry about setting a default
value for the option destinations in question; they can just leave the default as None and ensure_value() will
take care of getting it right when it’s needed.
OptionError
Raised if an Option instance is created with invalid or inconsistent arguments.
OptionConflictError
Raised if conflicting options are added to an OptionParser.
OptionValueError
Raised if an invalid option value is encountered on the command line.
BadOptionError
Raised if an invalid option is passed on the command line.
The ossaudiodev module is deprecated (see PEP
594 for details).
This module allows you to access the OSS (Open Sound System) audio interface.
OSS is available for a wide range of
open-source and commercial Unices, and is the standard audio interface for Linux and recent versions of FreeBSD.
Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.
See also:
Open Sound System Programmer’s Guide the official documentation for the OSS C API
The module defines a large number of constants supplied by the OSS device driver; see <sys/soundcard.h> on
either Linux or FreeBSD for a listing.
This exception is raised on certain errors.
The argument is a string describing what went wrong.
This object supports many file-like methods, such as
read(), write(), and fileno() (although there are subtle differences between conventional Unix read/write
semantics and those of OSS audio devices).
It also supports a number of audio-specific methods; see below for the
complete list of methods.
If it is not specified, this module first looks in the environment variable
AUDIODEV for a device to use.
If not found, it falls back to /dev/dsp.
mode is one of 'r' for read-only (record) access, 'w' for write-only (playback) access and 'rw' for both.
Since
many sound cards only allow one process to have the recorder or player open at a time, it is a good idea to open the
device only for the activity needed.
Further, some sound cards are half-duplex: they can be opened for reading or
writing, but not both at once.
Note the unusual calling syntax: the first argument is optional, and the second is required.
This is a historical
artifact for compatibility with the older linuxaudiodev module which ossaudiodev supersedes.
If it is not
specified, this module first looks in the environment variable MIXERDEV for a device to use.
This is more
convenient, but may not be as flexible in all cases.
The audio device objects returned by open() define the following methods and (read-only) attributes:
oss_audio_device.close()
Explicitly close the audio device.
When you are done writing to or reading from an audio device, you should
explicitly close it.
A closed device cannot be used again.
Unlike most Unix device drivers, OSS audio
devices in blocking mode (the default) will block read() until the entire requested amount of data is available.
If the audio device is
in blocking mode (the default), the entire data is always written (again, this is different from usual Unix device
semantics).
If the device is in non-blocking mode, some data may not be written—see writeall().
Writable bytes-like object is now accepted.
If the device is in blocking mode (the
default), this has the same effect as write(); writeall() is only useful in non-blocking mode.
Has no return
value, since the amount of data written is always equal to the amount of data supplied.
Writable bytes-like object is now accepted.
Changed in version 3.2: Audio device objects also support the context management protocol, i.e. they can be used in a
with statement.
The following methods each map to exactly one ioctl() system call.
The correspondence is obvious: for example,
setfmt() corresponds to the SNDCTL_DSP_SETFMT ioctl, and sync() to SNDCTL_DSP_SYNC (this can be
useful when consulting the OSS documentation).
If the underlying ioctl() fails, they all raise OSError.
Once in non-blocking mode, there is no way to return it to blocking mode.
Some older devices only support AFMT_U8; the most common format used today is
AFMT_S16_LE.
oss_audio_device.setfmt(format)
Try to set the current audio format to format—see getfmts() for a list.
Returns the audio format that the device
was set to, which may not be the requested format.
May also be used to return the current audio format—do this
by passing an “audio format” of AFMT_QUERY.
oss_audio_device.channels(nchannels)
Set the number of output channels to nchannels.
A value of 1 indicates monophonic sound, 2 stereophonic.
Some
devices may have more than 2 channels, and some high-end devices may not support mono.
Returns the number
of channels the device was set to.
Returns the rate actually set.
Most sound
devices don’t support arbitrary sampling rates.
The OSS documentation recommends closing and re-opening the device rather than using sync().
The OSS
documentation recommends closing and re-opening the device after calling reset().
You might use this after playing a spot sound effect, before waiting for user input, or before
doing disk I/O.
If strict is true, setparameters() checks to see if each parameter was actually set to the requested
value, and raises OSSAudioError if not.
Returns a tuple (format, nchannels, samplerate) indicating the pa-
rameter values that were actually set by the device driver (i.e., the same as the return values of setfmt(),
channels(), and speed()).
For example,
(fmt, channels, rate) = dsp.setparameters(fmt, channels, rate)
is equivalent to
fmt = dsp.setfmt(fmt)
channels = dsp.channels(channels)
rate = dsp.rate(rate)
oss_audio_device.bufsize()
Returns the size of the hardware buffer, in samples.
Audio device objects also support several read-only attributes:
oss_audio_device.closed
Boolean indicating whether the device has been closed.
This method closes the open mixer device file.
Any further attempts to use the mixer after this file is closed will
raise an OSError.
Changed in version 3.2: Mixer objects also support the context management protocol.
This method returns a bitmask specifying the available mixer controls (“Control” being a specific mixable “channel”,
such as SOUND_MIXER_PCM or SOUND_MIXER_SYNTH).
This bitmask indicates a subset of all available mixer
controls—the SOUND_MIXER_* constants defined at module level.
For most purposes, the SOUND_MIXER_VOLUME (master volume) and SOUND_MIXER_PCM controls should
suffice—but code that uses the mixer should be flexible when it comes to choosing mixer controls.
On the Gravis
Ultrasound, for example, SOUND_MIXER_VOLUME does not exist.
If a bit is set, the corresponding control is stereo; if it is
unset, the control is either monophonic or not supported by the mixer (use in combination with controls() to
determine which).
See the code example for the controls() function for an example of getting data from a bitmask.
Returns the volume of a given mixer control.
The returned volume is a 2-tuple (left_volume,
right_volume).
Volumes are specified as numbers from 0 (silent) to 100 (full volume).
If the control is
monophonic, a 2-tuple is still returned, but both volumes are the same.
Raises OSSAudioError if an invalid control is specified, or OSError if an unsupported control is specified.
Sets the volume for a given mixer control to (left,right).
On success, the new volume is returned as a 2-tuple.
Note that this may not be
exactly the same as the volume specified, because of the limited resolution of some soundcard’s mixers.
Raises OSSAudioError if an invalid mixer control was specified, or if the specified volumes were out-of-range.
This method returns a bitmask indicating which control(s) are currently being used as a recording source.
Call this function to specify a recording source.
Returns a bitmask indicating the new recording source (or sources)
if successful; raises OSError if an invalid source was specified.
The pipes module is deprecated (see PEP 594 for
details).
Please use the subprocess module instead.
The pipes module defines a class to abstract the concept of a pipeline — a sequence of converters from one file to
another.
36.14.
Because the module uses /bin/sh command lines, a POSIX or compatible shell for os.system() and os.
popen() is required.
Availability: Unix, not VxWorks.
The pipes module defines the following class:
class pipes.
Template
An abstraction of a pipeline.
Restore a pipeline template to its initial state.
Template.clone()
Return a new, equivalent, pipeline template.
If flag is true, turn debugging on.
Otherwise, turn debugging off.
Append a new action at the end.
The cmd variable must be a valid bourne shell command.
The kind variable
consists of two letters.
Template.prepend(cmd, kind)
Add a new action at the beginning.
See append() for explanations of the arguments.
Template.open(file, mode)
Return a file-like object, open to file, but read from or written to by the pipeline.
Note that only one of 'r', 'w'
may be given.
Template.copy(infile, outfile)
Copy infile to outfile through the pipe.
The sndhdr module is deprecated (see PEP 594 for
details and alternatives).
The sndhdr provides utility functions which attempt to determine the type of sound data which is in a file.
When these
functions are able to determine what type of sound data is stored in a file, they return a namedtuple(), containing five
attributes: (filetype, framerate, nchannels, nframes, sampwidth).
The value for type indicates the data
type and will be one of the strings 'aifc', 'aiff', 'au', 'hcom', 'sndr', 'sndt', 'voc', 'wav', '8svx',
'sb', 'ub', or 'ul'.
The sampling_rate will be either the actual value or 0 if unknown or difficult to decode.
Similarly,
channels will be either the number of channels or 0 if it cannot be determined or if the value is difficult to decode.
The
value for frames will be either the number of frames or -1.
Determines the type of sound data stored in the file filename using whathdr().
If it succeeds, returns a named-
tuple as described above, otherwise None is returned.
Determines the type of sound data stored in a file based on the file header.
The name of the file is given by filename.
This function returns a namedtuple as described above on success, or None.
Result changed from a tuple to a namedtuple.
Sndtool Sound Files
'voc'
Creative Labs Audio Files
'wav'
Waveform Audio File Format Files
'8svx'
8-Bit Sampled Voice Files
'sb'
Signed Byte Audio Data Files
'ub'
UB Files
'ul'
uLAW Audio Files
sndhdr.tests
A list of functions performing the individual tests.
Each function takes two arguments: the byte-stream and an open
file-like object.
When what() is called with a byte-stream, the file-like object will be None.
The test function should return a string describing the image type if the test succeeded, or None if it failed.
Example:
'wav'
(continues on next page)
36.15.
The spwd module is deprecated (see PEP 594 for details
and alternatives).
This module provides access to the Unix shadow password database.
It is available on various Unix versions.
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
You must have enough privileges to access the shadow password database (this usually means you have to be root).
Shadow password database entries are reported as a tuple-like object, whose attributes correspond to the members of the
spwd structure (Attribute field below, see <shadow.h>):
Index
Attribute
Meaning

sp_namp
Login name

sp_pwdp
Encrypted password

sp_lstchg
Date of last change

sp_min
Minimal number of days between changes

sp_max
Maximum number of days between changes

sp_warn
Number of days before password expires to warn user about it

sp_inact
Number of days after password expires until account is disabled

sp_expire
Number of days since 1970-01-01 when account expires

sp_flag
Reserved
The sp_namp and sp_pwdp items are strings, all others are integers.
KeyError is raised if the entry asked for cannot
be found.
Return the shadow password database entry for the given user name.
Raises a PermissionError instead of KeyError if the user doesn’t have privileges.
See also:
Module grp An interface to the group database, similar to this.
Module pwd An interface to the normal password database, similar to this.
The sunau module is deprecated (see PEP 594 for
details).
The sunau module provides a convenient interface to the Sun AU sound format.
Note that this module is interface-
compatible with the modules aifc and wave.
An audio file consists of a header followed by the data.
The fields of the header are:
Field
Contents
magic word
The four bytes .snd.
Apart from the info field, all header fields are 4 bytes in size.
They are all 32-bit unsigned integers encoded in big-endian
byte order.
If file is a string, open the file by that name, otherwise treat it as a seekable file-like object.
Note that it does not allow read/write files.
A mode of 'r' returns an AU_read object, while a mode of 'w' or 'wb' returns an AU_write object.
The sunau module defines the following exception:
exception sunau.
Error
An error raised when something is impossible because of Sun AU specs or implementation deficiency.
The sunau module defines the following data items:
sunau.
An integer every valid Sun AU file begins with, stored in big-endian form.
This is the string .snd interpreted as
an integer.
sunau.
AUDIO_FILE_ENCODING_MULAW_8
sunau.
AUDIO_FILE_ENCODING_LINEAR_8
sunau.
AUDIO_FILE_ENCODING_LINEAR_16
sunau.
AUDIO_FILE_ENCODING_LINEAR_24
sunau.
AUDIO_FILE_ENCODING_LINEAR_32
36.17.
AUDIO_FILE_ENCODING_ALAW_8
Values of the encoding field from the AU header which are supported by this module.
AUDIO_FILE_ENCODING_FLOAT
sunau.
AUDIO_FILE_ENCODING_DOUBLE
sunau.
AUDIO_FILE_ENCODING_ADPCM_G721
sunau.
AUDIO_FILE_ENCODING_ADPCM_G722
sunau.
AUDIO_FILE_ENCODING_ADPCM_G723_3
sunau.
AUDIO_FILE_ENCODING_ADPCM_G723_5
Additional known values of the encoding field from the AU header, but which are not supported by this module.
AU_read.getnchannels()
Returns number of audio channels (1 for mono, 2 for stereo).
AU_read.getsampwidth()
Returns sample width in bytes.
AU_read.getframerate()
Returns sampling frequency.
AU_read.getnframes()
Returns number of audio frames.
AU_read.getcomptype()
Returns compression type.
Supported compression types are 'ULAW', 'ALAW' and 'NONE'.
AU_read.getcompname()
Human-readable version of getcomptype().
The supported types have the respective names 'CCITT G.711
u-law', 'CCITT G.711 A-law' and 'not compressed'.
AU_read.getparams()
Returns
a
namedtuple()
(nchannels, sampwidth, framerate, nframes, comptype,
compname), equivalent to output of the get*() methods.
AU_read.readframes(n)
Reads and returns at most n frames of audio, as a bytes object.
The data will be returned in linear format.
If the
original data is in u-LAW format, it will be converted.
AU_read.rewind()
Rewind the file pointer to the beginning of the audio stream.
Set the file pointer to the specified position.
Note that the returned value has nothing to do with the actual position in the
file.
The following two functions are defined for compatibility with the aifc, and don’t do anything interesting.
AU_read.getmarkers()
Returns None.
AU_read.getmark(id)
Raise an error.
Set the frame rate.
AU_write.setnframes(n)
Set the number of frames.
This can be later changed, when and if more frames are written.
AU_write.setcomptype(type, name)
Set the compression type and description.
Only 'NONE' and 'ULAW' are supported on output.
The
tuple
should
be
(nchannels, sampwidth, framerate, nframes, comptype,
compname), with values valid for the set*() methods.
Set all parameters.
Write audio frames, without correcting nframes.
Changed in version 3.4: Any bytes-like object is now accepted.
AU_write.writeframes(data)
Write audio frames and make sure nframes is correct.
Changed in version 3.4: Any bytes-like object is now accepted.
AU_write.close()
Make sure nframes is correct, and close the file.
This method is called upon deletion.
Note that it is invalid to set any parameters after calling writeframes() or writeframesraw().
The telnetlib module is deprecated (see PEP 594
for details and alternatives).
The telnetlib module provides a Telnet class that implements the Telnet protocol.
See RFC 854 for details about
the protocol.
In addition, it provides symbolic constants for the protocol characters (see below), and for the telnet op-
tions.
The symbolic names of the telnet options follow the definitions in arpa/telnet.h, with the leading TELOPT_
removed.
For symbolic names of options which are traditionally not included in arpa/telnet.h, see the module
source itself.
The symbolic constants for the telnet commands are: IAC, DONT, DO, WONT, WILL, SE (Subnegotiation End), NOP
(No Operation), DM (Data Mark), BRK (Break), IP (Interrupt process), AO (Abort output), AYT (Are You There), EC
(Erase Character), EL (Erase Line), GA (Go Ahead), SB (Subnegotiation Begin).
Availability: not Emscripten, not WASI.
This module does not work or is not available on WebAssembly platforms wasm32-emscripten and
wasm32-wasi.
See WebAssembly platforms for more information.
Telnet(host=None, port=0[, timeout])
Telnet represents a connection to a Telnet server.
The instance is initially not connected by default; the open()
method must be used to establish a connection.
Alternatively, the host name and optional port number can be
passed to the constructor too, in which case the connection to the server will be established before the constructor
returns.
The optional timeout parameter specifies a timeout in seconds for blocking operations like the connection
attempt (if not specified, the global default timeout setting will be used).
Do not reopen an already connected instance.
This class has many read_*() methods.
Note that some of them raise EOFError when the end of the connec-
tion is read, because they can return an empty string for other reasons.
See the individual descriptions below.
A Telnet object is a context manager and can be used in a with statement.
When no match is found, return whatever is available instead, possibly empty bytes.
Raise EOFError if the
connection is closed and no cooked data is available.
Telnet.read_all()
Read all data until EOF as bytes; block until connection closed.
Telnet.read_some()
Read at least one byte of cooked data unless EOF is hit.
Return b'' if EOF is hit.
Block if no data is immediately
available.
Telnet.read_very_eager()
Read everything that can be without blocking in I/O (eager).
Raise EOFError if connection closed and no cooked data available.
Return b'' if no cooked data available
otherwise.
Do not block unless in the midst of an IAC sequence.
Telnet.read_eager()
Read readily available data.
Raise EOFError if connection closed and no cooked data available.
Return b'' if no cooked data available
otherwise.
Do not block unless in the midst of an IAC sequence.
Process and return data already in the queues (lazy).
Raise EOFError if connection closed and no data available.
Return b'' if no cooked data available otherwise.
Do not block unless in the midst of an IAC sequence.
Telnet.read_very_lazy()
Return any data available in the cooked queue (very lazy).
Raise EOFError if connection closed and no data available.
Return b'' if no cooked data available otherwise.
This method never blocks.
Telnet.read_sb_data()
Return the data collected between a SB/SE pair (suboption begin/end).
The callback should access these data when
it was invoked with a SE command.
This method never blocks.
Telnet.open(host, port=0[, timeout])
Connect to a host.
The optional second argument is the port number, which defaults to the standard Telnet port
(23).
The optional timeout parameter specifies a timeout in seconds for blocking operations like the connection
attempt (if not specified, the global default timeout setting will be used).
Do not try to reopen an already connected instance.
Raises an auditing event telnetlib.Telnet.open with arguments self, host, port.
Print a debug message when the debug level is > 0.
If extra arguments are present, they are substituted in the
message using the standard string formatting operator.
Telnet.set_debuglevel(debuglevel)
Set the debug level.
The higher the value of debuglevel, the more debug output you get (on sys.stdout).
Telnet.get_socket()
Return the socket object used internally.
Telnet.fileno()
Return the file descriptor of the socket object used internally.
Telnet.write(buffer)
Write a byte string to the socket, doubling any IAC characters.
This can block if the connection is blocked.
May
raise OSError if the connection is closed.
Raises an auditing event telnetlib.Telnet.write with arguments self, buffer.
This method used to raise socket.error, which is now an alias of OSError.
Telnet.interact()
Interaction function, emulates a very dumb Telnet client.
Telnet.mt_interact()
Multithreaded version of interact().
Telnet.expect(list, timeout=None)
Read until one from a list of a regular expressions matches.
The first argument is a list of regular expressions, either compiled (regex objects) or uncompiled (byte strings).
The
optional second argument is a timeout, in seconds; the default is to block indefinitely.
Return a tuple of three items: the index in the list of the first regular expression that matches; the match object
returned; and the bytes read up till and including the match.
If end of file is found and no bytes were read, raise EOFError.
Otherwise, when nothing matches, return (-1,
None, data) where data is the bytes received so far (may be empty bytes if a timeout happened).
If a regular expression ends with a greedy match (such as .*) or if more than one expression can match the same
input, the results are non-deterministic, and may depend on the I/O timing.
Each time a telnet option is read on the input flow, this callback (if set) is called with the following parameters:
callback(telnet socket, command (DO/DONT/WILL/WONT), option).
No other action is done afterwards by
telnetlib.
The uu module is deprecated (see PEP 594 for details).
This module encodes and decodes files in uuencode format, allowing arbitrary binary data to be transferred over ASCII-
only connections.
Wherever a file argument is expected, the methods accept a file-like object.
For backwards compati-
bility, a string containing a pathname is also accepted, and the corresponding file will be opened for reading and writing;
the pathname '-' is understood to mean the standard input or output.
However, this interface is deprecated; it’s better
for the caller to open the file itself, and be sure that, when required, the mode is 'rb' or 'wb' on Windows.
This code was contributed by Lance Ellinghouse, and modified by Jack Jansen.
The uu module defines the following functions:
uu.encode(in_file, out_file, name=None, mode=None, *, backtick=False)
Uuencode file in_file into file out_file.
The uuencoded file will have the header specifying name and mode as
the defaults for the results of decoding the file.
The default defaults are taken from in_file, or '-' and 0o666
respectively.
If backtick is true, zeros are represented by '`' instead of spaces.
Added the backtick parameter.
This call decodes uuencoded file in_file placing the result on file out_file.
If out_file is a pathname, mode is used
to set the permission bits if the file must be created.
Defaults for out_file and mode are taken from the uuencode
header.
However, if the file specified in the header already exists, a uu.
Error is raised.
Setting quiet to a true value silences this warning.
Error
Subclass of Exception, this can be raised by uu.decode() under various situations, such as described above,
but also including a badly formatted header, or truncated input file.
See also:
Module binascii Support module containing ASCII-to-binary and binary-to-ASCII conversions.
36.19.
The xdrlib module is deprecated (see PEP 594 for
details).
The xdrlib module supports the External Data Representation Standard as described in RFC 1014, written by Sun
Microsystems, Inc. June 1987.
It supports most of the data types described in the RFC.
The xdrlib module defines two classes, one for packing variables into XDR representation, and another for unpacking
from XDR representation.
There are also two exception classes.
Packer
Packer is the class for packing data into XDR representation.
The Packer class is instantiated with no argu-
ments.
Unpacker is the complementary class which unpacks XDR data values from a string buffer.
The input buffer is
given as data.
See also:
RFC 1014 - XDR: External Data Representation Standard This RFC defined the encoding of data which was XDR
at the time this module was originally written.
It has apparently been obsoleted by RFC 1832.
RFC 1832 - XDR: External Data Representation Standard Newer RFC that provides a revised definition of XDR.
Packer.reset()
Resets the pack buffer to the empty string.
In general, you can pack any of the most common XDR data types by calling the appropriate pack_type() method.
Each method takes a single argument, the value to pack.
The following simple data type packing methods are supported:
pack_uint(), pack_int(), pack_enum(), pack_bool(), pack_uhyper(), and pack_hyper().
Packer.pack_float(value)
Packs the single-precision floating point number value.
Packer.pack_double(value)
Packs the double-precision floating point number value.
The following methods support packing strings, bytes, and opaque data:
Packer.pack_fstring(n, s)
Packs a fixed length string, s. n is the length of the string but it is not packed into the data buffer.
The string is
padded with null bytes if necessary to guaranteed 4 byte alignment.
The length of the string is first packed as an unsigned integer, then the string data
is packed with pack_fstring().
Packs a variable length byte stream, similarly to pack_string().
The following methods support packing arrays and lists:
Packer.pack_list(list, pack_item)
Packs a list of homogeneous items.
This method is useful for lists with an indeterminate size; i.e. the size is not
available until the entire list has been walked.
For each item in the list, an unsigned integer 1 is packed first,
followed by the data value from the list.
At the
end of the list, an unsigned integer 0 is packed.
For example, to pack a list of integers, the code might appear like this:
import xdrlib
p = xdrlib.
Packer()
p.pack_list([1, 2, 3], p.pack_int)
Packer.pack_farray(n, array, pack_item)
Packs a fixed length list (array) of homogeneous items.
Packer.pack_array(list, pack_item)
Packs a variable length list of homogeneous items.
First, the length of the list is packed as an unsigned integer, then
each element is packed as in pack_farray() above.
Unpacker Objects
The Unpacker class offers the following methods:
Unpacker.reset(data)
Resets the string buffer with the given data.
Unpacker.get_position()
Returns the current unpack position in the data buffer.
Unpacker.set_position(position)
Sets the data buffer unpack position to position.
You should be careful about using get_position() and
set_position().
Unpacker.get_buffer()
Returns the current unpack data buffer as a string.
Unpacker.done()
Indicates unpack completion.
Raises an Error exception if all of the data has not been unpacked.
In addition, every data type that can be packed with a Packer, can be unpacked with an Unpacker.
Unpacking
methods are of the form unpack_type(), and take no arguments.
They return the unpacked object.
36.20.
Unpacker.unpack_double()
Unpacks a double-precision floating point number, similarly to unpack_float().
In addition, the following methods unpack strings, bytes, and opaque data:
Unpacker.unpack_fstring(n)
Unpacks and returns a fixed length string.
Padding with null bytes to
guaranteed 4 byte alignment is assumed.
Unpacker.unpack_fopaque(n)
Unpacks and returns a fixed length opaque data stream, similarly to unpack_fstring().
Unpacker.unpack_string()
Unpacks and returns a variable length string.
The length of the string is first unpacked as an unsigned integer, then
the string data is unpacked with unpack_fstring().
Unpacker.unpack_opaque()
Unpacks and returns a variable length opaque data string, similarly to unpack_string().
Unpacker.unpack_bytes()
Unpacks and returns a variable length byte stream, similarly to unpack_string().
The following methods support unpacking arrays and lists:
Unpacker.unpack_list(unpack_item)
Unpacks and returns a list of homogeneous items.
The list is unpacked one element at a time by first unpacking an
unsigned integer flag.
If the flag is 1, then the item is unpacked and appended to the list.
A flag of 0 indicates the
end of the list.
Unpacker.unpack_farray(n, unpack_item)
Unpacks and returns (as a list) a fixed length array of homogeneous items.
As above, unpack_item is the function used to unpack each element.
Unpacker.unpack_array(unpack_item)
Unpacks and returns a variable length list of homogeneous items.
First, the length of the list is unpacked as an
unsigned integer, then each element is unpacked as in unpack_farray() above.
Error
The base exception class.
Error has a single public attribute msg containing the description of the error.
ConversionError
Class derived from Error.
Contains no additional instance variables.
Here is an example of how you would catch one of these exceptions:
import xdrlib
p = xdrlib.
Packer()
try:
p.pack_double(8.01)
except xdrlib.
The following modules have specific security considerations:
• base64: base64 security considerations in RFC 4648
• cgi: CGI security considerations
• hashlib: all constructors take a “usedforsecurity” keyword-only argument disabling known insecure and blocked
algorithms
• http.server is not suitable for production use, only implementing basic security checks.
See the security
considerations.
When it cannot be used, the -P option or the
PYTHONSAFEPATH environment variable can be used to not prepend a potentially unsafe path to sys.path such as
the current directory, the script’s directory or an empty string.
Security Considerations
APPENDIX
A
GLOSSARY
in the interpreter.
A tool that tries to convert Python 2.x code to Python 3.x code by handling most of the incompatibilities which can
be detected by parsing the source and traversing the parse tree.
See 2to3 — Automated Python 2 to 3 code translation.
ABCs introduce
virtual subclasses, which are classes that don’t inherit from a class but are still recognized by isinstance() and
issubclass(); see the abc module documentation.
Python comes with many built-in ABCs for data structures
(in the collections.abc module), numbers (in the numbers module), streams (in the io module), import
finders and loaders (in the importlib.abc module).
See variable annotation, function annotation, PEP 484 and PEP 526, which describe this functionality.
Also see
annotations-howto for best practices on working with annotations.
There are two kinds of argument:
• keyword argument: an argument preceded by an identifier (e.g. name=) in a function call or passed as a value
in a dictionary preceded by **.
For example, 3 and 5 are both keyword arguments in the following calls to
complex():
complex(real=3, imag=5)
complex(**{'real': 3, 'imag': 5})
• positional argument: an argument that is not a keyword argument.
Positional arguments can appear at the
beginning of an argument list and/or be passed as elements of an iterable preceded by *.
Arguments are assigned to the named local variables in a function body.
See the calls section for the rules governing
this assignment.
Syntactically, any expression can be used to represent an argument; the evaluated value is assigned
to the local variable.
See also the parameter glossary entry, the FAQ question on the difference between arguments and parameters, and
PEP 362.
asynchronous context manager An object which controls the environment seen in an async with statement by
defining __aenter__() and __aexit__() methods.
Introduced by PEP 492.
It looks like a coroutine function
defined with async def except that it contains yield expressions for producing a series of values usable in an
async for loop.
Usually refers to an asynchronous generator function, but may refer to an asynchronous generator iterator in some
contexts.
In cases where the intended meaning isn’t clear, using the full terms avoids ambiguity.
An asynchronous generator function may contain await expressions as well as async for, and async with
statements.
This is an asynchronous iterator which when called using the __anext__() method returns an awaitable object
which will execute the body of the asynchronous generator function until the next yield expression.
Each yield temporarily suspends processing, remembering the location execution state (including local variables
and pending try-statements).
When the asynchronous generator iterator effectively resumes with another awaitable
returned by __anext__(), it picks up where it left off.
See PEP 492 and PEP 525.
Must return an asynchronous iterator
from its __aiter__() method.
Introduced by PEP 492.
asynchronous iterator An
object
that
implements
the
__aiter__()
and
__anext__()
methods.
Introduced by
PEP 492.
attribute A value associated with an object which is usually referenced by name using dotted expressions.
For example,
if an object o has an attribute a it would be referenced as o.a.
It is possible to give an object an attribute whose name is not an identifier as defined by identifiers, for example
using setattr(), if the object allows it.
Such an attribute will not be accessible using a dotted expression, and
would instead need to be retrieved with getattr().
Can be a coroutine or an object with an __await__()
method.
See also PEP 492.
Guido van Rossum, Python’s creator.
Examples of binary files are files opened in binary mode
('rb', 'wb' or 'rb+'), sys.stdin.buffer, sys.stdout.buffer, and instances of io.BytesIO
and gzip.
GzipFile.
See also text file for a file object able to read and write str objects.
It becomes a dangling pointer if the object is destroyed.
For example, a garbage
collection can remove the last strong reference to the object and so destroy it.
The Py_NewRef()
function can be used to create a new strong reference.
This includes all
bytes, bytearray, and array.array objects, as well as many common memoryview objects.
Bytes-
like objects can be used for various operations that work with binary data; these include compression, saving to a
binary file, and sending over a socket.
Some operations need the binary data to be mutable.
The documentation often refers to these as “read-write bytes-
like objects”.
Example mutable buffer objects include bytearray and a memoryview of a bytearray.
Other operations require the binary data to be stored in immutable objects (“read-only bytes-like objects”); examples
of these include bytes and a memoryview of a bytes object.
The bytecode is also cached in .pyc files so that executing the same file is faster the second time
(recompilation from source to bytecode can be avoided).
This “intermediate language” is said to run on a virtual
machine that executes the machine code corresponding to each bytecode.
Do note that bytecodes are not expected
to work between different Python virtual machines, nor to be stable between Python releases.
A list of bytecode instructions can be found in the documentation for the dis module.
An instance of a class that implements the __call__()
method is also a callable.
Class definitions normally contain method definitions which operate
on instances of the class.
Imaginary numbers are real multiples of the imaginary unit (the square root of -1),
often written i in mathematics or j in engineering.
Python has built-in support for complex numbers, which are
written with this latter notation; the imaginary part is written with a j suffix, e.g., 3+1j.
To get access to complex
equivalents of the math module, use cmath.
Use of complex numbers is a fairly advanced mathematical feature.
If you’re not aware of a need for them, it’s almost certain you can safely ignore them.
See PEP 343.
context variable A variable which can have different values depending on its context.
This is similar to Thread-Local
Storage in which each execution thread may have a different value for a variable.
However, with context variables,
there may be several contexts in one execution thread and the main usage for context variables is to keep track of
variables in concurrent asynchronous tasks.
See contextvars.
Zero-dimensional
buffers are C and Fortran contiguous.
In one-dimensional arrays, the items must be laid out in memory next to each
other, in order of increasing indexes starting from zero.
In multidimensional C-contiguous arrays, the last index
varies the fastest when visiting items in order of memory address.
However, in Fortran contiguous arrays, the first
index varies the fastest.
Subroutines are entered at one point and exited at
another point.
Coroutines can be entered, exited, and resumed at many different points.
They can be implemented
with the async def statement.
See also PEP 492.
coroutine function A function which returns a coroutine object.
A coroutine function may be defined with the async
def statement, and may contain await, async for, and async with keywords.
These were introduced by
PEP 492.
CPython The canonical implementation of the Python programming language, as distributed on python.org.
The term
“CPython” is used when necessary to distinguish this implementation from others such as Jython or IronPython.
Common examples for decorators are classmethod() and staticmethod().
The same concept exists for classes, but is less commonly used there.
See the documentation for function definitions
and class definitions for more about decorators.
When a class
attribute is a descriptor, its special binding behavior is triggered upon attribute lookup.
Normally, using a.b to
get, set or delete an attribute looks up the object named b in the class dictionary for a, but if b is a descriptor, the
respective descriptor method gets called.
Understanding descriptors is a key to a deep understanding of Python
because they are the basis for many features including functions, methods, properties, class methods, static methods,
and reference to super classes.
For more information about descriptors’ methods, see descriptors or the Descriptor How To Guide.
The keys can be any object with
__hash__() and __eq__() methods.
Called a hash in Perl.
dictionary comprehension A compact way to process all or part of the elements in an iterable and return a dictionary
with the results.
See comprehensions.
They provide a dynamic view on the dictionary’s entries, which means that when the dictionary
changes, the view reflects these changes.
To force the dictionary view to become a full list use list(dictview).
See Dictionary view objects.
While ignored when
the suite is executed, it is recognized by the compiler and put into the __doc__ attribute of the enclosing class,
function or module.
Since it is available via introspection, it is the canonical place for documentation of the object.
By emphasizing interfaces rather than specific types, well-designed code improves its flexibility by
allowing polymorphic substitution.
Duck-typing avoids tests using type() or isinstance().
Easier to ask for forgiveness than permission.
This common Python coding style assumes the existence of valid
keys or attributes and catches exceptions if the assumption proves false.
The technique contrasts with the LBYL style common to
many other languages such as C.
expression A piece of syntax which can be evaluated to some value.
In other words, an expression is an accumulation
of expression elements like literals, names, attribute access, operators or function calls which all return a value.
In contrast to many other languages, not all language constructs are expressions.
There are also statements which
cannot be used as expressions, such as while.
Assignments are also statements, not expressions.
See also PEP 498.
file object An object exposing a file-oriented API (with methods such as read() or write()) to an underlying re-
source.
Depending on the way it was created, a file object can mediate access to a real on-disk file or to another
type of storage or communication device (for example standard input/output, in-memory buffers, sockets, pipes,
etc.).
File objects are also called file-like objects or streams.
There are actually three categories of file objects: raw binary files, buffered binary files and text files.
Their interfaces
are defined in the io module.
The canonical way to create a file object is by using the open() function.
The filesystem encoding must guarantee to successfully decode all bytes below 128.
If the file system encoding
fails to provide this guarantee, API functions can raise UnicodeError.
The sys.getfilesystemencoding() and sys.getfilesystemencodeerrors() functions can
be used to get the filesystem encoding and error handler.
The filesystem encoding and error handler are configured at Python startup by the PyConfig_Read() function:
see filesystem_encoding and filesystem_errors members of PyConfig.
See also the locale encoding.
Since Python 3.3, there are two types of finder: meta path finders for use with sys.meta_path, and path entry
finders for use with sys.path_hooks.
See PEP 302, PEP 420 and PEP 451 for much more detail.
The floor division operator is //.
For example,
the expression 11 // 4 evaluates to 2 in contrast to the 2.75 returned by float true division.
Note that (-11)
// 4 is -3 because that is -2.75 rounded downward.
See PEP 238.
function A series of statements which returns some value to a caller.
It can also be passed zero or more arguments which
may be used in the execution of the body.
See also parameter, method, and the function section.
Function annotations are usually used for type hints: for example, this function is expected to take two int argu-
ments and is also expected to have an int return value:
def sum_two_numbers(a: int, b: int) -> int:

Function annotation syntax is explained in section function.
See variable annotation and PEP 484, which describe this functionality.
The process of freeing memory when it is not used anymore.
Python performs garbage collection
via reference counting and a cyclic garbage collector that is able to detect and break reference cycles.
The garbage
collector can be controlled using the gc module.
It looks like a normal function except that it contains yield
expressions for producing a series of values usable in a for-loop or that can be retrieved one at a time with the
next() function.
Usually refers to a generator function, but may refer to a generator iterator in some contexts.
In cases where the
intended meaning isn’t clear, using the full terms avoids ambiguity.
Each yield temporarily suspends processing, remembering the location execution state (including local variables
and pending try-statements).
When the generator iterator resumes, it picks up where it left off (in contrast to
functions which start fresh on every invocation).
It looks like a normal expression followed by a for clause
defining a loop variable, range, and an optional if clause.
The combined expression generates values for an en-
closing function:
# sum of squares 0, 1, 4, ... 81

generic function A function composed of multiple functions implementing the same operation for different types.
Which
implementation should be used during a call is determined by the dispatch algorithm.
See also the single dispatch glossary entry, the functools.singledispatch() decorator, and PEP 443.
Used for type hints
and annotations.
For more details, see generic alias types, PEP 483, PEP 484, PEP 585, and the typing module.
GIL See global interpreter lock.
This simplifies the CPython implementation by making the object model (including critical
built-in types such as dict) implicitly safe against concurrent access.
Locking the entire interpreter makes it
easier for the interpreter to be multi-threaded, at the expense of much of the parallelism afforded by multi-processor
machines.
However, some extension modules, either standard or third-party, are designed so as to release the GIL when doing
computationally intensive tasks such as compression or hashing.
Also, the GIL is always released when doing I/O.
Past efforts to create a “free-threaded” interpreter (one which locks shared data at a much finer granularity) have not
been successful because performance suffered in the common single-processor case.
It is believed that overcoming
this performance issue would make the implementation much more complicated and therefore costlier to maintain.
hash-based pyc A bytecode cache file that uses the hash rather than the last-modified time of the corresponding source
file to determine its validity.
See pyc-invalidation.
Hashable objects which compare
equal must have the same hash value.
Hashability makes an object usable as a dictionary key and a set member, because these data structures use the hash
value internally.
Most of Python’s immutable built-in objects are hashable; mutable containers (such as lists or dictionaries) are not;
immutable containers (such as tuples and frozensets) are only hashable if their elements are hashable.
Objects which
are instances of user-defined classes are hashable by default.
They all compare unequal (except with themselves),
and their hash value is derived from their id().
IDLE An Integrated Development and Learning Environment for Python.
IDLE is a basic editor and interpreter envi-
ronment which ships with the standard distribution of Python.
Immutable objects include numbers, strings and tuples.
Such an object cannot
be altered.
A new object has to be created if a different value has to be stored.
They play an important role in
places where a constant hash value is needed, for example as a key in a dictionary.
During
import, this list of locations usually comes from sys.path, but for subpackages it may also come from the parent
package’s __path__ attribute.
importing The process by which Python code in one module is made available to Python code in another module.
Just launch python with no arguments (possibly by
selecting it from your computer’s main menu).
It is a very powerful way to test out new ideas or inspect modules
and packages (remember help(x)).
This means that source files can be run directly without explicitly creating
an executable which is then run.
Interpreted languages typically have a shorter development/debug cycle than
compiled ones, though their programs generally also run more slowly.
See also interactive.
It also makes several calls to the
garbage collector.
This can trigger the execution of code in user-defined destructors or weakref callbacks.
Code
executed during the shutdown phase can encounter various exceptions as the resources it relies on may not function
anymore (common examples are library modules or the warnings machinery).
The main reason for interpreter shutdown is that the __main__ module or the script being run has finished
executing.
Examples of iterables include all sequence types (such
as list, str, and tuple) and some non-sequence types like dict, file objects, and objects of any classes you
define with an __iter__() method or with a __getitem__() method that implements sequence semantics.
Iterables can be used in a for loop and in many other places where a sequence is needed (zip(), map(), …).
When an iterable object is passed as an argument to the built-in function iter(), it returns an iterator for the
object.
This iterator is good for one pass over the set of values.
When using iterables, it is usually not necessary to
call iter() or deal with iterator objects yourself.
The for statement does that automatically for you, creating
a temporary unnamed variable to hold the iterator for the duration of the loop.
See also iterator, sequence, and
generator.
When no more data are available a
StopIteration exception is raised instead.
At this point, the iterator object is exhausted and any further calls
to its __next__() method just raise StopIteration again.
One notable exception is code which attempts multiple iteration passes.
A container
object (such as a list) produces a fresh new iterator each time you pass it to the iter() function or use it in a
for loop.
Attempting this with an iterator will just return the same exhausted iterator object used in the previous
iteration pass, making it appear like an empty container.
More information can be found in Iterator Types.
CPython implementation detail: CPython does not consistently apply the requirement that an iterator define
__iter__().
For
example, locale.strxfrm() is used to produce a sort key that is aware of locale specific sort conventions.
A number of tools in Python accept key functions to control how elements are ordered or grouped.
They in-
clude min(), max(), sorted(), list.sort(), heapq.merge(), heapq.nsmallest(), heapq.
There are several ways to create a key function.
For example.
Alternatively, a key function can be built from a lambda expression such as
lambda r: (r[0], r[2]).
Also, operator.attrgetter(), operator.itemgetter(), and
operator.methodcaller() are three key function constructors.
See the Sorting HOW TO for examples of
how to create and use key functions.
This coding style explicitly tests for pre-conditions before making calls or lookups.
This
style contrasts with the EAFP approach and is characterized by the presence of many if statements.
In a multi-threaded environment, the LBYL approach can risk introducing a race condition between “the looking”
and “the leaping”.
For example, the code, if key in mapping: return mapping[key] can fail if
another thread removes key from mapping after the test, but before the lookup.
This issue can be solved with locks
or by using the EAFP approach.
It can be set with locale.
LC_CTYPE, new_locale).
On Windows, it is the ANSI code page (ex: "cp1252").
On Android and VxWorks, Python uses "utf-8" as the locale encoding.
See also the filesystem encoding and error handler.
Despite its name it is more akin to an array in other languages than to a linked list since
access to elements is O(1).
The if clause is optional.
If omitted,
all elements in range(256) are processed.
It must define a method named load_module().
A loader is typically returned
by a finder.
See PEP 302 for details and importlib.abc.
Loader for an abstract base class.
Mapping
or
collections.abc.
MutableMapping
abstract
base
classes.
OrderedDict
and
collections.
Counter.
Meta path finders are related to, but different
from path entry finders.
See importlib.abc.
MetaPathFinder for the methods that meta path finders implement.
Class definitions create a class name, a class dictionary, and a list of base classes.
The
metaclass is responsible for taking those three arguments and creating the class.
Most object oriented program-
ming languages provide a default implementation.
What makes Python special is that it is possible to create custom
metaclasses.
Most users never need this tool, but when the need arises, metaclasses can provide powerful, ele-
gant solutions.
They have been used for logging attribute access, adding thread-safety, tracking object creation,
implementing singletons, and many other tasks.
More information can be found in metaclasses.
If called as an attribute of an instance of that class, the method
will get the instance object as its first argument (which is usually called self).
See function and nested scope.
See The Python 2.3 Method Resolution Order for details of the algorithm used by the Python interpreter
since the 2.3 release.
Modules have a namespace containing arbitrary
Python objects.
Modules are loaded into Python by the process of importing.
See also package.
A namespace containing the import-related information used to load a module.
An instance of
importlib.machinery.ModuleSpec.
MRO See method resolution order.
See also immutable.
The type or class may have other features as well.
Several built-in types are named tuples, including the values returned by time.localtime() and os.
stat().
Another example is sys.float_info:
# indexed access

# named field access

# kind of tuple
True
Some named tuples are built-in types (such as the above examples).
Alternatively, a named tuple can be created
from a regular class definition that inherits from tuple and that defines named fields.
Such a class can be written
by hand or it can be created with the factory function collections.namedtuple().
The latter technique
also adds some extra methods that may not be found in hand-written or built-in named tuples.
Namespaces are implemented as dictionaries.
There are the local, global
and built-in namespaces as well as nested namespaces in objects (in methods).
Namespaces support modularity by
preventing naming conflicts.
For instance, the functions builtins.open and os.open() are distinguished
by their namespaces.
Namespaces also aid readability and maintainability by making it clear which module im-
plements a function.
Namespace packages may
have no physical representation, and specifically are not like a regular package because they have no __init__.
See also module.
For instance, a function defined inside another
function can refer to variables in the outer function.
Note that nested scopes by default work only for reference and
not for assignment.
Local variables both read and write in the innermost scope.
Likewise, global variables read and
write to the global namespace.
The nonlocal allows writing to outer scopes.
In earlier Python versions,
only new-style classes could use Python’s newer, versatile features like __slots__, descriptors, properties,
__getattribute__(), class methods, and static methods.
Also the ultimate base class of any
new-style class.
Technically, a package is a Python
module with a __path__ attribute.
See also regular package and namespace package.
There are five kinds of parameter:
• positional-or-keyword: specifies an argument that can be passed either positionally or as a keyword argument.
This is the default kind of parameter, for example foo and bar in the following:

• positional-only: specifies an argument that can be supplied only by position.
Positional-only parameters can
be defined by including a / character in the parameter list of the function definition after them, for example
posonly1 and posonly2 in the following:

• keyword-only: specifies an argument that can be supplied only by keyword.
Keyword-only parameters can be
defined by including a single var-positional parameter or bare * in the parameter list of the function definition
before them, for example kw_only1 and kw_only2 in the following:

• var-positional: specifies that an arbitrary sequence of positional arguments can be provided (in addition to any
positional arguments already accepted by other parameters).
Such a parameter can be defined by prepending
the parameter name with *, for example args in the following:

• var-keyword: specifies that arbitrarily many keyword arguments can be provided (in addition to any key-
word arguments already accepted by other parameters).
Such a parameter can be defined by prepending the
parameter name with **, for example kwargs in the example above.
Parameters can specify both optional and required arguments, as well as default values for some optional arguments.
See also the argument glossary entry, the FAQ question on the difference between arguments and parameters, the
inspect.
See importlib.abc.
PathEntryFinder for the methods that path entry finders implement.
A path-like object is either a str or bytes object represent-
ing a path, or an object implementing the os.
PathLike protocol.
An object that supports the os.
PathLike
protocol can be converted to a str or bytes file system path by calling the os.fspath() function; os.
Introduced by PEP 519.
PEP Python Enhancement Proposal.
A PEP is a design document providing information to the Python community,
or describing a new feature for Python or its processes or environment.
PEPs should provide a concise technical
specification and a rationale for proposed features.
PEPs are intended to be the primary mechanisms for proposing major new features, for collecting community input
on an issue, and for documenting the design decisions that have gone into Python.
The PEP author is responsible
for building consensus within the community and documenting dissenting opinions.
See PEP 1.
portion A set of files in a single directory (possibly stored in a zip file) that contribute to a namespace package, as defined
in PEP 420.
positional argument See argument.
While major changes to such interfaces are not expected, as long as they are marked
provisional, backwards incompatible changes (up to and including removal of the interface) may occur if deemed
necessary by core developers.
Such changes will not be made gratuitously – they will occur only if serious funda-
mental flaws are uncovered that were missed prior to the inclusion of the API.
Even for provisional APIs, backwards incompatible changes are seen as a “solution of last resort” - every attempt
will still be made to find a backwards compatible resolution to any identified problems.
This process allows the standard library to continue to evolve over time, without locking in problematic design
errors for extended periods of time.
See PEP 411 for more details.
This is also abbreviated “Py3k”.
Pythonic An idea or piece of code which closely follows the most common idioms of the Python language, rather than
implementing code using concepts common to other languages.
For example, a common idiom in Python is to loop
over all elements of an iterable using a for statement.
When used to refer to modules, the fully qualified name means the entire dotted path to the module, including any
parent packages, e.g. email.mime.text:
'email.mime.text'
reference count The number of references to an object.
When the reference count of an object drops to zero, it is
deallocated.
Some objects are “immortal” and have reference counts that are never modified, and therefore the
objects are never deallocated.
Reference counting is generally not visible to Python code, but it is a key element of
the CPython implementation.
Programmers can call the sys.getrefcount() function to return the reference
count for a particular object.
See also namespace package.
A declaration inside a class that saves memory by pre-declaring space for instance attributes and eliminating
instance dictionaries.
Though popular, the technique is somewhat tricky to get right and is best reserved for rare
cases where there are large numbers of instances in a memory-critical application.
Some built-in sequence types
are list, str, tuple, and bytes.
Note that dict also supports __getitem__() and __len__(), but is
considered a mapping rather than a sequence because the lookups use arbitrary immutable keys rather than integers.
The collections.abc.
Sequence abstract base class defines a much richer interface that goes be-
yond just __getitem__() and __len__(), adding count(), index(), __contains__(), and
__reversed__().
Types that implement this expanded interface can be registered explicitly using
register().
See comprehensions.
The bracket (subscript)
notation uses slice objects internally.
Such methods have names starting and ending with double underscores.
A statement is either an expression or one of several
constructs with a keyword, such as if, while or for.
See also type hints and the typing module.
The strong reference is taken by calling Py_INCREF() when the reference is created and released
with Py_DECREF() when the reference is deleted.
The Py_NewRef() function can be used to create a strong reference to an object.
Usually, the Py_DECREF()
function must be called on the strong reference before exiting the scope of the strong reference, to avoid leaking
one reference.
See also borrowed reference.
To store or
transfer a string, it needs to be serialized as a sequence of bytes.
Serializing a string into a sequence of bytes is known as “encoding”, and recreating the string from the sequence
of bytes is known as “decoding”.
There are a variety of different text serialization codecs, which are collectively referred to as “text encodings”.
Often, a text file actually accesses a byte-oriented datastream
and handles the text encoding automatically.
Examples of text files are files opened in text mode ('r' or 'w'),
sys.stdin, sys.stdout, and instances of io.StringIO.
See also binary file for a file object able to read and write bytes-like objects.
While they don’t provide any functionality not available with single-quoted strings, they are useful for a number of
reasons.
They allow you to include unescaped single and double quotes within a string and they can span multiple
lines without the use of the continuation character, making them especially useful when writing docstrings.
An object’s type is
accessible as its __class__ attribute or can be retrieved with type(obj).
A synonym for a type, created by assigning the type to an identifier.
Type aliases are useful for simplifying type hints.
Type hints are optional and are not enforced by Python but they are useful to static type checkers.
They can also
aid IDEs with code completion and refactoring.
See typing and PEP 484, which describe this functionality.
See PEP 278 and PEP 3116, as well as bytes.splitlines() for an additional use.
When annotating a variable or a class attribute, assignment is optional:
class C:
field: 'annotation'
Variable annotations are usually used for type hints: for example this variable is expected to take int values:
count: int = 0
Variable annotation syntax is explained in section annassign.
See function annotation, PEP 484 and PEP 526, which describe this functionality.
Also see annotations-howto
for best practices on working with annotations.
virtual environment A cooperatively isolated runtime environment that allows Python users and applications to install
and upgrade Python distribution packages without interfering with the behaviour of other Python applications
running on the same system.
See also venv.
Python’s virtual machine executes the bytecode emitted by
the bytecode compiler.
Zen of Python Listing of Python design principles and philosophies that are helpful in understanding and using the
language.
The listing can be found by typing “import this” at the interactive prompt.

Appendix A. Glossary
APPENDIX
B
ABOUT THESE DOCUMENTS
These documents are generated from reStructuredText sources by Sphinx, a document processor specifically written for
the Python documentation.
Development of the documentation and its toolchain is an entirely volunteer effort, just like Python itself.
If you want
to contribute, please take a look at the reporting-bugs page for information on how to do so.
New volunteers are always
welcome!
Many thanks go to:
• Fred L. Drake, Jr., the creator of the original Python documentation toolset and writer of much of the content;
• the Docutils project for creating reStructuredText and the Docutils suite;
• Fredrik Lundh for his Alternative Python Reference project from which Sphinx got many good ideas.
B.1 Contributors to the Python Documentation
Many people have contributed to the Python language, the Python standard library, and the Python documentation.
See
Misc/ACKS in the Python source distribution for a partial list of contributors.
